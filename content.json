[{"title":"以 Keystone 作為 Kubernetes 使用者認證","date":"2018-05-30T09:08:54.000Z","path":"2018/05/30/kubernetes/k8s-integration-keystone/","text":"本文章將說明如何整合 Keystone 來提供給 Kubernetes 進行使用者認證。但由於 Keystone 整合 Kubernetes 認證在 1.10.x 版本已從原生移除(--experimental-keystone-url, --experimental-keystone-ca-file)，並轉而使用 cloud-provider-openstack 中的 Webhook 來達成，而篇將說明如何建置與設定以整合該 Webhook。 節點資訊本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統以Ubuntu 16.x進行測試： IP Address Hostname CPU Memory 172.22.132.20 k8s 4 8G 172.22.132.21 keystone 4 8G k8s為 all-in-one Kubernetes 節點(就只是個執行 kubeadm init 的節點)。 keystone利用 DevStack 部署一台 all-in-one OpenStack。 事前準備開始安裝前需要確保以下條件已達成： k8s節點以 kubeadm 部署成 Kubernetes v1.9+ all-in-one 環境。請參考 用 kubeadm 部署 Kubernetes 叢集。 在k8s節點安裝 openstack-client： $ sudo apt-get update &amp;&amp; sudo apt-get install -y python-pip $ export LC_ALL=C; sudo pip install python-openstackclient keystone節點部署成 OpenStack all-in-one 環境。請參考 DevStack。 Kubernetes 與 Keystone 整合本節將逐節說明如何設定以整合 Keystone。 建立 Keystone User 與 Roles當keystone節點的 OpenStack 部署完成後，進入到節點建立測試用 User 與 Roles： $ sudo su - stack $ cd devstack $ source openrc admin admin # 建立 Roles $ for role in &quot;k8s-admin&quot; &quot;k8s-viewer&quot; &quot;k8s-editor&quot;; do openstack role create $role; done # 建立 User $ openstack user create demo_editor --project demo --password secret $ openstack user create demo_admin --project demo --password secret # 加入 User 至 Roles $ openstack role add --user demo --project demo k8s-viewer $ openstack role add --user demo_editor --project demo k8s-editor $ openstack role add --user demo_admin --project demo k8s-admin 在 Kubernetes 安裝 Keystone Webhook進入k8s節點，首先導入下載的檔案來源： $ export URL=&quot;https://kairen.github.io/files/openstack/keystone&quot; 新增一些腳本，來提供導入不同使用者環境變數給 OpenStack Client 使用： $ export KEYSTONE_HOST=&quot;172.22.132.21&quot; $ export USER_PASSWORD=&quot;secret&quot; $ for n in &quot;admin&quot; &quot;demo&quot; &quot;demoadmin&quot; &quot;demoeditor&quot; &quot;altdemo&quot;; do wget ${URL}/openrc-${n} -O ~/openrc-${n} sed -i &quot;s/KEYSTONE_HOST/${KEYSTONE_HOST}/g&quot; ~/openrc-${n} sed -i &quot;s/USER_PASSWORD/${USER_PASSWORD}/g&quot; ~/openrc-${n} done 下載 Keystone Webhook Policy 檔案，然後執行指令修改內容： $ sudo wget ${URL}/webhook-policy.json -O /etc/kubernetes/webhook-policy.json $ source ~/openrc-demo $ PROJECT_ID=$(openstack project list | awk &#39;/demo/ {print$2}&#39;) $ sudo sed -i &quot;s/PROJECT_ID/${PROJECT_ID}/g&quot; /etc/kubernetes/webhook-policy.json 然後下載與部署 Keystone Webhook YAML 檔： $ wget ${URL}/keystone-webhook-ds.conf -O keystone-webhook-ds.yml $ KEYSTONE_HOST=&quot;172.22.132.21&quot; $ sed -i &quot;s/KEYSTONE_HOST/${KEYSTONE_HOST}/g&quot; keystone-webhook-ds.yml $ kubectl create -f keystone-webhook-ds.yml configmap &quot;keystone-webhook-kubeconfig&quot; created daemonset.apps &quot;keystone-auth-webhook&quot; created 透過 kubectl 確認 Keystone Webhook 是否部署成功： $ kubectl -n kube-system get po -l component=k8s-keystone NAME READY STATUS RESTARTS AGE keystone-auth-webhook-5qqwn 1/1 Running 0 1m 透過 cURL 確認是否能夠正確存取： $ source ~/openrc-demo $ TOKEN=$(openstack token issue -f yaml -c id | awk &#39;{print $2}&#39;) $ cat &lt;&lt; EOF | curl -kvs -XPOST -d @- https://localhost:8443/webhook | python -mjson.tool { &quot;apiVersion&quot;: &quot;authentication.k8s.io/v1beta1&quot;, &quot;kind&quot;: &quot;TokenReview&quot;, &quot;metadata&quot;: { &quot;creationTimestamp&quot;: null }, &quot;spec&quot;: { &quot;token&quot;: &quot;$TOKEN&quot; } } EOF # output { &quot;apiVersion&quot;: &quot;authentication.k8s.io/v1beta1&quot;, &quot;kind&quot;: &quot;TokenReview&quot;, &quot;metadata&quot;: { &quot;creationTimestamp&quot;: null }, &quot;spec&quot;: { &quot;token&quot;: &quot;gAAAAABbFi1SacEPNstSuSuiBXiBG0Y_DikfbiR75j3P-CJ8CeaSKXa5kDQvun4LZUq8U6ehuW_RrQwi-N7j8t086uN6a4hLnPPGmvc6K_Iw0BZHZps7G1R5WniHZ8-WTUxtkMJROSz9eG7m33Bp18mvgx-P179QiwNYxLivf_rjnxePmvujNow&quot; }, &quot;status&quot;: { &quot;authenticated&quot;: true, &quot;user&quot;: { &quot;extra&quot;: { &quot;alpha.kubernetes.io/identity/project/id&quot;: [ &quot;3ebcb1da142d427db04b8df43f6cb76a&quot; ], &quot;alpha.kubernetes.io/identity/project/name&quot;: [ &quot;demo&quot; ], &quot;alpha.kubernetes.io/identity/roles&quot;: [ &quot;k8s-viewer&quot;, &quot;Member&quot;, &quot;anotherrole&quot; ] }, &quot;groups&quot;: [ &quot;3ebcb1da142d427db04b8df43f6cb76a&quot; ], &quot;uid&quot;: &quot;19748c0131504b87a4117e49c67383c6&quot;, &quot;username&quot;: &quot;demo&quot; } } } 設定 kube-apiserver 使用 Webhook進入k8s節點，然後修改/etc/kubernetes/manifests/kube-apiserver.yaml檔案，加入以下內容： ... spec: containers: - command: ... # authorization-mode 加入 Webhook - --authorization-mode=Node,RBAC,Webhook - --runtime-config=authentication.k8s.io/v1beta1=true - --authentication-token-webhook-config-file=/srv/kubernetes/webhook-auth - --authorization-webhook-config-file=/srv/kubernetes/webhook-auth - --authentication-token-webhook-cache-ttl=5m volumeMounts: ... - mountPath: /srv/kubernetes/webhook-auth name: webhook-auth-file readOnly: true volumes: ... - hostPath: path: /srv/kubernetes/webhook-auth type: File name: webhook-auth-file 完成後重新啟動 kubelet(或者等待 static pod 自己更新)： $ sudo systemctl restart kubelet 驗證部署結果進入k8s節點，然後設定 kubectl context 並使用 openstack provider： $ kubectl config set-credentials openstack --auth-provider=openstack $ kubectl config \\ set-context --cluster=kubernetes \\ --user=openstack \\ openstack@kubernetes \\ --namespace=default $ kubectl config use-context openstack@kubernetes 測試 demo 使用者的存取權限是否有被限制： $ source ~/openrc-demo $ kubectl get pods No resources found. $ cat &lt;&lt;EOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: nginx-pod spec: restartPolicy: Never containers: - image: nginx name: nginx-app EOF # output Error from server (Forbidden): error when creating &quot;STDIN&quot;: pods is forbidden: User &quot;demo&quot; cannot create pods in the namespace &quot;default&quot; 由於 demo 只擁有 k8s-viewer role，因此只能進行 get, list 與 watch API。 測試 demo_editor 使用者是否能夠建立 Pod： $ source ~/openrc-demoeditor $ cat &lt;&lt;EOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: nginx-pod spec: restartPolicy: Never containers: - image: nginx name: nginx-app EOF # output pod &quot;nginx-pod&quot; created 這邊可以看到 demo_editor 因為擁有 k8s-editor role，因此能夠執行 create API。 測試 alt_demo 是否被禁止存取任何 API： $ source ~/openrc-altdemo $ kubectl get po Error from server (Forbidden): pods is forbidden: User &quot;alt_demo&quot; cannot list pods in the namespace &quot;default&quot; 由於 alt_demo 不具備任何 roles，因此無法存取任何 API。","tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"},{"name":"Keystone","slug":"Keystone","permalink":"https://kairen.github.io/tags/Keystone/"}]},{"title":"在 AWS 上建立跨地區的 Kubernetes Federation 叢集","date":"2018-04-21T09:08:54.000Z","path":"2018/04/21/kubernetes/aws-k8s-federation/","text":"本篇延續先前 On-premises Federation 與 Kops 經驗來嘗試在 AWS 上建立 Federaion 叢集，這邊架構如下圖所示： 本次安裝的軟體版本： Kubernetes v1.9.3 kops v1.9.0 kubefed v1.10 節點資訊測試環境為 AWS EC2 虛擬機器，共有三組叢集： US West(Oregon) 叢集，也是 Federation 控制平面叢集： Host vCPU RAM us-west-m1 1 2G us-west-n1 1 2G us-west-n2 1 2G US East(Ohio) 叢集: Host vCPU RAM us-east-m1 1 2G us-east-n1 1 2G us-east-n2 1 2G Asia Pacific(Tokyo) 叢集: Host vCPU RAM ap-northeast-m1 1 2G ap-northeast-n1 1 2G ap-northeast-n2 1 2G 事前準備開始前，需要先安裝下列工具到操作機器上來提供使用： kubectl：用來操作部署完成的 Kubernetes 叢集。 kops：用來部署與管理公有雲上的 Kubernetes 叢集。 Mac OS X： $ brew update &amp;&amp; brew install kops Linux distro： $ curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d &#39;&quot;&#39; -f 4)/kops-linux-amd64 $ chmod +x kops-linux-amd64 &amp;&amp; sudo mv kops-linux-amd64 /usr/local/bin/kops kubefed：用來建立 Federation 控制平面與管理 Federation 叢集的工具。 Mac OS X： $ git clone https://github.com/kubernetes/federation.git $GOPATH/src/k8s.io/federation $ cd $GOPATH/src/k8s.io/federation $ make quick-release $ cp _output/dockerized/bin/linux/amd64/kubefed /usr/local/bin/kubefed Linux distro： $ wget https://storage.googleapis.com/kubernetes-federation-release/release/v1.9.0-alpha.3/federation-client-linux-amd64.tar.gz $ tar xvf federation-client-linux-amd64.tar.gz $ cp federation/client/bin/kubefed /usr/local/bin/ $ kubefed version Client Version: version.Info{Major:&quot;1&quot;, Minor:&quot;9+&quot;, GitVersion:&quot;v1.9.0-alpha.3&quot;, GitCommit:&quot;85c06145286da663755b140efa2b65f793cce9ec&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-02-14T12:54:40Z&quot;, GoVersion:&quot;go1.9.1&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;} Server Version: version.Info{Major:&quot;1&quot;, Minor:&quot;9&quot;, GitVersion:&quot;v1.9.6&quot;, GitCommit:&quot;9f8ebd171479bec0ada837d7ee641dec2f8c6dd1&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-03-21T15:13:31Z&quot;, GoVersion:&quot;go1.9.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;} AWS CLI：用來操作 AWS 服務的工具。 $ sudo pip install awscli $ aws --version aws-cli/1.15.4 上述工具完成後，我們還要準備一下資訊： 申請 AWS 帳號，並在 IAM 服務新增一個 User 設定存取所有服務(AdministratorAccess)。另外這邊要記住 AccessKey 與 SecretKey。 一般來說只需開啟 S3、Route53、EC2、EBS、ELB 與 VPC 就好，但由於偷懶就全開。以下為各 AWS 服務在本次安裝的用意： IAM: 提供身份認證與存取管理。 EC2: Kubernetes 叢集部署的虛擬機環境。 ELB: Kubernetes 元件與 Service 負載平衡。 Route53: 提供 Public domain 存取 Kubernetes 環境。 S3: 儲存 Kops 狀態。 VPC: 提供 Kubernetes 與 EC2 的網路環境。 擁有自己的 Domain Name，這邊可以在 AWS Route53 註冊，或者是到 GoDaddy 購買。 部署 Kubernetes Federation 叢集本節將說明如何利用自己撰寫好的腳本 aws-k8s-federation 來部署 Kubernetes 叢集與 Federation 叢集。首先在操作節點下載： $ git clone https://github.com/kairen/aws-k8s-federation $ cd aws-k8s-federation $ cp .env.sample .env 編輯.env檔案來提供後續腳本的環境變數： # 你的 Domain Name(這邊為 &lt;hoste_dzone_name&gt;.&lt;domain_name&gt;) export DOMAIN_NAME=&quot;k8s.example.com&quot; # Regions and zones export US_WEST_REGION=&quot;us-west-2&quot; export US_EAST_REGION=&quot;us-east-2&quot; export AP_NORTHEAST_REGION=&quot;ap-northeast-1&quot; export ZONE=&quot;a&quot; # Cluster contexts name export FED_CONTEXT=&quot;aws-fed&quot; export US_WEST_CONTEXT=&quot;us-west.${DOMAIN_NAME}&quot; export US_EAST_CONTEXT=&quot;us-east.${DOMAIN_NAME}&quot; export AP_NORTHEAST_CONTEXT=&quot;ap-northeast.${DOMAIN_NAME}&quot; # S3 buckets name export US_WEST_BUCKET_NAME=&quot;us-west-k8s&quot; export US_EAST_BUCKET_NAME=&quot;us-east-k8s&quot; export AP_NORTHEAST_BUCKET_NAME=&quot;ap-northeast-k8s&quot; # Get domain name id export HOSTED_ZONE_ID=$(aws route53 list-hosted-zones \\ | jq -r &#39;.HostedZones[] | select(.Name==&quot;&#39;${DOMAIN_NAME}&#39;.&quot;) | .Id&#39; \\ | sed &#39;s/\\/hostedzone\\///&#39;) # Kubernetes master and node size, and node count. export MASTER_SIZE=&quot;t2.micro&quot; export NODE_SIZE=&quot;t2.micro&quot; export NODE_COUNT=&quot;2&quot; # Federation simple apps deploy and service name export DNS_RECORD_PREFIX=&quot;nginx&quot; export SERVICE_NAME=&quot;nginx&quot; 建立 Route53 Hosted Zone首先透過 aws 工具進行設定使用指定 AccessKey 與 SecretKey： $ aws configure AWS Access Key ID [****************QGEA]: AWS Secret Access Key [****************zJ+w]: Default region name [None]: Default output format [None]: 設定的 Keys 可以在~/.aws/credentials找到。 接著需要在 Route53 建立一個 Hosted Zone，並在 Domain Name 供應商上設定 NameServers： $ ./0-create-hosted-domain.sh # output ... { &quot;HostedZone&quot;: { &quot;ResourceRecordSetCount&quot;: 2, &quot;CallerReference&quot;: &quot;2018-04-25-16:16&quot;, &quot;Config&quot;: { &quot;PrivateZone&quot;: false }, &quot;Id&quot;: &quot;/hostedzone/Z2JR49ADZ0P3WC&quot;, &quot;Name&quot;: &quot;k8s.example.com.&quot; }, &quot;DelegationSet&quot;: { &quot;NameServers&quot;: [ &quot;ns-1547.awsdns-01.co.uk&quot;, &quot;ns-1052.awsdns-03.org&quot;, &quot;ns-886.awsdns-46.net&quot;, &quot;ns-164.awsdns-20.com&quot; ] }, &quot;Location&quot;: &quot;https://route53.amazonaws.com/2013-04-01/hostedzone/Z2JR49ADZ0P3WC&quot;, &quot;ChangeInfo&quot;: { &quot;Status&quot;: &quot;PENDING&quot;, &quot;SubmittedAt&quot;: &quot;2018-04-25T08:16:57.462Z&quot;, &quot;Id&quot;: &quot;/change/C3802PE0C1JVW2&quot; } } 之後將上述NameServers新增至自己的 Domain name 的 record 中，如 Godaddy： 在每個 Region 建立 Kubernetes 叢集當 Hosted Zone 建立完成後，就可以接著建立每個 Region 的 Kubernetes 叢集，這邊腳本已包含建立叢集與 S3 Bucket 指令，因此只需要執行以下腳本即可： $ ./1-create-clusters.sh .... Cluster is starting. It should be ready in a few minutes. ... 這邊會需要等待一點時間進行初始化與部署，也可以到 AWS Console 查看狀態。 完成後，即可透過 kubectl 來操作叢集： $ ./us-east/kc get no + kubectl --context=us-east.k8s.example.com get no NAME STATUS ROLES AGE VERSION ip-172-20-43-26.us-east-2.compute.internal Ready node 1m v1.9.3 ip-172-20-56-167.us-east-2.compute.internal Ready master 3m v1.9.3 ip-172-20-63-133.us-east-2.compute.internal Ready node 2m v1.9.3 $ ./ap-northeast/kc get no + kubectl --context=ap-northeast.k8s.example.com get no NAME STATUS ROLES AGE VERSION ip-172-20-42-184.ap-northeast-1.compute.internal Ready master 2m v1.9.3 ip-172-20-52-176.ap-northeast-1.compute.internal Ready node 20s v1.9.3 ip-172-20-56-88.ap-northeast-1.compute.internal Ready node 22s v1.9.3 $ ./us-west/kc get no + kubectl --context=us-west.k8s.example.com get no NAME STATUS ROLES AGE VERSION ip-172-20-33-22.us-west-2.compute.internal Ready node 1m v1.9.3 ip-172-20-55-237.us-west-2.compute.internal Ready master 2m v1.9.3 ip-172-20-63-77.us-west-2.compute.internal Ready node 35s v1.9.3 建立 Kubernetes Federation 叢集當三個地區的叢集建立完成後，接著要在 US West 的叢集上部署 Federation 控制平面元件： $ ./2-init-federation.sh ... Federation API server is running at: abba6864f490111e8b4bd028106a7a79-793027324.us-west-2.elb.amazonaws.com $ ./us-west/kc -n federation-system get po + kubectl --context=us-west.k8s.example.com -n federation-system get po NAME READY STATUS RESTARTS AGE apiserver-5d46898995-tmzvl 2/2 Running 0 1m controller-manager-6cc78c68d5-2pbg5 0/1 Error 3 1m 這邊會發現controller-manager會一直掛掉，這是因為它需要取得 AWS 相關權限，因此需要透過 Patch 方式來把 AccessKey 與 SecretKey 注入到 Deployment 中： $ ./3-path-federation.sh Switched to context &quot;us-west.k8s.example.com&quot;. deployment &quot;controller-manager&quot; patched $ ./us-west/kc -n federation-system get po + kubectl --context=us-west.k8s.example.com -n federation-system get po NAME READY STATUS RESTARTS AGE apiserver-5d46898995-tmzvl 2/2 Running 0 3m controller-manager-769bd95fbc-dkssr 1/1 Running 0 21s 確認上述沒問題後，透過 kubectl 確認 contexts： $ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE ap-northeast.k8s.example.com ap-northeast.k8s.example.com ap-northeast.k8s.example.com aws-fed aws-fed aws-fed us-east.k8s.example.com us-east.k8s.example.com us-east.k8s.example.com * us-west.k8s.example.com us-west.k8s.example.com us-west.k8s.example.com 接著透過以下腳本來加入us-west叢集至 aws-fed 的 Federation 中： $ ./4-join-us-west.sh + kubectl config use-context aws-fed Switched to context &quot;aws-fed&quot;. + kubefed join us-west --host-cluster-context=us-west.k8s.example.com --cluster-context=us-west.k8s.example.com cluster &quot;us-west&quot; created 加入ap-northeast叢集至 aws-fed 的 Federation 中： $ ./5-join-ap-northeast.sh + kubectl config use-context aws-fed Switched to context &quot;aws-fed&quot;. + kubefed join ap-northeast --host-cluster-context=us-west.k8s.example.com --cluster-context=ap-northeast.k8s.example.com cluster &quot;ap-northeast&quot; created 加入us-east叢集至 aws-fed 的 Federation 中： $ ./6-join-us-east.sh + kubectl config use-context aws-fed Switched to context &quot;aws-fed&quot;. + kubefed join us-east --host-cluster-context=us-west.k8s.example.com --cluster-context=us-east.k8s.example.com cluster &quot;us-east&quot; created 完成後，在 Federation 建立 Federated Namespace，並列出叢集： $ ./7-create-fed-ns.sh + kubectl --context=aws-fed create namespace default namespace &quot;default&quot; created + kubectl --context=aws-fed get clusters NAME AGE ap-northeast 2m us-east 1m us-west 2m 完成這些過程表示你已經建立了一套 Kubernetes Federation 叢集了，接下來就可以進行測試。 測試叢集首先建立一個簡單的 Nginx 來提供服務的測試，這邊可以透過以下腳本達成： $ ./8-deploy-fed-nginx.sh + cat + kubectl --context=aws-fed apply -f - deployment &quot;nginx&quot; created + cat + kubectl --context=aws-fed apply -f - service &quot;nginx&quot; created $ kubectl get deploy,svc NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/nginx 3 3 3 3 3m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/nginx LoadBalancer &lt;none&gt; a4d86547a4903... 80/TCP 2m 這裡的 nginx deployment 有設定deployment-preferences，因此在 scale 時會依據下面資訊來分配： { &quot;rebalance&quot;: true, &quot;clusters&quot;: { &quot;us-west&quot;: { &quot;minReplicas&quot;: 2, &quot;maxReplicas&quot;: 10, &quot;weight&quot;: 200 }, &quot;us-east&quot;: { &quot;minReplicas&quot;: 0, &quot;maxReplicas&quot;: 2, &quot;weight&quot;: 150 }, &quot;ap-northeast&quot;: { &quot;minReplicas&quot;: 1, &quot;maxReplicas&quot;: 5, &quot;weight&quot;: 150 } } } 檢查每個叢集的 Pod： # us-west context(這邊策略為 2 - 10) $ ./us-west/kc get po + kubectl --context=us-west.k8s.example.com get po NAME READY STATUS RESTARTS AGE nginx-679dc9c764-4x78c 1/1 Running 0 3m nginx-679dc9c764-fzv9z 1/1 Running 0 3m # us-east context(這邊策略為 0 - 2) $ ./us-east/kc get po + kubectl --context=us-east.k8s.example.com get po No resources found. # ap-northeast context(這邊策略為 1 - 5) $ ./ap-northeast/kc get po + kubectl --context=ap-northeast.k8s.example.com get po NAME READY STATUS RESTARTS AGE nginx-679dc9c764-hmwzq 1/1 Running 0 4m 透過擴展副本數來查看分配狀況： $ ./9-scale-fed-nginx.sh + kubectl --context=aws-fed scale deploy nginx --replicas=10 deployment &quot;nginx&quot; scaled $ kubectl get deploy NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx 10 10 10 10 8m 再次檢查每個叢集的 Pod： # us-west context(這邊策略為 2 - 10) $ ./us-west/kc get po + kubectl --context=us-west.k8s.example.com get po NAME READY STATUS RESTARTS AGE nginx-679dc9c764-4x78c 1/1 Running 0 8m nginx-679dc9c764-7958k 1/1 Running 0 50s nginx-679dc9c764-fzv9z 1/1 Running 0 8m nginx-679dc9c764-j6kc9 1/1 Running 0 50s nginx-679dc9c764-t6rvj 1/1 Running 0 50s # us-east context(這邊策略為 0 - 2) $ ./us-east/kc get po + kubectl --context=us-east.k8s.example.com get po NAME READY STATUS RESTARTS AGE nginx-679dc9c764-8t7qz 1/1 Running 0 1m nginx-679dc9c764-zvqmx 1/1 Running 0 1m # ap-northeast context(這邊策略為 1 - 5) $ ./ap-northeast/kc get po + kubectl --context=ap-northeast.k8s.example.com get po NAME READY STATUS RESTARTS AGE nginx-679dc9c764-f79v7 1/1 Running 0 1m nginx-679dc9c764-hmwzq 1/1 Running 0 9m nginx-679dc9c764-vj7hb 1/1 Running 0 1m 可以看到結果符合我們預期範圍內。 最後因為服務是透過 ELB 來提供，為了統一透過 Domain name 存取相同服務，這邊更新 Hosted Zone Record 來轉發： $ ./10-update-fed-nginx-record.sh 完成後透過 cURL 工作來測試： $ curl nginx.k8s.example.com ... &lt;title&gt;Welcome to nginx!&lt;/title&gt; ... 最後透過該腳本來清楚叢集與 AWS 服務上建立的東西： $ ./99-purge.sh","tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"},{"name":"AWS","slug":"AWS","permalink":"https://kairen.github.io/tags/AWS/"},{"name":"Kops","slug":"Kops","permalink":"https://kairen.github.io/tags/Kops/"},{"name":"Federation","slug":"Federation","permalink":"https://kairen.github.io/tags/Federation/"}]},{"title":"使用 Kops 部署 Kubernetes 至公有雲(AWS)","date":"2018-04-18T09:08:54.000Z","path":"2018/04/18/kubernetes/deploy/kops-aws/","text":"Kops 是 Kubernetes 官方維護的專案，是一套 Production ready 的 Kubernetes 部署、升級與管理工具，早期用於 AWS 公有雲上建置 Kubernetes 叢集使用，但隨著社群的推進已支援 GCP、vSphere(Alpha)，未來也會有更多公有雲平台慢慢被支援(Maybe)。本篇簡單撰寫使用 Kops 部署一個叢集，過去自己因為公司都是屬於建置 On-premises 的 Kubernetes，因此很少使用 Kops，剛好最近社群分享又再一次接觸的關析，所以就來寫個文章。 本次安裝的軟體版本： Kubernetes v1.9.3 Kops v1.9.0 事前準備開始使用 Kops 前，需要先安裝下列工具到操作機器上來提供使用： kubectl：用來操作部署完成的 Kubernetes 叢集。 kops：本次使用工具，用來部署與管理公有雲上的 Kubernetes 叢集。 Mac OS X： $ brew update &amp;&amp; brew install kops Linux distro： $ curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d &#39;&quot;&#39; -f 4)/kops-linux-amd64 $ chmod +x kops-linux-amd64 &amp;&amp; sudo mv kops-linux-amd64 /usr/local/bin/kops AWS CLI：用來操作 AWS 服務的工具。 $ sudo pip install awscli $ aws --version aws-cli/1.15.4 上述工具完成後，我們還要準備一下資訊： 申請 AWS 帳號，並在 IAM 服務新增一個 User 設定存取所有服務(AdministratorAccess)。另外這邊要記住 AccessKey 與 SecretKey。 一般來說只需開啟 S3、Route53、EC2、EBS 與 ELB 就好，但由於偷懶就全開。 擁有自己的 Domain Name，這邊可以在 AWS Route53 註冊，或者是到 GoDaddy 購買。 建立 S3 Bucket 與 Route53 Hosted Zone首先透過 aws 工具進行設定使用指定 AccessKey 與 SecretKey： $ aws configure AWS Access Key ID [****************QGEA]: AWS Secret Access Key [****************zJ+w]: Default region name [None]: Default output format [None]: 設定的 Keys 可以在~/.aws/credentials找到。 完成後建立一個 S3 bucket 用來儲存 Kops 狀態： $ aws s3 mb s3://kops-k8s-1 --region us-west-2 make_bucket: kops-k8s-1 這邊 region 可自行選擇，這邊選用 Oregon。 接著建立一個 Route53 Hosted Zone： $ aws route53 create-hosted-zone \\ --name k8s.example.com \\ --caller-reference $(date &#39;+%Y-%m-%d-%H:%M&#39;) # output { &quot;HostedZone&quot;: { &quot;ResourceRecordSetCount&quot;: 2, &quot;CallerReference&quot;: &quot;2018-04-25-16:16&quot;, &quot;Config&quot;: { &quot;PrivateZone&quot;: false }, &quot;Id&quot;: &quot;/hostedzone/Z2JR49ADZ0P3WC&quot;, &quot;Name&quot;: &quot;k8s.example.com.&quot; }, &quot;DelegationSet&quot;: { &quot;NameServers&quot;: [ &quot;ns-1547.awsdns-01.co.uk&quot;, &quot;ns-1052.awsdns-03.org&quot;, &quot;ns-886.awsdns-46.net&quot;, &quot;ns-164.awsdns-20.com&quot; ] }, &quot;Location&quot;: &quot;https://route53.amazonaws.com/2013-04-01/hostedzone/Z2JR49ADZ0P3WC&quot;, &quot;ChangeInfo&quot;: { &quot;Status&quot;: &quot;PENDING&quot;, &quot;SubmittedAt&quot;: &quot;2018-04-25T08:16:57.462Z&quot;, &quot;Id&quot;: &quot;/change/C3802PE0C1JVW2&quot; } } 請修改--name為自己所擁有的 domain name。 之後將上述NameServers新增至自己的 Domain name 的 record 中，如 Godaddy： 部署 Kubernetes 叢集當上述階段完成後，在自己機器建立 SSH key，就可以使用 Kops 來建立 Kubernetes 叢集： $ ssh-keygen -t rsa $ kops create cluster \\ --name=k8s.example.com \\ --state=s3://kops-k8s-1 \\ --zones=us-west-2a \\ --master-size=t2.micro \\ --node-size=t2.micro \\ --node-count=2 \\ --dns-zone=k8s.example.com # output ... Finally configure your cluster with: kops update cluster k8s.example.com --yes 若過程沒有發生錯誤的話，最後會提示再執行 update 來正式進行部署： $ kops update cluster k8s.example.com --state=s3://kops-k8s-1 --yes # output ... Cluster is starting. It should be ready in a few minutes. 當看到上述資訊時，表示叢集已建立，這時候等待環境初始化完成後就可以使用 kubectl 來操作： $ kubectl get node NAME STATUS ROLES AGE VERSION ip-172-20-32-194.us-west-2.compute.internal Ready master 1m v1.9.3 ip-172-20-32-21.us-west-2.compute.internal Ready node 22s v1.9.3 ip-172-20-54-100.us-west-2.compute.internal Ready node 28s v1.9.3 測試完成後就可以進行功能測試，這邊簡單建立 Nginx app： $ kubectl run nginx --image nginx --port 80 $ kubectl expose deploy nginx --type=LoadBalancer --port 80 $ kubectl get po,svc NAME READY STATUS RESTARTS AGE po/nginx-7587c6fdb6-7qtlr 1/1 Running 0 50s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kubernetes ClusterIP 100.64.0.1 &lt;none&gt; 443/TCP 8m svc/nginx LoadBalancer 100.68.96.3 ad99f206f486e... 80:30174/TCP 28s 這邊會看到EXTERNAL-IP會直接透過 AWS ELB 建立一個 Load Balancer，這時只要更新 Route53 的 record set 就可以存取到服務： $ export DOMAIN_NAME=k8s.example.com $ export NGINX_LB=$(kubectl get svc/nginx \\ --template=&quot;{{range .status.loadBalancer.ingress}} {{.hostname}} {{end}}&quot;) $ cat &lt;&lt;EOF &gt; dns-record.json { &quot;Comment&quot;: &quot;Create/Update a latency-based CNAME record for a federated Deployment&quot;, &quot;Changes&quot;: [ { &quot;Action&quot;: &quot;UPSERT&quot;, &quot;ResourceRecordSet&quot;: { &quot;Name&quot;: &quot;nginx.${DOMAIN_NAME}&quot;, &quot;Type&quot;: &quot;CNAME&quot;, &quot;Region&quot;: &quot;us-west-2&quot;, &quot;TTL&quot;: 300, &quot;SetIdentifier&quot;: &quot;us-west-2&quot;, &quot;ResourceRecords&quot;: [ { &quot;Value&quot;: &quot;${NGINX_LB}&quot; } ] } } ] } EOF $ export HOSTED_ZONE_ID=$(aws route53 list-hosted-zones \\ | jq -r &#39;.HostedZones[] | select(.Name==&quot;&#39;${DOMAIN_NAME}&#39;.&quot;) | .Id&#39; \\ | sed &#39;s/\\/hostedzone\\///&#39;) $ aws route53 change-resource-record-sets \\ --hosted-zone-id ${HOSTED_ZONE_ID} \\ --change-batch file://dns-record.json # output { &quot;ChangeInfo&quot;: { &quot;Status&quot;: &quot;PENDING&quot;, &quot;Comment&quot;: &quot;Create/Update a latency-based CNAME record for a federated Deployment&quot;, &quot;SubmittedAt&quot;: &quot;2018-04-25T10:06:02.545Z&quot;, &quot;Id&quot;: &quot;/change/C79MFJRHCF05R&quot; } } 完成後透過 cURL 工作來測試： $ curl nginx.k8s.example.com ... &lt;title&gt;Welcome to nginx!&lt;/title&gt; ... 刪除節點當叢集測完後，可以利用以下指令來刪除： $ kops delete cluster \\ --name=k8s.example.com \\ --state=s3://kops-k8s-1 --yes Deleted cluster: &quot;k8s.k2r2bai.com&quot; $ aws s3 rb s3://kops-k8s-1 --force remove_bucket: kops-k8s-1 接著清除 Route53 所有 record 並刪除 hosted zone： $ aws route53 list-resource-record-sets \\ --hosted-zone-id ${HOSTED_ZONE_ID} | jq -c &#39;.ResourceRecordSets[]&#39; | while read -r resourcerecordset; do read -r name type &lt;&lt;&lt;$(echo $(jq -r &#39;.Name,.Type&#39; &lt;&lt;&lt;&quot;$resourcerecordset&quot;)) if [ $type != &quot;NS&quot; -a $type != &quot;SOA&quot; ]; then aws route53 change-resource-record-sets \\ --hosted-zone-id ${HOSTED_ZONE_ID} \\ --change-batch &#39;{&quot;Changes&quot;:[{&quot;Action&quot;:&quot;DELETE&quot;,&quot;ResourceRecordSet&quot;: &#39;&quot;$resourcerecordset&quot;&#39; }]}&#39; \\ --output text --query &#39;ChangeInfo.Id&#39; fi done $ aws route53 delete-hosted-zone --id ${HOSTED_ZONE_ID}","tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"},{"name":"AWS","slug":"AWS","permalink":"https://kairen.github.io/tags/AWS/"}]},{"title":"整合 OpenLDAP 進行 Kubernetes 身份認證","date":"2018-04-15T09:08:54.000Z","path":"2018/04/15/kubernetes/k8s-integration-ldap/","text":"本文將說明如何整合 OpenLDAP 來提供給 Kubernetes 進行使用者認證。Kubernetes 官方並沒有提供針對 LDAP 與 AD 的整合，但是可以藉由 Webhook Token Authentication 以及 Authenticating Proxy 來達到整合功能。概念是開發一個 HTTP Server 提供 POST Method 來塞入 Bearer Token，而該 HTTP Server 利用 LDAP library 檢索對應 Token 的 User 進行認證，成功後回傳該 User 的所有 Group 等資訊，而這時可以利用 Kubernetes 針對該 User 的 Group 設定對應的 RBAC role 進行權限控管。 節點資訊本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統可採用Ubuntu 16.x與CentOS 7.x： IP Address Hostname CPU Memory 192.16.35.11 k8s-m1 1 2G 192.16.35.12 k8s-n1 1 2G 192.16.35.13 k8s-n2 1 2G 192.16.35.20 ldap-server 1 1G 這邊m為 K8s master，n為 K8s node。 所有操作全部用root使用者進行(方便用)，以 SRE 來說不推薦。 可以下載 Vagrantfile 來建立 Virtualbox 虛擬機叢集。不過需要注意機器資源是否足夠。 事前準備開始安裝前需要確保以下條件已達成： 所有節點需要安裝 Docker CE 版本的容器引擎： $ curl -fsSL &quot;https://get.docker.com/&quot; | sh 不管是在 Ubuntu 或 CentOS 都只需要執行該指令就會自動安裝最新版 Docker。CentOS 安裝完成後需要再執行以下指令： $ systemctl enable docker &amp;&amp; systemctl start docker 所有節點以 kubeadm 部署成 Kubernetes v1.9+ 叢集。請參考 用 kubeadm 部署 Kubernetes 叢集。 OpenLDAP 與 phpLDAPadmin本節將說明如何部署、設定與操作 OpenLDAP。 部署進入ldap-server節點透過 Docker 來進行部署： $ docker run -d \\ -p 389:389 -p 636:636 \\ --env LDAP_ORGANISATION=&quot;Kubernetes LDAP&quot; \\ --env LDAP_DOMAIN=&quot;k8s.com&quot; \\ --env LDAP_ADMIN_PASSWORD=&quot;password&quot; \\ --env LDAP_CONFIG_PASSWORD=&quot;password&quot; \\ --name openldap-server \\ osixia/openldap:1.2.0 $ docker run -d \\ -p 443:443 \\ --env PHPLDAPADMIN_LDAP_HOSTS=192.16.35.20 \\ --name phpldapadmin \\ osixia/phpldapadmin:0.7.1 這邊為cn=admin,dc=k8s,dc=com為admin DN ，而cn=admin,cn=config為config。另外這邊僅做測試用，故不使用 Persistent Volumes，需要可以參考 Docker OpenLDAP。 完成後就可以透過瀏覽器來 phpLDAPadmin website。這邊點選Login輸入 DN 與 Password。 成功登入後畫面，這時可以自行新增其他資訊。 建立 Kubenretes Token Schema進入openldap-server 容器，接著建立 Kubernetes token schema 物件的設定檔： $ docker exec -ti openldap-server sh $ mkdir ~/kubernetes_tokens $ cat &lt;&lt;EOF &gt; ~/kubernetes_tokens/kubernetesToken.schema attributeType ( 1.3.6.1.4.1.18171.2.1.8 NAME &#39;kubernetesToken&#39; DESC &#39;Kubernetes authentication token&#39; EQUALITY caseExactIA5Match SUBSTR caseExactIA5SubstringsMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 SINGLE-VALUE ) objectClass ( 1.3.6.1.4.1.18171.2.3 NAME &#39;kubernetesAuthenticationObject&#39; DESC &#39;Object that may authenticate to a Kubernetes cluster&#39; AUXILIARY MUST kubernetesToken ) EOF $ echo &quot;include /root/kubernetes_tokens/kubernetesToken.schema&quot; &gt; ~/kubernetes_tokens/schema_convert.conf $ slaptest -f ~/kubernetes_tokens/schema_convert.conf -F ~/kubernetes_tokens config file testing succeeded 修改以下檔案內容，如以下所示： $ vim ~/kubernetes_tokens/cn=config/cn=schema/cn\\=\\{0\\}kubernetestoken.ldif # AUTO-GENERATED FILE - DO NOT EDIT!! Use ldapmodify. # CRC32 e502306e dn: cn=kubernetestoken,cn=schema,cn=config objectClass: olcSchemaConfig cn: kubernetestoken olcAttributeTypes: {0}( 1.3.6.1.4.1.18171.2.1.8 NAME &#39;kubernetesToken&#39; DESC &#39;Kubernetes authentication token&#39; EQUALITY caseExactIA5Match SUBSTR caseExa ctIA5SubstringsMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 SINGLE-VALUE ) olcObjectClasses: {0}( 1.3.6.1.4.1.18171.2.3 NAME &#39;kubernetesAuthenticationO bject&#39; DESC &#39;Object that may authenticate to a Kubernetes cluster&#39; AUXILIAR Y MUST kubernetesToken ) 新增 Schema 物件至 LDAP Server 中： $ cd ~/kubernetes_tokens/cn=config/cn=schema $ ldapadd -c -Y EXTERNAL -H ldapi:/// -f cn\\=\\{0\\}kubernetestoken.ldif SASL/EXTERNAL authentication started SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth SASL SSF: 0 adding new entry &quot;cn=kubernetestoken,cn=schema,cn=config&quot; 完成後查詢是否成功新增 Entry： $ ldapsearch -x -H ldap:/// -LLL -D &quot;cn=admin,cn=config&quot; -w password -b &quot;cn=schema,cn=config&quot; &quot;(objectClass=olcSchemaConfig)&quot; dn -Z Enter LDAP Password: dn: cn=schema,cn=config ... dn: cn={14}kubernetestoken,cn=schema,cn=config 新增測試用 LDAP Groups 與 Users當上面 Schema 建立完成後，這邊需要新增一些測試用 Groups： $ cat &lt;&lt;EOF &gt; groups.ldif dn: ou=People,dc=k8s,dc=com ou: People objectClass: top objectClass: organizationalUnit description: Parent object of all UNIX accounts dn: ou=Groups,dc=k8s,dc=com ou: Groups objectClass: top objectClass: organizationalUnit description: Parent object of all UNIX groups dn: cn=kubernetes,ou=Groups,dc=k8s,dc=com cn: kubernetes gidnumber: 100 memberuid: user1 memberuid: user2 objectclass: posixGroup objectclass: top EOF $ ldapmodify -x -a -H ldap:// -D &quot;cn=admin,dc=k8s,dc=com&quot; -w password -f groups.ldif adding new entry &quot;ou=People,dc=k8s,dc=com&quot; adding new entry &quot;ou=Groups,dc=k8s,dc=com&quot; adding new entry &quot;cn=kubernetes,ou=Groups,dc=k8s,dc=com&quot; Group 建立完成後再接著建立 User： $ cat &lt;&lt;EOF &gt; users.ldif dn: uid=user1,ou=People,dc=k8s,dc=com cn: user1 gidnumber: 100 givenname: user1 homedirectory: /home/users/user1 loginshell: /bin/sh objectclass: inetOrgPerson objectclass: posixAccount objectclass: top objectClass: shadowAccount objectClass: organizationalPerson sn: user1 uid: user1 uidnumber: 1000 userpassword: user1 dn: uid=user2,ou=People,dc=k8s,dc=com homedirectory: /home/users/user2 loginshell: /bin/sh objectclass: inetOrgPerson objectclass: posixAccount objectclass: top objectClass: shadowAccount objectClass: organizationalPerson cn: user2 givenname: user2 sn: user2 uid: user2 uidnumber: 1001 gidnumber: 100 userpassword: user2 EOF $ ldapmodify -x -a -H ldap:// -D &quot;cn=admin,dc=k8s,dc=com&quot; -w password -f users.ldif adding new entry &quot;uid=user1,ou=People,dc=k8s,dc=com&quot; adding new entry &quot;uid=user2,ou=People,dc=k8s,dc=com&quot; 這邊可以登入 phpLDAPadmin 查看，結果如以下所示： 確認沒問題後，將 User dump 至一個文字檔案中： $ cat &lt;&lt;EOF &gt; users.txt dn: uid=user1,ou=People,dc=k8s,dc=com dn: uid=user2,ou=People,dc=k8s,dc=com EOF 這邊偷懶直接用 cat。 執行以下腳本來更新每個 LDAP User 的 kubernetesToken： $ while read -r user; do fname=$(echo $user | grep -E -o &quot;uid=[a-z0-9]+&quot; | cut -d&quot;=&quot; -f2) token=$(dd if=/dev/urandom bs=128 count=1 2&gt;/dev/null | base64 | tr -d &quot;=+/&quot; | dd bs=32 count=1 2&gt;/dev/null) cat &lt;&lt; EOF &gt; &quot;${fname}.ldif&quot; $user changetype: modify add: objectClass objectclass: kubernetesAuthenticationObject - add: kubernetesToken kubernetesToken: $token EOF ldapmodify -a -H ldapi:/// -D &quot;cn=admin,dc=k8s,dc=com&quot; -w password -f &quot;${fname}.ldif&quot; done &lt; users.txt # output Enter LDAP Password: modifying entry &quot;uid=user1,ou=Users,dc=k8s,dc=com&quot; Enter LDAP Password: modifying entry &quot;uid=user2,ou=Users,dc=k8s,dc=com&quot; 部署 Kubernetes LDAP當 Kubernetes 環境建立完成後，首先進入k8s-m1節點，透過 git 取得 kube-ldap-authn 原始碼專案： $ git clone https://github.com/kairen/kube-ldap-authn.git $ cd kube-ldap-authn 若想使用 Go 語言實作的版本，可以參考 kube-ldap-webhook. 新增一個config.py檔案來提供相關設定內容： LDAP_URL=&#39;ldap://192.16.35.20/ ldap://192.16.35.20&#39; LDAP_START_TLS = False LDAP_BIND_DN = &#39;cn=admin,dc=k8s,dc=com&#39; LDAP_BIND_PASSWORD = &#39;password&#39; LDAP_USER_NAME_ATTRIBUTE = &#39;uid&#39; LDAP_USER_UID_ATTRIBUTE = &#39;uidNumber&#39; LDAP_USER_SEARCH_BASE = &#39;ou=People,dc=k8s,dc=com&#39; LDAP_USER_SEARCH_FILTER = &quot;(&amp;(kubernetesToken={token}))&quot; LDAP_GROUP_NAME_ATTRIBUTE = &#39;cn&#39; LDAP_GROUP_SEARCH_BASE = &#39;ou=Groups,dc=k8s,dc=com&#39; LDAP_GROUP_SEARCH_FILTER = &#39;(|(&amp;(objectClass=posixGroup)(memberUid={username}))(&amp;(member={dn})(objectClass=groupOfNames)))&#39; 變數詳細說明可以參考 Config example 建立 kube-ldap-authn secret 來提供給 pod 使用，並部署 kube-ldap-authn pod 到所有 master 節點上： $ kubectl -n kube-system create secret generic ldap-authn-config --from-file=config.py=config.py $ kubectl create -f daemonset.yaml $ kubectl -n kube-system get po -l app=kube-ldap-authn -o wide NAME READY STATUS RESTARTS AGE IP NODE kube-ldap-authn-sx994 1/1 Running 0 13s 192.16.35.11 k8s-m1 這邊若成功部署的話，可以用 curl 進行測試： $ curl -X POST -H &quot;Content-Type: application/json&quot; \\ -d &#39;{&quot;apiVersion&quot;: &quot;authentication.k8s.io/v1beta1&quot;, &quot;kind&quot;: &quot;TokenReview&quot;, &quot;spec&quot;: {&quot;token&quot;: &quot;&lt;LDAP_K8S_TOKEN&gt;&quot;}}&#39; \\ http://localhost:8087/authn # output { &quot;apiVersion&quot;: &quot;authentication.k8s.io/v1beta1&quot;, &quot;kind&quot;: &quot;TokenReview&quot;, &quot;status&quot;: { &quot;authenticated&quot;: true, &quot;user&quot;: { &quot;groups&quot;: [ &quot;kubernetes&quot; ], &quot;uid&quot;: &quot;1000&quot;, &quot;username&quot;: &quot;user1&quot; } } } 在所有master節點上新增一個名稱為/srv/kubernetes/webhook-authn的檔案，並加入以下內容： $ mkdir /srv/kubernetes $ cat &lt;&lt;EOF &gt; /srv/kubernetes/webhook-authn clusters: - name: ldap-authn cluster: server: http://localhost:8087/authn users: - name: apiserver current-context: webhook contexts: - context: cluster: ldap-authn user: apiserver name: webhook EOF 修改所有master節點上的kube-apiserver.yaml Static Pod 檔案，該檔案會存在於/etc/kubernetes/manifests目錄中，請修改加入以下內容： ... spec: containers: - command: ... - --runtime-config=authentication.k8s.io/v1beta1=true - --authentication-token-webhook-config-file=/srv/kubernetes/webhook-authn - --authentication-token-webhook-cache-ttl=5m volumeMounts: ... - mountPath: /srv/kubernetes/webhook-authn name: webhook-authn readOnly: true volumes: ... - hostPath: path: /srv/kubernetes/webhook-authn type: File name: webhook-authn 這邊...表示已存在的內容，請不要刪除與變更。這邊也可以用 kubeadmconfig 來設定，請參考 Using kubeadm init with a configuration file。 測試功能首先進入k8s-m1，建立一個綁定在 user1 namespace 的唯讀 Role 與 RoleBinding： $ kubectl create ns user1 # 建立 Role $ cat &lt;&lt;EOF | kubectl create -f - kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: readonly-role namespace: user1 rules: - apiGroups: [&quot;&quot;] resources: [&quot;pods&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] EOF # 建立 RoleBinding $ cat &lt;&lt;EOF | kubectl create -f - kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: readonly-role-binding namespace: user1 subjects: - kind: Group name: kubernetes apiGroup: &quot;&quot; roleRef: kind: Role name: readonly-role apiGroup: &quot;&quot; EOF 注意!!這邊的Group是 LDAP 中的 Group。 在任意台 Kubernetes client 端設定 Kubeconfig 來存取叢集，這邊直接在k8s-m1進行： $ cd $ kubectl config set-credentials user1 --kubeconfig=.kube/config --token=&lt;user-ldap-token&gt; $ kubectl config set-context user1-context \\ --kubeconfig=.kube/config \\ --cluster=kubernetes \\ --namespace=user1 --user=user1 接著透過 kubeclt 來測試權限是否正確設定： $ kubectl --context=user1-context get po No resources found $ kubectl --context=user1-context run nginx --image nginx --port 80 Error from server (Forbidden): deployments.extensions is forbidden: User &quot;user1&quot; cannot create deployments.extensions in the namespace &quot;user1&quot; $ kubectl --context=user1-context get po -n default Error from server (Forbidden): pods is forbidden: User &quot;user1&quot; cannot list pods in the namespace &quot;default&quot; 參考資料 https://github.com/osixia/docker-openldap https://icicimov.github.io/blog/virtualization/Kubernetes-LDAP-Authentication/ https://github.com/torchbox/kube-ldap-authn","tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"},{"name":"LDAP","slug":"LDAP","permalink":"https://kairen.github.io/tags/LDAP/"}]},{"title":"Kubernetes v1.10.x HA 全手動苦工安裝教學(TL;DR)","date":"2018-04-05T09:08:54.000Z","path":"2018/04/05/kubernetes/deploy/manual-v1.10/","text":"本篇延續過往手動安裝方式來部署 Kubernetes v1.10.x 版本的 High Availability 叢集，主要目的是學習 Kubernetes 安裝的一些元件關析與流程。若不想這麼累的話，可以參考 Picking the Right Solution 來選擇自己最喜歡的方式。 本次安裝的軟體版本： Kubernetes v1.10.0 CNI v0.6.0 Etcd v3.1.13 Calico v3.0.4 Docker CE latest version 節點資訊本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統可採用Ubuntu 16.x與CentOS 7.x： IP Address Hostname CPU Memory 192.16.35.11 k8s-m1 1 4G 192.16.35.12 k8s-m2 1 4G 192.16.35.13 k8s-m3 1 4G 192.16.35.14 k8s-n1 1 4G 192.16.35.15 k8s-n2 1 4G 192.16.35.16 k8s-n2 1 4G 另外由所有 master 節點提供一組 VIP 192.16.35.10。 這邊m為主要控制節點，n為應用程式工作節點。 所有操作全部用root使用者進行(方便用)，以 SRE 來說不推薦。 可以下載 Vagrantfile 來建立 Virtualbox 虛擬機叢集。不過需要注意機器資源是否足夠。 事前準備開始安裝前需要確保以下條件已達成： 所有節點彼此網路互通，並且k8s-m1 SSH 登入其他節點為 passwdless。 所有防火牆與 SELinux 已關閉。如 CentOS： $ systemctl stop firewalld &amp;&amp; systemctl disable firewalld $ setenforce 0 $ vim /etc/selinux/config SELINUX=disabled 所有節點需要設定/etc/hosts解析到所有叢集主機。 ... 192.16.35.11 k8s-m1 192.16.35.12 k8s-m2 192.16.35.13 k8s-m3 192.16.35.14 k8s-n1 192.16.35.15 k8s-n2 192.16.35.16 k8s-n3 所有節點需要安裝 Docker CE 版本的容器引擎： $ curl -fsSL &quot;https://get.docker.com/&quot; | sh 不管是在 Ubuntu 或 CentOS 都只需要執行該指令就會自動安裝最新版 Docker。CentOS 安裝完成後，需要再執行以下指令： $ systemctl enable docker &amp;&amp; systemctl start docker 所有節點需要設定/etc/sysctl.d/k8s.conf的系統參數。 $ cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF $ sysctl -p /etc/sysctl.d/k8s.conf Kubernetes v1.8+ 要求關閉系統 Swap，若不關閉則需要修改 kubelet 設定參數，在所有節點利用以下指令關閉： $ swapoff -a &amp;&amp; sysctl -w vm.swappiness=0 記得/etc/fstab也要註解掉SWAP掛載。 在所有節點下載 Kubernetes 二進制執行檔： $ export KUBE_URL=&quot;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64&quot; $ wget &quot;${KUBE_URL}/kubelet&quot; -O /usr/local/bin/kubelet $ chmod +x /usr/local/bin/kubelet # node 請忽略下載 kubectl $ wget &quot;${KUBE_URL}/kubectl&quot; -O /usr/local/bin/kubectl $ chmod +x /usr/local/bin/kubectl 在所有節點下載 Kubernetes CNI 二進制檔案： $ mkdir -p /opt/cni/bin &amp;&amp; cd /opt/cni/bin $ export CNI_URL=&quot;https://github.com/containernetworking/plugins/releases/download&quot; $ wget -qO- --show-progress &quot;${CNI_URL}/v0.6.0/cni-plugins-amd64-v0.6.0.tgz&quot; | tar -zx 在k8s-m1需要安裝CFSSL工具，這將會用來建立 TLS Certificates。 $ export CFSSL_URL=&quot;https://pkg.cfssl.org/R1.2&quot; $ wget &quot;${CFSSL_URL}/cfssl_linux-amd64&quot; -O /usr/local/bin/cfssl $ wget &quot;${CFSSL_URL}/cfssljson_linux-amd64&quot; -O /usr/local/bin/cfssljson $ chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson 建立叢集 CA keys 與 Certificates在這個部分，將需要產生多個元件的 Certificates，這包含 Etcd、Kubernetes 元件等，並且每個叢集都會有一個根數位憑證認證機構(Root Certificate Authority)被用在認證 API Server 與 Kubelet 端的憑證。 P.S. 這邊要注意 CA JSON 檔的CN(Common Name)與O(Organization)等內容是會影響 Kubernetes 元件認證的。 Etcd首先在k8s-m1建立/etc/etcd/ssl資料夾，然後進入目錄完成以下操作。 $ mkdir -p /etc/etcd/ssl &amp;&amp; cd /etc/etcd/ssl $ export PKI_URL=&quot;https://kairen.github.io/files/manual-v1.10/pki&quot; 下載ca-config.json與etcd-ca-csr.json檔案，並從 CSR json 產生 CA keys 與 Certificate： $ wget &quot;${PKI_URL}/ca-config.json&quot; &quot;${PKI_URL}/etcd-ca-csr.json&quot; $ cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare etcd-ca 下載etcd-csr.json檔案，並產生 Etcd 證書： $ wget &quot;${PKI_URL}/etcd-csr.json&quot; $ cfssl gencert \\ -ca=etcd-ca.pem \\ -ca-key=etcd-ca-key.pem \\ -config=ca-config.json \\ -hostname=127.0.0.1,192.16.35.11,192.16.35.12,192.16.35.13 \\ -profile=kubernetes \\ etcd-csr.json | cfssljson -bare etcd -hostname需修改成所有 masters 節點。 完成後刪除不必要檔案： $ rm -rf *.json *.csr 確認/etc/etcd/ssl有以下檔案： $ ls /etc/etcd/ssl etcd-ca-key.pem etcd-ca.pem etcd-key.pem etcd.pem 複製相關檔案至其他 Etcd 節點，這邊為所有master節點： $ for NODE in k8s-m2 k8s-m3; do echo &quot;--- $NODE ---&quot; ssh ${NODE} &quot;mkdir -p /etc/etcd/ssl&quot; for FILE in etcd-ca-key.pem etcd-ca.pem etcd-key.pem etcd.pem; do scp /etc/etcd/ssl/${FILE} ${NODE}:/etc/etcd/ssl/${FILE} done done Kubernetes在k8s-m1建立pki資料夾，然後進入目錄完成以下章節操作。 $ mkdir -p /etc/kubernetes/pki &amp;&amp; cd /etc/kubernetes/pki $ export PKI_URL=&quot;https://kairen.github.io/files/manual-v1.10/pki&quot; $ export KUBE_APISERVER=&quot;https://192.16.35.10:6443&quot; 下載ca-config.json與ca-csr.json檔案，並產生 CA 金鑰： $ wget &quot;${PKI_URL}/ca-config.json&quot; &quot;${PKI_URL}/ca-csr.json&quot; $ cfssl gencert -initca ca-csr.json | cfssljson -bare ca $ ls ca*.pem ca-key.pem ca.pem API Server Certificate下載apiserver-csr.json檔案，並產生 kube-apiserver 憑證： $ wget &quot;${PKI_URL}/apiserver-csr.json&quot; $ cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=10.96.0.1,192.16.35.10,127.0.0.1,kubernetes.default \\ -profile=kubernetes \\ apiserver-csr.json | cfssljson -bare apiserver $ ls apiserver*.pem apiserver-key.pem apiserver.pem 這邊-hostname的10.96.0.1是 Cluster IP 的 Kubernetes 端點; 192.16.35.10為虛擬 IP 位址(VIP); kubernetes.default為 Kubernetes DN。 Front Proxy Certificate下載front-proxy-ca-csr.json檔案，並產生 Front Proxy CA 金鑰，Front Proxy 主要是用在 API aggregator 上: $ wget &quot;${PKI_URL}/front-proxy-ca-csr.json&quot; $ cfssl gencert \\ -initca front-proxy-ca-csr.json | cfssljson -bare front-proxy-ca $ ls front-proxy-ca*.pem front-proxy-ca-key.pem front-proxy-ca.pem 下載front-proxy-client-csr.json檔案，並產生 front-proxy-client 證書： $ wget &quot;${PKI_URL}/front-proxy-client-csr.json&quot; $ cfssl gencert \\ -ca=front-proxy-ca.pem \\ -ca-key=front-proxy-ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ front-proxy-client-csr.json | cfssljson -bare front-proxy-client $ ls front-proxy-client*.pem front-proxy-client-key.pem front-proxy-client.pem Admin Certificate下載admin-csr.json檔案，並產生 admin certificate 憑證： $ wget &quot;${PKI_URL}/admin-csr.json&quot; $ cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ admin-csr.json | cfssljson -bare admin $ ls admin*.pem admin-key.pem admin.pem 接著透過以下指令產生名稱為 admin.conf 的 kubeconfig 檔： # admin set cluster $ kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=../admin.conf # admin set credentials $ kubectl config set-credentials kubernetes-admin \\ --client-certificate=admin.pem \\ --client-key=admin-key.pem \\ --embed-certs=true \\ --kubeconfig=../admin.conf # admin set context $ kubectl config set-context kubernetes-admin@kubernetes \\ --cluster=kubernetes \\ --user=kubernetes-admin \\ --kubeconfig=../admin.conf # admin set default context $ kubectl config use-context kubernetes-admin@kubernetes \\ --kubeconfig=../admin.conf Controller Manager Certificate下載manager-csr.json檔案，並產生 kube-controller-manager certificate 憑證： $ wget &quot;${PKI_URL}/manager-csr.json&quot; $ cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ manager-csr.json | cfssljson -bare controller-manager $ ls controller-manager*.pem controller-manager-key.pem controller-manager.pem 若節點 IP 不同，需要修改manager-csr.json的hosts。 接著透過以下指令產生名稱為controller-manager.conf的 kubeconfig 檔： # controller-manager set cluster $ kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=../controller-manager.conf # controller-manager set credentials $ kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=controller-manager.pem \\ --client-key=controller-manager-key.pem \\ --embed-certs=true \\ --kubeconfig=../controller-manager.conf # controller-manager set context $ kubectl config set-context system:kube-controller-manager@kubernetes \\ --cluster=kubernetes \\ --user=system:kube-controller-manager \\ --kubeconfig=../controller-manager.conf # controller-manager set default context $ kubectl config use-context system:kube-controller-manager@kubernetes \\ --kubeconfig=../controller-manager.conf Scheduler Certificate下載scheduler-csr.json檔案，並產生 kube-scheduler certificate 憑證： $ wget &quot;${PKI_URL}/scheduler-csr.json&quot; $ cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ scheduler-csr.json | cfssljson -bare scheduler $ ls scheduler*.pem scheduler-key.pem scheduler.pem 若節點 IP 不同，需要修改scheduler-csr.json的hosts。 接著透過以下指令產生名稱為 scheduler.conf 的 kubeconfig 檔： # scheduler set cluster $ kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=../scheduler.conf # scheduler set credentials $ kubectl config set-credentials system:kube-scheduler \\ --client-certificate=scheduler.pem \\ --client-key=scheduler-key.pem \\ --embed-certs=true \\ --kubeconfig=../scheduler.conf # scheduler set context $ kubectl config set-context system:kube-scheduler@kubernetes \\ --cluster=kubernetes \\ --user=system:kube-scheduler \\ --kubeconfig=../scheduler.conf # scheduler use default context $ kubectl config use-context system:kube-scheduler@kubernetes \\ --kubeconfig=../scheduler.conf Master Kubelet Certificate接著在所有k8s-m1節點下載kubelet-csr.json檔案，並產生憑證： $ wget &quot;${PKI_URL}/kubelet-csr.json&quot; $ for NODE in k8s-m1 k8s-m2 k8s-m3; do echo &quot;--- $NODE ---&quot; cp kubelet-csr.json kubelet-$NODE-csr.json; sed -i &quot;s/\\$NODE/$NODE/g&quot; kubelet-$NODE-csr.json; cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=$NODE \\ -profile=kubernetes \\ kubelet-$NODE-csr.json | cfssljson -bare kubelet-$NODE done $ ls kubelet*.pem kubelet-k8s-m1-key.pem kubelet-k8s-m1.pem kubelet-k8s-m2-key.pem kubelet-k8s-m2.pem kubelet-k8s-m3-key.pem kubelet-k8s-m3.pem 這邊需要依據節點修改-hostname與$NODE。 完成後複製 kubelet 憑證至其他master節點： $ for NODE in k8s-m2 k8s-m3; do echo &quot;--- $NODE ---&quot; ssh ${NODE} &quot;mkdir -p /etc/kubernetes/pki&quot; for FILE in kubelet-$NODE-key.pem kubelet-$NODE.pem ca.pem; do scp /etc/kubernetes/pki/${FILE} ${NODE}:/etc/kubernetes/pki/${FILE} done done 接著執行以下指令產生名稱為kubelet.conf的 kubeconfig 檔： $ for NODE in k8s-m1 k8s-m2 k8s-m3; do echo &quot;--- $NODE ---&quot; ssh ${NODE} &quot;cd /etc/kubernetes/pki &amp;&amp; \\ kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=../kubelet.conf &amp;&amp; \\ kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=../kubelet.conf &amp;&amp; \\ kubectl config set-credentials system:node:${NODE} \\ --client-certificate=kubelet-${NODE}.pem \\ --client-key=kubelet-${NODE}-key.pem \\ --embed-certs=true \\ --kubeconfig=../kubelet.conf &amp;&amp; \\ kubectl config set-context system:node:${NODE}@kubernetes \\ --cluster=kubernetes \\ --user=system:node:${NODE} \\ --kubeconfig=../kubelet.conf &amp;&amp; \\ kubectl config use-context system:node:${NODE}@kubernetes \\ --kubeconfig=../kubelet.conf &amp;&amp; \\ rm kubelet-${NODE}.pem kubelet-${NODE}-key.pem&quot; done Service Account KeyService account 不是透過 CA 進行認證，因此不要透過 CA 來做 Service account key 的檢查，這邊建立一組 Private 與 Public 金鑰提供給 Service account key 使用： $ openssl genrsa -out sa.key 2048 $ openssl rsa -in sa.key -pubout -out sa.pub $ ls sa.* sa.key sa.pub 刪除不必要檔案所有資訊準備完成後，就可以將一些不必要檔案刪除： $ rm -rf *.json *.csr scheduler*.pem controller-manager*.pem admin*.pem kubelet*.pem 複製檔案至其他節點複製憑證檔案至其他master節點： $ for NODE in k8s-m2 k8s-m3; do echo &quot;--- $NODE ---&quot; for FILE in $(ls /etc/kubernetes/pki/); do scp /etc/kubernetes/pki/${FILE} ${NODE}:/etc/kubernetes/pki/${FILE} done done 複製 Kubernetes config 檔案至其他master節點： $ for NODE in k8s-m2 k8s-m3; do echo &quot;--- $NODE ---&quot; for FILE in admin.conf controller-manager.conf scheduler.conf; do scp /etc/kubernetes/${FILE} ${NODE}:/etc/kubernetes/${FILE} done done Kubernetes Masters本部分將說明如何建立與設定 Kubernetes Master 角色，過程中會部署以下元件： kube-apiserver：提供 REST APIs，包含授權、認證與狀態儲存等。 kube-controller-manager：負責維護叢集的狀態，如自動擴展，滾動更新等。 kube-scheduler：負責資源排程，依據預定的排程策略將 Pod 分配到對應節點上。 Etcd：儲存叢集所有狀態的 Key/Value 儲存系統。 HAProxy：提供負載平衡器。 Keepalived：提供虛擬網路位址(VIP)。 部署與設定首先在所有 master 節點下載部署元件的 YAML 檔案，這邊不採用二進制執行檔與 Systemd 來管理這些元件，全部採用 Static Pod 來達成。這邊將檔案下載至/etc/kubernetes/manifests目錄： $ export CORE_URL=&quot;https://kairen.github.io/files/manual-v1.10/master&quot; $ mkdir -p /etc/kubernetes/manifests &amp;&amp; cd /etc/kubernetes/manifests $ for FILE in kube-apiserver kube-controller-manager kube-scheduler haproxy keepalived etcd etcd.config; do wget &quot;${CORE_URL}/${FILE}.yml.conf&quot; -O ${FILE}.yml if [ ${FILE} == &quot;etcd.config&quot; ]; then mv etcd.config.yml /etc/etcd/etcd.config.yml sed -i &quot;s/\\${HOSTNAME}/${HOSTNAME}/g&quot; /etc/etcd/etcd.config.yml sed -i &quot;s/\\${PUBLIC_IP}/$(hostname -i)/g&quot; /etc/etcd/etcd.config.yml fi done $ ls /etc/kubernetes/manifests etcd.yml haproxy.yml keepalived.yml kube-apiserver.yml kube-controller-manager.yml kube-scheduler.yml 若IP與教學設定不同的話，請記得修改 YAML 檔案。 kube-apiserver 中的 NodeRestriction 請參考 Using Node Authorization。 產生一個用來加密 Etcd 的 Key： $ head -c 32 /dev/urandom | base64 SUpbL4juUYyvxj3/gonV5xVEx8j769/99TSAf8YT/sQ= 注意每台master節點需要用一樣的 Key。 在/etc/kubernetes/目錄下，建立encryption.yml的加密 YAML 檔案： $ cat &lt;&lt;EOF &gt; /etc/kubernetes/encryption.yml kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: SUpbL4juUYyvxj3/gonV5xVEx8j769/99TSAf8YT/sQ= - identity: {} EOF Etcd 資料加密可參考這篇 Encrypting data at rest。 在/etc/kubernetes/目錄下，建立audit-policy.yml的進階稽核策略 YAML 檔： $ cat &lt;&lt;EOF &gt; /etc/kubernetes/audit-policy.yml apiVersion: audit.k8s.io/v1beta1 kind: Policy rules: - level: Metadata EOF Audit Policy 請參考這篇 Auditing。 下載haproxy.cfg檔案來提供給 HAProxy 容器使用： $ mkdir -p /etc/haproxy/ $ wget &quot;${CORE_URL}/haproxy.cfg&quot; -O /etc/haproxy/haproxy.cfg 若與本教學 IP 不同的話，請記得修改設定檔。 下載kubelet.service相關檔案來管理 kubelet： $ mkdir -p /etc/systemd/system/kubelet.service.d $ wget &quot;${CORE_URL}/kubelet.service&quot; -O /lib/systemd/system/kubelet.service $ wget &quot;${CORE_URL}/10-kubelet.conf&quot; -O /etc/systemd/system/kubelet.service.d/10-kubelet.conf 若 cluster dns或domain有改變的話，需要修改10-kubelet.conf。 最後建立 var 存放資訊，然後啟動 kubelet 服務: $ mkdir -p /var/lib/kubelet /var/log/kubernetes /var/lib/etcd $ systemctl enable kubelet.service &amp;&amp; systemctl start kubelet.service 完成後會需要一段時間來下載映像檔與啟動元件，可以利用該指令來監看： $ watch netstat -ntlp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 127.0.0.1:10248 0.0.0.0:* LISTEN 10344/kubelet tcp 0 0 127.0.0.1:10251 0.0.0.0:* LISTEN 11324/kube-schedule tcp 0 0 0.0.0.0:6443 0.0.0.0:* LISTEN 11416/haproxy tcp 0 0 127.0.0.1:10252 0.0.0.0:* LISTEN 11235/kube-controll tcp 0 0 0.0.0.0:9090 0.0.0.0:* LISTEN 11416/haproxy tcp6 0 0 :::2379 :::* LISTEN 10479/etcd tcp6 0 0 :::2380 :::* LISTEN 10479/etcd tcp6 0 0 :::10255 :::* LISTEN 10344/kubelet tcp6 0 0 :::5443 :::* LISTEN 11295/kube-apiserve 若看到以上資訊表示服務正常啟動，若發生問題可以用docker指令來查看。 驗證叢集完成後，在任意一台master節點複製 admin kubeconfig 檔案，並透過簡單指令驗證： $ cp /etc/kubernetes/admin.conf ~/.kube/config $ kubectl get cs NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy {&quot;health&quot;: &quot;true&quot;} etcd-1 Healthy {&quot;health&quot;: &quot;true&quot;} etcd-0 Healthy {&quot;health&quot;: &quot;true&quot;} $ kubectl get node NAME STATUS ROLES AGE VERSION k8s-m1 NotReady master 52s v1.10.0 k8s-m2 NotReady master 51s v1.10.0 k8s-m3 NotReady master 50s v1.10.0 $ kubectl -n kube-system get po NAME READY STATUS RESTARTS AGE etcd-k8s-m1 1/1 Running 0 7s etcd-k8s-m2 1/1 Running 0 57s haproxy-k8s-m3 1/1 Running 0 1m ... 接著確認服務能夠執行 logs 等指令： $ kubectl -n kube-system logs -f kube-scheduler-k8s-m2 Error from server (Forbidden): Forbidden (user=kube-apiserver, verb=get, resource=nodes, subresource=proxy) ( pods/log kube-scheduler-k8s-m2) 這邊會發現出現 403 Forbidden 問題，這是因為 kube-apiserver user 並沒有 nodes 的資源存取權限，屬於正常。 由於上述權限問題，必需建立一個apiserver-to-kubelet-rbac.yml來定義權限，以供對 Nodes 容器執行 logs、exec 等指令。在任意一台master節點執行以下指令： $ kubectl apply -f &quot;${CORE_URL}/apiserver-to-kubelet-rbac.yml.conf&quot; clusterrole.rbac.authorization.k8s.io &quot;system:kube-apiserver-to-kubelet&quot; configured clusterrolebinding.rbac.authorization.k8s.io &quot;system:kube-apiserver&quot; configured # 測試 logs $ kubectl -n kube-system logs -f kube-scheduler-k8s-m2 ... I0403 02:30:36.375935 1 server.go:555] Version: v1.10.0 I0403 02:30:36.378208 1 server.go:574] starting healthz server on 127.0.0.1:10251 設定master節點允許 Taint： $ kubectl taint nodes node-role.kubernetes.io/master=&quot;&quot;:NoSchedule --all node &quot;k8s-m1&quot; tainted node &quot;k8s-m2&quot; tainted node &quot;k8s-m3&quot; tainted Taints and Tolerations。 建立 TLS Bootstrapping RBAC 與 Secret由於本次安裝啟用了 TLS 認證，因此每個節點的 kubelet 都必須使用 kube-apiserver 的 CA 的憑證後，才能與 kube-apiserver 進行溝通，而該過程需要手動針對每台節點單獨簽署憑證是一件繁瑣的事情，且一旦節點增加會延伸出管理不易問題; 而 TLS bootstrapping 目標就是解決該問題，透過讓 kubelet 先使用一個預定低權限使用者連接到 kube-apiserver，然後在對 kube-apiserver 申請憑證簽署，當授權 Token 一致時，Node 節點的 kubelet 憑證將由 kube-apiserver 動態簽署提供。具體作法可以參考 TLS Bootstrapping 與 Authenticating with Bootstrap Tokens。 首先在k8s-m1建立一個變數來產生BOOTSTRAP_TOKEN，並建立bootstrap-kubelet.conf的 Kubernetes config 檔： $ cd /etc/kubernetes/pki $ export TOKEN_ID=$(openssl rand 3 -hex) $ export TOKEN_SECRET=$(openssl rand 8 -hex) $ export BOOTSTRAP_TOKEN=${TOKEN_ID}.${TOKEN_SECRET} $ export KUBE_APISERVER=&quot;https://192.16.35.10:6443&quot; # bootstrap set cluster $ kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=../bootstrap-kubelet.conf # bootstrap set credentials $ kubectl config set-credentials tls-bootstrap-token-user \\ --token=${BOOTSTRAP_TOKEN} \\ --kubeconfig=../bootstrap-kubelet.conf # bootstrap set context $ kubectl config set-context tls-bootstrap-token-user@kubernetes \\ --cluster=kubernetes \\ --user=tls-bootstrap-token-user \\ --kubeconfig=../bootstrap-kubelet.conf # bootstrap use default context $ kubectl config use-context tls-bootstrap-token-user@kubernetes \\ --kubeconfig=../bootstrap-kubelet.conf 若想要用手動簽署憑證來進行授權的話，可以參考 Certificate。 接著在k8s-m1建立 TLS bootstrap secret 來提供自動簽證使用： $ cat &lt;&lt;EOF | kubectl create -f - apiVersion: v1 kind: Secret metadata: name: bootstrap-token-${TOKEN_ID} namespace: kube-system type: bootstrap.kubernetes.io/token stringData: token-id: ${TOKEN_ID} token-secret: ${TOKEN_SECRET} usage-bootstrap-authentication: &quot;true&quot; usage-bootstrap-signing: &quot;true&quot; auth-extra-groups: system:bootstrappers:default-node-token EOF secret &quot;bootstrap-token-65a3a9&quot; created 在k8s-m1建立 TLS Bootstrap Autoapprove RBAC： $ kubectl apply -f &quot;${CORE_URL}/kubelet-bootstrap-rbac.yml.conf&quot; clusterrolebinding.rbac.authorization.k8s.io &quot;kubelet-bootstrap&quot; created clusterrolebinding.rbac.authorization.k8s.io &quot;node-autoapprove-bootstrap&quot; created clusterrolebinding.rbac.authorization.k8s.io &quot;node-autoapprove-certificate-rotation&quot; created Kubernetes Nodes本部分將說明如何建立與設定 Kubernetes Node 角色，Node 是主要執行容器實例(Pod)的工作節點。 在開始部署前，先在k8-m1將需要用到的檔案複製到所有node節點上： $ cd /etc/kubernetes/pki $ for NODE in k8s-n1 k8s-n2 k8s-n3; do echo &quot;--- $NODE ---&quot; ssh ${NODE} &quot;mkdir -p /etc/kubernetes/pki/&quot; ssh ${NODE} &quot;mkdir -p /etc/etcd/ssl&quot; # Etcd for FILE in etcd-ca.pem etcd.pem etcd-key.pem; do scp /etc/etcd/ssl/${FILE} ${NODE}:/etc/etcd/ssl/${FILE} done # Kubernetes for FILE in pki/ca.pem pki/ca-key.pem bootstrap-kubelet.conf; do scp /etc/kubernetes/${FILE} ${NODE}:/etc/kubernetes/${FILE} done done 部署與設定在每台node節點下載kubelet.service相關檔案來管理 kubelet： $ export CORE_URL=&quot;https://kairen.github.io/files/manual-v1.10/node&quot; $ mkdir -p /etc/systemd/system/kubelet.service.d $ wget &quot;${CORE_URL}/kubelet.service&quot; -O /lib/systemd/system/kubelet.service $ wget &quot;${CORE_URL}/10-kubelet.conf&quot; -O /etc/systemd/system/kubelet.service.d/10-kubelet.conf 若 cluster dns或domain有改變的話，需要修改10-kubelet.conf。 最後建立 var 存放資訊，然後啟動 kubelet 服務: $ mkdir -p /var/lib/kubelet /var/log/kubernetes $ systemctl enable kubelet.service &amp;&amp; systemctl start kubelet.service 驗證叢集完成後，在任意一台master節點並透過簡單指令驗證： $ kubectl get csr NAME AGE REQUESTOR CONDITION csr-bvz9l 11m system:node:k8s-m1 Approved,Issued csr-jwr8k 11m system:node:k8s-m2 Approved,Issued csr-q867w 11m system:node:k8s-m3 Approved,Issued node-csr-Y-FGvxZWJqI-8RIK_IrpgdsvjGQVGW0E4UJOuaU8ogk 17s system:bootstrap:dca3e1 Approved,Issued node-csr-cnX9T1xp1LdxVDc9QW43W0pYkhEigjwgceRshKuI82c 19s system:bootstrap:dca3e1 Approved,Issued node-csr-m7SBA9RAGCnsgYWJB-u2HoB2qLSfiQZeAxWFI2WYN7Y 18s system:bootstrap:dca3e1 Approved,Issued $ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-m1 NotReady master 12m v1.10.0 k8s-m2 NotReady master 11m v1.10.0 k8s-m3 NotReady master 11m v1.10.0 k8s-n1 NotReady node 32s v1.10.0 k8s-n2 NotReady node 31s v1.10.0 k8s-n3 NotReady node 29s v1.10.0 Kubernetes Core Addons 部署當完成上面所有步驟後，接著需要部署一些插件，其中如Kubernetes DNS與Kubernetes Proxy等這種 Addons 是非常重要的。 Kubernetes ProxyKube-proxy 是實現 Service 的關鍵插件，kube-proxy 會在每台節點上執行，然後監聽 API Server 的 Service 與 Endpoint 資源物件的改變，然後來依據變化執行 iptables 來實現網路的轉發。這邊我們會需要建議一個 DaemonSet 來執行，並且建立一些需要的 Certificates。 在k8s-m1下載kube-proxy.yml來建立 Kubernetes Proxy Addon： $ kubectl apply -f &quot;https://kairen.github.io/files/manual-v1.10/addon/kube-proxy.yml.conf&quot; serviceaccount &quot;kube-proxy&quot; created clusterrolebinding.rbac.authorization.k8s.io &quot;system:kube-proxy&quot; created configmap &quot;kube-proxy&quot; created daemonset.apps &quot;kube-proxy&quot; created $ kubectl -n kube-system get po -o wide -l k8s-app=kube-proxy NAME READY STATUS RESTARTS AGE IP NODE kube-proxy-8j5w8 1/1 Running 0 29s 192.16.35.16 k8s-n3 kube-proxy-c4zvt 1/1 Running 0 29s 192.16.35.11 k8s-m1 kube-proxy-clpl6 1/1 Running 0 29s 192.16.35.12 k8s-m2 ... Kubernetes DNSKube DNS 是 Kubernetes 叢集內部 Pod 之間互相溝通的重要 Addon，它允許 Pod 可以透過 Domain Name 方式來連接 Service，其主要由 Kube DNS 與 Sky DNS 組合而成，透過 Kube DNS 監聽 Service 與 Endpoint 變化，來提供給 Sky DNS 資訊，已更新解析位址。 在k8s-m1下載kube-proxy.yml來建立 Kubernetes Proxy Addon： $ kubectl apply -f &quot;https://kairen.github.io/files/manual-v1.10/addon/kube-dns.yml.conf&quot; serviceaccount &quot;kube-dns&quot; created service &quot;kube-dns&quot; created deployment.extensions &quot;kube-dns&quot; created $ kubectl -n kube-system get po -l k8s-app=kube-dns NAME READY STATUS RESTARTS AGE kube-dns-654684d656-zq5t8 0/3 Pending 0 1m 這邊會發現處於Pending狀態，是由於 Kubernetes Pod Network 還未建立完成，因此所有節點會處於NotReady狀態，而造成 Pod 無法被排程分配到指定節點上啟動，由於為了解決該問題，下節將說明如何建立 Pod Network。 Calico Network 安裝與設定Calico 是一款純 Layer 3 的資料中心網路方案(不需要 Overlay 網路)，Calico 好處是它整合了各種雲原生平台，且 Calico 在每一個節點利用 Linux Kernel 實現高效的 vRouter 來負責資料的轉發，而當資料中心複雜度增加時，可以用 BGP route reflector 來達成。 本次不採用手動方式來建立 Calico 網路，若想了解可以參考 Integration Guide。 在k8s-m1下載calico.yaml來建立 Calico Network： $ kubectl apply -f &quot;https://kairen.github.io/files/manual-v1.10/network/calico.yml.conf&quot; configmap &quot;calico-config&quot; created daemonset &quot;calico-node&quot; created deployment &quot;calico-kube-controllers&quot; created clusterrolebinding &quot;calico-cni-plugin&quot; created clusterrole &quot;calico-cni-plugin&quot; created serviceaccount &quot;calico-cni-plugin&quot; created clusterrolebinding &quot;calico-kube-controllers&quot; created clusterrole &quot;calico-kube-controllers&quot; created serviceaccount &quot;calico-kube-controllers&quot; created $ kubectl -n kube-system get po -l k8s-app=calico-node -o wide NAME READY STATUS RESTARTS AGE IP NODE calico-node-22mbb 2/2 Running 0 1m 192.16.35.12 k8s-m2 calico-node-2qwf5 2/2 Running 0 1m 192.16.35.11 k8s-m1 calico-node-g2sp8 2/2 Running 0 1m 192.16.35.13 k8s-m3 calico-node-hghp4 2/2 Running 0 1m 192.16.35.14 k8s-n1 calico-node-qp6gf 2/2 Running 0 1m 192.16.35.15 k8s-n2 calico-node-zfx4n 2/2 Running 0 1m 192.16.35.16 k8s-n3 這邊若節點 IP 與網卡不同的話，請修改calico.yml檔案。 在k8s-m1下載 Calico CLI 來查看 Calico nodes: $ wget https://github.com/projectcalico/calicoctl/releases/download/v3.1.0/calicoctl -O /usr/local/bin/calicoctl $ chmod u+x /usr/local/bin/calicoctl $ cat &lt;&lt;EOF &gt; ~/calico-rc export ETCD_ENDPOINTS=&quot;https://192.16.35.11:2379,https://192.16.35.12:2379,https://192.16.35.13:2379&quot; export ETCD_CA_CERT_FILE=&quot;/etc/etcd/ssl/etcd-ca.pem&quot; export ETCD_CERT_FILE=&quot;/etc/etcd/ssl/etcd.pem&quot; export ETCD_KEY_FILE=&quot;/etc/etcd/ssl/etcd-key.pem&quot; EOF $ . ~/calico-rc $ calicoctl node status Calico process is running. IPv4 BGP status +--------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-------------------+-------+----------+-------------+ | 192.16.35.12 | node-to-node mesh | up | 04:42:37 | Established | | 192.16.35.13 | node-to-node mesh | up | 04:42:42 | Established | | 192.16.35.14 | node-to-node mesh | up | 04:42:37 | Established | | 192.16.35.15 | node-to-node mesh | up | 04:42:41 | Established | | 192.16.35.16 | node-to-node mesh | up | 04:42:36 | Established | +--------------+-------------------+-------+----------+-------------+ ... 查看 pending 的 pod 是否已執行： $ kubectl -n kube-system get po -l k8s-app=kube-dns kubectl -n kube-system get po -l k8s-app=kube-dns NAME READY STATUS RESTARTS AGE kube-dns-654684d656-j8xzx 3/3 Running 0 10m Kubernetes Extra Addons 部署本節說明如何部署一些官方常用的 Addons，如 Dashboard、Heapster 等。 DashboardDashboard 是 Kubernetes 社區官方開發的儀表板，有了儀表板後管理者就能夠透過 Web-based 方式來管理 Kubernetes 叢集，除了提升管理方便，也讓資源視覺化，讓人更直覺看見系統資訊的呈現結果。 在k8s-m1透過 kubectl 來建立 kubernetes dashboard 即可： $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml $ kubectl -n kube-system get po,svc -l k8s-app=kubernetes-dashboard NAME READY STATUS RESTARTS AGE kubernetes-dashboard-7d5dcdb6d9-j492l 1/1 Running 0 12s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard ClusterIP 10.111.22.111 &lt;none&gt; 443/TCP 12s 這邊會額外建立一個名稱為open-api Cluster Role Binding，這僅作為方便測試時使用，在一般情況下不要開啟，不然就會直接被存取所有 API: $ cat &lt;&lt;EOF | kubectl create -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: open-api namespace: &quot;&quot; roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: system:anonymous EOF 注意!管理者可以針對特定使用者來開放 API 存取權限，但這邊方便使用直接綁在 cluster-admin cluster role。 完成後，就可以透過瀏覽器存取 Dashboard。 在 1.7 版本以後的 Dashboard 將不再提供所有權限，因此需要建立一個 service account 來綁定 cluster-admin role： $ kubectl -n kube-system create sa dashboard $ kubectl create clusterrolebinding dashboard --clusterrole cluster-admin --serviceaccount=kube-system:dashboard $ SECRET=$(kubectl -n kube-system get sa dashboard -o yaml | awk &#39;/dashboard-token/ {print $3}&#39;) $ kubectl -n kube-system describe secrets ${SECRET} | awk &#39;/token:/{print $2}&#39; eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtdG9rZW4tdzVocmgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYWJmMTFjYzMtZjRlYi0xMWU3LTgzYWUtMDgwMDI3NjdkOWI5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZCJ9.Xuyq34ci7Mk8bI97o4IldDyKySOOqRXRsxVWIJkPNiVUxKT4wpQZtikNJe2mfUBBD-JvoXTzwqyeSSTsAy2CiKQhekW8QgPLYelkBPBibySjBhJpiCD38J1u7yru4P0Pww2ZQJDjIxY4vqT46ywBklReGVqY3ogtUQg-eXueBmz-o7lJYMjw8L14692OJuhBjzTRSaKW8U2MPluBVnD7M2SOekDff7KpSxgOwXHsLVQoMrVNbspUCvtIiEI1EiXkyCNRGwfnd2my3uzUABIHFhm0_RZSmGwExPbxflr8Fc6bxmuz-_jSdOtUidYkFIzvEWw2vRovPgs3MXTv59RwUw 複製token，然後貼到 Kubernetes dashboard。注意這邊一般來說要針對不同 User 開啟特定存取權限。 HeapsterHeapster 是 Kubernetes 社區維護的容器叢集監控與效能分析工具。Heapster 會從 Kubernetes apiserver 取得所有 Node 資訊，然後再透過這些 Node 來取得 kubelet 上的資料，最後再將所有收集到資料送到 Heapster 的後台儲存 InfluxDB，最後利用 Grafana 來抓取 InfluxDB 的資料源來進行視覺化。 在k8s-m1透過 kubectl 來建立 kubernetes monitor 即可： $ kubectl apply -f &quot;https://kairen.github.io/files/manual-v1.10/addon/kube-monitor.yml.conf&quot; $ kubectl -n kube-system get po,svc NAME READY STATUS RESTARTS AGE ... po/heapster-74fb5c8cdc-62xzc 4/4 Running 0 7m po/influxdb-grafana-55bd7df44-nw4nc 2/2 Running 0 7m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ... svc/heapster ClusterIP 10.100.242.225 &lt;none&gt; 80/TCP 7m svc/monitoring-grafana ClusterIP 10.101.106.180 &lt;none&gt; 80/TCP 7m svc/monitoring-influxdb ClusterIP 10.109.245.142 &lt;none&gt; 8083/TCP,8086/TCP 7m ··· 完成後，就可以透過瀏覽器存取 Grafana Dashboard。 Ingress ControllerIngress是利用 Nginx 或 HAProxy 等負載平衡器來曝露叢集內服務的元件，Ingress 主要透過設定 Ingress 規格來定義 Domain Name 映射 Kubernetes 內部 Service，這種方式可以避免掉使用過多的 NodePort 問題。 在k8s-m1透過 kubectl 來建立 Ingress Controller 即可： $ kubectl create ns ingress-nginx $ kubectl apply -f &quot;https://kairen.github.io/files/manual-v1.10/addon/ingress-controller.yml.conf&quot; $ kubectl -n ingress-nginx get po NAME READY STATUS RESTARTS AGE default-http-backend-5c6d95c48-rzxfb 1/1 Running 0 7m nginx-ingress-controller-699cdf846-982n4 1/1 Running 0 7m 這裡也可以選擇 Traefik 的 Ingress Controller。 測試 Ingress 功能這邊先建立一個 Nginx HTTP server Deployment 與 Service： $ kubectl run nginx-dp --image nginx --port 80 $ kubectl expose deploy nginx-dp --port 80 $ kubectl get po,svc $ cat &lt;&lt;EOF | kubectl create -f - apiVersion: extensions/v1beta1 kind: Ingress metadata: name: test-nginx-ingress annotations: ingress.kubernetes.io/rewrite-target: / spec: rules: - host: test.nginx.com http: paths: - path: / backend: serviceName: nginx-dp servicePort: 80 EOF 透過 curl 來進行測試： $ curl 192.16.35.10 -H &#39;Host: test.nginx.com&#39; &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Welcome to nginx!&lt;/title&gt; ... # 測試其他 domain name 是否會回傳 404 $ curl 192.16.35.10 -H &#39;Host: test.nginx.com1&#39; default backend - 404 Helm Tiller ServerHelm 是 Kubernetes Chart 的管理工具，Kubernetes Chart 是一套預先組態的 Kubernetes 資源套件。其中Tiller Server主要負責接收來至 Client 的指令，並透過 kube-apiserver 與 Kubernetes 叢集做溝通，根據 Chart 定義的內容，來產生與管理各種對應 API 物件的 Kubernetes 部署檔案(又稱為 Release)。 首先在k8s-m1安裝 Helm tool： $ wget -qO- https://kubernetes-helm.storage.googleapis.com/helm-v2.8.1-linux-amd64.tar.gz | tar -zx $ sudo mv linux-amd64/helm /usr/local/bin/ 另外在所有node節點安裝 socat： $ sudo apt-get install -y socat 接著初始化 Helm(這邊會安裝 Tiller Server)： $ kubectl -n kube-system create sa tiller $ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller $ helm init --service-account tiller ... Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster. Happy Helming! $ kubectl -n kube-system get po -l app=helm NAME READY STATUS RESTARTS AGE tiller-deploy-5f789bd9f7-tzss6 1/1 Running 0 29s $ helm version Client: &amp;version.Version{SemVer:&quot;v2.8.1&quot;, GitCommit:&quot;6af75a8fd72e2aa18a2b278cfe5c7a1c5feca7f2&quot;, GitTreeState:&quot;clean&quot;} Server: &amp;version.Version{SemVer:&quot;v2.8.1&quot;, GitCommit:&quot;6af75a8fd72e2aa18a2b278cfe5c7a1c5feca7f2&quot;, GitTreeState:&quot;clean&quot;} 測試 Helm 功能這邊部署簡單 Jenkins 來進行功能測試： $ helm install --name demo --set Persistence.Enabled=false stable/jenkins $ kubectl get po,svc -l app=demo-jenkins NAME READY STATUS RESTARTS AGE demo-jenkins-7bf4bfcff-q74nt 1/1 Running 0 2m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE demo-jenkins LoadBalancer 10.103.15.129 &lt;pending&gt; 8080:31161/TCP 2m demo-jenkins-agent ClusterIP 10.103.160.126 &lt;none&gt; 50000/TCP 2m # 取得 admin 帳號的密碼 $ printf $(kubectl get secret --namespace default demo-jenkins -o jsonpath=&quot;{.data.jenkins-admin-password}&quot; | base64 --decode);echo r6y9FMuF2u 完成後，就可以透過瀏覽器存取 Jenkins Web。 測試完成後，即可刪除： $ helm ls NAME REVISION UPDATED STATUS CHART NAMESPACE demo 1 Tue Apr 10 07:29:51 2018 DEPLOYED jenkins-0.14.4 default $ helm delete demo --purge release &quot;demo&quot; deleted 更多 Helm Apps 可以到 Kubeapps Hub 尋找。 測試叢集SSH 進入k8s-m1節點，然後關閉該節點： $ sudo poweroff 接著進入到k8s-m2節點，透過 kubectl 來檢查叢集是否能夠正常執行： # 先檢查 etcd 狀態，可以發現 etcd-0 因為關機而中斷 $ kubectl get cs NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-1 Healthy {&quot;health&quot;: &quot;true&quot;} etcd-2 Healthy {&quot;health&quot;: &quot;true&quot;} etcd-0 Unhealthy Get https://192.16.35.11:2379/health: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) # 測試是否可以建立 Pod $ kubectl run nginx --image nginx --restart=Never --port 80 $ kubectl get po NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 22s","tags":[{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"},{"name":"Calico","slug":"Calico","permalink":"https://kairen.github.io/tags/Calico/"}]},{"title":"使用 kubefed 建立 Kubernetes Federation(On-premises)","date":"2018-03-21T09:08:54.000Z","path":"2018/03/21/kubernetes/k8s-federation/","text":"Kubernetes Federation(聯邦) 是實現跨地區與跨服務商多個 Kubernetes 叢集的管理機制。Kubernetes Federation 的架構非常類似純 Kubenretes 叢集，Federation 會擁有自己的 API Server 與 Controller Manager 來提供一個標準的 Kubernetes API，以及管理聯邦叢集，並利用 Etcd 來儲存所有狀態，不過差異在於 Kubenretes 只管理多個節點，而 Federation 是管理所有被註冊的 Kubernetes 叢集。 Federation 使管理多個叢集更為簡單，這主要是透過兩個模型來實現： 跨叢集的資源同步(Sync resources across clusters)：提供在多個叢集中保持資源同步的功能，如確保一個 Deployment 可以存在於多個叢集中。 跨叢集的服務發現(Cross cluster discovery:)：提供自動配置 DNS 服務以及在所有叢集後端上進行負載平衡功能，如提供全域 VIP 或 DNS record，並透過此存取多個叢集後端。 Federation 有以下幾個好處： 跨叢集的資源排程，能讓 Pod 分配至不同叢集的不同節點上執行，如果當前叢集超出負荷，能夠將額外附載分配到空閒叢集上。 叢集的高可靠，能夠做到 Pod 故障自動遷移。 可管理多個 Kubernetes 叢集。 跨叢集的服務發現。 雖然 Federation 能夠降低管理多叢集門檻，但是目前依據不建議放到生產環境。以下幾個原因： 成熟度問題，目前還處與 Alpha 階段，故很多功能都還處於實現性質，或者不太穩定。 提升網路頻寬與成本，由於 Federation 需要監控所有叢集以確保當前狀態符合預期，因是會增加額外效能開銷。 跨叢集隔離差，Federation 的子叢集git有可能因為 Bug 的引發而影響其他叢集運行狀況。 個人用起來不是很穩定，例如建立的 Deployment 刪除很常會 Timeout。 支援的物件資源有限，如不支援 StatefulSets。可參考 API resources。 Federation 主要包含三個元件： federation-apiserver：主要提供跨叢集的 REST API 伺服器，類似 kube-apiserver。 federation-controller-manager：提供多個叢集之間的狀態同步，類似 kube-controller-manager。 kubefed：Federation CLI 工具，用來初始化 Federation 元件與加入子叢集。 節點資訊本次安裝作業系統採用Ubuntu 16.04 Server，測試環境為實體機器，共有三組叢集： Federation 控制平面叢集(簡稱 F): IP Address Host vCPU RAM 172.22.132.31 k8s-f-m1 4 16G 172.22.132.32 k8s-f-n1 4 16G 叢集 A: IP Address Host vCPU RAM 172.22.132.41 k8s-a-m1 8 16G 172.22.132.42 k8s-a-n1 8 16G 叢集 B: IP Address Host vCPU RAM 172.22.132.51 k8s-b-m1 8 16G 172.22.132.52 k8s-b-n1 8 16G 事前準備安裝與進行 Federation 之前，需要確保以下條件達成： 所有叢集的節點各自部署成一個 Kubernetes 叢集，請參考 用 kubeadm 部署 Kubernetes 叢集。 修改 F、A 與 B 叢集的 Kubernetes config，並將 A 與 B 複製到 F 節點，如修改成以下： ... ... name: k8s-a-cluster contexts: - context: cluster: k8s-a-cluster user: a-cluster-admin name: a-cluster-context current-context: a-cluster-context kind: Config preferences: {} users: - name: a-cluster-admin user: ... 這邊需要修改每個叢集 config。 接著在 F 叢集合併 F、A 與 B 三個 config，透過以下方式進行： $ ls a-cluster.conf b-cluster.conf f-cluster.conf $ KUBECONFIG=f-cluster.conf:a-cluster.conf:b-cluster.conf kubectl config view --flatten &gt; ~/.kube/config $ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE a-cluster-context k8s-a-cluster a-cluster-admin b-cluster-context k8s-b-cluster b-cluster-admin * f-cluster-context k8s-f-cluster f-cluster-admin 在 F 叢集安裝 kubefed 工具： $ wget https://storage.googleapis.com/kubernetes-federation-release/release/v1.9.0-alpha.3/federation-client-linux-amd64.tar.gz $ tar xvf federation-client-linux-amd64.tar.gz $ cp federation/client/bin/kubefed /usr/local/bin/ $ kubefed version Client Version: version.Info{Major:&quot;1&quot;, Minor:&quot;9+&quot;, GitVersion:&quot;v1.9.0-alpha.3&quot;, GitCommit:&quot;85c06145286da663755b140efa2b65f793cce9ec&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-02-14T12:54:40Z&quot;, GoVersion:&quot;go1.9.1&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;} Server Version: version.Info{Major:&quot;1&quot;, Minor:&quot;9&quot;, GitVersion:&quot;v1.9.6&quot;, GitCommit:&quot;9f8ebd171479bec0ada837d7ee641dec2f8c6dd1&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-03-21T15:13:31Z&quot;, GoVersion:&quot;go1.9.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;} 在 F 叢集安裝 Helm 工具，並進行初始化： $ wget -qO- https://kubernetes-helm.storage.googleapis.com/helm-v2.8.1-linux-amd64.tar.gz | tar -zxf $ sudo mv linux-amd64/helm /usr/local/bin/ $ kubectl -n kube-system create sa tiller $ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller $ helm init --service-account tiller # wait for a few minutes $ helm version Client: &amp;version.Version{SemVer:&quot;v2.8.1&quot;, GitCommit:&quot;6af75a8fd72e2aa18a2b278cfe5c7a1c5feca7f2&quot;, GitTreeState:&quot;clean&quot;} Server: &amp;version.Version{SemVer:&quot;v2.8.1&quot;, GitCommit:&quot;6af75a8fd72e2aa18a2b278cfe5c7a1c5feca7f2&quot;, GitTreeState:&quot;clean&quot;} 部署 Kubernetes Federation由於本篇是使用實體機器部署 Kubernetes 叢集，因此無法像是 GCP 可以提供 DNS 服務來給 Federation 使用，故這邊要用 CoreDNS 建立自定義 DNS 服務。 CoreDNS 安裝首先透過 Helm 來安裝 CoreDNS 使用到的 Etcd： $ helm install --namespace federation --name etcd-operator stable/etcd-operator $ helm upgrade --namespace federation --set cluster.enabled=true etcd-operator stable/etcd-operator $ kubectl -n federation get po NAME READY STATUS RESTARTS AGE etcd-operator-etcd-operator-etcd-backup-operator-577d56449zqkj2 1/1 Running 0 1m etcd-operator-etcd-operator-etcd-operator-56679fb56-fpgmm 1/1 Running 0 1m etcd-operator-etcd-operator-etcd-restore-operator-65b6cbccl7kzr 1/1 Running 0 1m 完成後就可以安裝 CoreDNS 來提供自定義 DNS 服務了： $ cat &lt;&lt;EOF &gt; Values.yaml isClusterService: false serviceType: NodePort middleware: kubernetes: enabled: false etcd: enabled: true zones: - &quot;kairen.com.&quot; endpoint: &quot;http://etcd-cluster.federation:2379&quot; EOF $ kubectl create clusterrolebinding federation-admin --clusterrole=cluster-admin --user=system:serviceaccount:federation:default $ helm install --namespace federation --name coredns -f Values.yaml stable/coredns # 測試 CoreDNS 可以查詢 Domain Name $ kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools dnstools# host kubernetes kubernetes.default.svc.cluster.local has address 10.96.0.1 安裝與初始化 Federation 控制平面元件完成 CoreDNS 後，接著透過 kubefed 安裝控制平面元件，由於使用到 CoreDNS，因此這邊要傳入相關 conf 檔，首先建立coredns-provider.conf檔案，加入以下內容： $ cat &lt;&lt;EOF &gt; coredns-provider.conf [Global] etcd-endpoints = http://etcd-cluster.federation:2379 zones = kairen.com. EOF 請自行修改etcd-endpoints與zones。 檔案建立並確認沒問題後，透過 kubefed 工具來初始化主叢集： $ kubefed init federation \\ --host-cluster-context=f-cluster-context \\ --dns-provider=&quot;coredns&quot; \\ --dns-zone-name=&quot;kairen.com.&quot; \\ --apiserver-enable-basic-auth=true \\ --apiserver-enable-token-auth=true \\ --dns-provider-config=&quot;coredns-provider.conf&quot; \\ --apiserver-arg-overrides=&quot;--anonymous-auth=false,--v=4&quot; \\ --api-server-service-type=&quot;NodePort&quot; \\ --api-server-advertise-address=&quot;172.22.132.31&quot; \\ --etcd-persistent-storage=true $ kubectl -n federation-system get po NAME READY STATUS RESTARTS AGE apiserver-848d584b5d-cwxdh 2/2 Running 0 1m controller-manager-5846c555c6-mw2jz 1/1 Running 1 1m 這邊可以改變--etcd-persistent-storage來選擇使用或不使用 PV，若使用請先建立一個 PV 來提供給 Federation Pod 的 PVC 索取使用，可以參考 Persistent Volumes。 加入 Federation 的 Kubernetes 子叢集$ kubectl config use-context federation # 加入 k8s-a-cluster $ kubefed join f-a-cluster \\ --cluster-context=a-cluster-context \\ --host-cluster-context=f-cluster-context # 加入 k8s-b-cluster $ kubefed join f-b-cluster \\ --cluster-context=b-cluster-context \\ --host-cluster-context=f-cluster-context $ kubectl get cluster NAME AGE f-a-cluster 57s f-b-cluster 53s 測試 Federation 叢集這邊利用 Nginx Deployment 來進行測試，先簡單建立一個副本為 4 的 Nginx： $ kubectl config use-context federation $ kubectl create ns default $ kubectl run nginx --image nginx --port 80 --replicas=4 查看 Cluster A： $ kubectl --context=a-cluster-context get po NAME READY STATUS RESTARTS AGE nginx-7587c6fdb6-dpjv5 1/1 Running 0 25s nginx-7587c6fdb6-sjv8v 1/1 Running 0 25s 查看 Cluster B： $ kubectl --context=b-cluster-context get po NAME READY STATUS RESTARTS AGE nginx-7587c6fdb6-dv45v 1/1 Running 0 1m nginx-7587c6fdb6-wxsmq 1/1 Running 0 1m 其他可測試功能： 設定 Replica set preferences，參考 Spreading Replicas in Underlying Clusters。 Federation 在 v1.7+ 加入了 ClusterSelector Annotation Scheduling Policy。 Refers Minikube Federation Global Kubernetes in 3 Steps","tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"},{"name":"Federation","slug":"Federation","permalink":"https://kairen.github.io/tags/Federation/"}]},{"title":"利用 Kubeflow 來管理 TensorFlow 應用程式","date":"2018-03-15T09:08:54.000Z","path":"2018/03/15/tensorflow/kubeflow/","text":"Kubeflow 是 Google 開源的機器學習工具，目標是簡化在 Kubernetes 上運行機器學習的過程，使之更簡單、可攜帶與可擴展。Kubeflow 目標不是在於重建其他服務，而是提供一個最佳開發系統來部署到各種基礎設施架構中，另外由於使用 Kubernetes 來做為基礎，因此只要有 Kubernetes 的地方，都能夠執行 Kubeflow。 該工具能夠建立以下幾項功能： 用於建議與管理互動式 Jupyter notebook 的 JupyterHub。 可以設定使用 CPU 或 GPU，並透過單一設定調整單個叢集大小的 Tensorflow Training Controller。 用 TensorFlow Serving 容器來提供模型服務。 Kubeflow 目標是透過 Kubernetes 的特性使機器學習更加簡單與快速： 在不同基礎設施上實現簡單、可重複的攜帶性部署(Laptop &lt;-&gt; ML rig &lt;-&gt; Training cluster &lt;-&gt; Production cluster)。 部署與管理松耦合的微服務。 根據需求進行縮放。 節點資訊本次安裝作業系統採用Ubuntu 16.04 Server，測試環境為實體機器： IP Address Role vCPU RAM Extra Device 172.22.132.51 gpu-node1 8 16G GTX 1060 3G 172.22.132.52 gpu-node2 8 16G GTX 1060 3G 172.22.132.53 master1 8 16G 無 事前準備使用 Kubeflow 之前，需要確保以下條件達成： 所有節點正確安裝指定版本的 NVIDIA driver、CUDA、Docker、NVIDIA Docker，請參考 安裝 Nvidia Docker 2。 (option)所有 GPU 節點安裝 cuDNN v7.1.2 for CUDA 9.1，請至 NVIDIA cuDNN 下載。 $ tar xvf cudnn-9.1-linux-x64-v7.1.tgz $ sudo cp cuda/include/cudnn.h /usr/local/cuda/include/ $ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/ 所有節點以 kubeadm 部署成 Kubernetes v1.9+ 叢集，請參考 用 kubeadm 部署 Kubernetes 叢集。 Kubernetes 叢集需要安裝 NVIDIA Device Plugins，請參考 安裝 Kubernetes NVIDIA Device Plugins。 建立 NFS server 並在 Kubernetes 節點安裝 NFS common，然後利用 Kubernetes 建立 PV 提供給 Kubeflow 使用： # 在 master 執行 $ sudo apt-get update &amp;&amp; sudo apt-get install -y nfs-server $ sudo mkdir /nfs-data $ echo &quot;/nfs-data *(rw,sync,no_root_squash,no_subtree_check)&quot; | sudo tee -a /etc/exports $ sudo /etc/init.d/nfs-kernel-server restart # 在 node 執行 $ sudo apt-get update &amp;&amp; sudo apt-get install -y nfs-common 安裝ksonnet 0.9.2，請參考以下： $ wget https://github.com/ksonnet/ksonnet/releases/download/v0.9.2/ks_0.9.2_linux_amd64.tar.gz $ tar xvf ks_0.9.2_linux_amd64.tar.gz $ sudo cp ks_0.9.2_linux_amd64/ks /usr/local/bin/ $ ks version ksonnet version: 0.9.2 jsonnet version: v0.9.5 client-go version: 1.8 部署 Kubeflow本節將說明如何利用 ksonnet 來部署 Kubeflow 到 Kubernetes 叢集中。首先在master節點初始化 ksonnet 應用程式目錄： $ ks init my-kubeflow 如果遇到以下問題的話，可以自己建立 GitHub Token 來存取 GitHub API，請參考 Github rate limiting errors。 ERROR GET https://api.github.com/repos/ksonnet/parts/commits/master: 403 API rate limit exceeded for 122.146.93.152. 接著安裝 Kubeflow 套件至應用程式目錄： $ cd my-kubeflow $ ks registry add kubeflow github.com/kubeflow/kubeflow/tree/master/kubeflow $ ks pkg install kubeflow/core $ ks pkg install kubeflow/tf-serving $ ks pkg install kubeflow/tf-job 然後建立 Kubeflow 核心元件，該元件包含 JupyterHub 與 TensorFlow job controller： $ kubectl create namespace kubeflow $ kubectl create clusterrolebinding tf-admin --clusterrole=cluster-admin --serviceaccount=default:tf-job-operator $ ks generate core kubeflow-core --name=kubeflow-core --namespace=kubeflow # 啟動收集匿名使用者使用量資訊，如果不想開啟則忽略 $ ks param set kubeflow-core reportUsage true $ ks param set kubeflow-core usageId $(uuidgen) # 部署 Kubeflow $ ks param set kubeflow-core jupyterHubServiceType LoadBalancer $ ks apply default -c kubeflow-core 詳細使用量資訊請參考 Usage Reporting 。 完成後檢查 Kubeflow 元件部署結果： $ kubectl -n kubeflow get po -o wide NAME READY STATUS RESTARTS AGE IP NODE ambassador-7956cf5c7f-6hngq 2/2 Running 0 34m 10.244.41.132 kube-gpu-node1 ambassador-7956cf5c7f-jgxnd 2/2 Running 0 34m 10.244.152.134 kube-gpu-node2 ambassador-7956cf5c7f-jww2d 2/2 Running 0 34m 10.244.41.133 kube-gpu-node1 spartakus-volunteer-8c659d4f5-bg7kn 1/1 Running 0 34m 10.244.152.135 kube-gpu-node2 tf-hub-0 1/1 Running 0 34m 10.244.152.133 kube-gpu-node2 tf-job-operator-78757955b-2jbdh 1/1 Running 0 34m 10.244.41.131 kube-gpu-node1 這時候就可以登入 Jupyter Notebook，但這邊需要修改 Kubernetes Service，透過以下指令進行： $ kubectl -n kubeflow get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR ambassador ClusterIP 10.101.157.91 &lt;none&gt; 80/TCP 45m service=ambassador ambassador-admin ClusterIP 10.107.24.138 &lt;none&gt; 8877/TCP 45m service=ambassador k8s-dashboard ClusterIP 10.111.128.104 &lt;none&gt; 443/TCP 45m k8s-app=kubernetes-dashboard tf-hub-0 ClusterIP None &lt;none&gt; 8000/TCP 45m app=tf-hub tf-hub-lb ClusterIP 10.105.47.253 &lt;none&gt; 80/TCP 45m app=tf-hub # 修改 svc 將 Type 修改成 LoadBalancer，並且新增 externalIPs 指定為 Master IP。 $ kubectl -n kubeflow edit svc tf-hub-lb ... spec: type: LoadBalancer externalIPs: - 172.22.132.41 ... 測試 Kubeflow開始測試前先建立一個 NFS PV 來提供給 Kubeflow Jupyter 使用： $ cat &lt;&lt;EOF | kubectl create -f - apiVersion: v1 kind: PersistentVolume metadata: name: nfs-pv spec: capacity: storage: 20Gi accessModes: - ReadWriteOnce nfs: server: 172.22.132.41 path: /nfs-data EOF 完成後連接 http://Master_IP，並輸入任意帳號密碼進行登入。 登入後點選Start My Server按鈕來建立 Server 的 Spawner options，預設會有多種映像檔可以使用： CPU：gcr.io/kubeflow-images-staging/tensorflow-notebook-cpu。 GPU：gcr.io/kubeflow-images-staging/tensorflow-notebook-gpu。 這邊也使用以下 GCP 建構的映像檔做測試使用(GPU 當前為 CUDA 8)： gcr.io/kubeflow/tensorflow-notebook-cpu:latest gcr.io/kubeflow/tensorflow-notebook-gpu:latest 若 CUDA 版本不同，請自行修改 GCP Tensorflow Notebook image 或是 Kubeflow Tensorflow Notebook image 重新建構。 如果使用 GPU 請執行以下指令確認是否可被分配資源： $ kubectl get nodes &quot;-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu&quot; NAME GPU kube-gpu-master1 &lt;none&gt; kube-gpu-node1 1 kube-gpu-node2 1 最後點選Spawn來完成建立 Server，如下圖所示： 這邊先用 CPU 進行測試，由於本篇是安裝 CUDA 9.1 + cuDNN 7，因此要自己建構映像檔。 接著等 Kubernetes 下載映像檔後，就會正常啟動，如下圖所示： 當正常啟動後，點選New &gt; Python 3建立一個 Notebook 並貼上以下範例程式： from __future__ import print_function import tensorflow as tf hello = tf.constant(&#39;Hello TensorFlow!&#39;) s = tf.Session() print(s.run(hello)) 正確執行會如以下圖所示： 若想關閉叢集的話，可以點選Control Plane。 另外由於 Kubeflow 會安裝 TF Operator 來管理 TFJob，這邊可以透過 Kubernetes 來手動建立 Job： $ kubectl create -f https://raw.githubusercontent.com/kubeflow/tf-operator/master/examples/tf_job.yaml $ kubectl get po NAME READY STATUS RESTARTS AGE example-job-ps-qq6x-0-pdx7v 1/1 Running 0 5m example-job-ps-qq6x-1-2mpfp 1/1 Running 0 5m example-job-worker-qq6x-0-m5fm5 1/1 Running 0 5m 若想從 Kubernetes 叢集刪除 Kubeflow 相關元件的話，可執行下列指令達成： $ ks delete default -c kubeflow-core","tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://kairen.github.io/tags/TensorFlow/"},{"name":"GPU","slug":"GPU","permalink":"https://kairen.github.io/tags/GPU/"},{"name":"DL/ML","slug":"DL-ML","permalink":"https://kairen.github.io/tags/DL-ML/"}]},{"title":"Kubernetes NVIDIA Device Plugins","date":"2018-03-01T09:08:54.000Z","path":"2018/03/01/kubernetes/nvidia-device-plugin/","text":"Device Plugins 是 Kubernetes v1.8 版本開始加入的 Alpha 功能，目標是結合 Extended Resource 來支援 GPU、FPGA、高效能 NIC、InfiniBand 等硬體設備介接的插件，這樣好處在於硬體供應商不需要修改 Kubernetes 核心程式，只需要依據 Device Plugins 介面來實作特定硬體設備插件，就能夠提供給 Kubernetes Pod 使用。而本篇會稍微提及 Device Plugin 原理，並說明如何使用 NVIDIA device plugin。 P.S. 傳統的alpha.kubernetes.io/nvidia-gpu將於 1.11 版本移除，因此與 GPU 相關的排程與部署原始碼都將從 Kubernetes 核心移除。 Device Plugins 原理Device Plugins 主要提供了一個 gRPC 介面來給廠商實現ListAndWatch()與Allocate()等 gRPC 方法，並監聽節點的/var/lib/kubelet/device-plugins/目錄中的 gRPC Server Unix Socket，這邊可以參考官方文件 Device Plugins。一旦啟動 Device Plugins 時，透過 Kubelet Unix Socket 註冊，並提供該 plugin 的 Unix Socket 名稱、API 版本號與插件資源名稱(vendor-domain/resource，例如 nvidia.com/gpu)，接著 Kubelet 會將這些曝露到 Node 狀態以便 Scheduler 使用。 Unix Socket 範例： $ ls /var/lib/kubelet/device-plugins/ kubelet_internal_checkpoint kubelet.sock nvidia.sock 一些 Device Plugins 列表： NVIDIA GPU RDMA Kubevirt SFC 節點資訊本次安裝作業系統採用Ubuntu 16.04 Server，測試環境為實體機器： IP Address Role vCPU RAM Extra Device 172.22.132.51 gpu-node1 8 16G GTX 1060 3G 172.22.132.52 gpu-node2 8 16G GTX 1060 3G 172.22.132.53 master1 8 16G 無 事前準備安裝 Device Plugin 前，需要確保以下條件達成： 所有節點正確安裝指定版本的 NVIDIA driver、CUDA、Docker、NVIDIA Docker。請參考 安裝 Nvidia Docker 2。 所有節點以 kubeadm 部署成 Kubernetes v1.9+ 叢集。請參考 用 kubeadm 部署 Kubernetes 叢集。 安裝 NVIDIA Device Plugin若上述要求以符合，再開始前需要在每台 GPU worker 節點修改/lib/systemd/system/docker.service檔案，將 Docker default runtime 改成 nvidia，依照以下內容來修改： ... ExecStart=/usr/bin/dockerd -H fd:// --default-runtime=nvidia ... 這邊也可以修改/etc/docker/daemon.json檔案，請參考 Configure and troubleshoot the Docker daemon。 完成後儲存，並重新啟動 Docker： $ sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart docker 接著由於 v1.9 版本的 Device Plugins 還是處於 Alpha 中，因此需要手動修改每台 GPU worker 節點的 kubelet drop-in /etc/systemd/system/kubelet.service.d/10-kubeadm.conf檔案，這邊在KUBELET_CERTIFICATE_ARGS加入一行 args： ... Environment=&quot;KUBELET_EXTRA_ARGS=--feature-gates=DevicePlugins=true&quot; ... 完成後儲存，並重新啟動 kubelet： $ sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart kubelet 確認上述完成，接著在Master節點安裝 NVIDIA Device Plugins，透過以下方式來進行： $ kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.9/nvidia-device-plugin.yml daemonset &quot;nvidia-device-plugin-daemonset&quot; created $ kubectl -n kube-system get po -o wide NAME READY STATUS RESTARTS AGE IP NODE ... nvidia-device-plugin-daemonset-bncw2 1/1 Running 0 2m 10.244.41.135 kube-gpu-node1 nvidia-device-plugin-daemonset-ddnhd 1/1 Running 0 2m 10.244.152.132 kube-gpu-node2 測試 GPU首先執行以下指令確認是否可被分配資源： $ kubectl get nodes &quot;-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu&quot; NAME GPU master1 &lt;none&gt; gpu-node1 1 gpu-node2 1 當 NVIDIA Device Plugins 部署完成後，即可建立一個簡單範例來進行測試： $ cat &lt;&lt;EOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: gpu-pod spec: restartPolicy: Never containers: - image: nvidia/cuda name: cuda command: [&quot;nvidia-smi&quot;] resources: limits: nvidia.com/gpu: 1 EOF pod &quot;gpu-pod&quot; created $ kubectl get po -a -o wide NAME READY STATUS RESTARTS AGE IP NODE gpu-pod 0/1 Completed 0 50s 10.244.41.136 kube-gpu-node1 $ kubectl logs gpu-pod Thu Mar 15 07:28:45 2018 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.30 Driver Version: 390.30 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 106... Off | 00000000:01:00.0 Off | N/A | | 0% 41C P8 10W / 120W | 0MiB / 3019MiB | 1% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ 從上面結果可以看到 Kubernetes Pod 正確的使用到 NVIDIA GPU，這邊也可以利用 TensorFlow 來進行測試，新增一個檔案tf-gpu-dp.yml加入以下內容： apiVersion: apps/v1 kind: Deployment metadata: name: tf-gpu spec: replicas: 1 selector: matchLabels: app: tf-gpu template: metadata: labels: app: tf-gpu spec: containers: - name: tensorflow image: tensorflow/tensorflow:latest-gpu ports: - containerPort: 8888 resources: limits: nvidia.com/gpu: 1 利用 kubectl 建立 Deployment，並曝露 Jupyter port： $ kubectl create -f tf-gpu-dp.yml deployment &quot;tf-gpu&quot; created $ kubectl expose deploy tf-gpu --type LoadBalancer --external-ip=172.22.132.53 --port 8888 --target-port 8888 service &quot;tf-gpu&quot; exposed $ kubectl get po,svc -o wide NAME READY STATUS RESTARTS AGE IP NODE po/tf-gpu-6f9464f94b-pq8t9 1/1 Running 0 1m 10.244.152.133 kube-gpu-node2 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR svc/kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 23h &lt;none&gt; svc/tf-gpu LoadBalancer 10.105.104.183 172.22.132.53 8888:30093/TCP 12s app=tf-gpu 確認無誤後，透過 logs 指令取得 token，並登入Jupyter Notebook，這邊 IP 為 :8888。 這邊執行一個簡單範例，並在用 logs 指令查看就能看到 Pod 透過 NVIDIA Device Plugins 使用 GPU： $ kubectl logs -f tf-gpu-6f9464f94b-pq8t9 ... 2018-03-15 07:37:22.022052: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA 2018-03-15 07:37:22.155254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2018-03-15 07:37:22.155565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7845 pciBusID: 0000:01:00.0 totalMemory: 2.95GiB freeMemory: 2.88GiB 2018-03-15 07:37:22.155586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0 2018-03-15 07:37:22.346590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2598 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:01:00.0, compute capability: 6.1) 最後因為目前 Pod 會綁整張 GPU 來使用，因此當無多餘顯卡時就讓 Pod 處於 Pending： $ kubectl scale deploy tf-gpu --replicas=3 $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE tf-gpu-6f9464f94b-42xcf 0/1 Pending 0 4s &lt;none&gt; &lt;none&gt; tf-gpu-6f9464f94b-nxdw5 1/1 Running 0 12s 10.244.41.138 kube-gpu-node1 tf-gpu-6f9464f94b-pq8t9 1/1 Running 0 5m 10.244.152.133 kube-gpu-node2","tags":[{"name":"NVIDIA GPU","slug":"NVIDIA-GPU","permalink":"https://kairen.github.io/tags/NVIDIA-GPU/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"}]},{"title":"安裝 NVIDIA Docker 2 來讓容器使用 GPU","date":"2018-02-17T09:08:54.000Z","path":"2018/02/17/container/docker-nvidia-install/","text":"本篇主要介紹如何使用 NVIDIA Docker v2 來讓容器使用 GPU，過去 NVIDIA Docker v1 需要使用 nvidia-docker 來取代 Docker 執行 GPU image，或是透過手動掛載 NVIDIA driver 與 CUDA 來使 Docker 能夠編譯與執行 GPU 應用程式 image，而新版本的 Docker 則可以透過 –runtime 來選擇使用 NVIDIA Docker v2 的 Runtime 來執行 GPU 應用。 安裝前需要確認滿足以下幾點： GNU/Linux x86_64 with kernel version &gt; 3.10 Docker CE or EE == v18.03.1 NVIDIA GPU with Architecture &gt; Fermi (2.1) NVIDIA drivers ~= 361.93 (untested on older versions) 首先透過 APT 安裝 Docker CE or EE v17.12 版本： $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - $ echo &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial edge&quot; | sudo tee /etc/apt/sources.list.d/docker.list $ sudo apt-get update &amp;&amp; sudo apt-get install -y docker-ce=18.03.1~ce-0~ubuntu 接著透過 APT 安裝 NVIDIA Driver(v390.30) 與 CUDA 9.1： $ wget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_9.1.85-1_amd64.deb $ sudo dpkg -i cuda-repo-ubuntu1604_9.1.85-1_amd64.deb $ sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub $ sudo apt-get update &amp;&amp; sudo apt-get install -y cuda 測試 NVIDIA Dirver 與 CUDA 是否有安裝完成： $ cat /usr/local/cuda/version.txt CUDA Version 9.1.85 $ sudo nvidia-smi Tue Mar 13 06:10:39 2018 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.30 Driver Version: 390.30 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 106... Off | 00000000:01:00.0 Off | N/A | | 0% 33C P0 15W / 120W | 0MiB / 3019MiB | 2% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ 確認上述無誤後，接著安裝 NVIDIA Docker v2，這邊透過 APT 來進行安裝： $ curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - $ curl -s -L https://nvidia.github.io/nvidia-docker/ubuntu16.04/amd64/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list $ sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-docker2=2.0.3+docker18.03.1-1 $ sudo pkill -SIGHUP dockerd 測試 NVIDIA runtime，這邊下載 NVIDIA image 來進行測試： $ docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi ... +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.30 Driver Version: 390.30 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 106... Off | 00000000:01:00.0 Off | N/A | | 0% 35C P0 15W / 120W | 0MiB / 3019MiB | 2% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ 透過 TensorFlow GPU image 來進行測試，這邊執行後登入 IP:8888 執行簡單範例程式： $ docker run --runtime=nvidia -it -p 8888:8888 tensorflow/tensorflow:latest-gpu ... 2018-03-13 06:44:21.719705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7845 pciBusID: 0000:01:00.0 totalMemory: 2.95GiB freeMemory: 2.88GiB 2018-03-13 06:44:21.719728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0 2018-03-13 06:44:21.919097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2598 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:01:00.0, compute capability: 6.1)","tags":[{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"Container","slug":"Container","permalink":"https://kairen.github.io/tags/Container/"},{"name":"NVIDIA GPU","slug":"NVIDIA-GPU","permalink":"https://kairen.github.io/tags/NVIDIA-GPU/"}]},{"title":"Ceph Luminous CRUSH map 400000000000000 問題","date":"2018-02-11T09:08:54.000Z","path":"2018/02/11/ceph/luminous-crush-issue/","text":"在 Ceph Luminous(v12) 版本中，預設開啟了一些 Kernel 特性，其中首先遇到的一般是 400000000000000 問題，即CEPH_FEATURE_NEW_OSDOPREPLY_ENCODING特性(可以從對照表得知CEPH_FEATURE Table and Kernel Version)，剛問題需要在 Kernel 4.5+ 才能夠被支援，但如果不想升級可以依據本篇方式解決。 在 L 版本中，當建立 RBD 並且想要 Map 時，會發生 timeout 問題，這時候可以透過 journalctl 來查看問題，如以下： $ journalctl -xe Feb 12 08:36:57 kube-server2 kernel: libceph: mon0 172.22.132.51:6789 feature set mismatch, my 106b84a842a42 &lt; server&#39;s 40106b84a842a42, missing 400000000000000 查詢發現是 400000000000000 問題，這時可以選擇兩個解決方式： 將作業系統更新到 Linux kernel v4.5+ 的版本。 修改 CRUSH 中的 tunables 參數。 若想修改 CRUSH tunnables 參數，可以先到任一 Monitor 或者 Admin 節點中，執行以下指令： $ ceph osd crush tunables jewel $ ceph osd crush reweight-all 只要執行以上指令即可。","tags":[{"name":"Ceph","slug":"Ceph","permalink":"https://kairen.github.io/tags/Ceph/"},{"name":"Storage","slug":"Storage","permalink":"https://kairen.github.io/tags/Storage/"}]},{"title":"利用 RBAC + SA 進行 Kubectl 權限控管","date":"2018-01-08T09:08:54.000Z","path":"2018/01/08/kubernetes/rbac-sa-kubectl/","text":"這邊說明如何建立不同 Service account user，以及 RBAC 來定義存取規則，並綁定於指定 Service account ，以對指定 Namespace 中資源進行存取權限控制。 Service accountService account 一般使用情境方便是 Pod 中的行程呼叫 Kubernetes API 或者其他服務設計而成，這可能會跟 Kubernetes user account 有所混肴，但是由於 Service account 有別於 User account 是可以針對 Namespace 進行建立，因此這邊嘗試拿 Service account 來提供資訊給 kubectl 使用，並利用 RBAC 來設定存取規則，以限制該 Account 存取 API 的資源。 RBACRBAC(Role-Based Access Control)是從 Kubernetes 1.6 開始支援的存取控制機制，叢集管理者能夠對 User 或 Service account 的角色設定指定資源存取權限，在 RBAC 中，權限與角色相互關聯，其透過成為適當的角色成員，以獲取這些角色的存取權限，這比起過去 ABAC 來的方便使用、更簡化等好處。 簡單範例首先建立一個 Namespace 與 Service account： $ kubectl create ns dev $ kubectl -n dev create sa dev # 取得 secret 資訊 $ SECRET=$(kubectl -n dev get sa dev -o go-template=&#39;{{range .secrets}}{{.name}}{{end}}&#39;) 建立一個 dev.conf 設定檔，添加以下內容： $ API_SERVER=&quot;https://172.22.132.51:6443&quot; $ CA_CERT=$(kubectl -n dev get secret ${SECRET} -o yaml | awk &#39;/ca.crt:/{print $2}&#39;) $ cat &lt;&lt;EOF &gt; dev.conf apiVersion: v1 kind: Config clusters: - cluster: certificate-authority-data: $CA_CERT server: $API_SERVER name: cluster EOF $ TOKEN=$(kubectl -n dev get secret ${SECRET} -o go-template=&#39;{{.data.token}}&#39;) $ kubectl config set-credentials dev-user \\ --token=`echo ${TOKEN} | base64 -d` \\ --kubeconfig=dev.conf $ kubectl config set-context default \\ --cluster=cluster \\ --user=dev-user \\ --kubeconfig=dev.conf $ kubectl config use-context default \\ --kubeconfig=dev.conf 在不同作業系統中，base64 的 decode 指令不一樣，有些是 -D(OS X)。 新增 RBAC role 來限制 dev-user 存取權限: $ cat &lt;&lt;EOF &gt; dev-user-role.yml kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: dev name: dev-user-pod rules: - apiGroups: [&quot;*&quot;] resources: [&quot;pods&quot;, &quot;pods/log&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;, &quot;update&quot;, &quot;create&quot;, &quot;delete&quot;] EOF $ kubectl create rolebinding dev-view-pod \\ --role=dev-user-pod \\ --serviceaccount=dev:dev \\ --namespace=dev apiGroups 為不同 API 的群組，如 rbac.authorization.k8s.io，[“*”] 為允許存取全部。 resources 為 API 存取資源，如 pods、pods/log、pod/exec，[“*”] 為允許存取全部。 verbs 為 API 存取方法，如 get、list、watch、create、update、 delete、proxy，[“*”] 為允許存取全部。 透過 kubectl 確認權限設定沒問題： $ kubectl --kubeconfig=dev.conf get po Error from server (Forbidden): pods is forbidden: User &quot;system:serviceaccount:dev:dev&quot; cannot list pods in the namespace &quot;default&quot; $ kubectl -n dev --kubeconfig=dev.conf run nginx --image nginx --port 80 --restart=Never $ kubectl -n dev --kubeconfig=dev.conf get po NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 39s $ kubectl -n dev --kubeconfig=dev.conf logs -f nginx 10.244.102.64 - - [04/Jan/2018:06:42:36 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;curl/7.47.0&quot; &quot;-&quot; $ kubectl -n dev --kubeconfig=dev.conf exec -ti nginx sh Error from server (Forbidden): pods &quot;nginx&quot; is forbidden: User &quot;system:serviceaccount:dev:dev&quot; cannot create pods/exec in the namespace &quot;dev&quot; 也可以用export KUBECONFIG=dev.conf來設定使用的 config。","tags":[{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"},{"name":"Kubernetes RBAC","slug":"Kubernetes-RBAC","permalink":"https://kairen.github.io/tags/Kubernetes-RBAC/"}]},{"title":"多租戶 Kubernetes 部署方案 Stackube","date":"2017-12-20T08:23:01.000Z","path":"2017/12/20/openstack/stackube/","text":"Stackube是一個 Kubernetes-centric 的 OpenStack 發行版本(架構如下圖所示)，該專案結合 Kubernetes 與 OpenStack 的技術來達到真正的 Kubernetes 租戶隔離，如租戶實例採用 Frakti 來進行隔離、網路採用 Neutron OVS 達到每個 Namespace 擁有獨立的網路資源等。本篇會簡單介紹如何用 DevStack 建立測試用 Stackube。 P.S. 目前 Stackube 已經不再維護，僅作為測試與研究程式碼使用。 節點資訊本次安裝作業系統採用Ubuntu 16.04 Server，測試環境為實體機器： IP Address Host vCPU RAM 172.22.132.42 stackube1 8 32G 部署 Stackube首先新增 Devstack 使用的 User： $ sudo useradd -s /bin/bash -d /opt/stack -m stack $ echo &quot;stack ALL=(ALL) NOPASSWD: ALL&quot; | sudo tee /etc/sudoers.d/stack $ sudo su - stack 透過 Git 取得 Ocata 版本的 Devstack： $ git clone https://git.openstack.org/openstack-dev/devstack -b stable/ocata $ cd devstack 取得單節範例設定檔： $ curl -sSL https://raw.githubusercontent.com/kairen/stackube/master/devstack/local.conf.sample -o local.conf 完成後即可進行安裝： $ ./stack.sh 測試基本功能完成後，就可以透過以下指令來引入 Kubernetes 與 OpenStack client 需要的環境變數： $ export KUBECONFIG=/opt/stack/admin.conf $ source /opt/stack/devstack/openrc admin admin Stackube 透過 CRD 新增了一個新抽象物件 Tenant，可以直接透過 Kubernetes API 來建立一個租戶，並將該租戶與 Kubernettes namespace 做綁定： $ cat &lt;&lt;EOF | kubectl create -f - apiVersion: &quot;stackube.kubernetes.io/v1&quot; kind: Tenant metadata: name: test spec: username: &quot;test&quot; password: &quot;password&quot; EOF $ kubectl get namespace test NAME STATUS AGE test Active 2h $ kubectl -n test get network test -o yaml apiVersion: stackube.kubernetes.io/v1 kind: Network metadata: clusterName: &quot;&quot; creationTimestamp: 2017-12-20T06:03:33Z generation: 0 name: test namespace: test resourceVersion: &quot;4631&quot; selfLink: /apis/stackube.kubernetes.io/v1/namespaces/test/networks/test uid: e9aef6fa-3316-11e8-8b66-448a5bd481f0 spec: cidr: 10.244.0.0/16 gateway: 10.244.0.1 networkID: &quot;&quot; status: state: Active 檢查 Neutron 網路狀況： $ neutron net-list +--------------------------------------+----------------------+----------------------------------+----------------------------------------------------------+ | id | name | tenant_id | subnets | +--------------------------------------+----------------------+----------------------------------+----------------------------------------------------------+ | 2a8e3b54-d76f-48a9-8380-7c2a5513b1fe | kube-test-test | f2f25d24fd9a4616bff41b018e8725d2 | 625909a9-6abf-4661-b259-ffc625bdf681 10.244.0.0/16 | P.S. 這邊個人只是研究 Stackube CNI，故不針對其於進行測試，可自行參考 Stackube。","tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"},{"name":"Openstack","slug":"Openstack","permalink":"https://kairen.github.io/tags/Openstack/"}]},{"title":"Deploy OpenStack on Kubernetes using OpenStack-helm","date":"2017-11-29T08:23:01.000Z","path":"2017/11/29/openstack/openstack-helm/","text":"OpenStack Helm 是一個提供部署建置的專案，其目的是為了推動 OpenStack 生產環境的解決方案，而這種部署方式採用容器化方式，並執行於 Kubernetes 系統上來提供 OpenStack 服務的管理與排程等使用。 而本篇文章將說明如何建置多節點的 OpenStack Helm 環境來進行功能驗證。 節點與安裝版本以下為各節點的硬體資訊。 IP Address Role CPU Memory 172.22.132.10 vip - - 172.22.132.101 master1 4 16G 172.22.132.22 node1 4 16G 172.22.132.24 node2 4 16G 172.22.132.28 node3 4 16G 使用 Kernel、作業系統與軟體版本： 資訊描述 作業系統版本 16.04.3 LTS (Xenial Xerus) Kernel 版本 4.4.0-101-generic Kubernetes v1.8.4 Docker Docker 17.09.0-ce Calico v2.6.2 Etcd v3.2.9 Ceph v10.2.10 Helm v2.7.0 Kubernetes 叢集本節說明如何建立 Kubernetes Cluster，這邊採用 kube-ansible 工具來建立。 初始化與設定基本需求安裝前需要確認以下幾個項目： 所有節點的網路之間可以互相溝通。 部署節點對其他節點不需要 SSH 密碼即可登入。 所有節點都擁有 Sudoer 權限，並且不需要輸入密碼。 所有節點需要安裝Python。 所有節點需要設定/etc/host解析到所有主機。 部署節點需要安裝 Ansible &gt;= 2.4.0。 # Ubuntu install $ sudo apt-get install -y software-properties-common $ sudo apt-add-repository -y ppa:ansible/ansible $ sudo apt-get update &amp;&amp; sudo apt-get install -y ansible git make # CentOS install $ sudo yum install -y epel-release $ sudo yum -y install ansible cowsay 安裝與設定 Kube-ansible首先取得最新穩定版本的 Kubernetes Ansible: $ git clone https://github.com/kairen/kube-ansible.git $ cd kube-ansible 然後新增inventory檔案來描述要部屬的主機角色: [etcds] 172.22.132.101 ansible_user=ubuntu [masters] 172.22.132.101 ansible_user=ubuntu [nodes] 172.22.132.22 ansible_user=ubuntu 172.22.132.24 ansible_user=ubuntu 172.22.132.28 ansible_user=ubuntu [kube-cluster:children] masters nodes [kube-addon:children] masters 接著編輯group_vars/all.yml檔案來添加與修改以下內容： # Kubenrtes version, only support 1.8.0+. kube_version: 1.8.4 # CNI plugin # Support: flannel, calico, canal, weave or router. network: calico pod_network_cidr: 10.244.0.0/16 # CNI opts: flannel(--iface=enp0s8), calico(interface=enp0s8), canal(enp0s8). cni_iface: &quot;&quot; # Kubernetes cluster network. cluster_subnet: 10.96.0 kubernetes_service_ip: &quot;{{ cluster_subnet }}.1&quot; service_ip_range: &quot;{{ cluster_subnet }}.0/12&quot; service_node_port_range: 30000-32767 api_secure_port: 5443 # Highly Available configuration. haproxy: true keepalived: true # set `lb_vip_address` as keepalived vip, if this enable. keepalived_vip_interface: &quot;{{ ansible_default_ipv4.interface }}&quot; lb_vip_address: 172.22.132.10 lb_secure_port: 6443 lb_api_url: &quot;https://{{ lb_vip_address }}:{{ lb_secure_port }}&quot; etcd_iface: &quot;&quot; insecure_registrys: - &quot;172.22.132.253:5000&quot; # 有需要的話 ceph_cluster: true 這邊insecure_registrys為 deploy 節點的 Docker registry ip 與 port。 Extra addons 部分針對需求開啟，預設不會開啟。 若想把 Etcd, VIP 與 Network plugin 綁定在指定網路的話，請修改etcd_iface, keepalived_vip_interface 與 cni_iface。其中cni_iface需要針對不同 Plugin 來改變。 若想要修改部署版本的 Packages 的話，請編輯roles/commons/packages/defaults/main.yml來修改版本。 接著由於 OpenStack-helm 使用的 Kubernetes Controller Manager 不同，因此要修改roles/commons/container-images/defaults/main.yml的 Image 來源如下： ... manager: name: kube-controller-manager repos: kairen/ tag: &quot;v{{ kube_version }}&quot; ... 完後成修改 storage roles 設定版本並進行安裝。 首先編輯roles/storage/ceph/defaults/main.yml修改版本為以下： ceph_version: jewel 接著編輯roles/storage/ceph/tasks/main.yml修改成以下內容： --- - name: Install Ceph dependency packages include_tasks: install-ceph.yml # - name: Create and copy generator config file # include_tasks: gen-config.yml # delegate_to: &quot;{{ groups['masters'][0] }}&quot; # run_once: true # # - name: Deploy Ceph components on Kubernetes # include_tasks: ceph-on-k8s.yml # delegate_to: &quot;{{ groups['masters'][0] }}&quot; # run_once: true # - name: Label all storage nodes # shell: &quot;kubectl label nodes node-type=storage&quot; # delegate_to: &quot;{{ groups['masters'][0] }}&quot; # run_once: true # ignore_errors: true 部屬 Kubernetes 叢集確認group_vars/all.yml與其他設定都完成後，就透過 ansible ping 來檢查叢集狀態： $ ansible -i inventory all -m ping ... 172.22.132.101 | SUCCESS =&gt; { &quot;changed&quot;: false, &quot;failed&quot;: false, &quot;ping&quot;: &quot;pong&quot; } ... 接著就可以透過以下指令進行部署叢集： $ ansible-playbook cluster.yml ... TASK [cni : Apply calico network daemonset] ********************************************************************************************************************************* changed: [172.22.132.101 -&gt; 172.22.132.101] PLAY RECAP ****************************************************************************************************************************************************************** 172.22.132.101 : ok=155 changed=58 unreachable=0 failed=0 172.22.132.22 : ok=117 changed=28 unreachable=0 failed=0 172.22.132.24 : ok=50 changed=18 unreachable=0 failed=0 172.22.132.28 : ok=51 changed=19 unreachable=0 failed=0 完成後，進入master節點執行以下指令確認叢集： $ kubectl get node NAME STATUS ROLES AGE VERSION kube-master1 Ready master 1h v1.8.4 kube-node1 Ready &lt;none&gt; 1h v1.8.4 kube-node2 Ready &lt;none&gt; 1h v1.8.4 kube-node3 Ready &lt;none&gt; 1h v1.8.4 $ kubectl -n kube-system get po NAME READY STATUS RESTARTS AGE calico-node-js6qp 2/2 Running 2 1h calico-node-kx9xn 2/2 Running 2 1h calico-node-lxrjl 2/2 Running 2 1h calico-node-vwn5f 2/2 Running 2 1h calico-policy-controller-d549764f6-9kn9l 1/1 Running 1 1h haproxy-kube-master1 1/1 Running 1 1h keepalived-kube-master1 1/1 Running 1 1h kube-apiserver-kube-master1 1/1 Running 1 1h kube-controller-manager-kube-master1 1/1 Running 1 1h kube-dns-7bd4879dc9-kxmx6 3/3 Running 3 1h kube-proxy-7tqkm 1/1 Running 1 1h kube-proxy-glzmm 1/1 Running 1 1h kube-proxy-krqxs 1/1 Running 1 1h kube-proxy-x9zdb 1/1 Running 1 1h kube-scheduler-kube-master1 1/1 Running 1 1h 檢查 kube-dns 是否連 host 都能夠解析: $ nslookup kubernetes Server: 10.96.0.10 Address: 10.96.0.10#53 Non-authoritative answer: Name: kubernetes.default.svc.cluster.local Address: 10.96.0.1 接著安裝 Ceph 套件： $ ansible-playbook storage.yml OpenStack-helm 叢集本節說明如何建立 OpenStack on Kubernetes 使用 Helm，部署是使用 openstack-helm。過程將透過 OpenStack-helm 來在 Kubernetes 建置 OpenStack 叢集。以下所有操作都在kube-master1上進行。 Helm init在開始前需要先將 Helm 進行初始化，以提供後續使用，然而這邊由於使用到 RBAC 的關係，因此需建立一個 Service account 來提供給 Helm 使用： $ kubectl -n kube-system create sa tiller $ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller $ helm init --service-account tiller 由於 kube-ansible 本身包含 Helm 工具, 因此不需要自己安裝，只需要依據上面指令進行 init 即可。 新增一個檔案openrc來提供環境變數： export HELM_HOST=$(kubectl describe svc/tiller-deploy -n kube-system | awk &#39;/Endpoints/{print $2}&#39;) export OSD_CLUSTER_NETWORK=172.22.132.0/24 export OSD_PUBLIC_NETWORK=172.22.132.0/24 export WORK_DIR=local export CEPH_RGW_KEYSTONE_ENABLED=true OSD_CLUSTER_NETWORK與OSD_PUBLIC_NETWORK都是使用實體機器網路，這邊 daemonset 會使用 hostNetwork。 CEPH_RGW_KEYSTONE_ENABLED 在 Kubernetes 版本有點不穩，可依需求關閉。 完成後，透過 source 指令引入: $ source openrc $ helm version Client: &amp;version.Version{SemVer:&quot;v2.7.0&quot;, GitCommit:&quot;08c1144f5eb3e3b636d9775617287cc26e53dba4&quot;, GitTreeState:&quot;clean&quot;} Server: &amp;version.Version{SemVer:&quot;v2.7.0&quot;, GitCommit:&quot;08c1144f5eb3e3b636d9775617287cc26e53dba4&quot;, GitTreeState:&quot;clean&quot;} 事前準備首先透過 Kubernetes label 來標示每個節點的角色： kubectl label nodes openstack-control-plane=enabled --all kubectl label nodes ceph-mon=enabled --all kubectl label nodes ceph-osd=enabled --all kubectl label nodes ceph-mds=enabled --all kubectl label nodes ceph-rgw=enabled --all kubectl label nodes ceph-mgr=enabled --all kubectl label nodes openvswitch=enabled --all kubectl label nodes openstack-compute-node=enabled --all 這邊為了避免過度的節點污染，因此不讓 masters 充當任何角色： kubectl label nodes kube-master1 openstack-control-plane- kubectl label nodes kube-master1 ceph-mon- kubectl label nodes kube-master1 ceph-osd- kubectl label nodes kube-master1 ceph-mds- kubectl label nodes kube-master1 ceph-rgw- kubectl label nodes kube-master1 ceph-mgr- kubectl label nodes kube-master1 openvswitch- kubectl label nodes kube-master1 openstack-compute-node- 由於使用 Kubernetes RBAC，而目前 openstack-helm 有 bug，不會正確建立 Service account 的 ClusterRoleBindings，因此要手動建立(這邊偷懶一下直接使用 Admin roles)： $ cat &lt;&lt;EOF | kubectl create -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: ceph-sa-admin roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: system:serviceaccount:ceph:default EOF $ cat &lt;&lt;EOF | kubectl create -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: openstack-sa-admin roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: system:serviceaccount:openstack:default EOF 若沒有建立的話，會有類似以下的錯誤資訊： Error from server (Forbidden): error when creating &quot;STDIN&quot;: secrets is forbidden: User &quot;system:serviceaccount:ceph:default&quot; cannot create secrets in the namespace &quot;ceph&quot; 下載最新版本 openstack-helm 專案： $ git clone https://github.com/openstack/openstack-helm.git $ cd openstack-helm 現在須建立 openstack-helm chart 來提供部署使用： $ helm serve &amp; $ helm repo add local http://localhost:8879/charts $ make # output ... 1 chart(s) linted, no failures if [ -d congress ]; then helm package congress; fi Successfully packaged chart and saved it to: /root/openstack-helm/congress-0.1.0.tgz make[1]: Leaving directory &#39;/root/openstack-helm&#39; Ceph Chart在部署 OpenStack 前，需要先部署 Ceph 叢集，這邊透過以下指令建置： $ helm install --namespace=ceph ${WORK_DIR}/ceph --name=ceph \\ --set endpoints.identity.namespace=openstack \\ --set endpoints.object_store.namespace=ceph \\ --set endpoints.ceph_mon.namespace=ceph \\ --set ceph.rgw_keystone_auth=${CEPH_RGW_KEYSTONE_ENABLED} \\ --set network.public=${OSD_PUBLIC_NETWORK} \\ --set network.cluster=${OSD_CLUSTER_NETWORK} \\ --set deployment.storage_secrets=true \\ --set deployment.ceph=true \\ --set deployment.rbd_provisioner=true \\ --set deployment.client_secrets=false \\ --set deployment.rgw_keystone_user_and_endpoints=false \\ --set bootstrap.enabled=true CEPH_RGW_KEYSTONE_ENABLED是否啟動 Ceph RGW Keystone。 OSD_PUBLIC_NETWORK與OSD_PUBLIC_NETWORK為 Ceph 叢集網路。 成功安裝 Ceph chart 後，就可以透過 kubectl 來查看結果： $ kubectl -n ceph get po NAME READY STATUS RESTARTS AGE ceph-mds-57798cc8f6-r898r 1/1 Running 2 10min ceph-mon-96p9r 1/1 Running 0 10min ceph-mon-check-bd8875f87-whvhd 1/1 Running 0 10min ceph-mon-qkj95 1/1 Running 0 10min ceph-mon-zx7tw 1/1 Running 0 10min ceph-osd-5fvfl 1/1 Running 0 10min ceph-osd-kvw9b 1/1 Running 0 10min ceph-osd-wcf5j 1/1 Running 0 10min ceph-rbd-provisioner-599ff9575-mdqnf 1/1 Running 0 10min ceph-rbd-provisioner-599ff9575-vpcr6 1/1 Running 0 10min ceph-rgw-7c8c5d4f6f-8fq9c 1/1 Running 3 10min 確認 Ceph 叢集建立正確： $ MON_POD=$(kubectl get pods \\ --namespace=ceph \\ --selector=&quot;application=ceph&quot; \\ --selector=&quot;component=mon&quot; \\ --no-headers | awk &#39;{ print $1; exit }&#39;) $ kubectl exec -n ceph ${MON_POD} -- ceph -s cluster 02ad8724-dee0-4f55-829f-3cc24e2c7571 health HEALTH_WARN too many PGs per OSD (856 &gt; max 300) monmap e2: 3 mons at {kube-node1=172.22.132.22:6789/0,kube-node2=172.22.132.24:6789/0,kube-node3=172.22.132.28:6789/0} election epoch 8, quorum 0,1,2 kube-node1,kube-node2,kube-node3 fsmap e5: 1/1/1 up {0=mds-ceph-mds-57798cc8f6-r898r=up:active} osdmap e21: 3 osds: 3 up, 3 in flags sortbitwise,require_jewel_osds pgmap v6053: 856 pgs, 10 pools, 3656 bytes data, 191 objects 43091 MB used, 2133 GB / 2291 GB avail 856 active+clean Warn 這邊忽略，OSD 機器太少….。 接著為了讓 Ceph 可以在其他 Kubernetes namespace 中存取 PVC，這邊要產生 client secret key 於 openstack namespace 中來提供給 OpenStack 元件使用，這邊執行以下 Chart 來產生： $ helm install --namespace=openstack ${WORK_DIR}/ceph --name=ceph-openstack-config \\ --set endpoints.identity.namespace=openstack \\ --set endpoints.object_store.namespace=ceph \\ --set endpoints.ceph_mon.namespace=ceph \\ --set ceph.rgw_keystone_auth=${CEPH_RGW_KEYSTONE_ENABLED} \\ --set network.public=${OSD_PUBLIC_NETWORK} \\ --set network.cluster=${OSD_CLUSTER_NETWORK} \\ --set deployment.storage_secrets=false \\ --set deployment.ceph=false \\ --set deployment.rbd_provisioner=false \\ --set deployment.client_secrets=true \\ --set deployment.rgw_keystone_user_and_endpoints=false 檢查 pod 與 secret 是否建立成功： $ kubectl -n openstack get secret,po -a NAME TYPE DATA AGE secrets/default-token-q2r87 kubernetes.io/service-account-token 3 2m secrets/pvc-ceph-client-key kubernetes.io/rbd 1 2m NAME READY STATUS RESTARTS AGE po/ceph-namespace-client-key-generator-w84n4 0/1 Completed 0 2m OpenStack Chart確認沒問題後，就可以開始部署 OpenStack chart 了。首先先安裝 Mariadb cluster: $ helm install --name=mariadb ./mariadb --namespace=openstack 這邊跑超久…34mins…，原因可能是 Storage 效能問題。 這邊正確執行後，會依序依據 StatefulSet 建立起 Pod 組成 Cluster： $ kubectl -n openstack get po NAME READY STATUS RESTARTS AGE mariadb-0 1/1 Running 0 37m mariadb-1 1/1 Running 0 4m mariadb-2 1/1 Running 0 2m 當 Mariadb cluster 完成後，就可以部署一些需要的服務，如 RabbitMQ, OVS 等： helm install --name=memcached ./memcached --namespace=openstack helm install --name=etcd-rabbitmq ./etcd --namespace=openstack helm install --name=rabbitmq ./rabbitmq --namespace=openstack helm install --name=ingress ./ingress --namespace=openstack helm install --name=libvirt ./libvirt --namespace=openstack helm install --name=openvswitch ./openvswitch --namespace=openstack 上述指令若正確執行的話，會分別建立起以下服務： $ kubectl -n openstack get po NAME READY STATUS RESTARTS AGE etcd-5c9bc8c97f-jpm2k 1/1 Running 0 4m ingress-api-jhjjv 1/1 Running 0 4m ingress-api-nx5qm 1/1 Running 0 4m ingress-api-vr8xf 1/1 Running 0 4m ingress-error-pages-86b9db69cc-mmq4p 1/1 Running 0 4m libvirt-94xq5 1/1 Running 0 4m libvirt-lzfzs 1/1 Running 0 4m libvirt-vswxb 1/1 Running 0 4m mariadb-0 1/1 Running 0 42m mariadb-1 1/1 Running 0 9m mariadb-2 1/1 Running 0 7m memcached-746fcc894-cwhpr 1/1 Running 0 4m openvswitch-db-7fjr2 1/1 Running 0 4m openvswitch-db-gtmcr 1/1 Running 0 4m openvswitch-db-hqmbt 1/1 Running 0 4m openvswitch-vswitchd-gptp9 1/1 Running 0 4m openvswitch-vswitchd-s4cwd 1/1 Running 0 4m openvswitch-vswitchd-tvxlg 1/1 Running 0 4m rabbitmq-6fdb8879df-6vmz8 1/1 Running 0 4m rabbitmq-6fdb8879df-875zz 1/1 Running 0 4m rabbitmq-6fdb8879df-h5wj6 1/1 Running 0 4m 一旦所有基礎服務與元件都建立完成後，就可以開始部署 OpenStack 的專案 Chart，首先建立 Keystone 來提供身份認證服務： $ helm install --namespace=openstack --name=keystone ./keystone \\ --set pod.replicas.api=1 $ kubectl -n openstack get po -l application=keystone NAME READY STATUS RESTARTS AGE keystone-api-74c774d448-dkqmj 0/1 Init:0/1 0 4m keystone-bootstrap-xpdtl 0/1 Init:0/1 0 4m keystone-db-sync-2bxtp 1/1 Running 0 4m 0 29s 這邊由於叢集規模問題，副本數都為一份。 這時候會先建立 Keystone database tables，完成後將啟動 API pod，如以下結果： $ kubectl -n openstack get po -l application=keystone NAME READY STATUS RESTARTS AGE keystone-api-74c774d448-dkqmj 1/1 Running 0 11m 如果安裝支援 RGW 的 Keystone endpoint 的話，可以使用以下方式建立： $ helm install --namespace=openstack ${WORK_DIR}/ceph --name=radosgw-openstack \\ --set endpoints.identity.namespace=openstack \\ --set endpoints.object_store.namespace=ceph \\ --set endpoints.ceph_mon.namespace=ceph \\ --set ceph.rgw_keystone_auth=${CEPH_RGW_KEYSTONE_ENABLED} \\ --set network.public=${OSD_PUBLIC_NETWORK} \\ --set network.cluster=${OSD_CLUSTER_NETWORK} \\ --set deployment.storage_secrets=false \\ --set deployment.ceph=false \\ --set deployment.rbd_provisioner=false \\ --set deployment.client_secrets=false \\ --set deployment.rgw_keystone_user_and_endpoints=true $ kubectl -n openstack get po -a -l application=ceph NAME READY STATUS RESTARTS AGE ceph-ks-endpoints-vfg4l 0/3 Completed 0 1m ceph-ks-service-tr9xt 0/1 Completed 0 1m ceph-ks-user-z5tlt 0/1 Completed 0 1m 完成後，安裝 Horizon chart 來提供 OpenStack dashbaord： $ helm install --namespace=openstack --name=horizon ./horizon \\ --set network.enable_node_port=true \\ --set network.node_port=31000 $ kubectl -n openstack get po -l application=horizon NAME READY STATUS RESTARTS AGE horizon-7c54878549-45668 1/1 Running 0 3m 接著安裝 Glance chart 來提供 OpenStack image service。目前 Glance 支援幾個 backend storage: pvc: 一個簡單的 Kubernetes PVCs 檔案後端。 rbd: 使用 Ceph RBD 來儲存 images。 radosgw: 使用 Ceph RGW 來儲存 images。 swift: 另用 OpenStack switf 所提供的物件儲存服務來儲存 images. 這邊可以利用以下方式來部署不同的儲存後端： $ export GLANCE_BACKEND=radosgw $ helm install --namespace=openstack --name=glance ./glance \\ --set pod.replicas.api=1 \\ --set pod.replicas.registry=1 \\ --set storage=${GLANCE_BACKEND} $ kubectl -n openstack get po -l application=glance NAME READY STATUS RESTARTS AGE glance-api-6cd8b856d6-lhzfs 1/1 Running 0 14m glance-registry-599f8b857b-gt4c6 1/1 Running 0 14m 接著安裝 Neutron chart 來提供 OpenStack 虛擬化網路服務： $ helm install --namespace=openstack --name=neutron ./neutron \\ --set pod.replicas.server=1 $ kubectl -n openstack get po -l application=neutron NAME READY STATUS RESTARTS AGE neutron-dhcp-agent-2z49d 1/1 Running 0 9h neutron-dhcp-agent-d2kn8 1/1 Running 0 9h neutron-dhcp-agent-mrstl 1/1 Running 0 9h neutron-l3-agent-9f9mw 1/1 Running 0 9h neutron-l3-agent-cshzw 1/1 Running 0 9h neutron-l3-agent-j5vb9 1/1 Running 0 9h neutron-metadata-agent-6bfb2 1/1 Running 0 9h neutron-metadata-agent-kxk9c 1/1 Running 0 9h neutron-metadata-agent-w8cnl 1/1 Running 0 9h neutron-ovs-agent-j2549 1/1 Running 0 9h neutron-ovs-agent-plj9t 1/1 Running 0 9h neutron-ovs-agent-xlx7z 1/1 Running 0 9h neutron-server-6f45d74b87-6wmck 1/1 Running 0 9h 接著安裝 Nova chart 來提供 OpenStack 虛擬機運算服務: $ helm install --namespace=openstack --name=nova ./nova \\ --set pod.replicas.api_metadata=1 \\ --set pod.replicas.osapi=1 \\ --set pod.replicas.conductor=1 \\ --set pod.replicas.consoleauth=1 \\ --set pod.replicas.scheduler=1 \\ --set pod.replicas.novncproxy=1 $ kubectl -n openstack get po -l application=nova NAME READY STATUS RESTARTS AGE nova-api-metadata-84fdc84fd7-ldzrh 1/1 Running 1 9h nova-api-osapi-57f599c6d6-pqrjv 1/1 Running 0 9h nova-compute-8rvm9 2/2 Running 0 9h nova-compute-cbk7h 2/2 Running 0 9h nova-compute-tf2jb 2/2 Running 0 9h nova-conductor-7f5bc76d79-bxwnb 1/1 Running 0 9h nova-consoleauth-6946b5884f-nss6n 1/1 Running 0 9h nova-novncproxy-d789dccff-7ft9q 1/1 Running 0 9h nova-placement-api-f7c79578f-hj2g9 1/1 Running 0 9h nova-scheduler-778866f555-mmksg 1/1 Running 0 9h 接著安裝 Cinfer chart 來提供 OpenStack 區塊儲存服務: $ helm install --namespace=openstack --name=cinder ./cinder \\ --set pod.replicas.api=1 $ kubectl -n openstack get po -l application=cinder NAME READY STATUS RESTARTS AGE cinder-api-5cc89f5467-ssm8k 1/1 Running 0 32m cinder-backup-67c4d8dfdb-zfsq4 1/1 Running 0 32m cinder-scheduler-65f9dd49bf-6htwg 1/1 Running 0 32m cinder-volume-69bfb67b4-bmst2 1/1 Running 0 32m (option)都完成後，將 Horizon 服務透過 NodePort 方式曝露出來(如果上面 Horizon chart 沒反應的話)，執行以下指令編輯： $ kubectl -n openstack edit svc horizon-int # 修改 type: type: NodePort 最後連接 Horizon Dashboard，預設使用者為admin/password。 其他 Chart 可以利用以下方式來安裝，如 Heat chart： $ helm install --namespace=openstack --name=heat ./heat $ kubectl -n openstack get po -l application=heat NAME READY STATUS RESTARTS AGE heat-api-5cf45d9d44-qrt69 1/1 Running 0 13m heat-cfn-79dbf55789-bq4wh 1/1 Running 0 13m heat-cloudwatch-bcc4647f4-4c4ln 1/1 Running 0 13m heat-engine-55cfcc86f8-cct4m 1/1 Running 0 13m 測試 OpenStack 功能在kube-master1安裝 openstack client: $ sudo pip install python-openstackclient 建立adminrc來提供 client 環境變數： export OS_PROJECT_DOMAIN_NAME=default export OS_USER_DOMAIN_NAME=default export OS_PROJECT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=password export OS_AUTH_URL=http://keystone.openstack.svc.cluster.local:80/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 引入環境變數，並透過 openstack client 測試： $ source adminrc $ openstack user list +----------------------------------+-----------+ | ID | Name | +----------------------------------+-----------+ | 42f0d2e7823e413cb469f9cce731398a | glance | | 556a2744811f450098f64b37d34192d4 | nova | | a97ec73724aa4445b2d575be54f23240 | cinder | | b28a5dcfd18948419e14acba7ecf6f63 | swift | | d1f312b6bb7c460eb7d8d78c8bf350fc | admin | | dc326aace22c4314a0100865fe4f57c2 | neutron | | ec5d6d3c529847b29a1c9187599c8a6b | placement | +----------------------------------+-----------+ 接著需要設定對外網路來提供給 VM 存取，在有neutron-l3-agent節點上，新增一個腳本setup-gateway.sh： #!/bin/bash set -x # Assign IP address to br-ex OSH_BR_EX_ADDR=&quot;172.24.4.1/24&quot; OSH_EXT_SUBNET=&quot;172.24.4.0/24&quot; sudo ip addr add ${OSH_BR_EX_ADDR} dev br-ex sudo ip link set br-ex up # Setup masquerading on default route dev to public subnet DEFAULT_ROUTE_DEV=&quot;enp3s0&quot; sudo iptables -t nat -A POSTROUTING -o ${DEFAULT_ROUTE_DEV} -s ${OSH_EXT_SUBNET} -j MASQUERADE 網卡記得修改DEFAULT_ROUTE_DEV。 這邊因為沒有額外提供其他張網卡，所以先用 bridge 處理。 然後透過執行該腳本建立一個 bridge 網路： $ chmod u+x setup-gateway.sh $ ./setup-gateway.sh 確認完成後，接著建立 Neutron ext net，透過以下指令進行建立： $ openstack network create \\ --share --external \\ --provider-physical-network external \\ --provider-network-type flat ext-net $ openstack subnet create --network ext-net \\ --allocation-pool start=172.24.4.10,end=172.24.4.100 \\ --dns-nameserver 8.8.8.8 --gateway 172.24.4.1 \\ --subnet-range 172.24.4.0/24 \\ --no-dhcp ext-subnet $ openstack router create router1 $ neutron router-gateway-set router1 ext-net 直接進入 Dashboard 新增 Self-service Network: 加入到 router1: 完成後，就可以建立 instance，這邊都透過 Dashboard 來操作： 透過 SSH 進入 instance： Refers sydney-workshop Multi Node","tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"},{"name":"Helm","slug":"Helm","permalink":"https://kairen.github.io/tags/Helm/"},{"name":"Openstack","slug":"Openstack","permalink":"https://kairen.github.io/tags/Openstack/"}]},{"title":"Kubernetes v1.8.x 全手動苦工安裝教學(TL;DR)","date":"2017-10-27T09:08:54.000Z","path":"2017/10/27/kubernetes/deploy/manual-v1.8/","text":"Kubernetes 提供了許多雲端平台與作業系統的安裝方式，本章將以全手動安裝方式來部署 Kubernetes v1.8.x 版本，主要是學習與了解 Kubernetes 建置流程。若想要瞭解更多平台的部署可以參考 Picking the Right Solution來選擇自己最喜歡的方式。 本次安裝版本為： Kubernetes v1.8.6 CNI v0.6.0 Etcd v3.2.9 Calico v2.6.2 Docker v17.10.0-ce 預先準備資訊本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統可採用Ubuntu 16.x與CentOS 7.x： IP Address Role CPU Memory 172.16.35.12 master1 1 2G 172.16.35.10 node1 1 2G 172.16.35.11 node2 1 2G 這邊 master 為主要控制節點也是部署節點，node 為應用程式工作節點。 所有操作全部用root使用者進行(方便用)，以 SRE 來說不推薦。 可以下載 Vagrantfile 來建立 Virtual box 虛擬機叢集。 首先安裝前要確認以下幾項都已經準備完成： 所有節點彼此網路互通，並且master1 SSH 登入其他節點為 passwdless。 所有防火牆與 SELinux 已關閉。如 CentOS： $ systemctl stop firewalld &amp;&amp; systemctl disable firewalld $ setenforce 0 $ vim /etc/selinux/config SELINUX=disabled 所有節點需要設定/etc/host解析到所有主機。 ... 172.16.35.10 node1 172.16.35.11 node2 172.16.35.12 master1 所有節點需要安裝Docker或rtk引擎。這邊採用Docker來當作容器引擎，安裝方式如下： $ curl -fsSL &quot;https://get.docker.com/&quot; | sh 不管是在 Ubuntu 或 CentOS 都只需要執行該指令就會自動安裝最新版 Docker。CentOS 安裝完成後，需要再執行以下指令： $ systemctl enable docker &amp;&amp; systemctl start docker 編輯/lib/systemd/system/docker.service，在ExecStart=..上面加入： ExecStartPost=/sbin/iptables -A FORWARD -s 0.0.0.0/0 -j ACCEPT 完成後，重新啟動 docker 服務： $ systemctl daemon-reload &amp;&amp; systemctl restart docker 所有節點需要設定/etc/sysctl.d/k8s.conf的系統參數。 $ cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF $ sysctl -p /etc/sysctl.d/k8s.conf 在master1需要安裝CFSSL工具，這將會用來建立 TLS certificates。 $ export CFSSL_URL=&quot;https://pkg.cfssl.org/R1.2&quot; $ wget &quot;${CFSSL_URL}/cfssl_linux-amd64&quot; -O /usr/local/bin/cfssl $ wget &quot;${CFSSL_URL}/cfssljson_linux-amd64&quot; -O /usr/local/bin/cfssljson $ chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson Etcd在開始安裝 Kubernetes 之前，需要先將一些必要系統建置完成，其中 Etcd 就是 Kubernetes 最重要的一環，Kubernetes 會將大部分資訊儲存於 Etcd 上，來提供給其他節點索取，以確保整個叢集運作與溝通正常。 建立叢集 CA 與 Certificates在這部分，將會需要產生 client 與 server 的各元件 certificates，並且替 Kubernetes admin user 產生 client 證書。 首先在master1建立/etc/etcd/ssl資料夾，然後進入目錄完成以下操作。 $ mkdir -p /etc/etcd/ssl &amp;&amp; cd /etc/etcd/ssl $ export PKI_URL=&quot;https://kairen.github.io/files/manual-v1.8/pki&quot; 下載ca-config.json與etcd-ca-csr.json檔案，並從 CSR json 產生 CA 金鑰與 Certificate： $ wget &quot;${PKI_URL}/ca-config.json&quot; &quot;${PKI_URL}/etcd-ca-csr.json&quot; $ cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare etcd-ca $ ls etcd-ca*.pem etcd-ca-key.pem etcd-ca.pem 下載etcd-csr.json檔案，並產生 Etcd certificate 證書： $ wget &quot;${PKI_URL}/etcd-csr.json&quot; $ cfssl gencert \\ -ca=etcd-ca.pem \\ -ca-key=etcd-ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ etcd-csr.json | cfssljson -bare etcd $ ls etcd*.pem etcd-ca-key.pem etcd-ca.pem etcd-key.pem etcd.pem 若節點 IP 不同，需要修改etcd-csr.json的hosts。 完成後刪除不必要檔案： $ rm -rf *.json 確認/etc/etcd/ssl有以下檔案： $ ls /etc/etcd/ssl etcd-ca.csr etcd-ca-key.pem etcd-ca.pem etcd.csr etcd-key.pem etcd.pem Etcd 安裝與設定首先在master1節點下載 Etcd，並解壓縮放到 /opt 底下與安裝： $ export ETCD_URL=&quot;https://github.com/coreos/etcd/releases/download&quot; $ cd &amp;&amp; wget -qO- --show-progress &quot;${ETCD_URL}/v3.2.9/etcd-v3.2.9-linux-amd64.tar.gz&quot; | tar -zx $ mv etcd-v3.2.9-linux-amd64/etcd* /usr/local/bin/ &amp;&amp; rm -rf etcd-v3.2.9-linux-amd64 完成後新建 Etcd Group 與 User，並建立 Etcd 設定檔目錄： $ groupadd etcd &amp;&amp; useradd -c &quot;Etcd user&quot; -g etcd -s /sbin/nologin -r etcd 下載etcd相關檔案，我們將來管理 Etcd： $ export ETCD_CONF_URL=&quot;https://kairen.github.io/files/manual-v1.8/master&quot; $ wget &quot;${ETCD_CONF_URL}/etcd.conf&quot; -O /etc/etcd/etcd.conf $ wget &quot;${ETCD_CONF_URL}/etcd.service&quot; -O /lib/systemd/system/etcd.service 若與該教學 IP 不同的話，請用自己 IP 取代172.16.35.12。 建立 var 存放資訊，然後啟動 Etcd 服務: $ mkdir -p /var/lib/etcd &amp;&amp; chown etcd:etcd -R /var/lib/etcd /etc/etcd $ systemctl enable etcd.service &amp;&amp; systemctl start etcd.service 透過簡單指令驗證： $ export CA=&quot;/etc/etcd/ssl&quot; $ ETCDCTL_API=3 etcdctl \\ --cacert=${CA}/etcd-ca.pem \\ --cert=${CA}/etcd.pem \\ --key=${CA}/etcd-key.pem \\ --endpoints=&quot;https://172.16.35.12:2379&quot; \\ endpoint health # output https://172.16.35.12:2379 is healthy: successfully committed proposal: took = 641.36µs Kubernetes MasterMaster 是 Kubernetes 的大總管，主要建置apiserver、Controller manager與Scheduler來元件管理所有 Node。本步驟將下載 Kubernetes 並安裝至 master1上，然後產生相關 TLS Cert 與 CA 金鑰，提供給叢集元件認證使用。 下載 Kubernetes 元件首先透過網路取得所有需要的執行檔案： # Download Kubernetes $ export KUBE_URL=&quot;https://storage.googleapis.com/kubernetes-release/release/v1.8.6/bin/linux/amd64&quot; $ wget &quot;${KUBE_URL}/kubelet&quot; -O /usr/local/bin/kubelet $ wget &quot;${KUBE_URL}/kubectl&quot; -O /usr/local/bin/kubectl $ chmod +x /usr/local/bin/kubelet /usr/local/bin/kubectl # Download CNI $ mkdir -p /opt/cni/bin &amp;&amp; cd /opt/cni/bin $ export CNI_URL=&quot;https://github.com/containernetworking/plugins/releases/download&quot; $ wget -qO- --show-progress &quot;${CNI_URL}/v0.6.0/cni-plugins-amd64-v0.6.0.tgz&quot; | tar -zx 建立叢集 CA 與 Certificates在這部分，將會需要產生 client 與 server 的各元件 certificates，並且替 Kubernetes admin user 產生 client 證書。 一樣在master1建立pki資料夾，然後進入目錄完成以下操作。 $ mkdir -p /etc/kubernetes/pki &amp;&amp; cd /etc/kubernetes/pki $ export PKI_URL=&quot;https://kairen.github.io/files/manual-v1.8/pki&quot; $ export KUBE_APISERVER=&quot;https://172.16.35.12:6443&quot; 下載ca-config.json與ca-csr.json檔案，並產生 CA 金鑰： $ wget &quot;${PKI_URL}/ca-config.json&quot; &quot;${PKI_URL}/ca-csr.json&quot; $ cfssl gencert -initca ca-csr.json | cfssljson -bare ca $ ls ca*.pem ca-key.pem ca.pem API server certificate下載apiserver-csr.json檔案，並產生 kube-apiserver certificate 證書： $ wget &quot;${PKI_URL}/apiserver-csr.json&quot; $ cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=10.96.0.1,172.16.35.12,127.0.0.1,kubernetes.default \\ -profile=kubernetes \\ apiserver-csr.json | cfssljson -bare apiserver $ ls apiserver*.pem apiserver-key.pem apiserver.pem 若節點 IP 不同，需要修改-hostname。 Front proxy certificate下載front-proxy-ca-csr.json檔案，並產生 Front proxy CA 金鑰，Front proxy 主要是用在 API aggregator 上: $ wget &quot;${PKI_URL}/front-proxy-ca-csr.json&quot; $ cfssl gencert \\ -initca front-proxy-ca-csr.json | cfssljson -bare front-proxy-ca $ ls front-proxy-ca*.pem front-proxy-ca-key.pem front-proxy-ca.pem 下載front-proxy-client-csr.json檔案，並產生 front-proxy-client 證書： $ wget &quot;${PKI_URL}/front-proxy-client-csr.json&quot; $ cfssl gencert \\ -ca=front-proxy-ca.pem \\ -ca-key=front-proxy-ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ front-proxy-client-csr.json | cfssljson -bare front-proxy-client $ ls front-proxy-client*.pem front-proxy-client-key.pem front-proxy-client.pem Bootstrap Token由於透過手動建立 CA 方式太過繁雜，只適合少量機器，因為每次簽證時都需要綁定 Node IP，隨機器增加會帶來很多困擾，因此這邊使用 TLS Bootstrapping 方式進行授權，由 apiserver 自動給符合條件的 Node 發送證書來授權加入叢集。 主要做法是 kubelet 啟動時，向 kube-apiserver 傳送 TLS Bootstrapping 請求，而 kube-apiserver 驗證 kubelet 請求的 token 是否與設定的一樣，若一樣就自動產生 kubelet 證書與金鑰。具體作法可以參考 TLS bootstrapping。 首先建立一個變數來產生BOOTSTRAP_TOKEN，並建立 bootstrap.conf 的 kubeconfig 檔： $ export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d &#39; &#39;) $ cat &lt;&lt;EOF &gt; /etc/kubernetes/token.csv ${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot; EOF # bootstrap set-cluster $ kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=../bootstrap.conf # bootstrap set-credentials $ kubectl config set-credentials kubelet-bootstrap \\ --token=${BOOTSTRAP_TOKEN} \\ --kubeconfig=../bootstrap.conf # bootstrap set-context $ kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=../bootstrap.conf # bootstrap set default context $ kubectl config use-context default --kubeconfig=../bootstrap.conf 若想要用 CA 方式來認證，可以參考 Kubelet certificate。 Admin certificate下載admin-csr.json檔案，並產生 admin certificate 證書： $ wget &quot;${PKI_URL}/admin-csr.json&quot; $ cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ admin-csr.json | cfssljson -bare admin $ ls admin*.pem admin-key.pem admin.pem 接著透過以下指令產生名稱為 admin.conf 的 kubeconfig 檔： # admin set-cluster $ kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=../admin.conf # admin set-credentials $ kubectl config set-credentials kubernetes-admin \\ --client-certificate=admin.pem \\ --client-key=admin-key.pem \\ --embed-certs=true \\ --kubeconfig=../admin.conf # admin set-context $ kubectl config set-context kubernetes-admin@kubernetes \\ --cluster=kubernetes \\ --user=kubernetes-admin \\ --kubeconfig=../admin.conf # admin set default context $ kubectl config use-context kubernetes-admin@kubernetes \\ --kubeconfig=../admin.conf Controller manager certificate下載manager-csr.json檔案，並產生 kube-controller-manager certificate 證書： $ wget &quot;${PKI_URL}/manager-csr.json&quot; $ cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ manager-csr.json | cfssljson -bare controller-manager $ ls controller-manager*.pem 若節點 IP 不同，需要修改manager-csr.json的hosts。 接著透過以下指令產生名稱為controller-manager.conf的 kubeconfig 檔： # controller-manager set-cluster $ kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=../controller-manager.conf # controller-manager set-credentials $ kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=controller-manager.pem \\ --client-key=controller-manager-key.pem \\ --embed-certs=true \\ --kubeconfig=../controller-manager.conf # controller-manager set-context $ kubectl config set-context system:kube-controller-manager@kubernetes \\ --cluster=kubernetes \\ --user=system:kube-controller-manager \\ --kubeconfig=../controller-manager.conf # controller-manager set default context $ kubectl config use-context system:kube-controller-manager@kubernetes \\ --kubeconfig=../controller-manager.conf Scheduler certificate下載scheduler-csr.json檔案，並產生 kube-scheduler certificate 證書： $ wget &quot;${PKI_URL}/scheduler-csr.json&quot; $ cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ scheduler-csr.json | cfssljson -bare scheduler $ ls scheduler*.pem scheduler-key.pem scheduler.pem 若節點 IP 不同，需要修改scheduler-csr.json的hosts。 接著透過以下指令產生名稱為 scheduler.conf 的 kubeconfig 檔： # scheduler set-cluster $ kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=../scheduler.conf # scheduler set-credentials $ kubectl config set-credentials system:kube-scheduler \\ --client-certificate=scheduler.pem \\ --client-key=scheduler-key.pem \\ --embed-certs=true \\ --kubeconfig=../scheduler.conf # scheduler set-context $ kubectl config set-context system:kube-scheduler@kubernetes \\ --cluster=kubernetes \\ --user=system:kube-scheduler \\ --kubeconfig=../scheduler.conf # scheduler set default context $ kubectl config use-context system:kube-scheduler@kubernetes \\ --kubeconfig=../scheduler.conf Kubelet master certificate下載kubelet-csr.json檔案，並產生 master node certificate 證書： $ wget &quot;${PKI_URL}/kubelet-csr.json&quot; $ sed -i &#39;s/$NODE/master1/g&#39; kubelet-csr.json $ cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=master1,172.16.35.12 \\ -profile=kubernetes \\ kubelet-csr.json | cfssljson -bare kubelet $ ls kubelet*.pem kubelet-key.pem kubelet.pem 這邊$NODE需要隨節點名稱不同而改變。 接著透過以下指令產生名稱為 kubelet.conf 的 kubeconfig 檔： # kubelet set-cluster $ kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=../kubelet.conf # kubelet set-credentials $ kubectl config set-credentials system:node:master1 \\ --client-certificate=kubelet.pem \\ --client-key=kubelet-key.pem \\ --embed-certs=true \\ --kubeconfig=../kubelet.conf # kubelet set-context $ kubectl config set-context system:node:master1@kubernetes \\ --cluster=kubernetes \\ --user=system:node:master1 \\ --kubeconfig=../kubelet.conf # kubelet set default context $ kubectl config use-context system:node:master1@kubernetes \\ --kubeconfig=../kubelet.conf Service account keyService account 不是透過 CA 進行認證，因此不要透過 CA 來做 Service account key 的檢查，這邊建立一組 Private 與 Public 金鑰提供給 Service account key 使用： $ openssl genrsa -out sa.key 2048 $ openssl rsa -in sa.key -pubout -out sa.pub $ ls sa.* sa.key sa.pub 完成後刪除不必要檔案： $ rm -rf *.json *.csr 確認/etc/kubernetes與/etc/kubernetes/pki有以下檔案： $ ls /etc/kubernetes/ admin.conf bootstrap.conf controller-manager.conf kubelet.conf pki scheduler.conf token.csv $ ls /etc/kubernetes/pki admin-key.pem apiserver-key.pem ca-key.pem controller-manager-key.pem front-proxy-ca-key.pem front-proxy-client-key.pem kubelet-key.pem sa.key scheduler-key.pem admin.pem apiserver.pem ca.pem controller-manager.pem front-proxy-ca.pem front-proxy-client.pem kubelet.pem sa.pub scheduler.pem 安裝 Kubernetes 核心元件首先下載 Kubernetes 核心元件 YAML 檔案，這邊我們不透過 Binary 方案來建立 Master 核心元件，而是利用 Kubernetes Static Pod 來達成，因此需下載所有核心元件的Static Pod檔案到/etc/kubernetes/manifests目錄： $ export CORE_URL=&quot;https://kairen.github.io/files/manual-v1.8/master&quot; $ mkdir -p /etc/kubernetes/manifests &amp;&amp; cd /etc/kubernetes/manifests $ for FILE in apiserver manager scheduler; do wget &quot;${CORE_URL}/${FILE}.yml.conf&quot; -O ${FILE}.yml done 若IP與教學設定不同的話，請記得修改apiserver.yml、manager.yml、scheduler.yml。apiserver 中的 NodeRestriction 請參考 Using Node Authorization。 產生一個用來加密 Etcd 的 Key： $ head -c 32 /dev/urandom | base64 SUpbL4juUYyvxj3/gonV5xVEx8j769/99TSAf8YT/sQ= 在/etc/kubernetes/目錄下，建立encryption.yml的加密 YAML 檔案： $ cat &lt;&lt;EOF &gt; /etc/kubernetes/encryption.yml kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: SUpbL4juUYyvxj3/gonV5xVEx8j769/99TSAf8YT/sQ= - identity: {} EOF Etcd 資料加密可參考這篇 Encrypting data at rest。 在/etc/kubernetes/目錄下，建立audit-policy.yml的進階稽核策略 YAML 檔： $ cat &lt;&lt;EOF &gt; /etc/kubernetes/audit-policy.yml apiVersion: audit.k8s.io/v1beta1 kind: Policy rules: - level: Metadata EOF Audit Policy 請參考這篇 Auditing。 下載kubelet.service相關檔案來管理 kubelet： $ export KUBELET_URL=&quot;https://kairen.github.io/files/manual-v1.8/master&quot; $ mkdir -p /etc/systemd/system/kubelet.service.d $ wget &quot;${KUBELET_URL}/kubelet.service&quot; -O /lib/systemd/system/kubelet.service $ wget &quot;${KUBELET_URL}/10-kubelet.conf&quot; -O /etc/systemd/system/kubelet.service.d/10-kubelet.conf 若cluster-dns或cluster-domain有改變的話，需要修改10-kubelet.conf。 最後建立 var 存放資訊，然後啟動 kubelet 服務: $ mkdir -p /var/lib/kubelet /var/log/kubernetes $ systemctl enable kubelet.service &amp;&amp; systemctl start kubelet.service 完成後會需要一段時間來下載映像檔與啟動元件，可以利用該指令來監看： $ watch netstat -ntlp tcp 0 0 127.0.0.1:10248 0.0.0.0:* LISTEN 23012/kubelet tcp 0 0 127.0.0.1:10251 0.0.0.0:* LISTEN 22305/kube-schedule tcp 0 0 127.0.0.1:10252 0.0.0.0:* LISTEN 22529/kube-controll tcp6 0 0 :::6443 :::* LISTEN 22956/kube-apiserve 若看到以上資訊表示服務正常啟動，若發生問題可以用docker cli來查看。 完成後，複製 admin kubeconfig 檔案，並透過簡單指令驗證： $ cp /etc/kubernetes/admin.conf ~/.kube/config $ kubectl get cs NAME STATUS MESSAGE ERROR etcd-0 Healthy {&quot;health&quot;: &quot;true&quot;} scheduler Healthy ok controller-manager Healthy ok $ kubectl get node NAME STATUS ROLES AGE VERSION master1 NotReady master 1m v1.8.6 $ kubectl -n kube-system get po NAME READY STATUS RESTARTS AGE kube-apiserver-master1 1/1 Running 0 4m kube-controller-manager-master1 1/1 Running 0 4m kube-scheduler-master1 1/1 Running 0 4m 確認服務能夠執行 logs 等指令： $ kubectl -n kube-system logs -f kube-scheduler-master1 Error from server (Forbidden): Forbidden (user=kube-apiserver, verb=get, resource=nodes, subresource=proxy) ( pods/log kube-apiserver-master1) 這邊會發現出現 403 Forbidden 問題，這是因為 kube-apiserver user 並沒有 nodes 的資源權限，屬於正常。 由於上述權限問題，我們必需建立一個 apiserver-to-kubelet-rbac.yml 來定義權限，以供我們執行 logs、exec 等指令： $ cd /etc/kubernetes/ $ export URL=&quot;https://kairen.github.io/files/manual-v1.8/master&quot; $ wget &quot;${URL}/apiserver-to-kubelet-rbac.yml.conf&quot; -O apiserver-to-kubelet-rbac.yml $ kubectl apply -f apiserver-to-kubelet-rbac.yml # 測試 logs $ kubectl -n kube-system logs -f kube-scheduler-master1 ... I1031 03:22:42.527697 1 leaderelection.go:184] successfully acquired lease kube-system/kube-scheduler Kubernetes NodeNode 是主要執行容器實例的節點，可視為工作節點。在這步驟我們會下載 Kubernetes binary 檔，並建立 node 的 certificate 來提供給節點註冊認證用。Kubernetes 使用Node Authorizer來提供Authorization mode，這種授權模式會替 Kubelet 生成 API request。 在開始前，我們先在master1將需要的 ca 與 cert 複製到 Node 節點上： $ for NODE in node1 node2; do ssh ${NODE} &quot;mkdir -p /etc/kubernetes/pki/&quot; ssh ${NODE} &quot;mkdir -p /etc/etcd/ssl&quot; # Etcd ca and cert for FILE in etcd-ca.pem etcd.pem etcd-key.pem; do scp /etc/etcd/ssl/${FILE} ${NODE}:/etc/etcd/ssl/${FILE} done # Kubernetes ca and cert for FILE in pki/ca.pem pki/ca-key.pem bootstrap.conf; do scp /etc/kubernetes/${FILE} ${NODE}:/etc/kubernetes/${FILE} done done 下載 Kubernetes 元件首先透過網路取得所有需要的執行檔案： # Download Kubernetes $ export KUBE_URL=&quot;https://storage.googleapis.com/kubernetes-release/release/v1.8.6/bin/linux/amd64&quot; $ wget &quot;${KUBE_URL}/kubelet&quot; -O /usr/local/bin/kubelet $ chmod +x /usr/local/bin/kubelet # Download CNI $ mkdir -p /opt/cni/bin &amp;&amp; cd /opt/cni/bin $ export CNI_URL=&quot;https://github.com/containernetworking/plugins/releases/download&quot; $ wget -qO- --show-progress &quot;${CNI_URL}/v0.6.0/cni-plugins-amd64-v0.6.0.tgz&quot; | tar -zx 設定 Kubernetes node接著下載 Kubernetes 相關檔案，包含 drop-in file、systemd service 檔案等： $ export KUBELET_URL=&quot;https://kairen.github.io/files/manual-v1.8/node&quot; $ mkdir -p /etc/systemd/system/kubelet.service.d $ wget &quot;${KUBELET_URL}/kubelet.service&quot; -O /lib/systemd/system/kubelet.service $ wget &quot;${KUBELET_URL}/10-kubelet.conf&quot; -O /etc/systemd/system/kubelet.service.d/10-kubelet.conf 若cluster-dns或cluster-domain有改變的話，需要修改10-kubelet.conf。 接著在所有node建立 var 存放資訊，然後啟動 kubelet 服務: $ mkdir -p /var/lib/kubelet /var/log/kubernetes /etc/kubernetes/manifests $ systemctl enable kubelet.service &amp;&amp; systemctl start kubelet.service P.S. 重複一樣動作來完成其他節點。 授權 Kubernetes Node當所有節點都完成後，在master1節點，因為我們採用 TLS Bootstrapping，所需要建立一個 ClusterRoleBinding： $ kubectl create clusterrolebinding kubelet-bootstrap \\ --clusterrole=system:node-bootstrapper \\ --user=kubelet-bootstrap 在master透過簡單指令驗證，會看到節點處於pending： $ kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-YWf97ZrLCTlr2hmXsNLfjVLwaLfZRsu52FRKOYjpcBE 2s kubelet-bootstrap Pending node-csr-eq4q6ffOwT4yqYQNU6sT7mphPOQdFN6yulMVZeu6pkE 2s kubelet-bootstrap Pending 透過 kubectl 來允許節點加入叢集： $ kubectl get csr | awk &#39;/Pending/ {print $1}&#39; | xargs kubectl certificate approve certificatesigningrequest &quot;node-csr-YWf97ZrLCTlr2hmXsNLfjVLwaLfZRsu52FRKOYjpcBE&quot; approved certificatesigningrequest &quot;node-csr-eq4q6ffOwT4yqYQNU6sT7mphPOQdFN6yulMVZeu6pkE&quot; approved $ kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-YWf97ZrLCTlr2hmXsNLfjVLwaLfZRsu52FRKOYjpcBE 30s kubelet-bootstrap Approved,Issued node-csr-eq4q6ffOwT4yqYQNU6sT7mphPOQdFN6yulMVZeu6pkE 30s kubelet-bootstrap Approved,Issued $ kubectl get no NAME STATUS ROLES AGE VERSION master1 NotReady master 21m v1.8.6 node1 NotReady node 8s v1.8.6 node2 NotReady node 8s v1.8.6 Kubernetes Core Addons 部署當完成上面所有步驟後，接著我們需要安裝一些插件，而這些有部分是非常重要跟好用的，如Kube-dns與Kube-proxy等。 Kube-proxy addonKube-proxy 是實現 Service 的關鍵元件，kube-proxy 會在每台節點上執行，然後監聽 API Server 的 Service 與 Endpoint 資源物件的改變，然後來依據變化執行 iptables 來實現網路的轉發。這邊我們會需要建議一個 DaemonSet 來執行，並且建立一些需要的 certificate。 首先在master1下載kube-proxy-csr.json檔案，並產生 kube-proxy certificate 證書： $ export PKI_URL=&quot;https://kairen.github.io/files/manual-v1.8/pki&quot; $ cd /etc/kubernetes/pki $ wget &quot;${PKI_URL}/kube-proxy-csr.json&quot; &quot;${PKI_URL}/ca-config.json&quot; $ cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-proxy-csr.json | cfssljson -bare kube-proxy $ ls kube-proxy*.pem kube-proxy-key.pem kube-proxy.pem 接著透過以下指令產生名稱為 kube-proxy.conf 的 kubeconfig 檔： # kube-proxy set-cluster $ kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=&quot;https://172.16.35.12:6443&quot; \\ --kubeconfig=../kube-proxy.conf # kube-proxy set-credentials $ kubectl config set-credentials system:kube-proxy \\ --client-key=kube-proxy-key.pem \\ --client-certificate=kube-proxy.pem \\ --embed-certs=true \\ --kubeconfig=../kube-proxy.conf # kube-proxy set-context $ kubectl config set-context system:kube-proxy@kubernetes \\ --cluster=kubernetes \\ --user=system:kube-proxy \\ --kubeconfig=../kube-proxy.conf # kube-proxy set default context $ kubectl config use-context system:kube-proxy@kubernetes \\ --kubeconfig=../kube-proxy.conf 完成後刪除不必要檔案： $ rm -rf *.json 確認/etc/kubernetes有以下檔案： $ ls /etc/kubernetes/ admin.conf bootstrap.conf encryption.yml kube-proxy.conf pki token.csv audit-policy.yml controller-manager.conf kubelet.conf manifests scheduler.conf 在master1將kube-proxy相關檔案複製到 Node 節點上： $ for NODE in node1 node2; do echo &quot;--- $NODE ---&quot; for FILE in pki/kube-proxy.pem pki/kube-proxy-key.pem kube-proxy.conf; do scp /etc/kubernetes/${FILE} ${NODE}:/etc/kubernetes/${FILE} done done 完成後，在master1透過 kubectl 來建立 kube-proxy daemon： $ export ADDON_URL=&quot;https://kairen.github.io/files/manual-v1.8/addon&quot; $ mkdir -p /etc/kubernetes/addons &amp;&amp; cd /etc/kubernetes/addons $ wget &quot;${ADDON_URL}/kube-proxy.yml.conf&quot; -O kube-proxy.yml $ kubectl apply -f kube-proxy.yml $ kubectl -n kube-system get po -l k8s-app=kube-proxy NAME READY STATUS RESTARTS AGE kube-proxy-bpp7q 1/1 Running 0 47s kube-proxy-cztvh 1/1 Running 0 47s kube-proxy-q7mm4 1/1 Running 0 47s Kube-dns addonKube DNS 是 Kubernetes 叢集內部 Pod 之間互相溝通的重要 Addon，它允許 Pod 可以透過 Domain Name 方式來連接 Service，其主要由 Kube DNS 與 Sky DNS 組合而成，透過 Kube DNS 監聽 Service 與 Endpoint 變化，來提供給 Sky DNS 資訊，已更新解析位址。 安裝只需要在master1透過 kubectl 來建立 kube-dns deployment 即可： $ export ADDON_URL=&quot;https://kairen.github.io/files/manual-v1.8/addon&quot; $ wget &quot;${ADDON_URL}/kube-dns.yml.conf&quot; -O kube-dns.yml $ kubectl apply -f kube-dns.yml $ kubectl -n kube-system get po -l k8s-app=kube-dns NAME READY STATUS RESTARTS AGE kube-dns-6cb549f55f-h4zr5 0/3 Pending 0 40s Calico Network 安裝與設定Calico 是一款純 Layer 3 的資料中心網路方案(不需要 Overlay 網路)，Calico 好處是他已與各種雲原生平台有良好的整合，而 Calico 在每一個節點利用 Linux Kernel 實現高效的 vRouter 來負責資料的轉發，而當資料中心複雜度增加時，可以用 BGP route reflector 來達成。 首先在master1透過 kubectl 建立 Calico policy controller： $ export CALICO_CONF_URL=&quot;https://kairen.github.io/files/manual-v1.8/network&quot; $ wget &quot;${CALICO_CONF_URL}/calico-controller.yml.conf&quot; -O calico-controller.yml $ kubectl apply -f calico-controller.yml $ kubectl -n kube-system get po -l k8s-app=calico-policy NAME READY STATUS RESTARTS AGE calico-policy-controller-5ff8b4549d-tctmm 0/1 Pending 0 5s 若節點 IP 不同，需要修改calico-controller.yml的ETCD_ENDPOINTS。 在master1下載 Calico CLI 工具： $ wget https://github.com/projectcalico/calicoctl/releases/download/v1.6.1/calicoctl $ chmod +x calicoctl &amp;&amp; mv calicoctl /usr/local/bin/ 然後在所有節點下載 Calico，並執行以下步驟： $ export CALICO_URL=&quot;https://github.com/projectcalico/cni-plugin/releases/download/v1.11.0&quot; $ wget -N -P /opt/cni/bin ${CALICO_URL}/calico $ wget -N -P /opt/cni/bin ${CALICO_URL}/calico-ipam $ chmod +x /opt/cni/bin/calico /opt/cni/bin/calico-ipam 接著在所有節點下載 CNI plugins設定檔，以及 calico-node.service： $ mkdir -p /etc/cni/net.d $ export CALICO_CONF_URL=&quot;https://kairen.github.io/files/manual-v1.8/network&quot; $ wget &quot;${CALICO_CONF_URL}/10-calico.conf&quot; -O /etc/cni/net.d/10-calico.conf $ wget &quot;${CALICO_CONF_URL}/calico-node.service&quot; -O /lib/systemd/system/calico-node.service 若節點 IP 不同，需要修改10-calico.conf的etcd_endpoints。 若部署的機器是使用虛擬機，如 Virtualbox 等的話，請修改calico-node.service檔案，並在IP_AUTODETECTION_METHOD(包含 IP6)部分指定綁定的網卡，以避免預設綁定到 NAT 網路上。 之後在所有節點啟動 Calico-node: $ systemctl enable calico-node.service &amp;&amp; systemctl start calico-node.service 在master1查看 Calico nodes: $ cat &lt;&lt;EOF &gt; ~/calico-rc export ETCD_ENDPOINTS=&quot;https://172.16.35.12:2379&quot; export ETCD_CA_CERT_FILE=&quot;/etc/etcd/ssl/etcd-ca.pem&quot; export ETCD_CERT_FILE=&quot;/etc/etcd/ssl/etcd.pem&quot; export ETCD_KEY_FILE=&quot;/etc/etcd/ssl/etcd-key.pem&quot; EOF $ . ~/calico-rc $ calicoctl get node -o wide NAME ASN IPV4 IPV6 master1 (64512) 172.16.35.12/24 node1 (64512) 172.16.35.10/24 node2 (64512) 172.16.35.11/24 查看 pending 的 pod 是否已執行： $ kubectl -n kube-system get po NAME READY STATUS RESTARTS AGE calico-policy-controller-5ff8b4549d-tctmm 1/1 Running 0 4m kube-apiserver-master1 1/1 Running 0 20m kube-controller-manager-master1 1/1 Running 0 20m kube-dns-6cb549f55f-h4zr5 3/3 Running 0 5m kube-proxy-fnrkb 1/1 Running 0 6m kube-proxy-l72bq 1/1 Running 0 6m kube-proxy-m6rfw 1/1 Running 0 6m kube-scheduler-master1 1/1 Running 0 20m 最後若想省事，可以直接用 Standard Hosted 方式安裝。 Kubernetes Extra Addons 部署本節說明如何部署一些官方常用的 Addons，如 Dashboard、Heapster 等。 Dashboard addonDashboard 是 Kubernetes 社區官方開發的儀表板，有了儀表板後管理者就能夠透過 Web-based 方式來管理 Kubernetes 叢集，除了提升管理方便，也讓資源視覺化，讓人更直覺看見系統資訊的呈現結果。 在master1透過 kubectl 來建立 kubernetes dashboard 即可： $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml $ kubectl -n kube-system get po,svc -l k8s-app=kubernetes-dashboard NAME READY STATUS RESTARTS AGE po/kubernetes-dashboard-747c4f7cf-md5m8 1/1 Running 0 56s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kubernetes-dashboard ClusterIP 10.98.120.209 &lt;none&gt; 443/TCP 56s 這邊會額外建立一個名稱為open-api Cluster Role Binding，這僅作為方便測試時使用，在一般情況下不要開啟，不然就會直接被存取所有 API: $ cat &lt;&lt;EOF | kubectl create -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: open-api namespace: &quot;&quot; roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: system:anonymous EOF P.S. 管理者可以針對特定使用者來開放 API 存取權限，但這邊方便使用直接綁在 cluster-admin cluster role。 完成後，就可以透過瀏覽器存取 Dashboard。 在 1.7 版本以後的 Dashboard 將不再提供所有權限，因此需要建立一個 service account 來綁定 cluster-admin role： $ kubectl -n kube-system create sa dashboard $ kubectl create clusterrolebinding dashboard --clusterrole cluster-admin --serviceaccount=kube-system:dashboard $ SECRET=$(kubectl -n kube-system get sa dashboard -o yaml | awk &#39;/dashboard-token/ {print $3}&#39;) $ kubectl -n kube-system describe secrets ${SECRET} | awk &#39;/token:/{print $2}&#39; eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtdG9rZW4tdzVocmgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYWJmMTFjYzMtZjRlYi0xMWU3LTgzYWUtMDgwMDI3NjdkOWI5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZCJ9.Xuyq34ci7Mk8bI97o4IldDyKySOOqRXRsxVWIJkPNiVUxKT4wpQZtikNJe2mfUBBD-JvoXTzwqyeSSTsAy2CiKQhekW8QgPLYelkBPBibySjBhJpiCD38J1u7yru4P0Pww2ZQJDjIxY4vqT46ywBklReGVqY3ogtUQg-eXueBmz-o7lJYMjw8L14692OJuhBjzTRSaKW8U2MPluBVnD7M2SOekDff7KpSxgOwXHsLVQoMrVNbspUCvtIiEI1EiXkyCNRGwfnd2my3uzUABIHFhm0_RZSmGwExPbxflr8Fc6bxmuz-_jSdOtUidYkFIzvEWw2vRovPgs3MXTv59RwUw 複製token，然後貼到 Kubernetes dashboard。 Heapster addonHeapster 是 Kubernetes 社區維護的容器叢集監控與效能分析工具。Heapster 會從 Kubernetes apiserver 取得所有 Node 資訊，然後再透過這些 Node 來取得 kubelet 上的資料，最後再將所有收集到資料送到 Heapster 的後台儲存 InfluxDB，最後利用 Grafana 來抓取 InfluxDB 的資料源來進行視覺化。 在master1透過 kubectl 來建立 kubernetes monitor 即可： $ export ADDON_URL=&quot;https://kairen.github.io/files/manual-v1.8/addon&quot; $ wget ${ADDON_URL}/kube-monitor.yml.conf -O kube-monitor.yml $ kubectl apply -f kube-monitor.yml $ kubectl -n kube-system get po,svc NAME READY STATUS RESTARTS AGE ... po/heapster-74fb5c8cdc-62xzc 4/4 Running 0 7m po/influxdb-grafana-55bd7df44-nw4nc 2/2 Running 0 7m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ... svc/heapster ClusterIP 10.100.242.225 &lt;none&gt; 80/TCP 7m svc/monitoring-grafana ClusterIP 10.101.106.180 &lt;none&gt; 80/TCP 7m svc/monitoring-influxdb ClusterIP 10.109.245.142 &lt;none&gt; 8083/TCP,8086/TCP 7m ··· 完成後，就可以透過瀏覽器存取 Grafana Dashboard。 簡單部署 Nginx 服務Kubernetes 可以選擇使用指令直接建立應用程式與服務，或者撰寫 YAML 與 JSON 檔案來描述部署應用程式的配置，以下將建立一個簡單的 Nginx 服務： $ kubectl run nginx --image=nginx --port=80 $ kubectl expose deploy nginx --port=80 --type=LoadBalancer --external-ip=172.16.35.12 $ kubectl get svc,po NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 1h svc/nginx LoadBalancer 10.97.121.243 172.16.35.12 80:30344/TCP 22s NAME READY STATUS RESTARTS AGE po/nginx-7cbc4b4d9c-7796l 1/1 Running 0 28s 192.160.57.181 ,172.16.35.12 80:32054/TCP 21s 這邊type可以選擇 NodePort 與 LoadBalancer，在本地裸機部署，兩者差異在於NodePort只映射 Host port 到 Container port，而LoadBalancer則繼承NodePort額外多出映射 Host target port 到 Container port。 確認沒問題後即可在瀏覽器存取 http://172.16.35.12。 擴展服務數量若叢集node節點增加了，而想讓 Nginx 服務提供可靠性的話，可以透過以下方式來擴展服務的副本： $ kubectl scale deploy nginx --replicas=2 $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-158599303-0h9lr 1/1 Running 0 25s 10.244.100.5 node2 nginx-158599303-k7cbt 1/1 Running 0 1m 10.244.24.3 node1","tags":[{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"},{"name":"Calico","slug":"Calico","permalink":"https://kairen.github.io/tags/Calico/"}]},{"title":"利用 Kuryr 整合 OpenStack 與 Kubernetes 網路","date":"2017-08-29T08:23:01.000Z","path":"2017/08/29/openstack/kuryr-kubernetes/","text":"Kubernetes Kuryr 是 OpenStack Neutron 的子專案，其主要目標是透過該專案來整合 OpenStack 與 Kubernetes 的網路。該專案在 Kubernetes 中實作了原生 Neutron-based 的網路，因此使用 Kuryr-Kubernetes 可以讓你的 OpenStack VM 與 Kubernetes Pods 能夠選擇在同一個子網路上運作，並且能夠使用 Neutron 的 L3 與 Security Group 來對網路進行路由，以及阻擋特定來源 Port。 Kuryr-Kubernetes 整合有兩個主要組成部分： Kuryr Controller:Controller 主要目的是監控 Kubernetes API 的來獲取 Kubernetes 資源的變化，然後依據 Kubernetes 資源的需求來執行子資源的分配和資源管理。 Kuryr CNI：主要是依據 Kuryr Controller 分配的資源來綁定網路至 Pods 上。 本篇我們將說明如何利用DevStack與Kubespray建立一個簡單的測試環境。 環境資源與事前準備準備兩台實體機器，這邊測試的作業系統為CentOS 7.x，該環境將在扁平的網路下進行。 IP Address 1 Role 172.24.0.34 controller, k8s-master 172.24.0.80 compute, k8s-node1 更新每台節點的 CentOS 7.x packages: $ sudo yum --enablerepo=cr update -y 然後關閉 firewalld 以及 SELinux 來避免實現發生問題： $ sudo setenforce 0 $ sudo systemctl disable firewalld &amp;&amp; sudo systemctl stop firewalld OpenStack Controller 安裝首先進入172.24.0.34（controller），並且執行以下指令。 然後執行以下指令來建立 DevStack 專用使用者： $ sudo useradd -s /bin/bash -d /opt/stack -m stack $ echo &quot;stack ALL=(ALL) NOPASSWD: ALL&quot; | sudo tee /etc/sudoers.d/stack 選用 DevStack 是因為現在都是用 Systemd 來管理服務，不用再用 screen 了，雖然都很方便。 接著切換至該使用者環境來建立 OpenStack： $ sudo su - stack 下載 DevStack 安裝套件： $ git clone https://git.openstack.org/openstack-dev/devstack $ cd devstack 新增local.conf檔案，來描述部署資訊： [[local|localrc]] HOST_IP=172.24.0.34 GIT_BASE=https://github.com ADMIN_PASSWORD=passwd DATABASE_PASSWORD=passwd RABBIT_PASSWORD=passwd SERVICE_PASSWORD=passwd SERVICE_TOKEN=passwd MULTI_HOST=1 [color=#fc9fca]Tips:修改 HOST_IP 為自己的 IP 位置。 完成後，執行以下指令開始部署： $ ./stack.sh Openstack Compute 安裝進入到172.24.0.80（compute），並且執行以下指令。 然後執行以下指令來建立 DevStack 專用使用者： $ sudo useradd -s /bin/bash -d /opt/stack -m stack $ echo &quot;stack ALL=(ALL) NOPASSWD: ALL&quot; | sudo tee /etc/sudoers.d/stack 選用 DevStack 是因為現在都是用 Systemd 來管理服務，不用再用 screen 了，雖然都很方便。 接著切換至該使用者環境來建立 OpenStack： $ sudo su - stack 下載 DevStack 安裝套件： $ git clone https://git.openstack.org/openstack-dev/devstack $ cd devstack 新增local.conf檔案，來描述部署資訊： [[local|localrc]] HOST_IP=172.24.0.80 GIT_BASE=https://github.com MULTI_HOST=1 LOGFILE=/opt/stack/logs/stack.sh.log ADMIN_PASSWORD=passwd DATABASE_PASSWORD=passwd RABBIT_PASSWORD=passwd SERVICE_PASSWORD=passwd DATABASE_TYPE=mysql SERVICE_HOST=172.24.0.34 MYSQL_HOST=$SERVICE_HOST RABBIT_HOST=$SERVICE_HOST GLANCE_HOSTPORT=$SERVICE_HOST:9292 ENABLED_SERVICES=n-cpu,q-agt,n-api-meta,c-vol,placement-client NOVA_VNC_ENABLED=True NOVNCPROXY_URL=&quot;http://$SERVICE_HOST:6080/vnc_auto.html&quot; VNCSERVER_LISTEN=$HOST_IP VNCSERVER_PROXYCLIENT_ADDRESS=$VNCSERVER_LISTEN Tips:修改 HOST_IP 為自己的主機位置。修改 SERVICE_HOST 為 Master 的IP位置。 完成後，執行以下指令開始部署： $ ./stack.sh 建立 Kubernetes 叢集環境首先確認所有節點之間不需要 SSH 密碼即可登入，接著進入到172.24.0.34（k8s-master）並且執行以下指令。 接著安裝所需要的套件： $ sudo yum -y install software-properties-common ansible git gcc python-pip python-devel libffi-devel openssl-devel $ sudo pip install -U kubespray 完成後，新增 kubespray 設定檔： $ cat &lt;&lt;EOF &gt; ~/.kubespray.yml kubespray_git_repo: &quot;https://github.com/kubernetes-incubator/kubespray.git&quot; # Logging options loglevel: &quot;info&quot; EOF 然後利用 kubespray-cli 快速產生環境的inventory檔，並修改部分內容： $ sudo -i $ kubespray prepare --masters master --etcds master --nodes node1 編輯/root/.kubespray/inventory/inventory.cfg，修改以下內容： [all] master ansible_host=172.24.0.34 ansible_user=root ip=172.24.0.34 node1 ansible_host=172.24.0.80 ansible_user=root ip=172.24.0.80 [kube-master] master [kube-node] master node1 [etcd] master [k8s-cluster:children] kube-node1 kube-master 完成後，即可利用 kubespray-cli 指令來進行部署： $ kubespray deploy --verbose -u root -k .ssh/id_rsa -n calico 經過一段時間後就會部署完成，這時候檢查節點是否正常： $ kubectl get no NAME STATUS AGE VERSION master Ready,master 2m v1.7.4 node1 Ready 2m v1.7.4 接著為了方便讓 Kuryr Controller 簡單取得 K8s API Server，這邊修改/etc/kubernetes/manifests/kube-apiserver.yml檔案，加入以下內容： - &quot;--insecure-bind-address=0.0.0.0&quot; - &quot;--insecure-port=8080&quot; Tips:將 insecure 綁定到 0.0.0.0 之上，以及開啟 8080 Port。 安裝 Openstack Kuryr進入到172.24.0.34（controller），並且執行以下指令。 首先在節點安裝所需要的套件： $ sudo yum -y install gcc libffi-devel python-devel openssl-devel install python-pip 然後下載 kuryr-kubernetes 並進行安裝： $ git clone http://git.openstack.org/openstack/kuryr-kubernetes $ pip install -e kuryr-kubernetes 新增kuryr.conf至/etc/kuryr目錄： $ cd kuryr-kubernetes $ ./tools/generate_config_file_samples.sh $ sudo mkdir -p /etc/kuryr/ $ sudo cp etc/kuryr.conf.sample /etc/kuryr/kuryr.conf 接著使用 OpenStack Dashboard 建立相關專案，在瀏覽器輸入Dashboard，並執行以下步驟。 新增 K8s project。 修改 K8s project member 加入到 service project。 在該 Project 中新增 Security Groups，參考 kuryr-kubernetes manually。 在該 Project 中新增 pod_subnet 子網路。 在該 Project 中新增 service_subnet 子網路。 完成後，修改/etc/kuryr/kuryr.conf檔案，加入以下內容： [DEFAULT] use_stderr = true bindir = /usr/local/libexec/kuryr [kubernetes] api_root = http://172.24.0.34:8080 [neutron] auth_url = http://172.24.0.34/identity username = admin user_domain_name = Default password = admin project_name = service project_domain_name = Default auth_type = password [neutron_defaults] ovs_bridge = br-int pod_security_groups = {id_of_secuirity_group_for_pods} pod_subnet = {id_of_subnet_for_pods} project = {id_of_project} service_subnet = {id_of_subnet_for_k8s_services} 完成後執行 kuryr-k8s-controller： $ kuryr-k8s-controller --config-file /etc/kuryr/kuryr.conf 安裝 Kuryr-CNI進入到172.24.0.80（node1）並且執行以下指令。 首先在節點安裝所需要的套件： $ sudo yum -y install gcc libffi-devel python-devel openssl-devel python-pip 然後安裝 Kuryr-CNI 來提供給 kubelet 使用： $ git clone http://git.openstack.org/openstack/kuryr-kubernetes $ sudo pip install -e kuryr-kubernetes 新增kuryr.conf至/etc/kuryr目錄： $ cd kuryr-kubernetes $ ./tools/generate_config_file_samples.sh $ sudo mkdir -p /etc/kuryr/ $ sudo cp etc/kuryr.conf.sample /etc/kuryr/kuryr.conf 修改/etc/kuryr/kuryr.conf檔案，加入以下內容： [DEFAULT] use_stderr = true bindir = /usr/local/libexec/kuryr [kubernetes] api_root = http://172.24.0.34:8080 建立 CNI bin 與 Conf 目錄： $ sudo mkdir -p /opt/cni/bin $ sudo ln -s $(which kuryr-cni) /opt/cni/bin/ $ sudo mkdir -p /etc/cni/net.d/ 新增/etc/cni/net.d/10-kuryr.conf CNI 設定檔： { &quot;cniVersion&quot;: &quot;0.3.0&quot;, &quot;name&quot;: &quot;kuryr&quot;, &quot;type&quot;: &quot;kuryr-cni&quot;, &quot;kuryr_conf&quot;: &quot;/etc/kuryr/kuryr.conf&quot;, &quot;debug&quot;: true } 完成後，更新 oslo 與 vif python 函式庫： $ sudo pip install &#39;oslo.privsep&gt;=1.20.0&#39; &#39;os-vif&gt;=1.5.0&#39; 最後重新啟動相關服務： sudo systemctl daemon-reload &amp;&amp; systemctl restart kubelet.service 測試結果我們這邊開一個 Pod 與 OpenStack VM 來進行溝通：","tags":[{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"OpenStack","slug":"OpenStack","permalink":"https://kairen.github.io/tags/OpenStack/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"}]},{"title":"利用 OpenStack Ironic 提供裸機部署服務","date":"2017-08-16T08:23:01.000Z","path":"2017/08/16/openstack/ironic/","text":"Ironic 是 OpenStack 專案之一，主要目的是提供裸機機器部署服務(Bare-metal service)。它能夠單獨或整合 OpenStack 其他服務被使用，而可整合服務包含 Keystone、Nova、Neutron、Glance 與 Swift 等核心服務。當使用 Compute 與 Network 服務對 Bare-metal 進行適當的配置時，OpenStack 可以透過 Compute API 同時部署虛擬機(Virtual machines)與裸機(Bare machines)。 本篇為了精簡安裝過程，故這邊不採用手動安裝教學(會在 Gitbook 書上更新)，因此採用 DevStack 來部署服務，再手動設定一些步驟。 本環境安裝資訊： OpenStack Pike DevStack Pike Pike Pike Pike …. P.S. 這邊因為我的 Manage net 已經有 MAAS 的服務，所以才用其他張網卡進行部署。 節點資訊本次安裝作業系統採用Ubuntu 16.04 Server，測試環境為實體主機： Role CPU Memory controller 4 16G bare-node1 4 16G 這邊 controller 為主要控制節點，將安裝大部分 OpenStack 服務。而 bare-node 為被用來做裸機部署的機器。 網卡若是實體主機，請設定為固定 IP，如以下： auto eth0 iface eth0 inet static address 172.20.3.93/24 gateway 172.20.3.1 dns-nameservers 8.8.8.8 若想修改主機的網卡名稱，可以編輯/etc/udev/rules.d/70-persistent-net.rules。 其中controller的eth2需設定為以下： auto &lt;ethx&gt; iface &lt;ethx&gt; inet manual up ip link set dev $IFACE up down ip link set dev $IFACE down 事前準備安裝前需要確認叢集滿足以下幾點： 確認所有節點網路可以溝通。 Bare-node IPMI 設定完成。包含 Address、User 與 Password。 修改 Controller 的 /etc/apt/sources.list，使用tw.archive.ubuntu.com。 安裝 OpenStack 服務這邊採用 DevStack 來部署測試環境，首先透過以下指令取得 DevStack： $ sudo useradd -s /bin/bash -d /opt/stack -m stack $ echo &quot;stack ALL=(ALL) NOPASSWD: ALL&quot; | sudo tee /etc/sudoers.d/stack $ sudo su - stack $ git clone https://git.openstack.org/openstack-dev/devstack $ cd devstack 接著撰寫 local.conf 來描述部署過程所需的服務： $ wget https://kairen.github.io/files/devstack/ironic-local.conf -O local.conf $ sed -i &#39;s/HOST_IP=.*/HOST_IP=172.22.132.93/g&#39; local.conf HOST_IP請更換為自己環境 IP。有其他 Driver 請記得加入。 完成後執行部署腳本進行建置： $ ./stack.sh 大約經過 15 min 就可以完成整個環境安裝。 測試 OpenStack 環境： $ source openrc admin $ openstack user list +----------------------------------+----------------+ | ID | Name | +----------------------------------+----------------+ | 3ba4e813270e4e98ad781f4103284e0d | demo | | 40c6014bc18f407fbfbc22aadedb1ca0 | placement | | 567156ad1c7b4ccdbcd4ea02e7c44ce3 | alt_demo | | 7a22ce5036614993a707dd976c505ccd | swift | | 8d392f051afe45008289abca4dadf3ca | swiftusertest1 | | a6e616af3bf04611bc23625e71a22e64 | swiftusertest4 | | a835f1674648427396a7c6ac7e5eef06 | neutron | | b2bf73ef2eaa425c93e4f552e9266056 | swiftusertest2 | | b7de1af8522b495c8a9fb743eb6e7f59 | nova | | cada5913a03e4f2794066902144264d3 | admin | | f03e39680b234474b139d00c3fbca989 | swiftusertest3 | | f0a4033463f64c00858ff05525545b6d | glance-swift | | f2a1b186e7e84b10ae7e8f810e5c2412 | glance | | ff31787d136f4fba96c19af419b8559c | ironic | +----------------------------------+----------------+ 測試 ironic 是否正常運行： $ ironic driver-list +---------------------+----------------+ | Supported driver(s) | Active host(s) | +---------------------+----------------+ | agent_ipmitool | ironic-dev | | fake | ironic-dev | | ipmi | ironic-dev | | pxe_ipmitool | ironic-dev | +---------------------+----------------+ 建立 Bare metal 網路首先我們需要設定一個網路來提供 DHCP, PXE 與其他需求使用，這部分會說明如何建立一個 Flat network 來提供裸機配置用。詳細可參考 Configure the Networking service for bare metal provisioning。 首先編輯/etc/neutron/plugins/ml2/ml2_conf.ini修改以下內容： [ml2_type_flat] flat_networks = public, physnet1 [ovs] datapath_type = system bridge_mappings = public:br-ex, physnet1:br-eth2 tunnel_bridge = br-tun local_ip = 172.22.132.93 接著建立 bridge 來處理實體網路與 OpenStack 之間的溝通： $ sudo ovs-vsctl add-br br-eth2 $ sudo ovs-vsctl add-port br-eth2 eth2 完成後重新啟動 Neutron server 與 agent： $ sudo systemctl restart devstack@q-svc.service $ sudo systemctl restart devstack@q-agt.service 建立完成後，OVS bridges 會類似如下： $ sudo ovs-vsctl show Bridge br-int fail_mode: secure Port &quot;int-br-eth2&quot; Interface &quot;int-br-eth2&quot; type: patch options: {peer=&quot;phy-br-eth2&quot;} Port br-int Interface br-int type: internal Bridge &quot;br-eth2&quot; Port &quot;phy-br-eth2&quot; Interface &quot;phy-br-eth2&quot; type: patch options: {peer=&quot;int-br-eth2&quot;} Port &quot;eth2&quot; Interface &quot;eth2&quot; Port &quot;br-eth2&quot; Interface &quot;br-eth2&quot; type: internal 接著建立 Neutron flat 網路來提供使用： $ neutron net-create sharednet1 \\ --shared \\ --provider:network_type flat \\ --provider:physical_network physnet1 $ neutron subnet-create sharednet1 172.22.132.0/24 \\ --name sharedsubnet1 \\ --ip-version=4 --gateway=172.22.132.254 \\ --allocation-pool start=172.22.132.180,end=172.22.132.200 \\ --enable-dhcp P.S. neutron-client 在未來會被移除，故請轉用 Provider network。 設定 Ironic cleaning network當使用到 Node cleaning 時，我們必須設定cleaning_network選項來提供使用。首先取得 Network 資訊，透過以下指令： $ openstack network list +--------------------------------------+------------+----------------------------------------------------------------------------+ | ID | Name | Subnets | +--------------------------------------+------------+----------------------------------------------------------------------------+ | 03de10a0-d4d2-43ce-83db-806a5277dd29 | private | 2a651bfb-776d-47f4-a958-f8a418f7fcd5, 99bdbd78-7a20-41b7-afa3-7cf7bf25b95b | | 349a6a5b-1e26-4e36-8444-f6a6bbbdd227 | public | 032a516e-3d55-4623-995d-06ee033eaee4, daf733a9-492e-4ea6-8a45-6364b88a8f6f | | ade096bd-6a86-4d90-9cf4-bce9921f7257 | sharednet1 | 3f9f2a47-fdd9-472b-a6a2-ce6570e490ff | +--------------------------------------+------------+----------------------------------------------------------------------------+ 編輯/etc/ironic/ironic.conf修改一下內容： [neutron] cleaning_network = sharednet1 完成後，重新啟動 Ironic 服務： $ sudo systemctl restart devstack@ir-api.service $ sudo systemctl restart devstack@ir-cond.service 建立 Deploy 與 User 映像檔裸機服務在配置時需要兩組映像檔，分別為 Deploy 與 User 映像檔，其功能如下： Deploy images: 用來準備裸機服務機器以進行實際的作業系統部署，在 Cleaning 等階段會使用到。 User images:最後安裝至裸機服務提供給使用者使用的作業系統映像檔。 由於 DevStack 預設會建立一組 Deploy 映像檔，這邊只針對 User 映像檔做手動建構說明，若要建構 Deploy 映像檔可以參考 Building or downloading a deploy ramdisk image。 首先我們必須先安裝disk-image-builder工具來提供建構映像檔： $ virtualenv dib $ source dib/bin/activate (dib) $ pip install diskimage-builder 接著執行以下指令來進行建構映像檔： $ cat &lt;&lt;EOF &gt; k8s.repo [kubernetes] name=Kubernetes baseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF $ DIB_YUM_REPO_CONF=k8s.repo \\ DIB_DEV_USER_USERNAME=kyle \\ DIB_DEV_USER_PWDLESS_SUDO=yes \\ DIB_DEV_USER_PASSWORD=r00tme \\ disk-image-create \\ centos7 \\ dhcp-all-interfaces \\ devuser \\ yum \\ epel \\ baremetal \\ -o k8s.qcow2 \\ -p vim,docker,kubelet,kubeadm,kubectl,kubernetes-cni ... Converting image using qemu-img convert Image file k8s.qcow2 created... 完成後會看到以下檔案： $ ls dib k8s.d k8s.initrd k8s.qcow2 k8s.repo k8s.vmlinuz 上傳至 Glance 以提供使用： # 上傳 Kernel $ openstack image create k8s.kernel \\ --public \\ --disk-format aki \\ --container-format aki &lt; k8s.vmlinuz # 上傳 Initrd $ openstack image create k8s.initrd \\ --public \\ --disk-format ari \\ --container-format ari &lt; k8s.initrd # 上傳 Qcow2 $ export MY_VMLINUZ_UUID=$(openstack image list | awk &#39;/k8s.kernel/ { print $2 }&#39;) $ export MY_INITRD_UUID=$(openstack image list | awk &#39;/k8s.initrd/ { print $2 }&#39;) $ openstack image create k8s \\ --public \\ --disk-format qcow2 \\ --container-format bare \\ --property kernel_id=$MY_VMLINUZ_UUID \\ --property ramdisk_id=$MY_INITRD_UUID &lt; k8s.qcow2 建立 Ironic 節點在所有服務配置都完成後，這時候要註冊實體機器資訊，來提供給 Compute 服務部署時使用。首先確認 Ironic 的 Driver 是否有資源機器的 Power driver： $ ironic driver-list +---------------------+----------------+ | Supported driver(s) | Active host(s) | +---------------------+----------------+ | agent_ipmitool | ironic-dev | | fake | ironic-dev | | ipmi | ironic-dev | | pxe_ipmitool | ironic-dev | +---------------------+----------------+ 若有缺少的話，請參考 Set up the drivers for the Bare Metal service。 確認有支援後，透過以下指令來建立 Node，並進行註冊： $ export DEPLOY_VMLINUZ_UUID=$(openstack image list | awk &#39;/ipmitool.kernel/ { print $2 }&#39;) $ export DEPLOY_INITRD_UUID=$(openstack image list | awk &#39;/ipmitool.initramfs/ { print $2 }&#39;) $ ironic node-create -d agent_ipmitool \\ -n bare-node-1 \\ -i ipmi_address=172.20.3.194 \\ -i ipmi_username=maas \\ -i ipmi_password=passwd \\ -i ipmi_port=623 \\ -i deploy_kernel=$DEPLOY_VMLINUZ_UUID \\ -i deploy_ramdisk=$DEPLOY_INITRD_UUID 若使用 Console 的話，要加入-i ipmi_terminal_port=9000，可參考 Configuring Web or Serial Console。 接著更新機器資訊，這邊透過手動方式來更新資訊： $ export NODE_UUID=$(ironic node-list | awk &#39;/bare-node-1/ { print $2 }&#39;) $ ironic node-update $NODE_UUID add \\ properties/cpus=4 \\ properties/memory_mb=8192 \\ properties/local_gb=100 \\ properties/root_gb=100 \\ properties/cpu_arch=x86_64 (option)也可以使用 inspector 來識別裸機機器的硬體資訊，但需要修改/etc/ironic-inspector/dnsmasq.conf修改一下： no-daemon port=0 interface=eth1 bind-interfaces dhcp-range=172.22.132.200,172.22.132.210 dhcp-match=ipxe,175 dhcp-boot=tag:!ipxe,undionly.kpxe dhcp-boot=tag:ipxe,http://172.22.132.93:3928/ironic-inspector.ipxe dhcp-sequential-ip 完成後，透過 systemctl 重新啟動背景服務devstack@ironic-inspector-dhcp.service與devstack@ironic-inspector.service。 透過 port create 來把 Node 的所有網路資訊進行註冊： $ ironic port-create -n $NODE_UUID -a NODE_MAC_ADDRESS 這邊NODE_MAC_ADDRESS是指bare-node-1節點的 PXE(eth1)網卡 Mac Address，如 54:a0:50:85:d5:fa。 完成後透過 validate 指令來檢查： $ ironic node-validate $NODE_UUID +------------+--------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Interface | Result | Reason | +------------+--------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | boot | False | Cannot validate image information for node 8e6fd86a-8eed-4e24-a510-3f5ebb0a336a because one or more parameters are missing from its instance_info. Missing are: [&#39;ramdisk&#39;, &#39;kernel&#39;, &#39;image_source&#39;] | | console | False | Missing &#39;ipmi_terminal_port&#39; parameter in node\\&#39;s driver_info. | | deploy | False | Cannot validate image information for node 8e6fd86a-8eed-4e24-a510-3f5ebb0a336a because one or more parameters are missing from its instance_info. Missing are: [&#39;ramdisk&#39;, &#39;kernel&#39;, &#39;image_source&#39;] | | inspect | True | | | management | True | | | network | True | | | power | True | | | raid | True | | | storage | True | | +------------+--------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ P.S. 這邊boot與deploy的錯誤若是如上所示的話，可以直接忽略，這是因為使用 Nova 來管理 baremetal 會出現的問題。 最後利用 provision 指令來測試節點是否能夠提供服務： $ ironic --ironic-api-version 1.34 node-set-provision-state $NODE_UUID manage $ ironic --ironic-api-version 1.34 node-set-provision-state $NODE_UUID provide $ ironic node-list +--------------------------------------+--------+---------------+-------------+--------------------+-------------+ | UUID | Name | Instance UUID | Power State | Provisioning State | Maintenance | +--------------------------------------+--------+---------------+-------------+--------------------+-------------+ | 0c20cf7d-0a36-46f4-ac38-721ff8bfb646 | bare-0 | None | power off | cleaning | False | +--------------------------------------+--------+---------------+-------------+--------------------+-------------+ 這時候機器會進行 clean 過程，經過一點時間就會完成，若順利完成則該節點就可以進行部署了。若要了解細節狀態，可以參考 Ironic’s State Machine。 透過 Nova 部署 baremetal 機器最後我們要透過 Nova API 來部署裸機，在開始前要建立一個 flavor 跟上傳 keypair 來提供使用： $ ssh-keygen -t rsa $ openstack keypair create --public-key ~/.ssh/id_rsa.pub default $ openstack flavor create --vcpus 4 --ram 8192 --disk 100 baremetal.large 完成後，即可透過以下指令進行部署： $ NET_ID=$(openstack network list | awk &#39;/sharednet1/ { print $2 }&#39;) $ openstack server create --flavor baremetal.large \\ --nic net-id=$NET_ID \\ --image k8s \\ --key-name default k8s-01 經過一段時間後，就會看到部署完成，這時候可以透過以下指令來確認部署結果： $ openstack server list +--------------------------------------+--------+--------+---------------------------+-------+-----------------+ | ID | Name | Status | Networks | Image | Flavor | +--------------------------------------+--------+--------+---------------------------+-------+-----------------+ | a40e5cb1-dfc6-44d5-b638-648e8c0975fb | k8s-01 | ACTIVE | sharednet1=172.22.132.187 | k8s | baremetal.large | +--------------------------------------+--------+--------+---------------------------+-------+-----------------+ $ openstack baremetal list +--------------------------------------+--------+--------------------------------------+-------------+--------------------+-------------+ | UUID | Name | Instance UUID | Power State | Provisioning State | Maintenance | +--------------------------------------+--------+--------------------------------------+-------------+--------------------+-------------+ | 0c20cf7d-0a36-46f4-ac38-721ff8bfb646 | bare-0 | a40e5cb1-dfc6-44d5-b638-648e8c0975fb | power on | active | False | +--------------------------------------+--------+--------------------------------------+-------------+--------------------+-------------+ 最後透過 ssh 來進入部署機器來建立應用： $ ssh kyle@172.22.132.187 [kyle@host-172-22-132-187 ~]$ sudo systemctl start kubelet.service [kyle@host-172-22-132-187 ~]$ sudo systemctl start docker.service [kyle@host-172-22-132-187 ~]$ sudo kubeadm init --service-cidr 10.96.0.0/12 \\ --kubernetes-version v1.7.4 \\ --pod-network-cidr 10.244.0.0/16 \\ --apiserver-advertise-address 172.22.132.187 \\ --token b0f7b8.8d1767876297d85c 整合Magnum有空再寫，先簡單玩玩吧。 若是懶人可以用 Dashboard 來部署，另外本教學的 DevStack 有使用 Ironic UI，因此可以在以下頁面看到 node 資訊。","tags":[{"name":"OpenStack","slug":"OpenStack","permalink":"https://kairen.github.io/tags/OpenStack/"},{"name":"DevStack","slug":"DevStack","permalink":"https://kairen.github.io/tags/DevStack/"},{"name":"Bare-metal","slug":"Bare-metal","permalink":"https://kairen.github.io/tags/Bare-metal/"}]},{"title":"利用 LinuxKit 建立 Kubernetes 叢集","date":"2017-07-21T16:00:00.000Z","path":"2017/07/22/kubernetes/deploy/linuxkit-k8s/","text":"LinuxKit 是以 Container 來建立最小、不可變的 Linux 作業系統映像檔框架，先前有簡單介紹與操作過，可以參考LinuxKit。本篇則將利用 LinuxKit 來建立 Kubernetes 的映像檔，並部署簡單的 Kubernetes 叢集。 本次教學會在Mac OS X作業系統上進行，而部署的軟體資訊如下： Kubernetes v1.7.2(2017-08-07, Update) Etcd v3 Weave Docker v17.06.0-ce 預先準備資訊 主機已安裝與啟動Docker工具。 主機已安裝Git工具。 主機以下載 LinuxKit 專案，並建構了 Moby 與 LinuxKit 工具。 建構 Moby 與 LinuxKit 方法如以下操作： $ git clone https://github.com/linuxkit/linuxkit.git $ cd linuxkit $ make $ ./bin/moby version moby version 0.0 commit: c2b081ed8a9f690820cc0c0568238e641848f58f $ ./bin/linuxkit version linuxkit version 0.0 commit: 0e3ca695d07d1c9870eca71fb7dd9ede31a38380 建構 Kubernetes 系統映像檔首先我們要建立一個包好 Kubernetes 的 Linux 映像檔，而官方已經有做好範例，只要利用以下方式即可建構： $ cd linuxkit/projects/kubernetes/ $ make build-vm-images ... Create outputs: kube-node-kernel kube-node-initrd.img kube-node-cmdline 建置 Kubernetes cluster完成建構映像檔後，就可以透過以下指令來啟動 Master OS 映像檔，然後獲取節點 IP： $ ./boot.sh (ns: getty) linuxkit-025000000002:~\\# ip addr show dev eth0 2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 02:50:00:00:00:02 brd ff:ff:ff:ff:ff:ff inet 192.168.65.3/24 brd 192.168.65.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::abf0:9fa4:d0f4:8da2/64 scope link valid_lft forever preferred_lft forever 啟動後，開啟新的 Console 來 SSH 進入 Master，來利用 kubeadm 初始化 Master： $ cd linuxkit/projects/kubernetes/ $ ./ssh_into_kubelet.sh 192.168.65.3 linuxkit-025000000002:/\\# kubeadm-init.sh ... kubeadm join --token 4236d3.29f61af661c49dbf 192.168.65.3:6443 一旦 kubeadm 完成後，就會看到 Token，這時請記住 Token 資訊。接著開啟新 Console，然後執行以下指令來啟動 Node： console1&gt;$ ./boot.sh 1 --token 4236d3.29f61af661c49dbf 192.168.65.3:6443 P.S. 開啟節點格式為./boot.sh &lt;n&gt; [&lt;join_args&gt; ...]。 接著分別在開兩個 Console 來加入叢集： console2&gt; $ ./boot.sh 2 --token 4236d3.29f61af661c49dbf 192.168.65.3:6443 console3&gt; $ ./boot.sh 3 --token 4236d3.29f61af661c49dbf 192.168.65.3:6443 完成後回到 Master 節點上，執行以下指令來查看節點狀況： $ kubectl get no NAME STATUS AGE VERSION linuxkit-025000000002 Ready 16m v1.7.2 linuxkit-025000000003 Ready 6m v1.7.2 linuxkit-025000000004 Ready 1m v1.7.2 linuxkit-025000000005 Ready 1m v1.7.2 簡單部署 Nginx 服務Kubernetes 可以選擇使用指令直接建立應用程式與服務，或者撰寫 YAML 與 JSON 檔案來描述部署應用程式的配置，以下將建立一個簡單的 Nginx 服務： $ kubectl run nginx --image=nginx --replicas=1 --port=80 $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-1423793266-v0hpb 1/1 Running 0 38s 10.42.0.1 linuxkit-025000000004 完成後要接著建立 svc(Service)，來提供外部網路存取應用程式，使用以下指令建立： $ kubectl expose deploy nginx --port=80 --type=NodePort $ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 10.96.0.1 &lt;none&gt; 443/TCP 19m nginx 10.108.41.230 &lt;nodes&gt; 80:31773/TCP 5s 由於這邊不是使用實體機器部署，因此網路使用 Docker namespace 網路，故這邊透過ubuntu-desktop-lxde-vnc來瀏覽 Nginx 應用： $ docker run -it --rm -p 6080:80 dorowu/ubuntu-desktop-lxde-vnc 完成後透過瀏覽器連接 HTLM VNC 最後關閉節點只需要執行以下即可： $ halt [ 1503.034689] reboot: Power down","tags":[{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"},{"name":"LinuxKit","slug":"LinuxKit","permalink":"https://kairen.github.io/tags/LinuxKit/"}]},{"title":"智能合約(Smart contracts)","date":"2017-05-28T09:08:54.000Z","path":"2017/05/28/blockchain/smart-contracts/","text":"智能合約(Smart Contracts) 是在 Ethereum 區塊鏈中所屬的物件。它們包含程式碼函式以及能夠與其他合約進行互動、做出決策、儲存資料與傳送乙太幣給其他人。合約是由創建者所定義，但是它們的執行與他們所提供的服務，都是由 Ethereum 網路本身提供。它們將存在且可被執行，只要整個網路存在，並且只會因程式中有撰寫自我銷毀的功能才會消失。 我可以用合約做什麼呢？只要想像力夠豐富，要做什麼幾乎都沒問題，但以下指南只會是入門，讓我們去實現一些簡單的事情。 Smart Sponsor本節將說明一智能合約範例，透過建構一個合約來允許以下賬戶持有人進行互動。 一個慈善機構舉行籌款活動，我們稱之為 thebenefactor。 一個受贊助的 runner 想為慈善機構募款，我們稱之為 therunner。 其他的人想要贊助 runner，我們稱之為 thesponsor。 一個 Ethereum 節點，用來開採區塊鏈以驗證交易，我們稱之為 theminer。 我們的合約(smartSponsor)： 是由一位 runner 透過贊助的執行來為慈善機構募款。 當建立合約時，runner 會任命為募集錢的捐助者。 runner 則邀情其他人去進行贊助。用戶透過呼叫一個在智能合約上的函式，將乙太幣從 贊助商的帳戶 轉移到 合約，並保持乙太幣於合約，直到有進一步的通知。 在合約的時限期間的所有人都能看到誰是 捐助者，有多少的乙太幣被從誰捐(雖然贊助者可以匿名，當然:p)。 那麼有兩件事情可能發生： 執行都按計劃進行，以及 runner 指示合約轉移到所有資金的捐助者。 執行由於謀些原因無法承擔，而 runner 指示合約將退還贊助商的抵押。 Ethereum 允許智能合約由撰寫 Solidity 語言來定義。Solidity 的合約是類似於 Java 的類別定義。成員變數的儲存採用區塊鍊交易與合約的方法，來詢問合約或改變的其狀態。作為區塊鏈的副本會分散到網路中的所有節點，任何人都可以詢問合約，以從中搜尋公開的訊息。 合約有以下幾種方法： smartSponsor：合約的建構子。它初始化合約的狀態。合約的建立者傳入賬戶的位址，有利於合約的 drawdown。 pledge：任何人都可以呼叫捐贈乙太幣贊助基金。贊助商提供支援的選擇性訊息 getPot：回傳當前儲存在合約的總乙太幣。 refund：把贊助商的錢退回給贊助商。只有合約的擁有者才能呼叫這個函式。 drawdown：傳送合約的總價值給捐助者賬戶。只有合約的擁有者才能呼叫這個函式。 這個想法是使一個合約擁有約束力。他們不能拿回任何資金，除非整個合約被退還。在這種情況下，所有資料都是被公開存取的，這意味著任何人都有存取 Ethereum 區塊鏈，來查看誰建立了合約，誰是捐助者，以及誰透過存取合約程式碼本身保證了每一筆資金。 要注意很重要的一點，任何改變合約的狀態(建立、承若、退還或 drawing down)都需要在區塊鏈上建立交易，這意味著這些交易不會被儲存，要直到這些交易的區塊被開採。操作只能讀取到一個現有合約狀態(getPot 或讀取公有成員變數)都不需要進行挖礦。這是一個很重要且微妙的點：寫入操作是很慢的(因為我們要等到採礦完成)。由於這情況合約可能永遠不會被建立到區塊鍊中，因此呼叫方需要提供一些獎勵，來促進礦工去工作。這是被稱為 gas 的 Ethereum 術語，所有的寫入操作都是需要 gas 的支出來改變區塊鍊的狀態。 幸運的是我們不需要購買真正的乙太幣，以及參與 Ethereum 網路。我們可以使用相同的軟體，但要配置它運行一個本地測試區塊鏈，以及產生自己的假乙太幣。 以下為一個 Solidity 語言的智能合約範例： contract smartSponsor { address public owner; address public benefactor; bool public refunded; bool public complete; uint public numPledges; struct Pledge { uint amount; address eth_address; bytes32 message; } mapping(uint =&gt; Pledge) public pledges; // constructor function smartSponsor(address _benefactor) { owner = msg.sender; numPledges = 0; refunded = false; complete = false; benefactor = _benefactor; } // add a new pledge function pledge(bytes32 _message) { if (msg.value == 0 || complete || refunded) throw; pledges[numPledges] = Pledge(msg.value, msg.sender, _message); numPledges++; } function getPot() constant returns (uint) { return this.balance; } // refund the backers function refund() { if (msg.sender != owner || complete || refunded) throw; for (uint i = 0; i &lt; numPledges; ++i) { pledges[i].eth_address.send(pledges[i].amount); } refunded = true; complete = true; } // send funds to the contract benefactor function drawdown() { if (msg.sender != owner || complete || refunded) throw; benefactor.send(this.balance); complete = true; } } 一個Pledge結構模型的捐贈，儲存著贊助商的帳戶 ID、承若押金，以及一些訊息字串。 這個pledges陣列儲存了一個承若方的列表。 合約中的所有成員變數都是公開的，所以getters將自動被建立。 throw被稱為某些函式(functions)，用以防止資料被寫入錯誤的資料到該區塊鏈中。 參考連結 Our thoughts on Ethereum Building a smart contract using the command line Block chain technology, smart contracts and Ethereum","tags":[{"name":"Ethereum","slug":"Ethereum","permalink":"https://kairen.github.io/tags/Ethereum/"},{"name":"Blockchain","slug":"Blockchain","permalink":"https://kairen.github.io/tags/Blockchain/"},{"name":"Solidity","slug":"Solidity","permalink":"https://kairen.github.io/tags/Solidity/"},{"name":"Smart Contract","slug":"Smart-Contract","permalink":"https://kairen.github.io/tags/Smart-Contract/"}]},{"title":"利用 Browser Solidity 部署智能合約","date":"2017-05-27T09:08:54.000Z","path":"2017/05/27/blockchain/browser-solidity/","text":"Browser Solidity 是一個 Web-based 的 Solidity 編譯器與 IDE。本節將說明如何安裝於 Linux 與 Docker 中。 這邊可以連結官方的 https://ethereum.github.io/browser-solidity 來使用; 該網站會是該專案的最新版本預覽。 Ubuntu Server 手動安裝首先安裝 Browser Solidity 要使用到的相關套件： $ sudo apt-get install -y apache2 make g++ git 接著安裝 node.js 平台，來建置 App： $ curl -sL https://deb.nodesource.com/setup_6.x | sudo -E bash - $ sudo apt-get install nodejs 然後透過 git 將專案抓到 local 端，並進入目錄： $ git clone https://github.com/ethereum/browser-solidity.git $ cd browser-solidity 安裝相依套件與建置應用程式： $ sudo npm install $ sudo npm run build 完成後，將所以有目錄的資料夾與檔案搬移到 Apache HTTP Server 的網頁根目錄： $ sudo cp ./* /var/www/html/ 完成後就可以開啟網頁了。 Docker 快速安裝目前 Browser Solidity 有提供 Docker Image 下載。這邊只需要透過以下指令就能夠建立 Browser Solidity Dashboard 環境： $ docker run -d \\ -p 80:80 \\ --name solidity \\ kairen/solidity","tags":[{"name":"Ethereum","slug":"Ethereum","permalink":"https://kairen.github.io/tags/Ethereum/"},{"name":"Blockchain","slug":"Blockchain","permalink":"https://kairen.github.io/tags/Blockchain/"},{"name":"Solidity","slug":"Solidity","permalink":"https://kairen.github.io/tags/Solidity/"},{"name":"Smart Contract","slug":"Smart-Contract","permalink":"https://kairen.github.io/tags/Smart-Contract/"}]},{"title":"監控 Go Ethereum 的區塊鏈狀況","date":"2017-05-26T09:08:54.000Z","path":"2017/05/26/blockchain/geth-monitoring/","text":"Ethereum 提供了一個 Web-based 的監控儀表板，可以部署該儀表板，並透過 Clinet 端傳送 Ethereum 節點的資訊，來查看整個區塊鏈狀態。本節將說明如何安裝監控儀表板於 Linux 與 Docker 容器中。 這邊可以連結官方的 https://ethstats.net/ 來查看主節點網路的狀態。 Ubuntu Server 手動安裝本部分說明如何手動安裝 eth-netstats 服務，其中會包含以下兩個部分： Monitoring site Client side Monitoring site首先安裝 Browser Solidity 要使用到的相關套件： $ sudo apt-get install -y make g++ git 接著安裝 node.js 平台，來建置 App： $ curl -sL https://deb.nodesource.com/setup_6.x | sudo -E bash - $ sudo apt-get install nodejs 然後透過 git 將專案抓到 local 端，並進入目錄： $ git clone https://github.com/cubedro/eth-netstats $ cd eth-netstats 安裝相依套件與建置應用程式，並啟動服務： $ sudo npm install $ sudo npm install -g grunt-cli $ grunt $ PORT=&quot;3000&quot; WS_SECRET=&quot;admin&quot; npm start 接著就可以開啟 eth-netstats。 在沒有任何 Clinet 節點連上情況下，會是一個空的網頁。 撰寫一個腳本eth-netstats.sh放置到背景服務執行： #!/bin/bash # History: # 2016/05/22 Kyle Bai Release # export PORT=&quot;3000&quot; export WS_SECRET=&quot;admin&quot; echo &quot;Starting private eth-netstats ...&quot; screen -dmS netstats /usr/bin/npm start 透過以下方式執行： $ chmod u+x eth-netstats.sh $ ./eth-netstats.sh Starting private eth-netstats ... 透過screen -x netstats取得當前畫面。 Client side首先安裝 Browser Solidity 要使用到的相關套件： $ sudo apt-get install -y make g++ git 接著安裝 node.js 平台，來建置 App： $ curl -sL https://deb.nodesource.com/setup_6.x | sudo -E bash - $ sudo apt-get install nodejs 然後透過 git 將專案抓到 local 端，並進入目錄： $ git clone https://github.com/cubedro/eth-net-intelligence-api $ cd eth-net-intelligence-api 安裝相依套件與建置應用程式： $ sudo npm install &amp;&amp; sudo npm install -g pm2 編輯app.json設定檔，並修改以下內容： [ { &quot;name&quot; : &quot;mynode&quot;, &quot;cwd&quot; : &quot;.&quot;, &quot;script&quot; : &quot;app.js&quot;, &quot;log_date_format&quot; : &quot;YYYY-MM-DD HH:mm Z&quot;, &quot;merge_logs&quot; : false, &quot;watch&quot; : false, &quot;exec_interpreter&quot; : &quot;node&quot;, &quot;exec_mode&quot; : &quot;fork_mode&quot;, &quot;env&quot;: { &quot;NODE_ENV&quot; : &quot;production&quot;, &quot;RPC_HOST&quot; : &quot;localhost&quot;, &quot;RPC_PORT&quot; : &quot;8545&quot;, &quot;INSTANCE_NAME&quot; : &quot;mynode-1&quot;, &quot;WS_SERVER&quot; : &quot;http://localhost:3000&quot;, &quot;WS_SECRET&quot; : &quot;admin&quot;, } }, ] RPC_HOST為 ethereum 的 rpc ip address。 RPC_PORT為 ethereum 的 rpc port。 INSTANCE_NAME為 ethereum 的監控實例名稱。 WS_SERVER為 eth-netstats 的 URL。 WS_SECRET為 eth-netstats 的 secret。 確認完成後，即可啟動服務： $ pm2 start app.json $ sudo tail -f $HOME/.pm2/logs/mynode-out-0.log Docker 快速安裝本部分說明如何手動安裝 eth-netstats 服務，其中會包含以下兩個部分： Docker Monitoring site Docker Client side Docker Monitoring site自動建置的映像檔現在可以在 DockerHub 找到，建議直接執行以下指令來啟動 eth-netstats 容器： $ docker run -d \\ -p 3000:3000 \\ -e WS_SECRET=&quot;admin&quot; \\ --name ethstats \\ kairen/ethstats 接著就可以開啟 eth-netstats。 在沒有任何 Clinet 節點連上情況下，會是一個空的網頁。 Docker Client side自動建置的映像檔現在可以在 DockerHub 找到，也推薦透過執行以下指令來啟動 eth-netintel 容器： $ docker run -d \\ -p 30303:30303 \\ -p 30303:30303/udp \\ -e NAME_PREFIX=&quot;geth-1&quot; \\ -e WS_SERVER=&quot;http://172.17.1.200:3000&quot; \\ -e WS_SECRET=&quot;admin&quot; \\ -e RPC_HOST=&quot;172.17.1.199&quot; \\ -e RPC_PORT=&quot;8545&quot; \\ --name geth-1 \\ kairen/ethnetintel Monitor 與 Client 需要統一WS_SECRET。","tags":[{"name":"Ethereum","slug":"Ethereum","permalink":"https://kairen.github.io/tags/Ethereum/"},{"name":"Blockchain","slug":"Blockchain","permalink":"https://kairen.github.io/tags/Blockchain/"},{"name":"Go lang","slug":"Go-lang","permalink":"https://kairen.github.io/tags/Go-lang/"}]},{"title":"建立 Go Ethereum 私有網路鏈","date":"2017-05-25T09:08:54.000Z","path":"2017/05/25/blockchain/multi-node-geth/","text":"Ethereum 專案是以區塊鏈原理，並進一步增加容納值、儲存資料，並且能封裝程式碼來建立智能合約(Smart Contracts)，形成區塊鏈應用程式，來執行運算任務。類似於比特幣(Bitcoin)，Ethereum 也具有一種貨幣，它叫做乙太幣(Ether)。乙太幣是開採於儲存在共享一致性的區塊鏈前驗證交易節點。乙太幣可以在賬戶(公有金鑰, Pubilc keys)與智能合約(Smart Contracts)之間進行轉移。 本節將說明如何透過 Ubuntu 部署 Go Ethereum。並利用簡單的指令來進行 Demo。 事前準備本次會使用到兩個節點來建立 Geth Instances，其規格如下： Role CPUs RAM Disk geth-1 2vCPU 4 GB 40 GB geth-2 2vCPU 4 GB 40 GB 首先在每個節點安裝 Ethereum 最新版本，可以依照官方透過以下方式快速安裝： $ sudo apt-get install -y software-properties-common $ sudo add-apt-repository -y ppa:ethereum/ethereum $ sudo apt-get update &amp;&amp; sudo apt-get install ethereum 在每個節點建立一個private.json檔案來定義起源區塊(Genesis Block)，內容如下： { &quot;coinbase&quot; : &quot;0x0000000000000000000000000000000000000000&quot;, &quot;difficulty&quot; : &quot;0x40000&quot;, &quot;extraData&quot; : &quot;Custem Ethereum Genesis Block&quot;, &quot;gasLimit&quot; : &quot;0xffffffff&quot;, &quot;nonce&quot; : &quot;0x0000000000000042&quot;, &quot;mixhash&quot; : &quot;0x0000000000000000000000000000000000000000000000000000000000000000&quot;, &quot;parentHash&quot; : &quot;0x0000000000000000000000000000000000000000000000000000000000000000&quot;, &quot;timestamp&quot; : &quot;0x00&quot;, &quot;config&quot;: { &quot;chainId&quot;: 123, &quot;homesteadBlock&quot;: 0, &quot;eip155Block&quot;: 0, &quot;eip158Block&quot;: 0 }, &quot;alloc&quot;: { } } 初始化創世區塊： $ geth init --datadir=data/ private.json 在每個節點新增一名稱為geth-private.sh的腳本程式，將用於啟動 geth，並放置背景： #!/bin/bash # Program: # This program is a private geth runner. # History: # 2016/05/22 Kyle Bai Release # echo &quot;Starting private geth&quot; screen -dmS geth /usr/bin/geth \\ --datadir data/ \\ --networkid 123 \\ --nodiscover \\ --maxpeers 5 \\ --port 30301 \\ --rpc \\ --rpcaddr &quot;0.0.0.0&quot; \\ --rpcport &quot;8545&quot; \\ --rpcapi &quot;admin,db,eth,debug,miner,net,shh,txpool,personal,web3&quot; \\ --rpccorsdomain &quot;*&quot; \\ -verbosity 6 更多的參數，請參考 Command-Line-Options。 建立完成後，修改執行權限： $ chmod u+x geth-private.sh 建立 Ethereum 環境首先進入到geth-1節點透過以下方式來啟動： $ ./geth-private.sh Starting private geth 這時候會透過 screen 執行於背景，我們可以透過screen -x geth來進行前景。若要回到背景則透過[Ctrl-A] + [Ctrl-D]來 detached。要關閉 screen 則透過 [Ctrl-C]。 接著為了確認是否正確啟動，我們可以透過 geth 的 attach 指令來連接 console： $ geth attach ipc:data/geth.ipc 也可以透過 HTTP 方式 attach，geth attach http://localhost:8545。 若一開始建立沒有 RPC，但想要加入 RPC 可以 attach 後，輸入以下 function： admin.startRPC(&quot;0.0.0.0&quot;, 8545, &quot;*&quot;, &quot;web3,db,net,eth&quot;) 進入後透過 admin API 來取得節點的資訊： &gt; admin.nodeInfo.enode &quot;enode://e3dd0392a2971c4b0c4c43a01cd682e19f31aaa573c43a9b227685af7ffed5070217392ae5ada278968d5c4bfddd9c93547bcf4592852196a8facbcdad64d257@[::]:30301?discport=0&quot; 這邊要取代[::]為主機 IP，如以下： &quot;enode://e3dd0392a2971c4b0c4c43a01cd682e19f31aaa573c43a9b227685af7ffed5070217392ae5ada278968d5c4bfddd9c93547bcf4592852196a8facbcdad64d257@172.16.1.99:30301?discport=0&quot; 上面沒問題後，接著進入到geth-2節點，然後透過以下指令開啟 console： $ geth init --datadir=data/ private.json $ geth --datadir data/ \\ --networkid 123 \\ --nodiscover \\ --maxpeers 5 \\ --port 30301 \\ --rpc \\ --rpcaddr &quot;0.0.0.0&quot; \\ --rpcport &quot;8545&quot; \\ --rpcapi &quot;admin,db,eth,debug,miner,net,shh,txpool,personal,web3&quot; \\ --rpccorsdomain &quot;*&quot; \\ -verbosity 6 \\ console 也可以透過上一個節點的方式將服務放到背景，在 attach。 完成上面指令會直接進入 console，接著透過以下方式來連接geth-1： &gt; admin.addPeer(&quot;enode://e3dd0392a2971c4b0c4c43a01cd682e19f31aaa573c43a9b227685af7ffed5070217392ae5ada278968d5c4bfddd9c93547bcf4592852196a8facbcdad64d257@172.16.1.99:30301?discport=0&quot;) true I0525 12:56:40.623642 eth/downloader/downloader.go:239] Registering peer e3dd0392a2971c4b I0525 12:57:10.622920 p2p/server.go:467] &lt;-taskdone: wait for dial hist expire (29.99999387s) 接著透過 net API 進行查看連接狀態： &gt; net.peerCount 1 &gt; admin.peers [{ caps: [&quot;eth/61&quot;, &quot;eth/62&quot;, &quot;eth/63&quot;], id: &quot;e3dd0392a2971c4b0c4c43a01cd682e19f31aaa573c43a9b227685af7ffed5070217392ae5ada278968d5c4bfddd9c93547bcf4592852196a8facbcdad64d257&quot;, name: &quot;Geth/v1.4.5-stable/linux/go1.5.1&quot;, network: { localAddress: &quot;172.16.1.100:51038&quot;, remoteAddress: &quot;172.16.1.99:30301&quot; }, protocols: { eth: { difficulty: 131072, head: &quot;882048e0d045ea48903eddb4c50825a4e3c6c1a055df6a32244e9a9239f8c5e8&quot;, version: 63 } } }] 驗證服務這部分將透過幾個指令與流程來驗證服務，首先在geth-1透過 attach 進入，並建立一個賬戶與查看乙太幣： $ geth attach http://localhost:8545 &gt; kairen = personal.newAccount(); Passphrase: Repeat passphrase: &quot;0xcb41ad8ba28c4b8b52eee159ef3bb6da197ff60b&quot; &gt; personal.listAccounts [&quot;0xcb41ad8ba28c4b8b52eee159ef3bb6da197ff60b&quot;] &gt; web3.fromWei(eth.getBalance(kairen), &quot;ether&quot;); 0 P.S. 若要移除帳號，可以刪除data/keystore底下的檔案。 接著在geth-2透過以下指令建立一個賬戶與查看乙太幣： &gt; pingyu = personal.newAccount(); Passphrase: Repeat passphrase: &quot;0xf8c70df559cb9225f6e426d0f139fd6e8752c644&quot; &gt; personal.listAccounts [&quot;0xf8c70df559cb9225f6e426d0f139fd6e8752c644&quot;] &gt; web3.fromWei(eth.getBalance(pingyu), &quot;ether&quot;); 0 接著回到geth-1來賺取一些要交易的乙太幣： &gt; miner.setEtherbase(kairen) true 當賬戶設定完成後，就可以執行以下指令進行採礦： &gt; miner.start(1) true 這邊需要一點時間產生 DAG，可以開一個新的命令列透過screen -x geth查看。 經過一段時間後，當 DAG 完成並開始採擴時就可以miner.stop()。 接著在geth-1查看賬戶的乙太幣： &gt; web3.fromWei(eth.getBalance(kairen), &quot;ether&quot;); 40.78125 當成開採區塊後，就可以查看geth-1共採集的 ether balance 的數值： &gt; eth.getBalance(eth.coinbase).toNumber() 40781250000000000000 即為40.78125乙太幣。 接著我們要在將geth-1的賬戶乙太幣轉移到geth-2上，首先在geth-1上建立一個變數來存geth-2的賬戶位址： &gt; consumer = &quot;0xf8c70df559cb9225f6e426d0f139fd6e8752c644&quot; &quot;0xf8c70df559cb9225f6e426d0f139fd6e8752c644&quot; 完成上述後，首先要將賬戶解鎖： &gt; personal.unlockAccount(kairen) true 輸入當初建立賬戶的密碼。 並透過 eth API 的交易函式還將 ether balance 數值轉移： $ eth.sendTransaction({from: kairen, to: consumer, value: web3.toWei(10, &quot;ether&quot;)}) &quot;0x1aee9082a55751c59077a273e7b08acd028d5099a4986f002518b0c8919d9e36&quot; 若有在每一台 geth 節點上進入 debug 模式的話，會發現該交易資訊被存到一個區塊，這邊也可以透過 txpool 與 eth API 來查看： &gt; txpool.status { pending: 1, queued: 0 } &gt; eth.getBlock(&quot;pending&quot;, true).transactions [{ blockHash: &quot;0x0b58d0b17e02f56746b0b5b22f195b6ae71d47343bf778763c4c476386ad7db7&quot;, blockNumber: 112, from: &quot;0xcb41ad8ba28c4b8b52eee159ef3bb6da197ff60b&quot;, gas: 90000, gasPrice: 20000000000, hash: &quot;0x1aee9082a55751c59077a273e7b08acd028d5099a4986f002518b0c8919d9e36&quot;, input: &quot;0x&quot;, nonce: 0, to: &quot;0xf8c70df559cb9225f6e426d0f139fd6e8752c644&quot;, transactionIndex: 0, value: 10000000000000000000 }] 這邊的pending表示目前還沒有被驗證，因此我們需要一些節點來進行採礦驗證。這邊也可以發現該交易資訊被存在區塊編號112，可以提供往後查詢之用。 接著回到geth-2節點，查看目前的數值變化： &gt; web3.fromWei(eth.getBalance(pingyu), &quot;ether&quot;); 0 這邊會發現沒有任何錢進來，Why? so sad。其實是因為該區塊還沒有被採集與認證，因此該交易不會被執行。 因此我們需要在任一節點提供運算，這邊在geth-1執行以下指令來進行採礦，這樣就可以看到該交易被驗證與接受： &gt; miner.start(1) true TX(1aee9082a55751c59077a273e7b08acd028d5099a4986f002518b0c8919d9e36) Contract: false From: cb41ad8ba28c4b8b52eee159ef3bb6da197ff60b To: f8c70df559cb9225f6e426d0f139fd6e8752c644 Nonce: 0 GasPrice: 20000000000 GasLimit 90000 Value: 10000000000000000000 Data: 0x V: 0x1c R: 0x9de7d843959f55a553577dc68a887893adf1b80eccd872021dfa6b8bcf3db43 S: 0x287f8e01640ccd5924308725d2d274def7edc4a18169b36ae26c95216fdf0fed Hex: f86d808504a817c80083015f9094f8c70df559cb9225f6e426d0f139fd6e8752c644888ac7230489e80000801ca009de7d843959f55a553577dc68a887893adf1b80eccd872021dfa6b8bcf3db43a0287f8e01640ccd5924308725d2d274def7edc4a18169b36ae26c95216fdf0fed 當該區塊的交易確認沒問題被執行後，就可以透過miner.stop()停止採礦。 這時再回到geth-2節點，查看目前的數值變化，會發現增加了 10 枚乙太幣： &gt; web3.fromWei(eth.getBalance(pingyu), &quot;ether&quot;); 10 之後可以在任一節點透過 eth web3 的 API 來查找指定區塊的交易資訊： &gt; eth.getTransactionFromBlock(40) { blockHash: &quot;0xe839c1392657731417fc04b9aecf7a181dd339086d5f7cdea0bccc2b1483b885&quot;, blockNumber: 112, from: &quot;0xcb41ad8ba28c4b8b52eee159ef3bb6da197ff60b&quot;, gas: 90000, gasPrice: 20000000000, hash: &quot;0x1aee9082a55751c59077a273e7b08acd028d5099a4986f002518b0c8919d9e36&quot;, input: &quot;0x&quot;, nonce: 0, to: &quot;0xf8c70df559cb9225f6e426d0f139fd6e8752c644&quot;, transactionIndex: 0, value: 10000000000000000000 } 簡單的 Contract這邊將說明如何建立一個簡單的合約(Contract)來部署於區塊鏈上，首先複製以下內容： contract SimpleStorage { uint storedData; function set(uint x) { storedData = x; } function get() constant returns (uint retVal) { return storedData; } } 接著將內容貼到 browser-solidity 進行編譯成 JavaScript。如快照畫面所示。 透過這個 IDE 可以將 Solidity 語言轉換成 web3 code(JavaScript)，複製 web3 code 的內容，並儲存成SimpleStorage.js檔案放置到geth-1上。接著 attach 進入 geth 執行以下指令： &gt; loadScript(&#39;SimpleStorage.js&#39;); 若有自行安裝browser-solidity的話，則可以使用如下圖一樣的方式連接。","tags":[{"name":"Ethereum","slug":"Ethereum","permalink":"https://kairen.github.io/tags/Ethereum/"},{"name":"Blockchain","slug":"Blockchain","permalink":"https://kairen.github.io/tags/Blockchain/"},{"name":"Go lang","slug":"Go-lang","permalink":"https://kairen.github.io/tags/Go-lang/"}]},{"title":"Enterprise 的 Docker registry 平台 Harbor","date":"2017-05-10T09:08:54.000Z","path":"2017/05/10/container/harbor-install/","text":"Harbor 是一個企業級 Registry 伺服器用於儲存和分散 Docker Image 的，透過新增一些企業常用的功能，例如：安全性、身分驗證和管理等功能擴展了開源的 Docker Distribution。作為一個企業級的私有 Registry 伺服器，Harbor 提供了更好的效能與安全性。Harbor 支援安裝多個 Registry 並將 Image 在多個 Registry 做 replicated。除此之外，Harbor 亦提供了高級的安全性功能，像是用戶管理(user managment)，存取控制(access control)和活動審核(activity auditing)。 功能特色 基於角色為基礎的存取控制(Role based access control)：使用者和 Repository 透過 Project 進行組織管理，一個使用者在同一個 Project 下，對於每個 Image 可以有不同權限。 基於 Policy 的 Image 複製：Image 可以在多得 Registry instance 中同步複製。適合於附載平衡、高可用性、混合雲與多雲的情境。 支援 LDAP/AD：Harbor 可以整合企業已有的 LDAP/AD，來管理使用者的認證與授權。 使用者的圖形化介面：使用者可以透過瀏覽器，查詢 Image 和管理 Project 審核管理：所有對 Repositroy 的操作都被記錄。 RESTful API：RESTful APIs 提供給管理的操作，可以輕易的整合額外的系統。 快速部署：提供 Online installer 與 Offline installer。 安裝指南Harbor 提供兩種方法進行安裝： Online installer 這種安裝方式會從 Docker hub 下載 Harbor 所需的映像檔，因此 installer 檔案較輕量。 Offline installer 當無任何網際網路連接的情況下使用此種安裝方式，預先將所需的映像檔打包，因此 installer 檔案較大。 事前準備Harbor 會部署數個 Docker container，所以部署的主機需要能支援 Docker 的 Linux distribution。而部署主機需要安裝以下套件： Python 版本2.7+。 Docker Engine 版本 1.10+。Docker 安裝方式，請參考：Install Docker Docker Compose 版本 1.6.0+。Docker Compose 安裝方式，請參考：Install Docker Compose 官方安裝指南說明是 Linux 且要支援 Docker，但 Windows 支援 Docker 部署 Harbor 還需要驗證是否可行。 安裝步驟大致可分為以下階段： 下載 installer 設定 Harbor 執行安裝腳本 下載 installerinstaller 的二進制檔案可以從 release page 下載，選擇您需要 Online installer 或者 Offline installer，下載完成後，使用tar將 package 解壓縮： Online installer： $ tar xvf harbor-online-installer-&lt;version&gt;.tgz Offline installer： $ tar xvf harbor-offline-installer-&lt;version&gt;.tgz 設定 HarborHarbor 的設定與參數都在harbor.cfg中。 harbor.cfg中的參數分為required parameters與optional parameters required parameters 這類的參數是必須設定的，且會影響使用者更新harbor.cfg後，重新執行安裝腳本來重新安裝 Harbor。 optional parameters 這類的參數為使用者自行決定是否設定，且只會在第一次安裝時，這些參數的配置才會生效。而 Harbor 啟動後，可以透過 Web UI 進行修改。 Configuring storage backend (optional)預設的情況下，Harbor 會將 Docker image 儲存在本機的檔案系統上，在生產環境中，您可以考慮使用其他 storage backend 而不是本機的檔案系統，像是 S3, OpenStack Swift, Ceph 等。而僅需更改 common/templates/registry/config.yml。以下為一個接 OpenStack Swift 的範例： storage: swift: username: admin password: ADMIN_PASS authurl: http://keystone_addr:35357/v3/auth tenant: admin domain: default region: regionOne container: docker_images 更多 storage backend 的資訊，請參考：Registry Configuration Reference。另外官方提供的是改 common/templates/registry/config.yml，感覺寫錯，需再測試其正確性。 執行安裝腳本一旦harbor.cfg與 storage backend (optional) 設定完成後，可以透過install.sh腳本開始安裝 Harbor。從 Harbor 1.1.0 版本之後，已經整合Notary，但是預設的情況下安裝是不包含Notary支援： $ sudo ./install.sh Online installer 會從 Docker hub 下載 Harbor 所需的映像檔，因此會花較久的時間。 如果安裝過程正常，您可以打開瀏覽器並輸入在harbor.cfg中設定的hostname，來存取 Harbor 的 Web UI。 預設的管理者帳號密碼為 admin/Harbor12345。 開始使用 Harbor登入成功後，可以創建一個新的 Project，並使用 Docker command 進行登入，但在登入之前，需要對 Docker daemon 新增--insecure-registry參數。新增--insecure-registry參數至/etc/default/docker中： DOCKER_OPTS=&quot;--insecure-registry &lt;your harbor.cfg hostname&gt;&quot; 其他細節，請參考：Test an insecure registry。 若在Ubuntu 16.04的作業系統版本，需要修改/lib/systemd/system/docker.service檔案，並加入一下內容。另外在 CentOS 7.x 版本則不需要加入-H fd://資訊： EnvironmentFile=/etc/default/docker ExecStart=/usr/bin/dockerd -H fd:// $DOCKER_OPTS 修改完成後，重新啟動服務： $ sudo systemctl daemon-reload 服務重啟成功後，透過 Docker command 進行 login： $ docker login &lt;your harbor.cfg hostname&gt; 將映像檔上 tag 之後，上傳至 Harbor： $ docker tag ubuntu:&lt;your harbor.cfg hostname&gt;/&lt;your project&gt;/ubuntu:16.04 $ docker push &lt;your harbor.cfg hostname&gt;/&lt;your project&gt;/ubunut:16.04 從 Harbor 抓取上傳的映像檔： $ docker pull &lt;your harbor.cfg hostname&gt;/&lt;your project&gt;/ubunut:16.04 更多使用者操作，請參考：Harbor User Guide。","tags":[{"name":"Linux Container","slug":"Linux-Container","permalink":"https://kairen.github.io/tags/Linux-Container/"},{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"Docker registry","slug":"Docker-registry","permalink":"https://kairen.github.io/tags/Docker-registry/"}]},{"title":"品嚐 Moby LinuxKit 的 Linux 作業系統","date":"2017-04-23T09:08:54.000Z","path":"2017/04/23/container/linuxkit/","text":"LinuxKit 是 DockerCon 2017 中推出的工具之一，其主要是以 Container 來建立最小、不可變的 Linux 作業系統映像檔框架，Docker 公司一直透過 LinuxKit 來建立相關產品，如 Docker for Mac 等。由於要最快的了解功能，因此這邊透過建立簡單的映像檔來學習。 在開始前需要準備完成一些事情： 安裝 Git client。 安裝 Docker engine，這邊建立使用 Docker-ce 17.04.0。 安裝 GUN make 工具。 安裝 GUN tar 工具。 建構 Moby 工具首先我們要建構名為 Moby 的工具，這個工具主要提供指定的 YAML 檔來執行描述的建構流程與功能，並利用 Docker 來建構出 Linux 作業系統。在本教學中，最後我們會利用 xhyve 這個 OS X 的虛擬化來提供執行系統實例，當然也可以透過官方的 HyperKit 來進行。 首先透過 Git 來抓取 LinuxKit repos，並進入建構 Moby： $ git clone https://github.com/linuxkit/linuxkit.git $ cd linuxkit $ make &amp;&amp; sudo make install $ moby version moby version 0.0 commit: 34d508562d7821cb812dd7b9caf4d9fbcdbc9fef 建立 Linux 映像檔當完成建構 Moby 工具後，就可以透過撰寫 YAML 檔來描述 Linux 的建構功能與流程了，這邊建立一個 Docker + SSH 的 Linux 映像檔。首先建立檔名為docker-sshd.yml的檔案，然後加入以下內容： kernel: image: &quot;linuxkit/kernel:4.9.x&quot; cmdline: &quot;console=ttyS0 console=tty0 page_poison=1&quot; init: - linuxkit/init:63eed9ca7a09d2ce4c0c5e7238ac005fa44f564b - linuxkit/runc:b0fb122e10dbb7e4e45115177a61a3f8d68c19a9 - linuxkit/containerd:18eaf72f3f4f9a9f29ca1951f66df701f873060b - linuxkit/ca-certificates:e091a05fbf7c5e16f18b23602febd45dd690ba2f onboot: - name: sysctl image: &quot;linuxkit/sysctl:1f5ec5d5e6f7a7a1b3d2ff9dd9e36fd6fb14756a&quot; net: host pid: host ipc: host capabilities: - CAP_SYS_ADMIN readonly: true - name: sysfs image: linuxkit/sysfs:6c1d06f28ddd9681799d3950cddf044b930b221c - name: binfmt image: &quot;linuxkit/binfmt:c7e69ebd918a237dd086a5c58dd888df772746bd&quot; binds: - /proc/sys/fs/binfmt_misc:/binfmt_misc readonly: true - name: format image: &quot;linuxkit/format:53748000acf515549d398e6ae68545c26c0f3a2e&quot; binds: - /dev:/dev capabilities: - CAP_SYS_ADMIN - CAP_MKNOD - name: mount image: &quot;linuxkit/mount:d2669e7c8ddda99fa0618a414d44261eba6e299a&quot; binds: - /dev:/dev - /var:/var:rshared,rbind capabilities: - CAP_SYS_ADMIN rootfsPropagation: shared command: [&quot;/mount.sh&quot;, &quot;/var/lib/docker&quot;] services: - name: rngd image: &quot;linuxkit/rngd:c42fd499690b2cb6e4e6cb99e41dfafca1cf5b14&quot; capabilities: - CAP_SYS_ADMIN oomScoreAdj: -800 readonly: true - name: dhcpcd image: &quot;linuxkit/dhcpcd:57a8ef29d3a910645b2b24c124f9ce9ef53ce703&quot; binds: - /var:/var - /tmp/etc:/etc capabilities: - CAP_NET_ADMIN - CAP_NET_BIND_SERVICE - CAP_NET_RAW net: host oomScoreAdj: -800 - name: ntpd image: &quot;linuxkit/openntpd:a570316d7fc49ca1daa29bd945499f4963d227af&quot; capabilities: - CAP_SYS_TIME - CAP_SYS_NICE - CAP_SYS_CHROOT - CAP_SETUID - CAP_SETGID net: host - name: docker image: &quot;linuxkit/docker-ce:741bf21513328f674e0cdcaa55492b0b75974e08&quot; capabilities: - all net: host mounts: - type: cgroup options: [&quot;rw&quot;,&quot;nosuid&quot;,&quot;noexec&quot;,&quot;nodev&quot;,&quot;relatime&quot;] binds: - /var/lib/docker:/var/lib/docker - /lib/modules:/lib/modules - name: sshd image: &quot;linuxkit/sshd:e108d208adf692c8a0954f602743e0eec445364e&quot; capabilities: - all net: host pid: host binds: - /root/.ssh:/root/.ssh - /etc/resolv.conf:/etc/resolv.conf - name: test-docker-bench image: &quot;linuxkit/test-docker-bench:2f941429d874c5dcf05e38005affb4f10192e1a8&quot; ipc: host pid: host net: host binds: - /run:/var/run capabilities: - all files: - path: etc/docker/daemon.json contents: &#39;{&quot;debug&quot;: true}&#39; - path: root/.ssh/authorized_keys contents: &#39;SSH_KEY&#39; trust: image: - linuxkit/kernel - linuxkit/binfmt - linuxkit/rngd outputs: - format: kernel+initrd - format: iso-bios P.S.請修改SSH_KEY內容為你的系統 ssh public key。 這邊說明幾個 YAML 格式意義： kernel: 指定 Docker 映像檔的核心版本，會包含一個 Linux 核心與檔案系統的 tar 檔，會將核心建構在/kernel目錄中。 init: 是一個 Docker Container 的 init 行程基礎，裡面包含init、containerd、runC與其他等工具。 onboot: 指定要建構的系統層級工具，會依據定義順序來執行，該類型如: dhcpd 與 ntpd 等。 services: 指定要建構服務，通常會是系統開啟後執行，如 ngnix、apache2。 files:要複製到該 Linux 系統映像檔中的檔案。 outputs:輸出的映像檔格式。 更多 YAML 格式說明可以參考官方 LinuxKit YAML。目前 LinuxKit 的映像檔來源可以參考 Docker Hub 撰寫完後，就可以透過 Moby 工具進行建構 Linux 映像檔了： $ moby build docker-sshd.yml Extract kernel image: linuxkit/kernel:4.9.x Pull image: linuxkit/kernel:4.9.x ... Create outputs: docker-sshd-kernel docker-sshd-initrd.img docker-sshd-cmdline docker-sshd.iso 完成後會看到以下幾個檔案： docker-sshd-kernel: 為 RAW Kernel 映像檔. docker-sshd-initrd.img: 為初始化 RAW Disk 檔案. docker-sshd-cmdline: Command line options 檔案. docker-sshd.iso: Docker SSHD ISO 格式映像檔. 測試映像檔當完成建構映像檔後，就可以透過一些工具來進行測試，這邊採用 xhyve 來執行實例，首先透過 Git 取得 xhyve repos，並建構與安裝： $ git clone https://github.com/mist64/xhyve $ cd xhyve $ make &amp;&amp; cp build/xhyve /usr/local/bin/ $ xhyve Usage: xhyve [-behuwxMACHPWY] [-c vcpus] [-g &lt;gdb port&gt;] [-l &lt;lpc&gt;] [-m mem] [-p vcpu:hostcpu] [-s &lt;pci&gt;] [-U uuid] -f &lt;fw&gt; xhyve 是 FreeBSD 虛擬化技術 bhyve 的 OS X 版本，是以 Hypervisor.framework 為基底的上層工具，這是除了 VirtualBox 與 VMwar 的另外選擇，並且該工具非常的輕巧，只有幾 KB 的容量。 接著撰寫 xhyve 腳本來啟動映像檔： #!/bin/sh KERNEL=&quot;docker-sshd-kernel&quot; INITRD=&quot;docker-sshd-initrd.img&quot; CMDLINE=&quot;console=ttyS0 console=tty0 page_poison=1&quot; MEM=&quot;-m 1G&quot; PCI_DEV=&quot;-s 0:0,hostbridge -s 31,lpc&quot; LPC_DEV=&quot;-l com1,stdio&quot; ACPI=&quot;-A&quot; #SMP=&quot;-c 2&quot; # sudo if you want networking enabled NET=&quot;-s 2:0,virtio-net&quot; xhyve $ACPI $MEM $SMP $PCI_DEV $LPC_DEV $NET -f kexec,$KERNEL,$INITRD,&quot;$CMDLINE&quot; 修改KERNEL與INITRD為 docker-sshd 的映像檔。 完成後就可以進行啟動測試： $ chmod u+x run.sh $ sudo ./run.sh Welcome to LinuxKit ## . ## ## ## == ## ## ## ## ## === /&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\\___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\_______/ ... / # ls bin etc lib root srv usr containers home media run sys var dev init proc sbin tmp / # ip ... 4: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 inet 192.168.64.4/24 brd 192.168.64.255 scope global eth0 valid_lft forever preferred_lft forever 14: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN inet 172.17.0.1/16 scope global docker0 valid_lft forever preferred_lft forever 驗證映像檔服務當看到上述結果後，表示作業系統開啟無誤，這時候我們要測試系統服務是否正常，首先透過 SSH 來進行測試，在剛剛新增的 ssh public key 主機上執行以下： $ ssh root@192.168.64.4 moby-aa16c789d03b:~# uname -r 4.9.25-linuxkit moby-aa16c789d03b:~# exit 查看 Docker 是否啟動： moby-aa16c789d03b:~# netstat -xp Active UNIX domain sockets (w/o servers) Proto RefCnt Flags Type State I-Node PID/Program name Path unix 2 [ ] DGRAM 33822 606/dhcpcd unix 3 [ ] STREAM CONNECTED 33965 748/ntpd: dns engin unix 3 [ ] STREAM CONNECTED 33960 747/ntpd: ntp engin unix 3 [ ] STREAM CONNECTED 33964 747/ntpd: ntp engin unix 3 [ ] STREAM CONNECTED 33959 642/ntpd unix 3 [ ] STREAM CONNECTED 34141 739/dockerd unix 3 [ ] STREAM CONNECTED 34142 751/docker-containe /var/run/docker/libcontainerd/docker-containerd.sock 最後關閉虛擬機可以透過以下指令完成： moby-aa16c789d03b:~# halt Terminated","tags":[{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"Linux","slug":"Linux","permalink":"https://kairen.github.io/tags/Linux/"},{"name":"Moby","slug":"Moby","permalink":"https://kairen.github.io/tags/Moby/"},{"name":"Microkernel","slug":"Microkernel","permalink":"https://kairen.github.io/tags/Microkernel/"}]},{"title":"TensorFlow 基本使用與分散式概念","date":"2017-04-10T08:23:01.000Z","path":"2017/04/10/tensorflow/intro/","text":"TensorFlow™ 是利用資料流圖(Data Flow Graphs)來表達數值運算的開放式原始碼函式庫。資料流圖中的節點(Nodes)被用來表示數學運算，而邊(Edges)則用來表示在節點之間互相聯繫的多維資料陣列，即張量(Tensors)。它靈活的架構讓你能夠在不同平台上執行運算，例如 PC 中的一個或多的 CPU(或GPU)、智慧手持裝置與伺服器等。TensorFlow 最初是 Google 機器智能研究所的研究員和工程師開發而成，主要用於機器學習與深度神經網路方面研究。 TensorFlow 其實在意思上是要用兩個部分來解釋，Tensor 與 Flow： Tensor：是中文翻譯是張量，其實就是一個n維度的陣列或列表。如一維 Tensor 就是向量，二維 Tensor 就是矩陣等等. Flow：是指 Graph 運算過程中的資料流. Data Flow Graphs資料流圖(Data Flow Graphs)是一種有向圖的節點(Node)與邊(Edge)來描述計算過程。圖中的節點表示數學操作，亦表示資料 I/O 端點; 而邊則表示節點之間的關析，用來傳遞操作之間互相使用的多維陣列(Tensors)，而 Tensor 是在圖中流動的資料表示。一旦節點相連的邊傳來資料流，這時節點就會被分配到運算裝置上異步(節點之間)或同步(節點之內)的執行。 TensorFlow 基本使用在開始進行 TensorFlow 之前，需要了解幾個觀念： 使用 tf.Graph 來表示計算任務. 採用tensorflow::Session的上下文(Context)來執行圖. 以 Tensor 來表示所有資料，可看成擁有靜態資料類型，但有動態大小的多維陣列與列表，如 Boolean 或 String 轉成數值類型. 透過tf.Variable來維護狀態. 透過 feed 與 fetch 來任意操作(Arbitrary operation)給予值或從中取得資料. TensorFlow 的圖中的節點被稱為 op(operation)。一個op會有 0 至多個 Tensor，而每個 Tensor 是一種類別化的多維陣列，例如把一個圖集合表示成四維浮點陣列，分別為[batch, height, width, channels]。 利用三種不同稱呼來描述 Tensor 的維度，Shape、Rank 與 Dimension。可參考 Rank, Shape, 和 Type。 一般只有 shape 能夠直接被 print，而 Tensor 則需要 Session 來提供，一般需要三個操作步驟： 建立 Tensor. 新增 op. 建立 Session(包含一個 Graph)來執行運算. 以下是一個簡單範例，說明如何建立運算： # coding=utf-8 import tensorflow as tf a = tf.constant(1) b = tf.constant(2) c = tf.constant(3) d = tf.constant(4) add1 = tf.add(a, b) mul1 = tf.multiply(b, c) add2 = tf.add(c, d) output = tf.add(add1, mul1) with tf.Session() as sess: print sess.run(output) 執行流程如下圖： 以下是一個簡單範例，說明如何建立多個 Graph： # coding=utf-8 import tensorflow as tf logs_path = &#39;./basic_tmp&#39; # 建立一個 graph，並建立兩個常數 op ，這些 op 稱為節點 g1 = tf.Graph() with g1.as_default(): a = tf.constant([1.5, 6.0]) b = tf.constant([1.5, 3.2]) c = a * b with tf.Graph().as_default() as g2: # 建立一個 1x2 矩陣與 2x1 矩陣 op m1 = tf.constant([[1., 0., 2.], [-1., 3., 1.]]) m2 = tf.constant([[3., 1.], [2., 1.], [1., 0.]]) m3 = tf.matmul(m1, m2) # 矩陣相乘 # 在 session 執行 graph，並進行資料數據操作 `c`。 # 然後指派給 cpu 做運算 with tf.Session(graph=g1) as sess_cpu: with tf.device(&quot;/cpu:0&quot;): writer = tf.summary.FileWriter(logs_path, graph=g1) print(sess_cpu.run(c)) with tf.Session(graph=g2) as sess_gpu: with tf.device(&quot;/gpu:0&quot;): result = sess_gpu.run(m3) print(result) # 使用 tf.InteractiveSession 方式來印出內容(不會實際執行) it_sess = tf.InteractiveSession() x = tf.Variable([1.0, 2.0]) a = tf.constant([3.0, 3.0]) # 使用初始器 initializer op 的 run() 方法初始化 &#39;x&#39; x.initializer.run() sub = tf.subtract(x, a) print sub.eval() it_sess.close() 範例來至 Basic Usage。 指定 Device 可以看這邊 Using GPU. 上面範例可以看到建立了一個 Graph 的計算過程c，而當直接執行到c時，並不會真的執行運算，而是在sess會話建立後，並透過sess執行分配給 CPU 或 GPU 之類設備進行運算後，才會回傳一個節點的 Tensor，在 Python 中 Tensor 是一個 Numpy 的 ndarry 物件。 TensorFlow 也可以透過變數來維護 Graph 的執行過程狀態，這邊提供一個簡單的累加器： # coding=utf-8 import tensorflow as tf # 建立一個變數 counter，並初始化為 0 state = tf.Variable(0, name=&quot;counter&quot;) # 建立一個常數 op 為 1，並用來累加 state one = tf.constant(1) new_value = tf.add(state, one) update = tf.assign(state, new_value) # 啟動 Graph 前，變數必須先被初始化(init) op init_op = tf.global_variables_initializer() # 啟動 Graph 來執行 op with tf.Session() as sess: sess.run(init_op) print sess.run(state) # 執行 op 並更新 state for _ in range(3): sess.run(update) print sess.run(state) 更多細節可以查看 Variables。 另外可以利用 Fetch 方式來一次取得多個節點的 Tensor，範例如下： # coding=utf-8 import tensorflow as tf input1 = tf.constant(3.0) input2 = tf.constant(2.0) input3 = tf.constant(5.0) intermed = tf.add(input2, input3) mul = tf.multiply(input1, intermed) with tf.Session() as sess: # 一次取得多個 Tensor result = sess.run([mul, intermed]) print result 而當我們想要在執行 Session 時，臨時替換 Tensor 內容的話，就可以利用 TensorFlow 內建的 Feed 方法來解決： # coding=utf-8 import tensorflow as tf input1 = tf.placeholder(tf.float32) input2 = tf.placeholder(tf.float32) output = tf.multiply(input1, input2) with tf.Session() as sess: # 透過 feed 來更改 op 內容，這只會在執行時有效 print sess.run([output], feed_dict={input1:[7.], input2:[2.]}) print sess.run([output]) TensorFlow 分散式運算本節將以 TensorFlow 分散式深度學習為例。 gRPCgRPC(google Remote Procedure Call) 是 Google 開發的基於 HTTP/2 和 Protocol Buffer 3 的 RPC 框架，該框架有各種常見語言的實作，如 C、Java 與 Go 等語言，提供輕鬆跨語言的呼叫。 概念說明客戶端(Client)、叢集(Cluster)、工作(Job)、任務(Task)、TensorFlow 伺服器、Master 與 Worker 服務。 如圖所示，幾個流程說明如下： 整個系统映射到 TensorFlow 叢集. 參數伺服器映射到一個 Job. 每個模型(Model)副本映射到一個 Job. 每台實體運算節點映射到其 Job 中的 Task. 每個 Task 都有一個 TF Server，並利用 Master 服務來進行溝通與協調工作，而 Worker 服務則透過本地裝置(CPU 或 GPU)進行 TF graph 運算. TensorFlow 叢集裡包含了一個或多個工作(Job)，每個工作又可以拆分成一個或多個任務(Task)，簡單說 Cluster 是 Job 的集合，而 Job 是 Task 的集合。叢集概念主要用在一個特定層次對象，如訓練神經網路、平行操作多台機器等，一個叢集物件可以透過tf.train.ClusterSpec來定義。 如上所述，TensorFlow 的叢集就是一組工作任務，每個任務是一個服務，而服務又分成Master與Worker這兩種，並提供給Client進行操作。 Client：是用於建立 TensorFlow 計算 Graph，並建立與叢集進行互動的tensorflow::Session行程，一般由 Python 或 C++ 實作，單一客戶端可以同時連接多個 TF 伺服器連接，同時也能被多個 TF 伺服器連接. Master Service：是一個 RPC 服務行程，用來遠端連線一系列分散式裝置，主要提供tensorflow::Session介面，並負責透過 Worker Service 與工作的任務進行溝通. Worker Service：是一個可以使用本地裝置(CPU 或 GPU)對部分 Graph 進行運算的 RPC 邏輯，透過worker_service.proto介面來實作，所有 TensorFlow 伺服器均包含了 Worker Service 邏輯. TensorFlow 伺服器是運行tf.train.Server實例的行程，其為叢集一員，並有 Master 與 Worker 之分。 而 TensorFlow 的工作(Job)可拆成多個相同功能的任務(Task)，這些工作又分成Parameter server與Worker，兩者功能說明如下： Parameter server(ps):是分散式系統縮放至工業大小機器學習的問題，它提供工作節點與伺服器節點之間的非同步與零拷貝 key-value 的溝通，並支援資料的一致性模型的分散式儲存。在 TensorFlow 中主要根據梯度更新變數，並儲存於tf.Variable，可理解成只儲存 TF Model 的變數，並存放 Variable 副本. Worker:通常稱為計算節點，一般管理無狀態(Stateless)，且執行密集型的 Graph 運算資源，並根據變數運算梯度。存放 Graph 副本. Parameter Server 詳解 一般對於小型規模訓練，這種資料與參數量不多時，可以用一個 CPU 來同時執行兩種任務。而中型規模訓練，資料量較大，但參數量不多時，計算梯度的工作負載較高，而參數更新負載較低，所以計算梯度交給若干個 CPU 或 GPU 去執行，而更新參數則交給一個 CPU 即可。對於大型規模訓練，資料與參數量多時，不僅計算梯度需要部署多個 CPU 或 GPU，連更新參數也要不說到多個 CPU 中。 然而單一節點能夠裝載的 CPU 與 GPU 是有限的，所以在大量訓練時就需要多台機器來提供運算能力的擴展。 分散式變數伺服器(Parameter Server)當在較大規模的訓練時，隨著模型的變數越來越多，很可能造成單一節點因為效能問題，而無法負荷模型變數儲存與更新時，這時候就需要將變數分開到不同機器來做儲存與更新。而 TensorFlow 提供了變數伺服器的邏輯實現，並可以用多台機器來組成叢集，類似分散式儲存結構，主要用來解決變數的儲存與更新效能問題。 撰寫分散式程式注意概念當我們在寫分散式程式時，需要知道使用的副本與訓練模式。 In-graph 與 Between-graph 副本模式下圖顯示兩者差異，而這邊也在進行描述。 In-graph：只有一個 Clinet(主要呼叫tf::Session行程)，並將裡面變數與 op 指定給對應的 Job 完成，因此資料分發只由一個 Client 完成。這種方式設定簡單，其他節點只需要 join 操作，並提供一個 gRPC 位址來等待任務。但是訓練資料只在單一節點，因此要把資料分發到不同機器時，會影響平行訓練效能。可理解成所有 op 都在同一個 Graph 中，伺服器只需要做join()功能. Between-graph：多個獨立 Client 建立相同 Graph(包含變數)，並透過tf.train.replica_device_setter將這些參數映射到 ps 上，即訓練的變數儲存在 Parameter Server，而資料不用分發，資料分片(Shards)會存在個計算節點，因此個節點自己算自己的，算完後，把要更新變數告知 Parameter Server 進行更新。適合在 TB 級別的資料量使用，節省大量資料傳輸時間，也是深度學習推薦模式。 同步(Synchronous)訓練與非同步(Asynchronous)訓鍊TensorFlow 的副本擁有 in-graph 和 between-graph 模式，這兩者都支援了同步與非同步更新。本節將說明同步與非同步兩者的差異為何。 Synchronous：每個 Graph 的副本讀取相同 Parameter 的值，然後平行計算梯度(gradients)，將所有計算完的梯度放在一起處理，當每次更新梯度時，需要等所以分發的資料計算完成，並回傳結果來把梯度累加計算平均，在進行更新變數。好處在於使用 loss 的下降時比較穩定，壞處就是要等最慢的分片計算時間。 可以利用tf.train.SyncReplicasOptimizer來解決這個問題(在 Between-graph 情況下)，而在 In-graph 則將所有梯度平均即可。 Asynchronous：自己計算完梯度後，就去更新 paramenter，不同副本之前不會進行協調進度，因此計算資源被充分的利用。缺點是 loss 的下降不穩定。 一般在資料量小，且各節點計算能力平均下，適合使用同步模式; 反之在資料量大與各節點效能差異不同時，適合用非同步。 簡單分散式訓練程式TensorFlow 提供建立 Server 函式來進行測試使用，以下是建立一個分散式訓練 Server 程式server.py： # coding=utf-8 import tensorflow as tf # 定義 Cluster cluster = tf.train.ClusterSpec({&quot;worker&quot;: [&quot;localhost:2222&quot;]}) # 建立 Worker server server = tf.train.Server(cluster,job_name=&quot;worker&quot;,task_index=0) server.join() 也可以透過tf.train.Server.create_local_server() 來建立 Local Server 當確認程式沒有任何問題後，就可以透過以下方式啟動： $ python server.py 2017-04-10 18:19:41.953448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 650, pci bus id: 0000:01:00.0) 2017-04-10 18:19:41.983913: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job local -&gt; {0 -&gt; localhost:2222} 2017-04-10 18:19:41.984946: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:240] Started server with target: grpc://localhost:2222 接著我們要撰寫 Client 端來進行定義 Graph 運算的程式client.py： # coding=utf-8 import tensorflow as tf # 執行目標 Session server_target = &quot;grpc://localhost:2222&quot; logs_path = &#39;./basic_tmp&#39; # 指定 worker task 0 使用 CPU 運算 with tf.device(&quot;/job:worker/task:0&quot;): with tf.device(&quot;/cpu:0&quot;): a = tf.constant([1.5, 6.0], name=&#39;a&#39;) b = tf.Variable([1.5, 3.2], name=&#39;b&#39;) c = (a * b) + (a / b) d = c * a y = tf.assign(b, d) # 啟動 Session with tf.Session(server_target) as sess: sess.run(tf.global_variables_initializer()) writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph()) print(sess.run(y)) 完成後即可透過以下指令測試： $ python client.py [ 4.875 126.45000458] 線性迴歸訓練程式上面範例提供了很簡單的 Client 與 Server 運算操作。而這邊建立一個 Between-graph 執行程式bg_dist.py： # coding=utf-8 import tensorflow as tf import numpy as np parameter_servers = [&quot;localhost:2222&quot;] workers = [&quot;localhost:2223&quot;, &quot;localhost:2224&quot;] tf.app.flags.DEFINE_string(&quot;job_name&quot;, &quot;&quot;, &quot;輸入 &#39;ps&#39; 或是 &#39;worker&#39;&quot;) tf.app.flags.DEFINE_integer(&quot;task_index&quot;, 0, &quot;Job 的任務 index&quot;) FLAGS = tf.app.flags.FLAGS def main(_): cluster = tf.train.ClusterSpec({&quot;ps&quot;: parameter_servers, &quot;worker&quot;: workers}) server = tf.train.Server(cluster,job_name=FLAGS.job_name,task_index=FLAGS.task_index) if FLAGS.job_name == &quot;ps&quot;: server.join() elif FLAGS.job_name == &quot;worker&quot;: train_X = np.linspace(-1.0, 1.0, 100) train_Y = 2.0 * train_X + np.random.randn(*train_X.shape) * 0.33 + 10.0 X = tf.placeholder(&quot;float&quot;) Y = tf.placeholder(&quot;float&quot;) # Assigns ops to the local worker by default. with tf.device(tf.train.replica_device_setter( worker_device=&quot;/job:worker/task:%d&quot; % FLAGS.task_index, cluster=cluster)): w = tf.Variable(0.0, name=&quot;weight&quot;) b = tf.Variable(0.0, name=&quot;bias&quot;) # 損失函式，用於描述模型預測值與真實值的差距大小，常見為`均方差(Mean Squared Error)` loss = tf.square(Y - tf.multiply(X, w) - b) global_step = tf.Variable(0) train_op = tf.train.AdagradOptimizer(0.01).minimize( loss, global_step=global_step) saver = tf.train.Saver() summary_op = tf.summary.merge_all() init_op = tf.global_variables_initializer() # 建立 &quot;Supervisor&quot; 來負責監督訓練過程 sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0), logdir=&quot;/tmp/train_logs&quot;, init_op=init_op, summary_op=summary_op, saver=saver, global_step=global_step, save_model_secs=600) with sv.managed_session(server.target) as sess: loss_value = 100 while not sv.should_stop() and loss_value &gt; 70.0: # 執行一個非同步 training 步驟. # 若要執行同步可利用`tf.train.SyncReplicasOptimizer` 來進行 for (x, y) in zip(train_X, train_Y): _, step = sess.run([train_op, global_step], feed_dict={X: x, Y: y}) loss_value = sess.run(loss, feed_dict={X: x, Y: y}) print(&quot;步驟: {}, loss: {}&quot;.format(step, loss_value)) sv.stop() if __name__ == &quot;__main__&quot;: tf.app.run() tf.train.replica_device_setter(ps_tasks=0, ps_device=&#39;/job:ps&#39;, worker_device=&#39;/job:worker&#39;, merge_devices=True, cluster=None, ps_ops=None) 指定方式。 撰寫完成後，透過以下指令來進行測試： $ python liner_dist.py --job_name=ps --task_index=0 $ python liner_dist.py --job_name=worker --task_index=0 $ python liner_dist.py --job_name=worker --task_index=1 Tensorboard 視覺化工具Tensorboard 是 TensorFlow 內建的視覺化工具，我們可以透過讀取事件紀錄結構化的資料，來顯示以下幾個項目來提供視覺化： Event：訓練過程中統計資料(平均值等)變化狀態. Image：訓練過程中紀錄的 Graph. Audio：訓練過程中紀錄的 Audio. Histogram：順練過程中紀錄的資料分散圖 一個範例程式如下所示： # coding=utf-8 import tensorflow as tf logs_path = &#39;./tmp/1&#39; # 建立一個 graph，並建立兩個常數 op ，這些 op 稱為節點 g1 = tf.Graph() with g1.as_default(): a = tf.constant([1.5, 6.0], name=&#39;a&#39;) b = tf.Variable([1.5, 3.2], name=&#39;b&#39;) c = (a * b) + (a / b) d = c * a y = tf.assign(b, d) # 在 session 執行 graph，並進行資料數據操作 `c`。 # 然後指派給 cpu 做運算 with tf.Session(graph=g1) as sess_cpu: with tf.device(&quot;/cpu:0&quot;): sess_cpu.run(tf.global_variables_initializer()) writer = tf.summary.FileWriter(logs_path, graph=g1) print(sess_cpu.run(y)) 執行後會看到當前目錄產生tmp_mnist logs 檔案，這時候就可以透過 thensorboard 來視覺化訓練結果： $ tensorboard --logdir=run1:./tmp/1 --port=6006 run1 是當有多次 log 被載入時做為區別用。","tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://kairen.github.io/tags/TensorFlow/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://kairen.github.io/tags/Machine-Learning/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://kairen.github.io/tags/Ubuntu/"}]},{"title":"Kuberentes Helm 介紹","date":"2017-03-25T09:08:54.000Z","path":"2017/03/25/kubernetes/helm-quickstart/","text":"Helm 是 Kubernetes Chart 的管理工具，Kubernetes Chart 是一套預先組態的 Kubernetes 資源套件。使用 Helm 有以下幾個好處： 查詢與使用熱門的 Kubernetes Chart 軟體套件。 以 Kuberntes Chart 來分享自己的應用程式。 可利用 Chart 來重複建立應用程式。 智能地管理 Kubernetes manifest 檔案。 管理釋出的 Helm 版本。 概念Helm 有三個觀念需要我們去了解，分別為 Chart、Release 與 Repository，其細節如下： Chart：主要定義要被執行的應用程式中，所需要的工具、資源、服務等資訊，有點類似 Homebrew 的 Formula 或是 APT 的 dpkg 檔案。 Release：一個被執行於 Kubernetes 的 Chart 實例。Chart 能夠在一個叢集中擁有多個 Release，例如 MySQL Chart，可以在叢集建立基於該 Chart 的兩個資料庫實例，其中每個 Release 都會有獨立的名稱。 Repository：主要用來存放 Chart 的倉庫，如 KubeApps。 可以理解 Helm 主要目標就是從 Chart Repository 中，查找部署者需要的應用程式 Chart，然後以 Release 形式來部署到 Kubernetes 中進行管理。 Helm 系統元件Helm 主要分為兩種元件，Helm Client 與 Tiller Server，兩者功能如下： Helm Client：一個安裝 Helm CLI 的機器，該機器透過 gRPC 連接 Tiller Server 來對 Repository、Chart 與 Release 等進行管理與操作，如建立、刪除與升級等操作，細節可以查看 Helm Documentation。 Tiller Server：主要負責接收來至 Client 的指令，並透過 kube-apiserver 與 Kubernetes 叢集做溝通，根據 Chart 定義的內容，來產生與管理各種對應 API 物件的 Kubernetes 部署檔案(又稱為 Release)。 兩者溝通架構圖如下所示： 事前準備安裝前需要確認環境滿足以下幾膽： 已部署 Kubernetes 叢集。 操作端安裝 kubectl 工具。 操作端可以透過 kubectl 工具管理到 Kubernetes（可用的 kubectl config）。 安裝 HelmHelm 有許多種安裝方式，這邊個人比較喜歡用 binary 檔案來進行安裝： $ wget -qO- https://kubernetes-helm.storage.googleapis.com/helm-v2.8.1-linux-amd64.tar.gz | tar -zx $ sudo mv linux-amd64/helm /usr/local/bin/ $ helm version OS X 為下載 helm-v2.4.1-darwin-amd64.tar.gz。 初始化 Helm在開始使用 Helm 之前，我們需要建置 Tiller Server 來對 Kubernetes 的管理，而 Helm CLI 內建也提供了快速初始化指令，如下： $ kubectl -n kube-system create sa tiller $ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller $ helm init --service-account tiller $HELM_HOME has been configured at /root/.helm. Tiller (the helm server side component) has been installed into your Kubernetes Cluster. Happy Helming! 若之前只用舊版想要更新可以透過以下指令helm init --upgrade來達到效果。 完成後，就可以透過 kubectl 來查看 Tiller Server 是否被建立： $ kubectl get po,svc -n kube-system -l app=helm NAME READY STATUS RESTARTS AGE po/tiller-deploy-1651596238-5lsdw 1/1 Running 0 3m NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/tiller-deploy 192.162.204.144 &lt;none&gt; 44134/TCP 3m 接著透過 helm ctl 來查看資訊： $ export KUBECONFIG=/etc/kubernetes/admin.conf $ export HELM_HOST=$(kubectl describe svc/tiller-deploy -n kube-system | awk &#39;/Endpoints/{print $2}&#39;) # wait for a few minutes $ helm version Client: &amp;version.Version{SemVer:&quot;v2.8.1&quot;, GitCommit:&quot;6af75a8fd72e2aa18a2b278cfe5c7a1c5feca7f2&quot;, GitTreeState:&quot;clean&quot;} Server: &amp;version.Version{SemVer:&quot;v2.8.1&quot;, GitCommit:&quot;6af75a8fd72e2aa18a2b278cfe5c7a1c5feca7f2&quot;, GitTreeState:&quot;clean&quot;} 部署 Chart Release 實例當完成初始化後，就可以透過 helm ctl 來管理與部署 Chart Release，我們可以到 KubeApps 查找想要部署的 Chart，如以下快速部屬 Jenkins 範例，首先先透過搜尋來查看目前應用程式版本： $ helm search jenkins NAME VERSION DESCRIPTION stable/jenkins 0.6.3 Open source continuous integration server. It s... 接著透過inspect指令查看該 Chart 的參數資訊： $ helm inspect stable/jenkins ... Persistence: Enabled: true 從中我們會發現需要建立一個 PVC 來提供持久性儲存。 因此需要建立一個 PVC 提供給 Jenkins Chart 來儲存使用，這邊我們自己手動建立jenkins-pv-pvc.yml檔案： apiVersion: v1 kind: PersistentVolume metadata: name: jenkins-pv labels: app: jenkins spec: capacity: storage: 10Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle nfs: path: /var/nfs/jenkins server: 172.20.3.91 --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: jenkins-pvc labels: app: jenkins spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi 接著透過 kubectl 來建立： $ kubectl create -f jenkins-pv-pvc.yml persistentvolumeclaim &quot;jenkins-pvc&quot; created persistentvolume &quot;jenkins-pv&quot; created $ kubectl get pv,pvc NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM STORAGECLASS REASON AGE pv/jenkins-pv 10Gi RWO Recycle Bound default/jenkins-pvc 20s NAME STATUS VOLUME CAPACITY ACCESSMODES STORAGECLASS AGE pvc/jenkins-pvc Bound jenkins-pv 10Gi RWO 20s 當 PVC 建立完成後，就可以開始透過 Helm 來建立 Jenkins Release： $ export PVC_NAME=$(kubectl get pvc -l app=jenkins --output=template --template=&quot;{{with index .items 0}}{{.metadata.name}}{{end}}&quot;) $ helm install --name demo --set Persistence.ExistingClaim=${PVC_NAME} stable/jenkins NAME: demo LAST DEPLOYED: Thu May 25 17:53:50 2017 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==&gt; v1beta1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE demo-jenkins 1 1 1 0 1s ==&gt; v1/Secret NAME TYPE DATA AGE demo-jenkins Opaque 2 1s ==&gt; v1/ConfigMap NAME DATA AGE demo-jenkins-tests 1 1s demo-jenkins 3 1s ==&gt; v1/Service NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE demo-jenkins 192.169.143.140 &lt;pending&gt; 8080:30152/TCP,50000:31806/TCP 1s ... P.S. install 指令可以安裝來至Chart repository、壓縮檔 Chart、一個 Chart 目錄與Chart URL。 這邊 install 可以額外透過以下兩種方式來覆寫參數，在這之前可以先透過helm inspect values &lt;chart&gt;來取得使用的變數。 –values：指定一個 YAML 檔案來覆寫設定。 $ echo -e &#39;Master:\\n AdminPassword: r00tme&#39; &gt; config.yaml $ helm install -f config.yaml stable/jenkins –sets：指定一對 Key/value 指令來覆寫。 $ helm install --set Master.AdminPassword=r00tme stable/jenkins 完成後就可以透過 helm 與 kubectl 來查看建立狀態： $ helm ls NAME REVISION UPDATED STATUS CHART NAMESPACE demo 1 Thu May 25 17:53:50 2017 DEPLOYED jenkins-0.6.3 default $ kubectl get po,svc NAME READY STATUS RESTARTS AGE po/demo-jenkins-3139496662-c0lzk 1/1 Running 0 1m NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/demo-jenkins 192.169.143.140 &lt;pending&gt; 8080:30152/TCP,50000:31806/TCP 1m 由於預設只使用 LoadBalancerSourceRanges 來定義存取策略，但沒有指定任何外部 IP，因此要手動加入以下內容： $ kubectl edit svc demo-jenkins spec: externalIPs: - 172.20.3.90 完成後再次查看 Service 資訊： $ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE demo-jenkins 192.169.143.140 ,172.20.3.90 8080:30152/TCP,50000:31806/TCP 10m 這時候就可以透過 http://172.20.3.90:8080 連進去 Jenkins 了，其預設帳號為 admin。 透過以下指令來取得 Jenkins admin 密碼： $ printf $(kubectl get secret --namespace default demo-jenkins -o jsonpath=&quot;{.data.jenkins-admin-password}&quot; | base64 --decode);echo buQ1ik2Q7x 該 Chart 會產生亂數密碼存放到 secret 中。 最後我們也可以透過upgrade指令來更新已經 Release 的 Chart： $ helm upgrade --set Master.AdminPassword=r00tme --set Persistence.ExistingClaim=jenkins-pvc demo stable/jenkins Release &quot;demo&quot; has been upgraded. Happy Helming! $ helm get values demo Master: AdminPassword: r00tme Persistence: ExistingClaim: jenkins-pvc $ helm ls NAME REVISION UPDATED STATUS CHART NAMESPACE demo 2 Tue May 30 21:18:43 2017 DEPLOYED jenkins-0.6.3 default 這邊會看到REVISION會 +1，這可以用來做 rollback 的版本號使用。 刪除 ReleaseHelm 除了基本的建立功能外，其還包含了整個 Release 的生命週期管理功能，如我們不需要該 Release 時，就可以透過以下方式刪除： $ helm del demo $ helm status demo | grep STATUS STATUS: DELETED 當刪除後，該 Release 並沒有真的被刪除，我們可以透過 helm ls 來查看被刪除的 Release： $ helm ls --all NAME REVISION UPDATED STATUS CHART NAMESPACE demo 2 Tue May 30 21:18:43 2017 DELETED jenkins-0.6.3 default 當執行 helm ls 指令為加入 --all 時，表示只列出DEPLOYED狀態的 Release。 而當 Release 處於 DELETED 狀態時，我們可以進行一些操作，如 Roll back 或完全刪除 Release： $ helm rollback demo 1 Rollback was a success! Happy Helming! $ printf $(kubectl get secret --namespace default demo-jenkins -o jsonpath=&quot;{.data.jenkins-admin-password}&quot; | base64 --decode);echo BIsLlQTN9l $ helm del demo --purge release &quot;demo&quot; deleted # 這時執行以下指令就不會再看到已刪除的 Release. $ helm ls --all 建立簡單 Chart 結構Helm 提供了 create 指令來建立一個 Chart 基本結構： $ helm create example $ tree example/ example/ ├── charts ├── Chart.yaml ├── templates │ ├── deployment.yaml │ ├── _helpers.tpl │ ├── ingress.yaml │ ├── NOTES.txt │ └── service.yaml └── values.yaml 當我們設定完 Chart 後，就可以透過 helm 指令來打包： $ helm package example/ example-0.1.0.tgz 最後可以用 helm 來安裝： $ helm install ./example-0.1.0.tgz 自己建立 RepositoryHelm 指令除了可以建立 Chart 基本結構外，很幸運的也提供了建立 Helm Repository 的功能，建立方式如下： $ helm serve --repo-path example-0.1.0.tgz $ helm repo add example http://repo-url 另外 helm repo 也可以加入來至於 Github 與 HTTP 伺服器的網址來提供服務。","tags":[{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"},{"name":"Helm","slug":"Helm","permalink":"https://kairen.github.io/tags/Helm/"}]},{"title":"Kubespray 部署實體機 Kubernetes v1.6 叢集","date":"2017-03-17T09:08:54.000Z","path":"2017/03/17/kubernetes/deploy/kubespray-baremetal/","text":"Kubespray 是 Kubernetes incubator 中的專案，目標是提供 Production Ready Kubernetes 部署方案，該專案基礎是透過 Ansible Playbook 來定義系統與 Kubernetes 叢集部署的任務，目前 Kubespray 有以下幾個特點： 可以部署在 AWS, GCE, Azure, OpenStack 或者 Baremetal. 部署 High Available Kubernetes 叢集. 可組合性(Composable)，可自行選擇 Network Plugin (flannel, calico, canal, weave) 來部署. 支援多種 Linux distributions(CoreOS, Debian Jessie, Ubuntu 16.04, CentOS/RHEL7). 本篇將說明如何透過 Kubespray 部署 Kubernetes 至實體機器節點，安裝版本如下所示： Kubernetes v1.6.4 Etcd v3.1.6 Flannel v0.7.1 Docker v17.04.0-ce 節點資訊本次安裝測試環境的作業系統採用Ubuntu 16.04 Server，其他細節內容如下： IP Address Role CPU Memory 192.168.121.179 master1 + deploy 2 4G 192.168.121.106 node1 2 4G 192.168.121.197 node2 2 4G 192.168.121.123 node3 2 4G 這邊 master 為主要控制節點，node 為應用程式工作節點。 預先準備資訊 所有節點的網路之間可以互相溝通。 部署節點(這邊為 master1)對其他節點不需要 SSH 密碼即可登入。 所有節點都擁有 Sudoer 權限，並且不需要輸入密碼。 所有節點需要安裝 Python。 所有節點需要設定/etc/host解析到所有主機。 修改所有節點/etc/resolv.conf： $ echo &quot;nameserver 8.8.8.8&quot; | sudo tee /etc/resolv.conf 部署節點(這邊為 master1)需要安裝 Ansible &gt; 2.3.0。 Ubuntu 16.04 安裝最新版 Ansible: $ sudo sed -i &#39;s/us.archive.ubuntu.com/tw.archive.ubuntu.com/g&#39; /etc/apt/sources.list $ sudo apt-get install -y software-properties-common $ sudo apt-add-repository -y ppa:ansible/ansible $ sudo apt-get update &amp;&amp; sudo apt-get install -y ansible git cowsay python-pip python-netaddr libssl-dev 安裝 Kubespray 與準備部署資訊首先透過 pypi 安裝 kubespray-cli，雖然官方說已經改成 Go 語言版本的工具，但是根本沒在更新，所以目前暫時用 pypi 版本： $ sudo pip install -U kubespray 安裝完成後，新增設定檔~/.kubespray.yml，並加入以下內容： $ mkdir /etc/kubespray $ cat &lt;&lt;EOF &gt; ~/.kubespray.yml kubespray_git_repo: &quot;https://github.com/kubernetes-incubator/kubespray.git&quot; # Logging options loglevel: &quot;info&quot; EOF 接著用 kubespray cli 來產生 inventory 檔案： $ kubespray prepare --masters master1 --etcds master1 --nodes node1 node2 node3 $ cat ~/.kubespray/inventory/inventory.cfg 也可以自己建立inventory來描述部署節點。 完成後就可以透過以下指令進行部署 Kubernetes 叢集： $ time kubespray deploy --verbose -u root -k .ssh/id_rsa -n flannel Run kubernetes cluster deployment with the above command ? [Y/n]y ... master1 : ok=368 changed=89 unreachable=0 failed=0 node1 : ok=305 changed=73 unreachable=0 failed=0 node2 : ok=276 changed=62 unreachable=0 failed=0 node3 : ok=276 changed=62 unreachable=0 failed=0 Kubernetes deployed successfuly 其中-n為部署的網路插件類型，目前支援 calico、flannel、weave 與 canal。 驗證叢集當 Ansible 執行完成後，若沒發生錯誤就可以開始進行操作 Kubernetes，如取得版本資訊： $ kubectl version Client Version: version.Info{Major:&quot;1&quot;, Minor:&quot;6&quot;, GitVersion:&quot;v1.6.4+coreos.0&quot;, GitCommit:&quot;9212f77ed8c169a0afa02e58dce87913c6387b3e&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2017-04-04T00:32:53Z&quot;, GoVersion:&quot;go1.7.5&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;} Server Version: version.Info{Major:&quot;1&quot;, Minor:&quot;6&quot;, GitVersion:&quot;v1.6.4+coreos.0&quot;, GitCommit:&quot;9212f77ed8c169a0afa02e58dce87913c6387b3e&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2017-04-04T00:32:53Z&quot;, GoVersion:&quot;go1.7.5&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;} 取得目前節點狀態： $ kubectl get node NAME STATUS AGE VERSION master1 Ready,SchedulingDisabled 11m v1.6.4+coreos.0 node1 Ready 11m v1.6.4+coreos.0 node2 Ready 11m v1.6.4+coreos.0 node3 Ready 11m v1.6.4+coreos.0 查看目前系統 Pod 狀態： $ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE dnsmasq-975202658-6jj3n 1/1 Running 0 14m dnsmasq-975202658-h4rn9 1/1 Running 0 14m dnsmasq-autoscaler-2349860636-kfpx0 1/1 Running 0 14m flannel-master1 1/1 Running 1 14m flannel-node1 1/1 Running 1 14m flannel-node2 1/1 Running 1 14m flannel-node3 1/1 Running 1 14m kube-apiserver-master1 1/1 Running 0 15m kube-controller-manager-master1 1/1 Running 0 15m kube-proxy-master1 1/1 Running 1 14m kube-proxy-node1 1/1 Running 1 14m kube-proxy-node2 1/1 Running 1 14m kube-proxy-node3 1/1 Running 1 14m kube-scheduler-master1 1/1 Running 0 15m kubedns-1519522227-thmrh 3/3 Running 0 14m kubedns-autoscaler-2999057513-tx14j 1/1 Running 0 14m nginx-proxy-node1 1/1 Running 1 14m nginx-proxy-node2 1/1 Running 1 14m nginx-proxy-node3 1/1 Running 1 14m","tags":[{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"},{"name":"Ansible","slug":"Ansible","permalink":"https://kairen.github.io/tags/Ansible/"}]},{"title":"Ubuntu 16.04 安裝 TensorFlow GPU GTX 1060","date":"2017-03-12T08:23:01.000Z","path":"2017/03/12/tensorflow/install-source/","text":"本篇主要因為自己買了一片Nvidia GTX 1060 6G顯卡，但是購買至今只用來玩過一個遊戲，因此才拿來試跑 TensorFlow。 本次安裝硬體與規格如下： 作業系統: Ubuntu 16.04 Desktop GPU: GeForce® GTX 1060 6G NVIDIA Driver: nvidia-367 Python: 2.7+ TensorFlow: r1.0.1 CUDA: v8.0 cuDNN: v5.1 環境部署如果要安裝 TensorFlow with GPU support 的話，需要滿足以下幾點： Nvidia Driver. 已安裝 CUDA® Toolkit 8.0. 已安裝 cuDNN v5.1. GPU card with CUDA Compute Capability 6.1(GTX 10-series). libcupti-dev 函式庫. Nvidia Driver 安裝由於預設 Ubuntu 的 Nvidia 版本比較舊，或者並沒有安裝相關驅動，因此這邊需要安裝顯卡對應的版本才能夠正常使用，可以透過以下方式進行： $ sudo add-apt-repository -y ppa:graphics-drivers/ppa $ sudo apt-get update $ sudo apt-get install -y nvidia-367 完成後，需重新啟動機器。 CUDA Toolkit 8.0 安裝由於 TensorFlow 支援 GPU 運算時，會需要使用到 CUDA Toolkit 相關功能，可以到 CUDA Toolkit 頁面下載，這邊會下載 Ubuntu Run file 檔案，來進行安裝： $ wget &quot;https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda_8.0.61_375.26_linux-run&quot; $ sudo chmod u+x cuda_8.0.61_375.26_linux-run $ ./cuda_8.0.61_375.26_linux-run Do you accept the previously read EULA? accept/decline/quit: accept Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 361.77? (y)es/(n)o/(q)uit: n Install the CUDA 8.0 Toolkit? (y)es/(n)o/(q)uit: y Enter Toolkit Location [ default is /usr/local/cuda-8.0]: enter Do you want to install a symbolic link at /usr/local/cuda? (y)es/(n)o/(q)uit:y Install the CUDA 8.0 Samples? (y)es/(n)o/(q)uit:y Enter CUDA Samples Location [ defualt is /home/kylebai ]: enter 這邊enter為鍵盤直接按壓，而不是輸入 enter。 安裝完成後，編輯 Home 目錄底下的.bashrc檔案加入以下內容： export PATH=${PATH}:/usr/local/cuda-8.0/bin export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64 最後 Source Bash 檔案與測試 CUDA Toolkit： $ source .bashrc $ sudo nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 375.39 Driver Version: 375.39 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 106... Off | 0000:01:00.0 On | N/A | | 28% 29C P8 6W / 120W | 130MiB / 6069MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1165 G /usr/lib/xorg/Xorg 98MiB | | 0 1764 G compiz 29MiB | +-----------------------------------------------------------------------------+ cuDNN 5.1 安裝NVIDIA cuDNN 是一個深度神經網路運算的 GPU 加速原函式庫，這邊需要點選前面的連結，下載cuDNN v5.1 Library for Linux檔案： $ tar xvf cudnn-8.0-linux-x64-v5.1.tgz $ sudo cp cuda/include/cudnn.h /usr/local/cuda/include/ $ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/ TensorFlow GPU 套件建構本次教學將透過 Source code 建構安裝檔，再進行安裝 TensorFlow，首先安裝相依套件： $ sudo add-apt-repository -y ppa:webupd8team/java $ sudo apt-get update $ sudo apt-get install -y libcupti-dev python-numpy python-dev python-setuptools python-pip python-wheel git oracle-java8-installer $ echo &quot;deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8&quot; | sudo tee /etc/apt/sources.list.d/bazel.list $ curl -s &quot;https://storage.googleapis.com/bazel-apt/doc/apt-key.pub.gpg&quot; | sudo apt-key add - $ sudo apt-get update &amp;&amp; sudo apt-get -y install bazel $ sudo apt-get upgrade -y bazel 接著取得 TensorFlow 專案原始碼，然後進入到 TensorFlow 專案目錄進行 bazel 設定： $ git clone &quot;https://github.com/tensorflow/tensorflow&quot; $ cd tensorflow $ ./configure ... Do you wish to build TensorFlow with CUDA support? [y/N] y Please specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0 Please specify the cuDNN version you want to use. [Leave empty to use system default]: 5 Please note that each additional compute capability significantly increases your build time and binary size. [Default is: &quot;3.5,5.2&quot;]: 6.1 ... Configuration finished 6.1為 GTX 10-series 系列顯卡，其他可以查看 CUDA GPUS。這邊除了上述特定要輸入外，其餘都是直接鍵盤enter。 當完成組態後，即可透過 bazel 進行建構 pip 套件腳本： $ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package 當腳本建構完成後，即可透過以下指令來建構 .whl 檔案： $ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tf_pkg 完成後，可以在/tmp/tf_pkg目錄底下找到安裝檔tensorflow-1.0.1-py2-none-any.whl，最後就可以透過 pip 來進行安裝了： $ sudo pip install /tmp/tf_pkg/tensorflow-1.0.1-cp27-cp27mu-linux_x86_64.whl 測試安裝結果最後透過簡單程式來驗證安裝是否成功： $ cat &lt;&lt;EOF &gt; simple.py import tensorflow as tf hello = tf.constant(&#39;Hello, TensorFlow!&#39;) sess = tf.Session() print(sess.run(hello)) EOF $ python simple.py ... roperties: name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate (GHz) 1.7845 pciBusID 0000:01:00.0 Total memory: 5.93GiB Free memory: 5.74GiB 2017-03-12 21:43:56.477084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 2017-03-12 21:43:56.477092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0: Y 2017-03-12 21:43:56.503464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0) Hello, TensorFlow! ...部分會顯示一些 GPU 使用狀態。","tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://kairen.github.io/tags/TensorFlow/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://kairen.github.io/tags/Machine-Learning/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://kairen.github.io/tags/Ubuntu/"}]},{"title":"kube-ansible 快速部署實體機 HA 叢集","date":"2017-02-19T09:08:54.000Z","path":"2017/02/19/kubernetes/deploy/kube-ansible-metal/","text":"本篇說明如何透過 kube-ansible 部署多節點實體機 Kubernetes 叢集。 本安裝各軟體版本如下： Kubernetes v1.8.3 Etcd v3.2.9 Docker v1.13.0+ 而在 OS 部分以支援Ubuntu 16.x及CentOS 7.x的虛擬機與實體機部署。 節點資訊本次安裝作業系統採用Ubuntu 16.04 Server，測試環境為實體主機： IP Address Role CPU Memory 172.20.3.90 VIP 172.20.3.91 master1 2 4G 172.20.3.92 master2 2 4G 172.20.3.93 master3 2 4G 172.20.3.94 node1 4 8G 172.20.3.95 node2 4 8G 172.20.3.96 node3 4 8G 172.20.3.97 node4 4 8G 172.20.3.98 node5 4 8G 事前準備安裝前需要確認以下幾個項目： 所有節點的網路之間可以互相溝通。 部署節點(這邊為 master1)對其他節點不需要 SSH 密碼即可登入。 所有節點都擁有 Sudoer 權限，並且不需要輸入密碼。 所有節點需要安裝 Python。 所有節點需要設定/etc/host解析到所有主機。 部署節點(這邊為 master1)需要安裝 Ansible。 Ubuntu 16.04 安裝 Ansible: $ sudo apt-get install -y software-properties-common git cowsay $ sudo apt-add-repository -y ppa:ansible/ansible $ sudo apt-get update &amp;&amp; sudo apt-get install -y ansible CentOS 7 安裝 Ansible： $ sudo yum install -y epel-release $ sudo yum -y install ansible cowsay 部署 Kubernetes 叢集首先透過 Git 取得 HA Kubernetes Ansible 的專案： $ git clone &quot;https://github.com/kairen/kube-ansible.git&quot; $ cd kube-ansible 然後編輯inventory檔案，來加入要部署的節點角色： [etcds] 172.20.3.[91:93] [masters] 172.20.3.[91:93] [nodes] 172.20.3.[94:98] [kube-cluster:children] masters nodes [kube-addon:children] masters 完成後接著編輯group_vars/all.yml，來根據需求設定參數，範例如下： # Kubenrtes version, only support 1.8.0+. kube_version: 1.8.3 # CRI plugin, # Supported runtime: docker, containerd. cri_plugin: docker # CNI plugin, # Supported network: flannel, calico, canal, weave or router. network: calico pod_network_cidr: 10.244.0.0/16 # Kubernetes cluster network cluster_subnet: 10.96.0 kubernetes_service_ip: &quot;{{ cluster_subnet }}.1&quot; service_ip_range: &quot;{{ cluster_subnet }}.0/12&quot; service_node_port_range: 30000-32767 # apiserver lb 與 vip lb_vip_address: 172.20.3.90 lb_secure_port: 6443 lb_api_url: &quot;https://{{ lb_vip_address }}:{{ lb_secure_port }}&quot; # 若有內部 registry 則需要設定 insecure_registrys: # - &quot;gcr.io&quot; # Core addons (Strongly recommend) kube_dns: true dns_name: cluster.local # cluster dns name dns_ip: &quot;{{ cluster_subnet }}.10&quot; kube_proxy: true kube_proxy_mode: iptables # &quot;ipvs(1.8+)&quot;, &quot;iptables&quot; or &quot;userspace&quot;. # Extra addons kube_dashboard: true # Kubenetes dasobhard console. kube_logging: false # EFK stack for Kubernetes kube_monitoring: true # Grafana + Infuxdb + Heapster monitoring # Ingress controller ingress: true ingress_type: nginx # &#39;nginx&#39;, &#39;haproxy&#39;, &#39;traefik&#39; 確認group_vars/all.yml完成後，透過 ansible ping 來檢查叢集狀態： $ ansible all -m ping 172.20.3.91 | SUCCESS =&gt; { &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot; } ... 接著就可以透過以下指令進行部署叢集： $ ansible-playbook cluster.yml 執行後需要等一點時間，當完成後就可以進入任何一台 Master 進行操作： $ kubectl get node NAME STATUS AGE master1 Ready,master 3m master2 Ready,master 3m master3 Ready,master 3m node1 Ready 1m node2 Ready 1m node3 Ready 1m node4 Ready 1m node5 Ready 1m 接著就可以部署 Addons 了，透過以下方式進行： $ ansible-playbook addons.yml 驗證叢集當完成上述步驟後，就可以在任一台master節點進行操作 Kubernetes： $ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE ... kubernetes-dashboard-1765530275-rxbkw 1/1 Running 0 1m 確認都是Running後，就可以進入 Dashboard。 接著透過 Etcd 來查看目前 Leader 狀態： $ export CA=&quot;/etc/etcd/ssl&quot; $ ETCDCTL_API=3 etcdctl \\ --cacert=${CA}/etcd-ca.pem \\ --cert=${CA}/etcd.pem \\ --key=${CA}/etcd-key.pem \\ --endpoints=&quot;https://172.20.3.91:2379&quot; \\ etcdctl member list 2de3b0eee054a36f: name=master1 peerURLs=http://172.20.3.91:2380 clientURLs=http://172.20.3.91:2379 isLeader=false 75809e2ee8d8d4b4: name=master2 peerURLs=http://172.20.3.92:2380 clientURLs=http://172.20.3.92:2379 isLeader=false af31edd02fc70872: name=master3 peerURLs=http://172.20.3.93:2380 clientURLs=http://172.20.3.93:2379 isLeader=true 重置叢集若想要將整個叢集進行重置的話，可以使用以下方式： $ ansible-playbook reset.yml","tags":[{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"},{"name":"Ansible","slug":"Ansible","permalink":"https://kairen.github.io/tags/Ansible/"}]},{"title":"kube-ansible 快速部署 HA 測試環境","date":"2017-02-17T09:08:54.000Z","path":"2017/02/17/kubernetes/deploy/kube-ansible-test/","text":"kube-ansible 提供自動化部署 Kubernetes High Availability 叢集於虛擬機與實體機上，並且支援部署 Ceph 叢集於 Kubernetes 中提供共享式儲存系統給 Pod 應用程式使用。該專案最主要是想要快速建立測試環境來進行 Kubernetes 練習與驗證。 kube-ansible 提供了以下幾項功能： Kubernetes 1.8.0+. Ceph on Kubernetes cluster. Common addons. 而在 OS 部分以支援Ubuntu 16.x及CentOS 7.x的虛擬機與實體機部署。未來會以 Python 工具形式來提供使用。 快速開始kube-ansible 支援了 Vagrant 腳本來快速提供 VirtualBox 環境，若想單一主機模擬 Kubernetes 叢集的話，主機需要安裝以下軟體工具： Vagrant &gt;= 1.7.0 VirtualBox &gt;= 5.0.0 當主機確認安裝完成後，即可透過 Git 下載最新版本程式，並使用setup-vagrant腳本： $ git clone &quot;https://github.com/kairen/kube-ansible.git&quot; $ cd kube-ansible $ ./tools/setup -h Usage : setup-vagrant [options] -b|--boss Number of master. -w|--worker Number of worker. -c|--cpu Number of cores per vm. -m|--memory Memory size per vm. -p|--provider Virtual machine provider(virtualbox, libvirt). -o|--os-image Virtual machine operation system(ubuntu16, centos7). -i|--interface Network bind interface. -n|--network Container Network plugin. -f|--force Force deployment. --combine-master Combine number of worker into masters. --combine-etcd Combine number of worker into etcds. 這邊執行以下指令來建立三台 Master 與三台 Node 的環境： $ ./tools/setup -m 2048 -n calico -i eth1 Cluster Size: 1 master, 2 worker. VM Size: 1 vCPU, 2048 MB VM Info: ubuntu16, virtualbox CNI: calico, Binding iface: eth1 Start deploying?(y): y 執行後需要等一點時間，當完成後就可以進入任何一台 Master 進行操作： $ kubectl -n kube-system get po NAME READY STATUS RESTARTS AGE calico-node-657hv 2/2 Running 0 57s calico-node-gmd8b 2/2 Running 0 57s calico-node-w7nj8 2/2 Running 0 57s calico-policy-controller-55dfcd9c69-t8s8z 1/1 Running 0 57s haproxy-master1 1/1 Running 0 22s haproxy-node2 1/1 Running 0 1m keepalived-master1 1/1 Running 0 30s keepalived-node2 1/1 Running 0 1m kube-apiserver-master1 1/1 Running 0 23s kube-apiserver-node2 1/1 Running 0 1m kube-controller-manager-master1 1/1 Running 0 17s kube-controller-manager-node2 1/1 Running 0 1m kube-dns-6cb549f55f-8mgsd 3/3 Running 0 46s kube-proxy-l54d7 1/1 Running 0 1m kube-proxy-rm4nn 1/1 Running 0 1m kube-proxy-tvfs7 1/1 Running 0 1m kube-scheduler-master1 1/1 Running 0 39s kube-scheduler-node2 1/1 Running 0 1m 這樣一個 HA 叢集就部署完成了，可以試著將一台 Master 關閉來驗證可靠性，若 Master 是三台的話，即表示可容忍最多一台故障。 簡單部署 Nginx 服務當完成部署後，可以透過簡單的應用程式部署來驗證系統是否正常運作： $ cat &lt;&lt;EOF &gt; deploy.yml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx spec: replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 EOF $ kubectl create -f deploy.yml $ kubectl get po NAME READY STATUS RESTARTS AGE nginx-4087004473-g6635 1/1 Running 0 15s 然後透過建置 Service 來提供外部存取 Nginx HTTP 伺服器服務： $ cat &lt;&lt;EOF &gt; service.yml apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: NodePort ports: - port: 80 targetPort: 80 protocol: TCP nodePort: 30000 selector: app: nginx EOF $ kubectl create -f service.yml $ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 192.160.0.1 &lt;none&gt; 443/TCP 15m nginx-service 192.173.165.220 &lt;nodes&gt; 80:30000/TCP 11s 由於範例使用 NodePort 的類型，所以任何一台節點都可以透過 TCP 30000 Port 來存取服務，包含 VIP 172.16.35.9 也可以存取。 最後，我們可以關閉 master1 來測試是否有 HA 效果。","tags":[{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"},{"name":"Ansible","slug":"Ansible","permalink":"https://kairen.github.io/tags/Ansible/"}]},{"title":"Kubernetes v1.6.x 全手動苦工安裝教學","date":"2016-12-16T09:08:54.000Z","path":"2016/12/16/kubernetes/deploy/manual-v1.6/","text":"Kubernetes 提供了許多雲端平台與作業系統的安裝方式，本章將以全手動安裝方式來部署，主要是學習與了解 Kubernetes 建置流程。若想要瞭解更多平台的部署可以參考 Picking the Right Solution來選擇自己最喜歡的方式。 本次安裝版本為： Kubernetes v1.6.4 Etcd v3.1.6 Flannel v0.7.1 Docker v17.05.0-ce 預先準備資訊本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統可採用Ubuntu 16.x與CentOS 7.x： IP Address Role CPU Memory 172.16.35.12 master 1 2G 172.16.35.10 node1 1 2G 172.16.35.11 node2 1 2G 這邊 master 為主要控制節點，node 為應用程式工作節點。 首先安裝前要確認以下幾項都已將準備完成： 所有節點彼此網路互通，並且不需要 SSH 密碼即可登入。 所有防火牆與 SELinux 已關閉。如 CentOS： $ systemctl stop firewalld &amp;&amp; systemctl disable firewalld $ setenforce 0 所有節點需要設定/etc/host解析到所有主機。 所有節點需要安裝Docker或rtk引擎。這邊採用Docker來當作容器引擎，安裝方式如下： $ curl -fsSL &quot;https://get.docker.com/&quot; | sh 不管是在 Ubuntu 或 CentOS 都只需要執行該指令就會自動安裝最新版 Docker。CentOS 安裝完成後，需要再執行以下指令： $ systemctl enable docker &amp;&amp; systemctl start docker Etcd 安裝與設定在開始安裝 Kubernetes 之前，需要先將一些必要系統建置完成，其中 Etcd 就是 Kubernetes 最為需要的一環，Kubernetes 會將部分資訊儲存於 Etcd 上，來提供給其他節點索取，以確保整個叢集的狀態。 首先在master節點下載 Etcd，並解壓縮放到 /opt 底下與安裝： $ cd /opt $ wget -qO- &quot;https://github.com/coreos/etcd/releases/download/v3.1.6/etcd-v3.1.6-linux-amd64.tar.gz&quot; | tar -zx $ mv etcd-v3.1.6-linux-amd64 etcd $ cd etcd/ &amp;&amp; ln etcd /usr/bin/ &amp;&amp; ln etcdctl /usr/bin/ 完成後新建 Etcd Group 與 User，並建立 Etcd 設定檔目錄： $ groupadd etcd $ useradd -c &quot;Etcd user&quot; -g etcd -s /sbin/nologin -r etcd $ mkdir /etc/etcd 新增/etc/etcd/etcd.conf檔案，加入以下內容： $ cat &lt;&lt;EOF &gt; /etc/etcd/etcd.conf ETCD_NAME=master ETCD_DATA_DIR=/var/lib/etcd ETCD_INITIAL_ADVERTISE_PEER_URLS=http://172.16.35.12:2380 ETCD_INITIAL_CLUSTER=master=http://172.16.35.12:2380 ETCD_INITIAL_CLUSTER_STATE=new ETCD_INITIAL_CLUSTER_TOKEN=etcd-k8s-cluster ETCD_LISTEN_PEER_URLS=http://0.0.0.0:2380 ETCD_ADVERTISE_CLIENT_URLS=http://172.16.35.12:2379 ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379 ETCD_PROXY=off EOF P.S. 若與該教學 IP 不同的話，請用自己 IP 取代172.16.35.12。 新增/lib/systemd/system/etcd.service來管理 Etcd，並加入以下內容： $ cat &lt;&lt;EOF &gt; /lib/systemd/system/etcd.service [Unit] Description=Etcd Service After=network.target [Service] Environment=ETCD_DATA_DIR=/var/lib/etcd/default EnvironmentFile=-/etc/etcd/etcd.conf Type=notify User=etcd PermissionsStartOnly=true ExecStart=/usr/bin/etcd Restart=always RestartSec=10 LimitNOFILE=65536 [Install] WantedBy=multi-user.target EOF 建立 var 存放資訊，然後啟動 Etcd 服務: $ mkdir -p /var/lib/etcd &amp;&amp; chown etcd:etcd -R /var/lib/etcd $ systemctl enable etcd.service &amp;&amp; systemctl start etcd.service 透過簡單指令驗證： $ etcdctl cluster-health member 95b428c288413b46 is healthy: got healthy result from http://172.16.35.12:2379 cluster is healthy 接著回到master節點，新增一個/tmp/flannel-config.json檔，並加入以下內容： $ cat &lt;&lt;EOF &gt; /tmp/flannel-config.json { &quot;Network&quot;: &quot;10.244.0.0/16&quot;, &quot;SubnetLen&quot;: 24, &quot;Backend&quot;: { &quot;Type&quot;: &quot;vxlan&quot; } } EOF 然後將 Flannel 網路設定儲存到 etcd 中： $ etcdctl --no-sync set /atomic.io/network/config &lt; /tmp/flannel-config.json $ etcdctl ls /atomic.io/network/ /atomic.io/network/config Flannel 安裝與設定Flannel 是 CoreOS 團隊針對 Kubernetes 設計的一個覆蓋網絡(Overlay Network)工具，其目的在於幫助每一個使用 Kuberentes 的主機擁有一個完整的子網路。 首先在所有節點下載 Flannel，並執行以下步驟。首先解壓縮放到 /opt 底下與安裝： $ cd /opt &amp;&amp; mkdir flannel $ wget -qO- &quot;https://github.com/coreos/flannel/releases/download/v0.7.1/flannel-v0.7.1-linux-amd64.tar.gz&quot; | tar -zxC flannel/ $ cd flannel/ &amp;&amp; ln flanneld /usr/bin/ &amp;&amp; ln mk-docker-opts.sh /usr/bin/ 建立 Docker Drop-in 目錄，並新增flannel.conf檔案： $ mkdir -p /etc/systemd/system/docker.service.d $ cat &lt;&lt;EOF &gt; /etc/systemd/system/docker.service.d/flannel.conf [Service] EnvironmentFile=-/run/flannel/docker EOF 新增/etc/default/flanneld檔案，加入以下內容： $ cat &lt;&lt;EOF &gt; /etc/default/flanneld FLANNEL_ETCD_ENDPOINTS=&quot;http://172.16.35.12:2379&quot; FLANNEL_ETCD_PREFIX=&quot;/atomic.io/network&quot; FLANNEL_OPTIONS=&quot;--iface=enp0s8&quot; EOF FLANNEL_ETCD_ENDPOINTS 請修改成自己的 master IP。FLANNEL_OPTIONS可以依據需求加入，這邊主要指定 flannel 使用的網卡。 新增/lib/systemd/system/flanneld.service來管理 Flannel： $ cat &lt;&lt;EOF &gt; /lib/systemd/system/flanneld.service [Unit] Description=Flanneld Service After=network-online.target Wants=network-online.target After=etcd.service Before=docker.service [Service] Type=notify EnvironmentFile=/etc/default/flanneld ExecStart=/usr/bin/flanneld -etcd-endpoints=\\${FLANNEL_ETCD_ENDPOINTS} -etcd-prefix=\\${FLANNEL_ETCD_PREFIX} \\${FLANNEL_OPTIONS} ExecStartPost=/usr/bin/mk-docker-opts.sh -d /run/flannel/docker Restart=always [Install] WantedBy=multi-user.target RequiredBy=docker.service EOF 之後到每台節點啟動 Flannel: $ systemctl enable flanneld.service &amp;&amp; systemctl start flanneld.service 完成後透過以下指令簡單驗證： $ ip -4 addr show flannel.1 5: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default inet 10.244.11.0/32 scope global flannel.1 valid_lft forever preferred_lft forever 確認有網路後，修改/lib/systemd/system/docker.service檔案以下內容： ExecStartPost=/sbin/iptables -I FORWARD -s 0.0.0.0/0 -j ACCEPT ExecStart=/usr/bin/dockerd -H fd:// $DOCKER_OPTS 若是 CentOS 7 則不需要加入 -H fd://。 重新啟動 Docker 來使用 Flannel： $ systemctl daemon-reload &amp;&amp; systemctl restart docker $ ip -4 a show docker0 4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default inet 10.244.11.1/24 scope global docker0 valid_lft forever preferred_lft forever 最後在任一台節點去 Ping 其他節點的 docker0 網路，若 Ping 的到表示部署沒問題。 Kubernetes Master 安裝與設定Master 是 Kubernetes 的大總管，主要建置API Server、Controller Manager Server與Scheduler來元件管理所有 Node。首先加入取得 Packages 來源並安裝： $ curl -s &quot;https://packages.cloud.google.com/apt/doc/apt-key.gpg&quot; | apt-key add - $ echo &quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&quot; &gt; /etc/apt/sources.list.d/kubernetes.list $ apt-get update &amp;&amp; apt-get install -y kubectl kubelet kubernetes-cni CentOS 7 則使用以下指令安裝： $ cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF $ yum install -y kubelet kubectl kubernetes-cni 然後準備 OpenSSL 的設定檔資訊： $ mkdir -p /etc/kubernetes/pki $ DIR=/etc/kubernetes/pki $ cat &lt;&lt;EOF &gt; ${DIR}/openssl.conf [req] req_extensions = v3_req distinguished_name = req_distinguished_name [req_distinguished_name] [ v3_req ] basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment subjectAltName = @alt_names [alt_names] DNS.1 = kubernetes DNS.2 = kubernetes.default DNS.3 = kubernetes.default.svc DNS.4 = kubernetes.default.svc.cluster.local IP.1 = 192.160.0.1 IP.2 = 172.16.35.12 EOF IP.2 請修改成自己的master IP。細節請參考Cluster TLS using OpenSSL。 建立 OpenSSL Keypairs 與 Certificate： DIR=/etc/kubernetes/pki openssl genrsa -out ${DIR}/ca-key.pem 2048 openssl req -x509 -new -nodes -key ${DIR}/ca-key.pem -days 1000 -out ${DIR}/ca.pem -subj &#39;/CN=kube-ca&#39; openssl genrsa -out ${DIR}/admin-key.pem 2048 openssl req -new -key ${DIR}/admin-key.pem -out ${DIR}/admin.csr -subj &#39;/CN=kube-admin&#39; openssl x509 -req -in ${DIR}/admin.csr -CA ${DIR}/ca.pem -CAkey ${DIR}/ca-key.pem -CAcreateserial -out ${DIR}/admin.pem -days 1000 openssl genrsa -out ${DIR}/apiserver-key.pem 2048 openssl req -new -key ${DIR}/apiserver-key.pem -out ${DIR}/apiserver.csr -subj &#39;/CN=kube-apiserver&#39; -config ${DIR}/openssl.conf openssl x509 -req -in ${DIR}/apiserver.csr -CA ${DIR}/ca.pem -CAkey ${DIR}/ca-key.pem -CAcreateserial -out ${DIR}/apiserver.pem -days 1000 -extensions v3_req -extfile ${DIR}/openssl.conf 細節請參考 Cluster TLS using OpenSSL。 接著下載 Kubernetes 相關檔案至/etc/kubernetes： cd /etc/kubernetes/ URL=&quot;https://kairen.github.io/files/manual/master&quot; wget ${URL}/kube-apiserver.conf -O manifests/kube-apiserver.yml wget ${URL}/kube-controller-manager.conf -O manifests/kube-controller-manager.yml wget ${URL}/kube-scheduler.conf -O manifests/kube-scheduler.yml wget ${URL}/admin.conf -O admin.conf wget ${URL}/kubelet.conf -O kubelet cat &lt;&lt;EOF &gt; /etc/kubernetes/user.csv p@ssw0rd,admin,admin EOF 若IP與教學設定不同的話，請記得修改kube-apiserver.yml、kube-controller-manager.yml、kube-scheduler.yml與admin.yml。 新增/lib/systemd/system/kubelet.service來管理 kubelet： $ cat &lt;&lt;EOF &gt; /lib/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Server After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet EnvironmentFile=-/etc/kubernetes/kubelet ExecStart=/usr/bin/kubelet \\$KUBELET_ADDRESS \\$KUBELET_POD_INFRA_CONTAINER \\ \\$KUBELET_ARGS \\$KUBE_NODE_LABEL \\$KUBE_LOGTOSTDERR \\ \\$KUBE_ALLOW_PRIV \\$KUBELET_NETWORK_ARGS \\ \\$KUBELET_DNS_ARGS Restart=always [Install] WantedBy=multi-user.target EOF /etc/systemd/system/kubelet.service為CentOS 7使用的路徑。 最後建立 var 存放資訊，然後啟動 kubelet 服務: $ mkdir -p /var/lib/kubelet $ systemctl daemon-reload &amp;&amp; systemctl restart kubelet.service 完成後會需要一段時間來下載與啟動元件，可以利用該指令來監看： $ watch -n 1 netstat -ntlp tcp 0 0 127.0.0.1:10248 0.0.0.0:* LISTEN 20613/kubelet tcp 0 0 127.0.0.1:10251 0.0.0.0:* LISTEN 19968/kube-schedule tcp 0 0 127.0.0.1:10252 0.0.0.0:* LISTEN 20815/kube-controll tcp6 0 0 :::8080 :::* LISTEN 20333/kube-apiserve 若看到以上已經被 binding 後，就可以透過瀏覽器存取 API Service，並輸入帳號admin與密碼p@ssw0rd。 透過簡單指令驗證： $ kubectl get node NAME STATUS AGE master Ready,master 1m $ kubectl get po --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system kube-apiserver-master1 1/1 Running 0 3m kube-system kube-controller-manager-master1 1/1 Running 0 2m kube-system kube-scheduler-master1 1/1 Running 0 2m Kubernetes Node 安裝與設定Node 是主要的工作節點，上面將運行許多容器應用。到所有node節點加入取得 Packages 來源，並安裝： $ curl -s &quot;https://packages.cloud.google.com/apt/doc/apt-key.gpg&quot; | apt-key add - $ echo &quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&quot; &gt; /etc/apt/sources.list.d/kubernetes.list $ apt-get update &amp;&amp; apt-get install -y kubelet kubernetes-cni CentOS 7 則使用以下指令安裝： $ cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF $ yum install -y kubelet kubernetes-cni 然後準備 OpenSSL 的設定檔資訊： $ mkdir -p /etc/kubernetes/pki $ DIR=/etc/kubernetes/pki $ cat &lt;&lt;EOF &gt; ${DIR}/openssl.conf [req] req_extensions = v3_req distinguished_name = req_distinguished_name [req_distinguished_name] [ v3_req ] basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment subjectAltName = @alt_names [alt_names] IP.1=172.16.35.10 DNS.1=node1 EOF P.S. 這邊IP.1與DNS.1需要隨機器不同設定。細節請參考 Cluster TLS using OpenSSL。 將master1上的 OpenSSL key 複製到/etc/kubernetes/pki： for file in ca-key.pem ca.pem admin.pem admin-key.pem; do scp /etc/kubernetes/pki/${file} &lt;NODE&gt;:/etc/kubernetes/pki/ done P.S. 該操作在master1執行。並記得修改&lt;NODE&gt;為所有工作節點。 建立 OpenSSL Keypairs 與 Certificate： DIR=/etc/kubernetes/pki openssl genrsa -out ${DIR}/node-key.pem 2048 openssl req -new -key ${DIR}/node-key.pem -out ${DIR}/node.csr -subj &#39;/CN=kube-node&#39; -config ${DIR}/openssl.conf openssl x509 -req -in ${DIR}/node.csr -CA ${DIR}/ca.pem -CAkey ${DIR}/ca-key.pem -CAcreateserial -out ${DIR}/node.pem -days 1000 -extensions v3_req -extfile ${DIR}/openssl.conf 細節請參考 Cluster TLS using OpenSSL。 接著下載 Kubernetes 相關檔案至/etc/kubernetes/： cd /etc/kubernetes/ URL=&quot;https://kairen.github.io/files/manual/node&quot; wget ${URL}/kubelet-user.conf -O kubelet-user.conf wget ${URL}/admin.conf -O admin.conf wget ${URL}/kubelet.conf -O kubelet 若IP與教學設定不同的話，請記得修改kubelet-user.conf與admin.conf。 新增/lib/systemd/system/kubelet.service來管理 kubelet： $ cat &lt;&lt;EOF &gt; /lib/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Server After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet EnvironmentFile=-/etc/kubernetes/kubelet ExecStart=/usr/bin/kubelet \\$KUBELET_ADDRESS \\$KUBELET_POD_INFRA_CONTAINER \\ \\$KUBELET_ARGS \\$KUBE_LOGTOSTDERR \\ \\$KUBE_ALLOW_PRIV \\$KUBELET_NETWORK_ARGS \\ \\$KUBELET_DNS_ARGS Restart=always [Install] WantedBy=multi-user.target EOF /etc/systemd/system/kubelet.service為CentOS 7使用的路徑。 最後建立 var 存放資訊，然後啟動 kubelet 服務: $ mkdir -p /var/lib/kubelet $ systemctl daemon-reload &amp;&amp; systemctl restart kubelet.service 當所有節點都完成後，回到master透過簡單指令驗證： $ kubectl get node NAME STATUS AGE VERSION master1 Ready,master 17m v1.6.4 node1 Ready 20s v1.6.4 node2 Ready 18s v1.6.4 Kubernetes Addons 部署當環境都建置完成後，就可以進行部署附加元件，首先到master1，並進入/etc/kubernetes/目錄下載 Addon 檔案： cd /etc/kubernetes/ &amp;&amp; mkdir addon URL=&quot;https://kairen.github.io/files/manual/addon&quot; wget ${URL}/kube-proxy.conf -O addon/kube-proxy.yml wget ${URL}/kube-dns.conf -O addon/kube-dns.yml wget ${URL}/kube-dash.conf -O addon/kube-dash.yml wget ${URL}/kube-monitor.conf -O addon/kube-monitor.yml 若IP與教學設定不同的話，請記得修改&lt;YOUR_MASTER_IP&gt;。 $ sed -i &#39;s/172.16.35.12/&lt;YOUR_MASTER_IP&gt;/g&#39; addon/kube-monitor.yml $ sed -i &#39;s/172.16.35.12/&lt;YOUR_MASTER_IP&gt;/g&#39; addon/kube-proxy.yml 接著透過 kubectl 來指定檔案建立附加元件： $ kubectl apply -f addon/ 若想要刪除則將apply改成delete即可。 透過以下指令來驗證部署是否有效： $ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE heapster-v1.2.0-1753406648-wsb3z 1/1 Running 0 2m influxdb-grafana-42195489-vtmnl 2/2 Running 0 2m kube-apiserver-master1 1/1 Running 0 33m kube-controller-manager-master1 1/1 Running 0 33m kube-dns-3701766129-0p28b 3/3 Running 0 2m kube-proxy-amd64-44rft 1/1 Running 0 2m kube-proxy-amd64-fz77b 1/1 Running 0 2m kube-proxy-amd64-gqq2p 1/1 Running 0 2m kube-scheduler-master1 1/1 Running 0 33m kubernetes-dashboard-210558060-zw814 1/1 Running 2 2m 確定都啟動後，可以開啟 https://172.16.35.12:6443/ui 來查看。 簡單部署 Nginx 服務Kubernetes 可以選擇使用指令直接建立應用程式與服務，或者撰寫 YAML 與 JSON 檔案來描述部署應用程式的配置，以下將建立一個簡單的 Nginx 服務： $ kubectl run nginx --image=nginx --replicas=1 --port=80 $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-158599303-k7cbt 1/1 Running 0 14s 10.244.24.3 node1 完成後要接著建立 svc(Service)，來提供外部網路存取應用程式，使用以下指令建立： $ kubectl expose deploy nginx --port=80 --type=LoadBalancer --external-ip=172.16.35.12 $ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kubernetes 192.160.0.1 &lt;none&gt; 443/TCP 2h svc/nginx 192.160.57.181 ,172.16.35.12 80:32054/TCP 21s 這邊type可以選擇 NodePort 與 LoadBalancer。另外需隨機器 IP 不同而修改 external-ip。 確認沒問題後即可在瀏覽器存取 http://172.16.35.12/。 擴展服務數量若叢集node節點增加了，而想讓 Nginx 服務提供可靠性的話，可以透過以下方式來擴展服務的副本： $ kubectl scale deploy nginx --replicas=2 $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-158599303-0h9lr 1/1 Running 0 25s 10.244.100.5 node2 nginx-158599303-k7cbt 1/1 Running 0 1m 10.244.24.3 node1","tags":[{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"}]},{"title":"Ceph 使用 SPDK 加速 NVMe SSD","date":"2016-12-03T09:08:54.000Z","path":"2016/12/03/ceph/ceph-spdk/","text":"SPDK(Storage Performance Development Kit) 是 Intel 釋出的儲存效能開發工具，主要提供一套撰寫高效能、可擴展與 User-mode 的儲存應用程式工具與函式庫，而中國公司 XSKY 藉由該開發套件來加速 Ceph 在 NVMe SSD 的效能。 首先進入 root，並 clone 專案到 local： $ sudo su - $ git clone http://github.com/ceph/ceph $ cd ceph 編輯CMakeLists.txt檔案，修改以下內容： option(WITH_SPDK &quot;Enable SPDK&quot; ON) 接著安裝一些相依套件與函式庫： $ ./install-deps.sh $ sudo apt-get install -y libpciaccess-dev 接著需要在環境安裝 DPDK 開發套件，首先進入 src 底下的 dpdk 目錄，編輯config/common_linuxapp檔案修改以下內容： CONFIG_RTE_BUILD_SHARED_LIB= 完成後建置與安裝 DPDK： $ make config T=x86_64-native-linuxapp-gcc $ make &amp;&amp; make install 接著回到 ceph root 目錄進行建構 Ceph 準備，透過以下指令進行： $ ./do_cmake.sh .... -- Configuring done -- Generating done -- Build files have been written to: /root/ceph/build + cat + echo 40000 + echo done. done. 確認上面無誤後就可以進行 compile 包含 SPDK 的 Ceph： $ cd build $ make -j2 完成後就可以執行 test cluster，首先建構 vstart 程式： $ make vstart $ ../src/vstart.sh -d -n -x -l $ ./bin/ceph -s 若要關閉則使用以下方式： $ ../src/stop.sh","tags":[{"name":"Ceph","slug":"Ceph","permalink":"https://kairen.github.io/tags/Ceph/"},{"name":"Storage","slug":"Storage","permalink":"https://kairen.github.io/tags/Storage/"},{"name":"Distribution System","slug":"Distribution-System","permalink":"https://kairen.github.io/tags/Distribution-System/"},{"name":"SPDK","slug":"SPDK","permalink":"https://kairen.github.io/tags/SPDK/"}]},{"title":"Using bluestore in Kraken","date":"2016-11-28T09:08:54.000Z","path":"2016/11/28/ceph/deploy/ceph-deploy-bluestore/","text":"本篇說明如何安裝 Kraken 版本的 Ceph，並將 objectstore backend 修改成 Bluestore，過程包含建立 RBD 等操作。 硬體規格說明本安裝由於實體機器數量受到限制，故只進行一台 MON 與兩台 OSD，而 OSD 數量則總共兩顆，硬體規格如下所示： Role RAM CPUs Disk IP Address mon1(deploy) 4 GB 4 core 500 GB 172.16.1.200 osd1 16 GB 8 core 2 TB 172.16.1.201 osd2 16 GB 8 core 2 TB 172.16.1.202 osd3 16 GB 8 core 2 TB 172.16.1.203 作業系統採用Ubuntu 16.04 LTS Server，Kernel 版本為Linux 4.4.0-31-generic。 事前準備在開始部署 Ceph 叢集之前，我們需要在每個節點做一些基本的準備，來確保叢集安裝的過程是流暢的，本次安裝會擁有四台節點。 首先在每一台節點新增以下內容到/etc/hosts： 127.0.0.1 localhost 172.16.1.200 mon1 172.16.1.201 osd1 172.16.1.202 osd2 172.16.1.203 osd3 然後設定各節點 sudo 指令的權限，使之不用輸入密碼(若使用 root 則忽略)： $ echo &quot;ubuntu ALL = (root) NOPASSWD:ALL&quot; | \\ sudo tee /etc/sudoers.d/ubuntu &amp;&amp; sudo chmod 440 /etc/sudoers.d/ubuntu 接著在設定deploy節點能夠以無密碼方式進行 SSH 登入其他節點，請依照以下執行： $ ssh-keygen -t rsa $ ssh-copy-id mon1 $ ssh-copy-id osd1 ... 若不同節點之間使用不同 User 進行 SSH 部署的話，可以設定 ~/.ssh/config 之後在deploy節點安裝部署工具，首先使用 apt-get 來進行安裝基本相依套件，再透過 pypi 進行安裝 ceph-deploy 工具： $ sudo apt-get install -y python-pip $ sudo pip install -U ceph-deploy 節點部署首先建立一個名稱為 local 的目錄，並進到目錄底下： $ sudo mkdir local &amp;&amp; cd local 接著透過 ceph-deploy 在各節點安裝 ceph： $ ceph-deploy install --release kraken mon1 osd1 osd2 osd3 完成後建立 Monitor 節點資訊到 ceph.conf 中： $ ceph-deploy new mon1 &lt;other_mons&gt; 接著編輯目錄底下的 ceph.conf，並加入以下內容： [global] ... rbd_default_features = 3 osd pool default size = 3 osd pool default min size = 1 public network = 172.16.1.0/24 cluster network = 172.16.1.0/24 filestore_xattr_use_omap = true enable experimental unrecoverable data corrupting features = bluestore rocksdb bluestore fsck on mount = true bluestore block db size = 134217728 bluestore block wal size = 268435456 bluestore block size = 322122547200 osd objectstore = bluestore [osd] bluestore = true 若確認沒問題，即可透過以下指令初始化 mon： $ ceph-deploy mon create-initial 上述沒有問題後，就可以開始部署實際作為儲存的 OSD 節點，我們可以透過以下指令進行： $ ceph-deploy osd prepare --bluestore osd1:&lt;device&gt; 系統驗證叢集檢查首先要驗證環境是否有部署成功，可以透過 ceph 提供的基本指令做檢查： $ ceph -v ceph version v11.0.2 (697fe64f9f106252c49a2c4fe4d79aea29363be7) $ ceph -s cluster 6da24ae5-755f-4077-bfa0-78681dfc6bde health HEALTH_OK monmap e1: 1 mons at {r-mon00=172.16.1.200:6789/0} election epoch 7, quorum 0 mon1 mgr no daemons active osdmap e256: 3 osds: 3 up, 3 in flags sortbitwise,require_jewel_osds pgmap v920162: 128 pgs, 1 pools, 6091 MB data, 1580 objects 12194 MB used, 588 GB / 600 GB avail 128 active+clean 另外也可以用 osd 指令來查看部屬的 osd 資訊： $ ceph osd tree ID WEIGHT TYPE NAME UP/DOWN REWEIGHT PRIMARY-AFFINITY -1 0.58618 root default -2 0.29309 host osd1 0 0.29309 osd.0 up 1.00000 1.00000 -3 0.29309 host osd2 1 0.29309 osd.1 up 1.00000 1.00000 -4 0.29309 host osd3 1 0.29309 osd.2 up 1.00000 1.00000 RBD 建立本節說明在 Kraken 版本建立 RBD 來進行使用，在預設部署起來的叢集下會存在一個儲存池 rbd，因此可以省略建立新的儲存池。 首先透過以下指令建立一個區塊裝置映像檔： $ rbd create rbd/bd -s 50G 接著透過 info 指令查看區塊裝置映像檔資訊： $ rbd info rbd/bd rbd image &#39;bd&#39;: size 51200 MB in 12800 objects order 22 (4096 kB objects) block_name_prefix: rbd_data.102d474b0dc51 format: 2 features: layering, striping flags: stripe unit: 4096 kB stripe count: 1 P.S. 這邊由於 Kernel 版本問題有些特性無法支援，因此在 conf 檔只設定使用 layering, striping。 P.S. 若預設未修改 feature 設定的話，可以透過以下指令修改: $ rbd feature disable rbd/bd &lt;feature_name&gt; 以下為目前支援的特性： 屬性名稱 說明 Bit Code layering 支援分層 1 striping 支援串連(v2) 2 exclusive-lock 支援互斥鎖定 4 object-map 支援物件映射(相依於 exclusive-lock ) 8 fast-diff 支援快速計算差異(相依於 object-map ) 16 deep-flatten 支援快照扁平化操作 32 journaling 支援紀錄 I/O 操作(相依於 exclusive-lock ) 64 接著就可以透過 Linux mkfs 指令來格式化 rbd： $ sudo mkfs.ext4 /dev/rbd0 $ sudo mount /dev/rbd0 /mnt 最後透過 dd 指令測試 rbd 寫入效能： $ dd if=/dev/zero of=/mnt/test bs=4096 count=4000000 4000000+0 records in 4000000+0 records out 16384000000 bytes (16 GB) copied, 119.947 s, 137 MB/s 另外有些需求為了測試 feature，卻又礙於 Kernel 不支援等問題，而造成無法 Map 時，可以透過 rbd-nbd 來進行 Map，安裝跟使用方式如下： $ sudo apt-get install -y rbd-nbd $ sudo rbd-nbd map rbd/bd /dev/nbd0 P.S. 在新版的 ceph 已經有內建 rbd nbd，參考 rbd - manage command。 最後透過 dd 指令測試 nbd 寫入效能： $ dd if=/dev/zero of=./mnt-nbd/test bs=4096 count=4000000 4000000+0 records in 4000000+0 records out 16384000000 bytes (16 GB) copied, 168.201 s, 97.4 MB/s","tags":[{"name":"Ceph","slug":"Ceph","permalink":"https://kairen.github.io/tags/Ceph/"},{"name":"Storage","slug":"Storage","permalink":"https://kairen.github.io/tags/Storage/"},{"name":"BlueStore","slug":"BlueStore","permalink":"https://kairen.github.io/tags/BlueStore/"}]},{"title":"簡單部署 Docker Swarm 測試叢集","date":"2016-11-16T09:08:54.000Z","path":"2016/11/16/container/docker-swarm/","text":"Docker Swarm 是 Docker 公司的 Docker 編配引擎，最早是在 2014 年 12 月發佈。Docker Swarm 目的即管理多台節點的 Docker 上應用程式與節點資源的排程等，並提供標準的 Docker API 介面當作前端存取入口，因此可以跟現有 Docker 工具與函式庫進行整合，本篇將介紹簡單的建立 Swarm cluster。 Docker Swarm 具備了以下幾個特性： Docker engine 原生支援。(Docker 1.12+)。 去中心化設計。 宣告式服務模型(Declarative Service Model)。 服務可擴展與容錯。 可協調預期狀態與實際狀態的一致性。 多種網路支援。 提供服務發現、負載平衡與安全策略。 支援滾動升級(Rolling Update)。 基本架構Docker Swarm 具備基本叢集功能，能讓多個 Docker 組合成一個群組，來提供容器服務。Docker 採用標準 Docker API 來管理容器的生命週期，而 Swarm 最主要核心是處理容器如何選擇一台主機來啟動容器這件事。以下為 Docker Swarm 架構： Docker Swarm 一般分為兩個角色Manager與Worker，兩者主要工作如下： Manager: 主要負責排程 Task，Task 可以表示為 Swarm 節點中的 Node 上啟動的容器。同時還負責編配容器與叢集管理功能，簡單說就是 Manager 具備管理 Node 的工作，除了以上外，Manager 還會維護叢集狀態。另外 Manager 也具備 Worker 的功能，當然也可以設定只做管理 Node 的職務。 Worker: Worker 主要接收來自 Manager 的 Task 指派，並依據指派內容啟動 Docker 容器服務，並在完成後向 Manager 匯報 Task 執行狀態。 預先準備資訊本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統可採用Ubuntu 16.x與CentOS 7.x： IP Address Role CPU Memory 172.16.35.12 manager 1 2G 172.16.35.10 node1 1 2G 172.16.35.11 node2 1 2G 這邊 Manager 為主要控制節點，node 為應用程式工作節點。 首先安裝前要確認以下幾項都已將準備完成： 所有節點彼此網路互通，並且不需要 SSH 密碼即可登入。 所有防火牆與 SELinux 已關閉。如 CentOS： $ systemctl stop firewalld &amp;&amp; systemctl disable firewalld $ setenforce 0 所有節點需要設定/etc/host解析到所有主機。 所有節點需要安裝Docker引擎，安裝方式如下： $ curl -fsSL &quot;https://get.docker.com/&quot; | sh 不管是在 Ubuntu 或 CentOS 都只需要執行該指令就會自動安裝最新版 Docker。CentOS 安裝完成後，需要再執行以下指令： $ systemctl enable docker &amp;&amp; systemctl start docker Manager 節點建置當我們完成安裝 Docker Engine 後，就可以透過 Docker 指令來初始化 Manager 節點： $ docker swarm init --advertise-addr 172.16.35.12 Swarm initialized: current node (olluuvvz340ze64zhjpw03uke) is now a manager. To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-0q0ohnexs40lb9z4kmvqb6zcrmp22hul9tmh6zpfztxzv5cv61-73yubitun1ufm0yhwx7h38p85 172.16.35.12:2377 To add a manager to this swarm, run &#39;docker swarm join-token manager&#39; and follow the instructions. 當看到上述內容，表示 Manager 初始化完成，這時候可以透過以下指令檢查： $ docker info $ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS olluuvvz340ze64zhjpw03uke * manager Ready Active Leader 接著建立 Docker swarm network 來提供容器跨節點的溝通： # Deploy network $ docker network create --driver=overlay --attachable cnblogs # Docker flow proxy network $ docker network create --driver overlay proxy 檢查 Docker 網路狀態： $ docker network ls | grep swarm NETWORK ID NAME DRIVER SCOPE 57nq0rux7akh cnblogs overlay swarm ihyg6uixeiov ingress overlay swarm b8vqturisod8 proxy overlay swarm Worker 節點建置完成 Manager 初始化後，就可以透過以下指令來將節點加入叢集： $ docker swarm join --token SWMTKN-1-0q0ohnexs40lb9z4kmvqb6zcrmp22hul9tmh6zpfztxzv5cv61-73yubitun1ufm0yhwx7h38p85 172.16.35.12:2377 This node joined a swarm as a worker. P.S. 其他節點一樣請用上述指令加入。 在Manager節點，查看節點狀態： $ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS cwkta4o37daxed3otrqab9zdq node2 Ready Active olluuvvz340ze64zhjpw03uke * manager Ready Active Leader sfs49249kv8mad2qzr4ev4fy0 node1 Ready Active (option)將節點改為 Manager： $ docker node promote &lt;HOSTNAME&gt; 另外降級為docker node demote &lt;HOSTNAME&gt;。 透過指令建立簡單服務要建立 Docker 服務，可以使用docker service指令來達成，如下指令： $ docker service create --replicas 1 --name ping alpine ping 8.8.8.8 $ docker service logs ping ping.1.auqefe3iq9yk@node2 | PING 8.8.8.8 (8.8.8.8): 56 data bytes ping.1.auqefe3iq9yk@node2 | 64 bytes from 8.8.8.8: seq=0 ttl=61 time=7.042 ms ping.1.auqefe3iq9yk@node2 | 64 bytes from 8.8.8.8: seq=1 ttl=61 time=7.029 ms ping.1.auqefe3iq9yk@node2 | 64 bytes from 8.8.8.8: seq=2 ttl=61 time=7.668 m ... 建立兩份副本數的應用，如以下指令： $ docker service create --replicas 2 --name redis redis $ docker service ps redis ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS ngtegx9vk4gu redis.1 redis:latest node1 Running Running 43 seconds ago n95vu3dzewu7 redis.2 redis:latest manager Running Running 44 seconds ago 完成後，想要刪除可以使用以下指令： $ docker service rm ping $ docker service rm redis 部署簡單的 Stack這邊利用簡單範例來部署應用程式於 Swarm 叢集中，首先新增stack.yml檔案，並加入以下內容： version: &#39;3.2&#39; services: api: image: open-api:latest deploy: replicas: 2 update_config: delay: 5s labels: - com.df.notify=true - com.df.distribute=true - com.df.serviceDomain=api.cnblogs.com - com.df.port=80 networks: - cnblogs - proxy networks: cnblogs: external: true proxy: external: true 完成後，透過以下指令來進行部署： $ docker stack deploy -c stack.yml openapi","tags":[{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"Docker Swarm","slug":"Docker-Swarm","permalink":"https://kairen.github.io/tags/Docker-Swarm/"}]},{"title":"Minikube 部署 Local 測試環境","date":"2016-10-23T09:08:54.000Z","path":"2016/10/23/kubernetes/deploy/minikube/","text":"Minikube 是提供簡單與快速部署本地 Kubernetes 環境的工具，透過執行虛擬機來執行單節點 Kubernetes 叢集，以便開發者使用 Kubernetes 與開發用。 本環境安裝資訊： Minikube v0.22.3 Kubernetes v1.7.5 事前準備安裝前需要確認叢集滿足以下幾點： 安裝 xhyve driver, VirtualBox 或 VMware Fusion。 安裝 kubectl 工具。 以下為Mac OS X下載方式： $ curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.7.5/bin/darwin/amd64/kubectl $ chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/ 如果是Linux則使用以下方式： $ curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.7.5/bin/linux/amd64/kubectl $ chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/ 快速開始Minikube 支援了許多作業系統，若是 OS X 的開發者，可以透過該指令安裝： $ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.22.3/minikube-darwin-amd64 $ chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/ 若是 Linux 開發者則利用以下指令安裝： $ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.22.3/minikube-linux-amd64 $ chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/ 下載完成後，就可以透過以下指令建立環境： $ minikube get-k8s-versions The following Kubernetes versions are available: - v1.8.0 - v1.7.5 ... $ minikube start Starting local Kubernetes v1.7.5 cluster... Starting VM... Getting VM IP address... Moving files into cluster... Setting up certs... Connecting to cluster... Setting up kubeconfig... Starting cluster components... Kubectl is now configured to use the cluster. 看到上述資訊表示已完成啟動 Kubernetes 虛擬機，這時候可以透過 kubectl 來查看資訊： $ kubectl get node NAME STATUS AGE VERSION minikube Ready 2m v1.7.5 $ kubectl get po,svc -n kube-system NAME READY STATUS RESTARTS AGE po/default-http-backend-2jk83 1/1 Running 0 2m po/kube-addon-manager-minikube 1/1 Running 0 2m po/kube-dns-1326421443-8br9x 3/3 Running 0 2m po/kubernetes-dashboard-mdc9f 1/1 Running 0 2m po/nginx-ingress-controller-dspc0 1/1 Running 0 2m NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/default-http-backend 10.0.0.237 &lt;nodes&gt; 80:30001/TCP 2m svc/kube-dns 10.0.0.10 &lt;none&gt; 53/UDP,53/TCP 2m svc/kubernetes-dashboard 10.0.0.222 &lt;nodes&gt; 80:30000/TCP 2m 新版本 Minikube 預設會自動啟動上述 Addons。這時可以透過瀏覽器進入 Dashboard。 想啟動 Extra addons 的話，可以透過以下指令來達成： $ minikube addons list $ minikube addons enable heapster 若要移除則使用minikube addons disable heapster指令。 取得虛擬機裡面的 Docker env： $ eval $(minikube docker-env) $ docker version Client: Version: 17.10.0-ce API version: 1.23 Go version: go1.8.3 Git commit: f4ffd25 Built: Tue Oct 17 19:00:43 2017 OS/Arch: darwin/amd64 Server: Version: 1.12.6 API version: 1.24 (minimum version ) Go version: go1.6.4 Git commit: 78d1802 Built: Wed Jan 11 00:23:16 2017 OS/Arch: linux/amd64 Experimental: false (option)若想要移除與刪除虛擬機的話，可以透過以下指令進行： $ minikube stop $ minikube delete 執行簡單測試應用程式這邊利用 echoserver 來測試 Minikube 功能，首先透過以下指令啟動一個 Deployment： $ kubectl run hello-minikube --image=gcr.io/google_containers/echoserver:1.4 --port=8080 $ kubectl get deploy,po NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/hello-minikube 1 1 1 1 11m NAME READY STATUS RESTARTS AGE po/hello-minikube-938614450-31rtv 1/1 Running 0 11m 接著 expose 服務來進行存取： $ kubectl expose deployment hello-minikube --type=NodePort $ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE hello-minikube 10.0.0.164 &lt;nodes&gt; 8080:30371/TCP 4s kubernetes 10.0.0.1 &lt;none&gt; 443/TCP 29m 最後透過 cURL 來存取服務： $ curl $(minikube service hello-minikube --url) CLIENT VALUES: client_address=172.17.0.1 command=GET real path=/ ...","tags":[{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"}]},{"title":"TensorFlow on Docker","date":"2016-10-01T08:23:01.000Z","path":"2016/10/01/tensorflow/tensorflow-docker/","text":"本篇主要整理使用 Docker 來執行 TensorFlow 的一些問題，這邊 Google 官方已經提供了相關的映像檔提供使用，因此會簡單說明安裝過程與需求。 環境準備環境採用 Ubuntu 16.04 Desktop 作業系統，然後顯卡是撿朋友不要的來使用，環境硬體資源如下： 名稱 描述 CPU i7-4790 CPU @ 3.60GHz Memory 32GB GPU GeForce GTX 650 事前準備開始進行 TensorFlow on Docker 之前，需要確認環境已經安裝以下驅動與軟體等。 系統安裝了 Docker Engine： $ curl -fsSL &quot;https://get.docker.com/&quot; | sh $ sudo iptables -P FORWARD ACCEPT 安裝最新版本 NVIDIA Driver 軟體： $ sudo add-apt-repository -y ppa:graphics-drivers/ppa $ sudo apt-get update $ sudo apt-get install -y nvidia-367 $ sudo dpkg -l | grep nvidia-367 ... 375.39-0ubuntu0.16.04.1 .. 編譯與安裝 nvidia-modprobe： $ sudo apt-get install -y m4 $ git clone &quot;https://github.com/NVIDIA/nvidia-modprobe.git&quot; $ cd nvidia-modprobe $ make &amp;&amp; sudo make install $ sudo nvidia-modprobe -u -c=0 安裝 Nvidia Docker Plugin: $ wget -P /tmp &quot;https://github.com/NVIDIA/nvidia-docker/releases/download/v1.0.1/nvidia-docker_1.0.1-1_amd64.deb&quot; $ sudo dpkg -i /tmp/nvidia-docker*.deb &amp;&amp; rm /tmp/nvidia-docker*.deb $ sudo systemctl start nvidia-docker.service $ sudo nvidia-docker run --rm nvidia/cuda nvidia-smi ... +-----------------------------------------------------------------------------+ | NVIDIA-SMI 375.39 Driver Version: 375.39 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 650 Off | 0000:01:00.0 N/A | N/A | | 10% 34C P8 N/A / N/A | 267MiB / 975MiB | N/A Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| 利用 Docker 執行 TensorFlowTensorFlow on Docker 官方已經提供了相關映像檔，這邊透過單一指令就可以取得該映像檔，並啟動提供使用，以下為只有 CPU 的版本： $ docker run -d -p 8888:8888 --name tf-cpu tensorflow/tensorflow $ docker logs tf-cpu ... to login with a token: http://localhost:8888/?token=7ddd6ef31fed5f22696c1003a905782b9219a6ec9a19b97c 這時候就可以登入 Jupyter notebook，這邊登入需要token後面的值。 若要支援 GPU(CUDA) 的容器的話，可以透過以下指令來提供： $ nvidia-docker run -d -p 8888:8888 --name tf-gpu tensorflow/tensorflow:latest-gpu $ docker logs tf-cpu 其他版本可以參考 tags。 利用 Docker 提供 ServingTensorFlow Serving 是靈活、高效能的機器學習模型服務系統，是專門為生產環境而設計的，它可以很簡單部署新的演算法與實驗來提供同樣的架構與 API 進行服務。 首先我們下載官方寫好的 Dockerfile 來進行建置： $ mkdir serving &amp;&amp; cd serving $ wget &quot;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/tools/docker/Dockerfile.devel&quot; $ sed -i &#39;s/BAZEL_VERSION.*0.4.2/BAZEL_VERSION 0.4.5/g&#39; Dockerfile.devel $ docker build --pull -t kyle/serving:0.1.0 -f Dockerfile.devel . 建置完成映像檔後，透過以下指令執行，並在容器內建置 Serving： $ docker run -itd --name=tf-serving kyle/serving:0.1.0 $ docker exec -ti tf-serving bash root@459a89a3cf5a$ git clone --recurse-submodules &quot;https://github.com/tensorflow/serving&quot; root@459a89a3cf5a$ cd serving/tensorflow root@459a89a3cf5a$ ./configure root@459a89a3cf5a$ cd .. &amp;&amp; bazel build -c opt tensorflow_serving/... 當建置完 Serving 後，就可以透過以下指令來確認是否正確： root@459a89a3cf5a$ bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server usage: bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server Flags: --port=8500 int32 port to listen on --enable_batching=false ... 接著使用 Inception v3 模型來提供服務，透過以下步驟來完成： root@459a89a3cf5a$ curl -O &quot;http://download.tensorflow.org/models/image/imagenet/inception-v3-2016-03-01.tar.gz&quot; root@459a89a3cf5a$ tar xzf inception-v3-2016-03-01.tar.gz root@459a89a3cf5a$ ls inception-v3 README.txt checkpoint model.ckpt-157585 root@459a89a3cf5a$ bazel-bin/tensorflow_serving/example/inception_saved_model --checkpoint_dir=inception-v3 --output_dir=inception-export Successfully exported model to inception-export root@459a89a3cf5a$ ls inception-export 1 當完成匯入後離開容器，並 commit 成新版本映像檔： $ docker commit tf-serving kyle/serving-inception:0.1.0 $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE kyle/serving-inception 0.1.0 1d866ff60d38 3 minutes ago 5.55 GB 接著執行剛 commit 的映像檔，並啟動 Serving 服務： $ docker run -it kyle/serving-inception:0.1.0 root@5b9a89eeef5a$ cd serving root@5b9a89eeef5a$ bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9000 --model_name=inception --model_base_path=inception-export &amp;&gt; inception_log &amp; [1] 15 最後透過 inception_client.py 來測試功能： root@5b9a89eeef5a$ curl &quot;https://s-media-cache-ak0.pinimg.com/736x/32/00/3b/32003bd128bebe99cb8c655a9c0f00f5.jpg&quot; --output rabbit.jpg root@5b9a89eeef5a$ bazel-bin/tensorflow_serving/example/inception_client --server=localhost:9000 --image=rabbit.jpg outputs { key: &quot;classes&quot; value { dtype: DT_STRING tensor_shape { dim { size: 1 } dim { size: 5 } } string_val: &quot;hare&quot; string_val: &quot;wood rabbit, cottontail, cottontail rabbit&quot; string_val: &quot;Angora, Angora rabbit&quot; string_val: &quot;mouse, computer mouse&quot; string_val: &quot;gazelle&quot; } } outputs { key: &quot;scores&quot; value { dtype: DT_FLOAT tensor_shape { dim { size: 1 } dim { size: 5 } } float_val: 10.3059120178 float_val: 8.19226741791 float_val: 4.00839996338 float_val: 2.34308481216 float_val: 2.00992465019 } }","tags":[{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://kairen.github.io/tags/TensorFlow/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://kairen.github.io/tags/Machine-Learning/"}]},{"title":"只要用 kubeadm 小朋友都能部署 Kubernetes","date":"2016-09-29T09:08:54.000Z","path":"2016/09/29/kubernetes/deploy/kubeadm/","text":"kubeadm是 Kubernetes 官方推出的部署工具，該工具實作類似 Docker swarm 一樣的部署方式，透過初始化 Master 節點來提供給 Node 快速加入，kubeadm 目前屬於測試環境用階段，但隨著時間推移會越來越多功能被支援，這邊可以看 kubeadm Roadmap 來更進一步知道功能發展狀態。 若想利用 Ansible 安裝的話，可以參考這邊 kubeadm-ansible。 本環境安裝資訊： Kubernetes v1.9.6 Etcd v3 Flannel v0.9.1 Docker v18.02.0-ce 節點資訊本次安裝作業系統採用Ubuntu 16.04 Server，測試環境為 Vagrant with Libvirt： IP Address Role CPU Memory 172.16.35.12 master1 1 2G 172.16.35.10 node1 1 2G 172.16.35.11 node2 1 2G 目前 kubeadm 只支援在Ubuntu 16.04+、CentOS 7與HypriotOS v1.0.1+作業系統上使用。 事前準備安裝前需要確認叢集滿足以下幾點： 所有節點網路可以溝通。 所有節點需要設定 APT Docker Repository： $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - $ sudo add-apt-repository \\ &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable&quot; CentOS 7 EPEL 有支援 Docker Package: $ sudo yum install -y epel-release 所有節點需要設定 APT 與 YUM Kubernetes Repository： $ curl -s &quot;https://packages.cloud.google.com/apt/doc/apt-key.gpg&quot; | sudo apt-key add - $ echo &quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&quot; | sudo tee /etc/apt/sources.list.d/kubernetes.list 若是 CentOS 7 則執行以下方式： $ cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF CentOS 7 要額外確認 SELinux 或 Firewall 關閉。 Kubernetes v1.8+ 要求關閉系統 Swap，若不關閉則需要修改 kubelet 設定參數，這邊可以利用以下指令關閉： $ swapoff -a &amp;&amp; sysctl -w vm.swappiness=0 # 不同機器有差異 $ sed &#39;/swap.img/d&#39; -i /etc/fstab 記得/etc/fstab也要註解掉SWAP掛載。 Kubernetes Master 建立首先更新 APT 來源，並且安裝 Kubernetes 元件與工具： $ export KUBE_VERSION=&quot;1.9.6&quot; $ sudo apt-get update &amp;&amp; sudo apt-get install -y kubelet=${KUBE_VERSION}-00 kubeadm=${KUBE_VERSION}-00 kubectl=${KUBE_VERSION}-00 docker-ce 進行初始化 Master，這邊需要進入root使用者執行以下指令： $ sudo su - $ kubeadm token generate b0f7b8.8d1767876297d85c $ kubeadm init --service-cidr 10.96.0.0/12 \\ --kubernetes-version v${KUBE_VERSION} \\ --pod-network-cidr 10.244.0.0/16 \\ --token b0f7b8.8d1767876297d85c \\ --apiserver-advertise-address 172.16.35.12 # output ... kubeadm join --token b0f7b8.8d1767876297d85c 172.16.35.12:6443 --discovery-token-ca-cert-hash sha256:739d936954a752d44d2f2282dd645083259826f2c24a651608a6ac2081106cd7 當出現如上面資訊後，表示 Master 初始化成功，不過這邊還是一樣透過 kubectl 測試一下： $ mkdir ~/.kube &amp;&amp; cp /etc/kubernetes/admin.conf ~/.kube/config $ kubectl get node NAME STATUS ROLES AGE VERSION master1 NotReady master 4m v1.9.6 當執行正確後要接著部署網路，但要注意一個叢集只能用一種網路，這邊採用 Flannel： $ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml clusterrole &quot;flannel&quot; created clusterrolebinding &quot;flannel&quot; created serviceaccount &quot;flannel&quot; configured configmap &quot;kube-flannel-cfg&quot; configured daemonset &quot;kube-flannel-ds&quot; configured 若參數 --pod-network-cidr=10.244.0.0/16 改變時，在kube-flannel.yml檔案也需修改net-conf.json欄位的 CIDR。 若使用 Virtualbox 的話，請修改kube-flannel.yml中的 command 綁定 iface，如command: [ &quot;/opt/bin/flanneld&quot;, &quot;--ip-masq&quot;, &quot;--kube-subnet-mgr&quot;, &quot;--iface=eth1&quot; ]。 其他 Pod Network 可以參考 Installing a pod network。 確認 Flannel 部署正確： $ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE kube-flannel-ds-3b66l 1/1 Running 0 9s kube-flannel-ds-m6874 1/1 Running 0 9s kube-flannel-ds-vmb38 1/1 Running 0 9s ... $ ip -4 a show flannel.1 5: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default inet 10.244.0.0/32 scope global flannel.1 valid_lft forever preferred_lft forever $ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 10.244.0.0 0.0.0.0 255.255.0.0 U 0 0 0 flannel.1 Kubernetes Node 建立首先更新 APT 來源，並且安裝 Kubernetes 元件與工具： $ export KUBE_VERSION=&quot;1.9.6&quot; $ sudo apt-get update &amp;&amp; sudo apt-get install -y kubelet=${KUBE_VERSION}-00 kubeadm=${KUBE_VERSION}-00 docker-ce 完成後就可以開始加入 Node，這邊需要進入root使用者執行以下指令： $ kubeadm join --token b0f7b8.8d1767876297d85c 172.16.35.12:6443 --discovery-token-ca-cert-hash sha256:739d936954a752d44d2f2282dd645083259826f2c24a651608a6ac2081106cd7 # output ... Run &#39;kubectl get nodes&#39; on the master to see this machine join. 回到master1查看節點狀態： $ kubectl get node NAME STATUS ROLES AGE VERSION master1 Ready master 10m v1.9.6 node1 Ready &lt;none&gt; 9m v1.9.6 node2 Ready &lt;none&gt; 9m v1.9.6 為了多加利用資源這邊透過 taint 來讓 masters 也會被排程執行容器： $ kubectl taint nodes --all node-role.kubernetes.io/master- Add-ons 建立當完成後就可以建立一些 Addons，如 Dashboard。這邊執行以下指令進行建立： $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml Dashboard 1.7.x 版本有做一些改變，會用到 SSL Cert，可參考這邊 Installation。 確認沒問題後，透過 kubectl 查看： kubectl get svc --namespace=kube-system NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns 10.96.0.10 &lt;none&gt; 53/UDP,53/TCP 36m kubernetes-dashboard 10.111.162.184 &lt;nodes&gt; 80:32546/TCP 33s 最後就可以存取 Kube Dashboard。 在 1.7 版本以後的 Dashboard 將不再提供所有權限，因此需要建立一個 service account 來綁定 cluster-admin role： $ kubectl -n kube-system create sa dashboard $ kubectl create clusterrolebinding dashboard --clusterrole cluster-admin --serviceaccount=kube-system:dashboard $ kubectl -n kube-system get sa dashboard -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: 2017-11-27T17:06:41Z name: dashboard namespace: kube-system resourceVersion: &quot;69076&quot; selfLink: /api/v1/namespaces/kube-system/serviceaccounts/dashboard uid: 56b880bf-d395-11e7-9528-448a5ba4bd34 secrets: - name: dashboard-token-vg52j $ kubectl -n kube-system describe secrets dashboard-token-vg52j ... token: eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtdG9rZW4tdmc1MmoiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNTZiODgwYmYtZDM5NS0xMWU3LTk1MjgtNDQ4YTViYTRiZDM0Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZCJ9.bVRECfNS4NDmWAFWxGbAi1n9SfQ-TMNafPtF70pbp9Kun9RbC3BNR5NjTEuKjwt8nqZ6k3r09UKJ4dpo2lHtr2RTNAfEsoEGtoMlW8X9lg70ccPB0M1KJiz3c7-gpDUaQRIMNwz42db7Q1dN7HLieD6I4lFsHgk9NPUIVKqJ0p6PNTp99pBwvpvnKX72NIiIvgRwC2cnFr3R6WdUEsuVfuWGdF-jXyc6lS7_kOiXp2yh6Ym_YYIr3SsjYK7XUIPHrBqWjF-KXO_AL3J8J_UebtWSGomYvuXXbbAUefbOK4qopqQ6FzRXQs00KrKa8sfqrKMm_x71Kyqq6RbFECsHPA 複製token，然後貼到 Kubernetes dashboard。 簡單部署一個服務這邊利用 Weave 公司提供的服務來驗證系統，透過以下方式建立： $ kubectl create namespace sock-shop $ kubectl apply -n sock-shop -f &quot;https://github.com/microservices-demo/microservices-demo/blob/master/deploy/kubernetes/complete-demo.yaml?raw=true&quot; 接著透過 kubectl 查看資訊： $ kubectl describe svc front-end -n sock-shop $ kubectl get pods -n sock-shop 最後存取 http://172.16.35.12:30001 即可看到服務的 Frontend。 移除節點最後，若要將現有節點移除的話，kubeadm 已經有內建的指令來完成這件事，只要執行以下即可： $ kubeadm reset","tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://kairen.github.io/tags/Ubuntu/"},{"name":"CentOS","slug":"CentOS","permalink":"https://kairen.github.io/tags/CentOS/"}]},{"title":"Go 語言環境安裝","date":"2016-08-19T09:08:54.000Z","path":"2016/08/19/golang/go-install/","text":"Go 語言是 Google 開發的該世代 C 語言，延續 C 語言的一些優點，是一種靜態強刑別、編譯型，且具有並行機制與垃圾回收功能的語言。由於其並行機制讓 Go 在撰寫多執行緒與網路程式都非常容易。值得一提的是 Go 語言的設計者也包含過去設計 C 語言的 Ken Thompson。目前 Go 語言基於 1.x 每半年發布一個版本。 Go 語言安裝Go 語言安裝非常容易，目前已支援多個平台的作業系統，以下針對幾個常見的作業系統進行教學。 P.S. 以下教學皆使用 64 bit 進行安裝。 Linux首先透過網路下載 Go 語言的壓縮檔： $ wget https://storage.googleapis.com/golang/go1.8.linux-amd64.tar.gz 然後將壓縮檔內的資料全部解壓縮到/usr/local底下： $ sudo tar -C /usr/local -xzf go1.8.linux-amd64.tar.gz 之後編輯.bashrc檔案，在最下面加入以下內容： export GOROOT=/usr/local/go export GOPATH=${HOME}/go export PATH=$PATH:$GOPATH/bin:$GOROOT/bin Mac OS XMac OS X 安裝可以透過官方封裝好的檔案來進行安裝，下載 go1.8.darwin-amd64.pkg，然後雙擊進行安裝。 完成後，編輯.bashrc檔案來加入套件的安裝路徑： export PATH=$PATH:/usr/local/go/bin export GOPATH=~/Go export PATH=$PATH:$GOPATH/bin 簡單入門建立目錄hello-go，並新增檔案hello.go： $ mkdir hello-go $ cd hello-go &amp;&amp; touch hello.go 接著編輯hello.go加入以下內容： package main import &quot;fmt&quot; func main() { fmt.Println(&quot;Hello world, GO !&quot;) } 完成後執行以下指令： $ go build $ ./hello Hello world, GO ! 其他 Framework 與網站以下整理相關 Go 語言的套件與不錯網站。 種類 名稱 Web 框架 Beego、Martini、Gorilla、GoCraft、Net/HTTP、Revel、girl、XWeb、go-start、goku、web.go 系統處理框架 apifs、goIRC 影音處理 Gopher-Talkie、Videq Social 框架 ChannelMail.io 參考資訊 Gopher Gala 2015 Finalists Web application frameworks","tags":[{"name":"Linux","slug":"Linux","permalink":"https://kairen.github.io/tags/Linux/"},{"name":"Golang","slug":"Golang","permalink":"https://kairen.github.io/tags/Golang/"},{"name":"OS X","slug":"OS-X","permalink":"https://kairen.github.io/tags/OS-X/"}]},{"title":"簡單部署 DC/OS 於 CentOS 上","date":"2016-08-17T09:08:54.000Z","path":"2016/08/17/data-engineer/dcos-install/","text":"DC/OS(Data Center Operating System，資料中心作業系統)是 Mesosphere 公司開源的系統，該平台提供了諸多巨量資料處理框架與系統的建置，並以分散式系統 Mesos 作為核心，提供系統資源的隔離與調度，使用者可以根據需求與策略來應用系統資源。 而本篇將說明如何透過 UI 與 CLI 進行安裝 DC/OS。 節點配置DC/OS 最低需求要四台主機，以下為本次安裝的硬體設備： Role RAM Disk CPUs IP Address bootstrap 16 GB 記憶體 250 GB 儲存空間 四核處理器 10.0.0.101 master 8 GB 記憶體 250 GB 儲存空間 四核處理器 10.0.0.102 agent-1 8 GB 記憶體 250 GB 儲存空間 四核處理器 10.0.0.103 agent-2 8 GB 記憶體 250 GB 儲存空間 四核處理器 10.0.0.104 請以上節點都分別安裝 RHEL 或者 CentOS 作業系統。並且設定 IP 為靜態固定，編輯/etc/sysconfig/network-scripts/ifcfg-&lt;name&gt;檔案，加入以下內容： ONBOOT=&quot;yes&quot; IPADDR=&quot;10.0.0.104&quot; PREFIX=&quot;24&quot; GATEWAY=&quot;10.0.0.1&quot; DNS1=&quot;8.8.8.8&quot; DNS2=&quot;8.8.8.4&quot; 安裝前準備在開始安裝以前，首先需要在每一台節點將基本環境的軟體更新： $ sudo yum upgrade -y 完成後檢查是否是最新版本，可以透過以下方式查看 Kernel： $ uname -r 3.10.0-327.13.1.el7.x86_64 如果不是以上版本，請執行以下指令： $ sudo yum upgrade --assumeyes --tolerant $ sudo yum update --assumeyes 由於在 CentOS 與 RHEL 預設會開啟防火牆，故要關閉防火牆與開機時自動啟動： $ sudo systemctl stop firewalld &amp;&amp; sudo systemctl disable firewalld 接著安裝一些基本工具軟體： $ sudo yum install -y tar xz unzip curl ipset vim 設定啟動 OverlayFS : $ sudo tee /etc/modules-load.d/overlay.conf &lt;&lt;-&#39;EOF&#39; overlay EOF 設定關閉 SELinux 與設定一些資訊，並重新啟動： $ sudo sed -i s/SELINUX=enforcing/SELINUX=permissive/g /etc/selinux/config &amp;&amp; sudo groupadd nogroup &amp;&amp; sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1 &amp;&amp; sudo sysctl -w net.ipv6.conf.default.disable_ipv6=1 &amp;&amp; sudo reboot 完成重新啟動後，在每一台節點安裝 Docker，首先要取得 Repos，設定以下來讓 yum 可以抓取： $ sudo tee /etc/yum.repos.d/docker.repo &lt;&lt;-&#39;EOF&#39; [dockerrepo] name=Docker Repository baseurl=https://yum.dockerproject.org/repo/main/centos/$releasever/ enabled=1 gpgcheck=1 gpgkey=https://yum.dockerproject.org/gpg EOF 設定 systemd 執行 Docker daemon 於 OverlayFS： $ sudo mkdir -p /etc/systemd/system/docker.service.d &amp;&amp; sudo tee /etc/systemd/system/docker.service.d/override.conf &lt;&lt;- EOF [Service] ExecStart= ExecStart=/usr/bin/docker daemon --storage-driver=overlay -H fd:// EOF 安裝 Docker engine，並啟動 docker 與設定開機啟動： $ sudo yum install --assumeyes --tolerant docker-engine $ sudo systemctl start docker $ sudo systemctl enable docker 這邊可以設定使用者加入 docker 群組： $ sudo gpasswd -a $(whoami) docker 安裝 Bootstrap NodeBootstrap 節點主要提供佈署的功能，可以採用 UI 或 CLI 來進行部署，以下將說明如何透過建置 Bootstrap 來完成 DC/OS 佈署。 GUI 安裝這邊採用 GUI 方式來佈署 DC/OS，首先下載 DC/OS installer： $ wget https://downloads.dcos.io/dcos/EarlyAccess/dcos_generate_config.sh 接著執行啟動 DC/OS UI： $ sudo bash dcos_generate_config.sh --web -v 完成後，即可開啟瀏覽器輸入 bootstrap web 這邊需要輸入 Bootstrap 節點的 SSH 私有金鑰，透過以下方式產生與印出： $ ssh-keygen -t rsa $ cat .ssh/id_rsa 並建立一個腳本檔案ip-detect，並加入以下內容： #!/bin/bash set -o nounset -o errexit IP=&lt;MASTER_IP&gt; echo $(ip route get ${IP} | awk &#39;{print $NF; exit}&#39;) 上傳以上資訊，並填入 Master 與 Agent 的 IP Address。 CLI 安裝DC/OS 除了可以透過 GUI 進行安裝，也可以透過 CLI 的方式來佈署，首先建立目錄genconf： $ mkdir -p dcos_cluster $ cd dcos_cluster $ mkdir -p genconf 然後下載 DC/OS installer 來進行安裝，並且查看所有指令： $ wget https://downloads.dcos.io/dcos/EarlyAccess/dcos_generate_config.sh $ sudo bash dcos_generate_config.sh --help 建立一個腳本檔案genconf/ip-detect，並加入以下內容： #!/bin/bash set -o nounset -o errexit IP=&lt;MASTER_IP&gt; echo $(ip route get ${IP} | awk &#39;{print $NF; exit}&#39;) MASTER_IP為修改成主節點 IP。 接著建立設定檔genconf/config.yaml，並加入以下內容： --- agent_list: - &lt;agent-private-ip-1&gt; - &lt;agent-private-ip-2&gt; bootstrap_url: file:///opt/dcos_install_tmp cluster_name: &quot;DC/OS Cluster&quot; master_discovery: static master_list: - &lt;master-private-ip-1&gt; resolvers: - 8.8.4.4 - 8.8.8.8 ssh_port: 22 ssh_user: &lt;username&gt; &lt;agent-private-ip-1&gt; 修改成 10.0.0.103。 &lt;agent-private-ip-2&gt; 修改成 10.0.0.104。 &lt;master-private-ip-1&gt; 修改成 10.0.0.102。 &lt;username&gt; 修改成 cloud-user。P.S 這邊是用 cloud image。 完成後複製 Bootstrap 節點的 SSH 私有金鑰到目錄底下： $ cp &lt;path-to-key&gt; genconf/ssh_key &amp;&amp; chmod 0600 genconf/ssh_key 透過 DC/OS installer 產生設定資訊： $ sudo bash dcos_generate_config.sh --genconf Extracting image from this script and loading into docker daemon, this step can take a few minutes dcos-genconf.e060aa49ac4ab62d5e-1e14856f55e5d5d07b.tar Running mesosphere/dcos-genconf docker with BUILD_DIR set to /home/centos/genconf ====&gt; EXECUTING CONFIGURATION GENERATION ... 成功的話目錄結構會類似以下： ├── dcos-genconf.14509fe1e7899f4395-3a2b7e03c45cd615da.tar ├── dcos_generate_config.sh └── genconf ├── cluster_packages.json ├── config.yaml ├── ip-detect ├── serve │ ├── bootstrap │ │ ├── 3a2b7e03c45cd615da8dfb1b103943894652cd71.active.json │ │ └── 3a2b7e03c45cd615da8dfb1b103943894652cd71.bootstrap.tar.xz │ ├── bootstrap.latest │ ├── cluster-package-info.json │ ├── dcos_install.sh │ ├── fetch_packages.sh │ └── packages │ ├── dcos-config │ │ └── dcos-config--setup_6dac2d99011d6219b32d9f66cafa9845b7cf6d74.tar.xz │ └── dcos-metadata │ └── dcos-metadata--setup_6dac2d99011d6219b32d9f66cafa9845b7cf6d74.tar.xz ├── ssh_key └── state 若沒問題就可以 prerequisites 佈署階段： $ sudo bash dcos_generate_config.sh --install-prereqs Running mesosphere/dcos-genconf docker with BUILD_DIR set to /home/centos/genconf ====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; EXECUTING INSTALL PREREQUISITES ====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; START install_prereqs 若沒問題就可以執行 preflight 來驗證叢集是否可以安裝： $ sudo bash dcos_generate_config.sh --preflight Running mesosphere/dcos-genconf docker with BUILD_DIR set to /home/centos/genconf ====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; EXECUTING PREFLIGHT ====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; START run_preflight ====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; STAGE preflight 一樣若沒問題就可以執行 deploy 來安裝 DC/OS： $ sudo bash dcos_generate_config.sh --deploy Running mesosphere/dcos-genconf docker with BUILD_DIR set to /home/centos/genconf ====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; EXECUTING DC/OS INSTALLATION ====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; START deploy_master 最後沒問題就可以執行 postflight 來確認服務是否有啟動： $ sudo bash dcos_generate_config.sh --postflight Running mesosphere/dcos-genconf docker with BUILD_DIR set to /home/centos/genconf ====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; EXECUTING POSTFLIGHT ====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; START run_postflight 都完成後就可以查看 Zookpeer 與 DC/OS。","tags":[{"name":"Spark","slug":"Spark","permalink":"https://kairen.github.io/tags/Spark/"},{"name":"DC/OS","slug":"DC-OS","permalink":"https://kairen.github.io/tags/DC-OS/"},{"name":"Mesos","slug":"Mesos","permalink":"https://kairen.github.io/tags/Mesos/"}]},{"title":"OpenStack Kolla 初體驗","date":"2016-07-21T08:23:01.000Z","path":"2016/07/21/openstack/kolla-ansible/","text":"Kolla 提供 OpenStack 生產環境就緒的容器與部署工具，其具備快速、擴展、可靠等特性，並提供社群版本的最佳本版升級實現。 節點配置本安裝將使用三台實體主機與一台虛擬機器來進行簡單叢集部署，主機規格如以下所示： Role RAM CPUs Disk IP Address controller1 8 GB 4vCPU 250 GB 10.0.0.11 network1 2 GB 1vCPU 250 GB 10.0.0.21 compute1 8 GB 8vCPU 500 GB 10.0.0.31 docker-registry 2 GB 1vCPU 50 GB 10.0.0.90 作業系統皆為Ubuntu Server 14.04。 另外每台實體主機的網卡網路分別為以下： Role Management Tunnel Public controller1 eth0 eth1 eth2 network1 eth0 eth1 eth2 compute1 eth0 eth1 eth2 網卡若是實體主機，請設定為固定 IP，如以下： auto eth0 iface eth0 inet static address 10.0.0.11 netmask 255.255.255.0 gateway 10.0.0.1 dns-nameservers 8.8.8.8 若想修改主機的網卡名稱，可以編輯/etc/udev/rules.d/70-persistent-net.rules。 其中network節點的 Public 需設定網卡為以下： auto &lt;ethx&gt; iface &lt;ethx&gt; inet manual up ip link set dev $IFACE up down ip link set dev $IFACE down 事前準備在開始部署 OpenStack 之前，我們需要先將一些相依軟體與函式庫安裝完成，並且需要部署一用於存取映像檔的倉庫(Docker registry)。 部署 Docker Registry首先進入到 docker-registry 節點，本教學採用一台虛擬機作為部署使用，並安裝 Docker engine： $ curl -fsSL &quot;https://get.docker.com/&quot; | sh 安裝完成 Docker engine 後，透過以下指令建置 Docker registry： $ docker run -d -p 5000:5000 --restart=always --name registry \\ -v $(pwd)/data:/var/lib/registry \\ registry:2 接著為了方便檢視 Docker image，這邊另外部署 Docker registry UI： $ docker run -d -p 8080:80 \\ -e ENV_DOCKER_REGISTRY_HOST=10.26.1.49 \\ -e ENV_DOCKER_REGISTRY_PORT=5000 \\ konradkleine/docker-registry-frontend:v2 完成以上即可透過瀏覽器查看該主機 8080 Port。也可以透過以下指令檢查是否部署成功： $ docker pull ubuntu:14.04 $ docker tag ubuntu:14.04 localhost:5000/ubuntu:14.04 $ docker push localhost:5000/ubuntu:14.04 The push refers to a repository [localhost:5000/ubuntu] 447f88c8358f: Pushed df9a135a6949: Pushed ... 其他 Docker registry 列表： Portus Atomic Registry Private Registries in RancherOS 部署節點準備當完成上述後，即可進行部署節點的相依軟體安裝，首先在每台節點透過以下指令更新與安裝一些套件： $ sudo apt-get update -y &amp;&amp; sudo apt-get upgrade -y $ sudo apt-get install -y python-pip python-dev $ curl -fsSL &quot;https://get.docker.com/&quot; | sh $ sudo timedatectl set-timezone Asia/Taipei 由於使用Ubuntu server 14.04，故需要更新 Kernel: $ sudo apt-get install -y linux-image-generic-lts-wily 接著更新 pip，並安裝 docker-python 函式庫： $ sudo pip install -U pip docker-py 編輯每台節點的/etc/default/docker檔案，加入以下內容： DOCKER_OPTS=&quot;--insecure-registry &lt;registry-ip&gt;:5000&quot; 這邊{registry-ip}為 Docker registry 的 IP 位址。若使用 Ubuntu Server 16.04 版本的話，需要編輯/lib/systemd/system/docker.service檔案，修改以下： EnvironmentFile=-/etc/default/%p ExecStart=/usr/bin/dockerd -H fd:// \\ $DOCKER_OPTS 完成上述後，需要 reload service： $ sudo systemctl daemon-reload 接著編輯每台節點的/etc/rc.local檔案，加入以下內容： mount --make-shared /run # 在 compute 節點額外加入 mount --make-shared /var/lib/nova/mnt 接著在compute1節點執行以下指令： sudo mkdir -p /var/lib/nova/mnt /var/lib/nova/mnt1 sudo mount --bind /var/lib/nova/mnt1 /var/lib/nova/mnt sudo mount --make-shared /var/lib/nova/mnt 完成後重新啟動每台節點。 當上述步驟都完成後，進入到controller1的root使用者執行以下指令安裝與設定額外套件： $ sudo apt-get install -y software-properties-common $ sudo apt-add-repository -y ppa:ansible/ansible &amp;&amp; sudo apt-get update $ sudo apt-get install -y ansible libffi-dev libssl-dev gcc git NTP 部分可自行設定，這邊不再說明。 接著繼續在controller1節點的root使用者執行以下指令： $ ssh-keygen -t rsa $ cat .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys 複製controller1節點的root底下的.ssh/id_rsa.pub公有金鑰到其他節點的root使用者底下的.ssh/authorized_keys。 建立 OpenStack Kolla Docker 映像檔在使用 Kolla 部署 OpenStack 叢集之前，我們需要預先建立用於部署的映像檔，才能進行節點的部署。首先進入到controller1節點，並下載 Kolla 專案： $ git clone &quot;https://github.com/openstack/kolla.git&quot; -b stable/newton $ cd kolla &amp;&amp; pip install . $ pip install tox python-openstackclient &amp;&amp; tox -e genconfig $ cp -r etc/kolla /etc/ 接著執行指令進行建立 Docker 映像檔，若不指定名稱預設下將建立全部映像檔，如以下： $ kolla-build --base ubuntu --type source --registry {registry-ip}:5000 --push 這邊{registry-ip}為 Docker registry 的 IP 位址。 若要改變 OS 與版本，可以編輯/etc/kolla/kolla-build.conf檔案修改以下內容： base = centos base_tag = 3.0.3 push = true install_type = rdo registry = {registry-ip}:5000 可參考官方文件 Building Container Images。 當映像檔建置完成後，即可查看docker-regisrtry節點的 UI 介面，來確認是否上傳成功。 部署 OpenStack 節點當完成映像檔建立後，即可開始進行部署 OpenStack 節點。 首先到controller1進入剛下載的kolla-ansible專案目錄，並編輯ansible/inventory/multinode檔案，修改以下內容： [control] controller1 [network] network1 [compute] compute1 [storage] controller1 這邊由於機器不夠，所以將 storage 也放到 controller。 接著編輯/etc/kolla/globals.yml檔案，修改以下內容： config_strategy: &quot;COPY_ALWAYS&quot; kolla_base_distro: &quot;ubuntu&quot; kolla_install_type: &quot;source&quot; openstack_release: &quot;3.0.3&quot; kolla_internal_vip_address: &quot;10.0.0.10&quot; docker_registry: &quot;10.0.0.90:5000&quot; network_interface: &quot;eth0&quot; tunnel_interface: &quot;eth1&quot; neutron_external_interface: &quot;eth2&quot; neutron_plugin_agent: &quot;openvswitch&quot; # OpenStack service enable_cinder: &quot;yes&quot; enable_neutron_lbaas: &quot;yes&quot; cinder_volume_group: &quot;cinder-volumes&quot; 若要將 API 開放給 Public network 存取的話，需要設定以下內容： kolla_external_vip_address: &quot;10.26.1.251&quot; kolla_external_vip_interface: &quot;eth3&quot; (Option)建立 Cinder LVM volume group: $ sudo apt-get install lvm2 -y $ sudo pvcreate /dev/sdb $ sudo vgcreate cinder-volumes /dev/sdb $ sudo pvdisplay --- Physical volume --- PV Name /dev/sdb VG Name cinder-volumes PV Size 465.76 GiB / not usable 4.02 MiB Allocatable yes PE Size 4.00 MiB Total PE 119234 Free PE 119234 Allocated PE 0 PV UUID 5g01hz-Ebds-EVSF-yGm4-evNm-5Kfp-2h8kUR 然後透過以下指令產生亂數密碼： $ kolla-genpwd 接著要執行預先檢查確認節點是否可以進行部署，透過以下指令： $ kolla-ansible prechecks -i ansible/inventory/multinode PLAY RECAP ********************************************************************* compute1 : ok=8 changed=0 unreachable=0 failed=0 controller1 : ok=35 changed=0 unreachable=0 failed=0 network1 : ok=29 changed=0 unreachable=0 failed=0 若中途發生錯誤，請檢查錯誤訊息。 確認沒問題後即可部署 OpenStack，透過以下指令進行： $ kolla-ansible deploy -i ansible/inventory/multinode PLAY RECAP ********************************************************************* compute1 : ok=44 changed=31 unreachable=0 failed=0 controller1 : ok=154 changed=96 unreachable=0 failed=0 network1 : ok=37 changed=28 unreachable=0 failed=0 P.S. 若發生映像檔無法下載，請自行透過以下指令傳到該節點： $ docker save {image} &gt; image.tar $ scp image.tar network1:~/ $ ssh network1 &quot;docker load &lt; image.tar&quot; 驗證服務完成後，就可以產生 Credential 檔案來進行系統驗證： $ kolla-ansible post-deploy $ source /etc/kolla/admin-openrc.sh 透過 OpenStack Client 來檢查 Keystone，並查看所有使用者： $ openstack user list +----------------------------------+-------------------+ | ID | Name | +----------------------------------+-------------------+ | 19af62000d334c4d9de3aa29b9fd06df | heat_domain_admin | | 83fa8ba1deff46e58e716470dbdaeb03 | heat | | 9661e2a4b1f648a6a3ed5c52f22c52bb | nova | | 9fa2a71716d046a893dcbaef3d7375ce | admin | | acdfdc4fae2248b0af4c8788a9858cf3 | glance | | b683b3c611dd4bdba2c25321a0f03cf0 | cinder | | dc57ebf4e6c548e990767f00ffda19d8 | neutron | +----------------------------------+-------------------+ 透過 OpenStack Client 檢查 Nova 的服務列表： $ openstack compute service list +----+------------------+-------------+----------+---------+-------+----------------------------+-----------------+ | Id | Binary | Host | Zone | Status | State | Updated_at | Disabled Reason | +----+------------------+-------------+----------+---------+-------+----------------------------+-----------------+ | 3 | nova-consoleauth | controller1 | internal | enabled | up | 2016-08-16T05:40:16.000000 | - | | 5 | nova-scheduler | controller1 | internal | enabled | up | 2016-08-16T05:40:20.000000 | - | | 6 | nova-conductor | controller1 | internal | enabled | up | 2016-08-16T05:40:12.000000 | - | | 10 | nova-compute | compute1 | nova | enabled | up | 2016-08-16T05:40:16.000000 | - | +----+------------------+-------------+----------+---------+-------+----------------------------+-----------------+ 透過 OpenStack Client 檢查 Neutron 的服務列表： $ openstack network agent list +------------------+------------------+----------+-------------------+-------+----------------+------------------+ | id | agent_type | host | availability_zone | alive | admin_state_up | binary | +------------------+------------------+----------+-------------------+-------+----------------+------------------+ | 2254dcb0-9d5b- | Open vSwitch | network1 | | :-) | True | neutron- | | 46da-b1f2-b03719 | agent | | | | | openvswitch- | | 5913b1 | | | | | | agent | | acb81021-2efd-4b | DHCP agent | network1 | nova | :-) | True | neutron-dhcp- | | b6-9af7-a3df95ce | | | | | | agent | | 479b | | | | | | | | b564c005-779d- | Metadata agent | network1 | | :-) | True | neutron- | | 45f4-b10f- | | | | | | metadata-agent | | 788bb788491b | | | | | | | | d95d75e5-6429-44 | Open vSwitch | compute1 | | :-) | True | neutron- | | 65-aef1-a6a9f4e7 | agent | | | | | openvswitch- | | 995f | | | | | | agent | | e0dbbff4-d7a7-4f | L3 agent | network1 | nova | :-) | True | neutron-l3-agent | | a2-8fa2-b3d3de42 | | | | | | | | d899 | | | | | | | +------------------+------------------+----------+-------------------+-------+----------------+------------------+ 從網路上下載一個測試用映像檔cirros-0.3.4-x86_64，並上傳至 Glance 服務： $ wget &quot;http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img&quot; $ openstack image create &quot;cirros-0.3.4-x86_64&quot; \\ --file cirros-0.3.4-x86_64-disk.img \\ --disk-format qcow2 --container-format bare \\ --public 這邊可以透過 Neutron client 來查看建立外部網路： $ openstack network create --external \\ --provider-physical-network physnet1 \\ --provider-network-type flat public1 $ openstack subnet create --no-dhcp --network public1 \\ --allocation-pool start=10.26.1.230,end=10.26.1.240 \\ --subnet-range 10.26.1.0/24 --gateway 10.26.1.254 public1-subnet $ openstack network create --provider-network-type vxlan admin-net $ openstack subnet create --subnet-range 192.168.10.0/24 \\ --network admin-net --gateway 192.168.10.1 \\ --dns-nameserver 8.8.8.8 admin-subnet $ openstack router create admin-router $ openstack router add subnet admin-router admin-subnet $ openstack router set --external-gateway public1 admin-router 建立 flavor 來提供虛擬機樣板： openstack flavor create --id 1 --ram 512 --disk 1 --vcpus 1 m1.tiny openstack flavor create --id 2 --ram 2048 --disk 20 --vcpus 1 m1.small openstack flavor create --id 3 --ram 4096 --disk 40 --vcpus 2 m1.medium openstack flavor create --id 4 --ram 8192 --disk 80 --vcpus 4 m1.large openstack flavor create --id 5 --ram 16384 --disk 160 --vcpus 8 m1.xlarge 上傳 Keypair 來提供虛擬機 SSH 登入認證用： $ openstack keypair create --public-key ~/.ssh/id_rsa.pub mykey 開啟一台虛擬機進行測試： $ openstack server create --image cirros-0.3.4-x86_64 \\ --flavor m1.tiny \\ --nic net-id=admin-net \\ --key-name mykey \\ admin-instance 查看 instance 列表： $ openstack server list +--------------------------------------+----------------+--------+------------------------+---------------------+ | ID | Name | Status | Networks | Image Name | +--------------------------------------+----------------+--------+------------------------+---------------------+ | bb3b31b6-56a4-4c68-bd53-b7efb451f0fc | admin-instance | ACTIVE | admin-net=192.168.10.6 | cirros-0.3.4-x86_64 | +--------------------------------------+----------------+--------+------------------------+---------------------+","tags":[{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"OpenStack","slug":"OpenStack","permalink":"https://kairen.github.io/tags/OpenStack/"},{"name":"Ansible","slug":"Ansible","permalink":"https://kairen.github.io/tags/Ansible/"}]},{"title":"用 Bcache 來加速硬碟效能","date":"2016-07-05T08:23:01.000Z","path":"2016/07/05/linux/ubuntu/bcache/","text":"Bcache 是按照固態硬碟特性來設計的技術，只按擦除 Bucket 的大小進行分配，並使用 btree 和 journal 混合方法來追蹤快取資料，快取資料可以是 Bucket 上的任意一個 Sector。Bcache 最大程度上減少了隨機寫入的代價，它按循序的方式填充一個 Bucket，重新使用時只需將 Bucket 設置為無效即可。Bcache 也支援了類似 Flashcache 的快取策略，如write-back、write-through 與 write-around。 安裝與設定首先要先安裝 bcache-tools，這邊採用 ubuntu 的apt-get來進行安裝： $ sudo add-apt-repository ppa:g2p/storage $ sudo apt-get update $ sudo apt-get install -y bcache-tools 完成安裝後，要準備一顆 SSD 與 HDD，並安裝於同一台主機上，如以下硬碟結構： +-------+----------+ +--------+---------+ | [ 固態硬碟(SSD)] | | [ 傳統硬碟(HDD)] | | System disk +-------+ System disk + | (/dev/sdb) | | (/dev/sdc) | +------------------+ +------------------+ 當確認以上都沒問題後，即可用 bcache 指令來建立快取，首先建立後端儲存裝置： $ sudo make-bcache -B /dev/sdc UUID: 3b62c662-c739-4621-aca3-80efbf5e1da2 Set UUID: 67828232-2427-46d3-a473-e92e1f213f87 version: 1 block_size: 1 data_offset: 16 -C為快取層。 -B為 bcache 後端儲存層。 --block 為 Block Size，預設為 1k。 --discard為 SSD 上使用 TRIM。 --writeback為使用 writeback 模式，預設為 writethrough。 P.S 如果有任何錯誤，請使用以下指令： $ sudo wipefs -a /dev/sdb 之後在透過指令建立快取儲存裝置，如以下： $ sudo make-bcache --block 4k --bucket 2M -C /dev/sdb -B /dev/sdc --wipe-bcache UUID: 192dfaf6-fd2a-4246-b4be-f159c3346850 Set UUID: ed865522-96a7-43e5-8dab-e8c024fe85db version: 0 nbuckets: 228946 block_size: 1 bucket_size: 1024 nr_in_set: 1 nr_this_dev: 0 first_bucket: 1 完成後，可以用bcache-super-show指令確認是否有建立，並取得 UUID： $ sudo bcache-super-show /dev/sdb | grep cset.uuid cset.uuid b6295aac-34c3-4630-8872-9aa18618daea 也可以用其他指令查看儲存建立狀況： $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 232.9G 0 disk └─sda1 8:1 0 232.9G 0 part / sdb 8:16 0 111.8G 0 disk └─bcache0 251:0 0 465.8G 0 disk sdc 8:32 0 465.8G 0 disk └─bcache0 251:0 0 465.8G 0 disk 接著將快取儲存裝置附加到後端儲存裝置： $ echo &quot;&lt;cset.uuid&gt;&quot; &gt; /sys/block/bcache0/bcache/attach bcache0會隨建立的不同而改變。 之後可以依需求設定 cache mode，透過以下方式： $ echo writeback &gt; /sys/block/bcache0/bcache/cache_mode 一切完成後，可以透過以下方式來檢查 Cache 狀態： $ cat /sys/block/bcache0/bcache/state clean no cache：表示沒有任何快取裝置連接到後台儲存裝置。 clean：表示快取已連接，且快取是乾淨的。 dirty：表示一切設定完成，但必須啟用 writeback，且快取不是乾淨的。 inconsistent：這表示後端是不被同步的高速快取儲存裝置。記得換爛一點。 測試寫入速度這邊採用 Linux 的 dd 工具來看寫入速度： $ dd if=/dev/zero of=/dev/bcache0 bs=1G count=1 oflag=direct","tags":[{"name":"Storage","slug":"Storage","permalink":"https://kairen.github.io/tags/Storage/"},{"name":"Linux","slug":"Linux","permalink":"https://kairen.github.io/tags/Linux/"},{"name":"SSD","slug":"SSD","permalink":"https://kairen.github.io/tags/SSD/"}]},{"title":"Building Spark Source Code","date":"2016-06-24T09:08:54.000Z","path":"2016/06/24/data-engineer/build-spark/","text":"本節將說明如何透過 mvn 與 sbt 來建置 Spark 最新版的相關檔案，透過提供最新版本來觀看 API 的變動。 事前準備首先準備一台裝有 Ubuntu 14.04 LTS Server 的主機或 Docker 容器，然後在裡面安裝相依套件： sudo apt-get purge openjdk* sudo apt-get -y autoremove sudo apt-get install -y software-properties-common sudo add-apt-repository -y ppa:webupd8team/java sudo apt-get update echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections sudo apt-get -y install oracle-java8-installer git 接著安裝 maven 3.3.1 + 工具： wget http://ftp.tc.edu.tw/pub/Apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -zxf apache-maven-3.3.9-bin.tar.gz sudo cp -R apache-maven-3.3.9 /usr/local/ sudo ln -s /usr/local/apache-maven-3.3.9/bin/mvn /usr/bin/mvn mvn --version 安裝 Scala 語言： wget www.scala-lang.org/files/archive/scala-2.11.7.deb sudo dpkg -i scala-2.11.7.deb 安裝 sbt 工具： echo &quot;deb http://dl.bintray.com/sbt/debian /&quot; | sudo tee /etc/apt/sources.list.d/sbt.list sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 642AC823 sudo apt-get update sudo apt-get install sbt 安裝 Python 2.7 語言： $ sudo apt-get install -y python 透過 Git 指令取得 Spark 最新原始碼： $ git clone https://github.com/apache/spark.git 使用 sbt 來建置 sparksbt 的 spark 建置指令如下所示，若使用 sbt 需要大約 10 分鐘時間：： $ ./build/sbt -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver -DskipTests clean assembly 當建置完成後，可以透過 spark-shell 查看版本： $ ./bin/spark-shell --version 使用 Apache Maven 來建置 sparkApache Maven 的 spark 建置指令如下所示: $ ./build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver -DskipTests clean install 當建置完成後，可以透過 spark-shell 查看版本： $ ./bin/spark-shell --version Making Distributionmake-distribution.sh 是一個 shell 腳本用於建立分散式應用。它使用跟 sbt 與 mvn 一樣的配置檔案。首先新增 Java 環境參數： $ export JAVA_HOME=&quot;/usr/lib/jvm/java-8-oracle&quot; 使用--tgz選項建立一個 tar gz 的 Spark 分散檔案： $ ./dev/make-distribution.sh --tgz -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver -DskipTests 一旦完成後，你會在當前目錄看到檔案，名稱會是spark-2.0.0-SNAPSHOT-bin-2.6.0.tgz。","tags":[{"name":"Spark","slug":"Spark","permalink":"https://kairen.github.io/tags/Spark/"},{"name":"Maven","slug":"Maven","permalink":"https://kairen.github.io/tags/Maven/"}]},{"title":"Docker 串接 OpenStack Neutron Kuryr 網路","date":"2016-06-02T08:23:01.000Z","path":"2016/06/02/openstack/kuryr-install/","text":"Kuryr 是 Docker network plugin 之一，主要是使用 Neutron 來提供網路服務給不同主機的 Docker 容器使用，目前也提供了容器化的 Neutron plugin 容器映像檔。 本篇說明如何透過 CentOS 來部署簡單的 Kuryr 與 Docker 串接。 節點配置OpenStack Kuryr 我們使用到三台節點，以下為本次安裝的規格： Role RAM Disk CPUs IP Address controller 4 GB 50GB 2vCPU 172.16.1.115 network-1 4 GB 50GB 2vCPU 172.16.1.118 network-2 4 GB 50 GB 2vCPU 172.16.1.119 請以上節點都分別安裝 RHEL 或者 CentOS 作業系統。並且設定 IP 為靜態固定，編輯/etc/sysconfig/network-scripts/ifcfg-&lt;name&gt;檔案，加入以下內容： ONBOOT=&quot;yes&quot; IPADDR=&quot;10.0.0.104&quot; PREFIX=&quot;24&quot; GATEWAY=&quot;10.0.0.1&quot; DNS1=&quot;8.8.8.8&quot; DNS2=&quot;8.8.8.4&quot; P.S. 若是虛擬機則不需要設定。 安裝前準備在開始安裝以前，首先需要在每一台節點將基本環境的軟體更新： $ sudo yum update -y 完成後檢查是否是最新版本，可以透過以下方式查看 Kernel： $ uname -r 3.10.0-327.13.1.el7.x86_64 如果不是以上版本，請執行以下指令： $ sudo yum upgrade --assumeyes --tolerant $ sudo yum update --assumeyes 由於在 CentOS 與 RHEL 預設會開啟防火牆，故要關閉防火牆與開機時自動啟動： $ sudo systemctl stop firewalld &amp;&amp; sudo systemctl disable firewalld 設定關閉 SELinux 與設定一些資訊，並重新啟動： $ sudo sed -i s/SELINUX=enforcing/SELINUX=permissive/g /etc/selinux/config &amp;&amp; sudo groupadd nogroup &amp;&amp; sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1 &amp;&amp; sudo sysctl -w net.ipv6.conf.default.disable_ipv6=1 &amp;&amp; sudo reboot 完成重新啟動後，在所有Network節點安裝 Docker，首先要取得 repos，設定以下來讓 yum 可以抓取： $ sudo tee /etc/yum.repos.d/docker.repo &lt;&lt;-&#39;EOF&#39; [dockerrepo] name=Docker Repository baseurl=https://yum.dockerproject.org/repo/main/centos/$releasever/ enabled=1 gpgcheck=1 gpgkey=https://yum.dockerproject.org/gpg EOF 在Network節點安裝 Docker engine，並啟動 docker 與設定開機啟動： $ sudo yum install --assumeyes --tolerant docker-engine $ sudo systemctl start docker $ sudo systemctl enable docker 在所有節點安裝一些基本工具軟體： $ sudo yum install -y tar xz unzip curl ipset vim wget git python-pip $ sudo pip install --upgrade pip 在Controller節點，進入root使用者，並建置 ssh keys： $ ssh-keygen -t rsa $ cat .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys 複製Controller節點的.ssh/id_rsa.pub內容，並貼到Network節點的root使用者的.ssh/authorized_keys。並驗證 ssh 是否可以無密碼登入： $ ssh 172.16.1.118 安裝 OpenStack這邊使用 RDO 進行安裝。由於只需要 Neutron 與 Keystone 服務，所以可以修改部署的answer-file設定檔。由於這邊使用的是虛擬機，因此 Neutron 網路採用 VXLAN 方式進行安裝。進入到Controller節點，並且進入到root使用者安裝 PackStack： $ yum install -y centos-release-openstack-mitaka.noarch $ yum install -y openstack-packstack $ wget https://gist.githubusercontent.com/kairen/637c707b960e188d32aba9044e652c0b/raw/6724a5177ca82ced555554635c6b0893e8c398ab/answer-file 這邊可以更改安裝版本，如更改成 Liberty 的穩定版centos-release-openstack-liberty.noarch 編輯answer-file設定檔，修改以下內容： CONFIG_CONTROLLER_HOST=172.16.1.115 CONFIG_COMPUTE_HOSTS=172.16.1.118,172.16.1.119 CONFIG_NETWORK_HOSTS=172.16.1.115,172.16.1.118,172.16.1.119 CONFIG_STORAGE_HOST=172.16.1.115 CONFIG_AMQP_HOST=172.16.1.115 CONFIG_MARIADB_HOST=172.16.1.115 CONFIG_KEYSTONE_LDAP_URL=ldap://172.16.1.115 設定檔都確認無誤後，透過以下指令進行安裝： $ packstack --answer-file=answer-file ... **** Installation completed successfully ****** * To access the OpenStack Dashboard browse to http://172.16.1.115/dashboard . 中途若發生安裝套件失敗問題，請直接重新執行一次。 當成功安裝完成後，透過簡單的 OpenStack 指令來確認： $ . keystonerc_admin $ openstack user list +----------------------------------+---------+ | ID | Name | +----------------------------------+---------+ | 70b80593320543bbb32e15d7f06036f0 | admin | | 9600aaa2447940e789e548b2f5515690 | neutron | +----------------------------------+---------+ 安裝 Kuryr進入Network節點，並且進入到root使用者，下載 Kuryr 最新的專案： $ git clone https://github.com/openstack/kuryr.git $ pip install -r requirements.txt $ python setup.py install 建立 Kuryr 設定檔與 Log 目錄： $ mkdir -p /var/log/kuryr /etc/kuryr $ wget https://gist.githubusercontent.com/kairen/637c707b960e188d32aba9044e652c0b/raw/6724a5177ca82ced555554635c6b0893e8c398ab/kuryr.conf -O /etc/kuryr/kuryr.conf 編輯/etc/kuryr/kuryr.conf設定檔，修改一下內容： [keystone_client] auth_uri = http://172.16.1.115:35357/v2.0 [neutron_client] neutron_uri = http://172.16.1.115:9696 接著啟動 Kuryr 服務： $ ./scripts/run_kuryr.sh &amp; 2016-06-02 03:51:33.578 4758 INFO werkzeug [-] * Running on http://0.0.0.0:2377/ (Press CTRL+C to quit) 服務驗證當所有Network節點都完成 Kuryr 安裝後，就可以透過以下方式來驗證，首先在network-1建立網路： $ docker network create --driver=kuryr \\ --ipam-driver=kuryr \\ --subnet 10.0.0.0/16 \\ --gateway 10.0.0.1 \\ --ip-range 10.0.0.0/24 kuryr ... 821d6cd53af6c656969c1c96063a60c695a0313c9a119e44d4325ce2a9f2f935 透過 docker 指令來查看網路： $ docker network inspect kuryr [ { &quot;Name&quot;: &quot;kuryr&quot;, &quot;Id&quot;: &quot;821d6cd53af6c656969c1c96063a60c695a0313c9a119e44d4325ce2a9f2f935&quot;, &quot;Scope&quot;: &quot;local&quot;, &quot;Driver&quot;: &quot;kuryr&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: { &quot;Driver&quot;: &quot;kuryr&quot;, &quot;Options&quot;: {}, &quot;Config&quot;: [ { &quot;Subnet&quot;: &quot;10.0.0.0/16&quot;, &quot;IPRange&quot;: &quot;10.0.0.0/24&quot;, &quot;Gateway&quot;: &quot;10.0.0.1&quot; } ] }, &quot;Internal&quot;: false, &quot;Containers&quot;: { &quot;367763a677ad180c57818631ee3e9151683d0218a15bb002d0995ab5c6e30446&quot;: { &quot;Name&quot;: &quot;awesome_dijkstra&quot;, &quot;EndpointID&quot;: &quot;326aba6247266cfd0ca3771b0283e2142a3e934bb994d1b77fc93bb467c0df48&quot;, &quot;MacAddress&quot;: &quot;fa:16:3e:7c:d8:b6&quot;, &quot;IPv4Address&quot;: &quot;10.0.0.5/24&quot;, &quot;IPv6Address&quot;: &quot;&quot; }, &quot;b740abd7976d274de233df0bad052c418516ba036132b92ad022a4b52a2d7d25&quot;: { &quot;Name&quot;: &quot;awesome_engelbart&quot;, &quot;EndpointID&quot;: &quot;fcc642dae6323ce7d86ddda61c9583a19801eae70d8126d35b4b5846cd8598a4&quot;, &quot;MacAddress&quot;: &quot;fa:16:3e:5b:6e:7f&quot;, &quot;IPv4Address&quot;: &quot;10.0.0.3/24&quot;, &quot;IPv6Address&quot;: &quot;&quot; } }, &quot;Options&quot;: {}, &quot;Labels&quot;: {} } ] 接著建立一個 container 來取得 IP： $ docker run -it -d --net kuryr --privileged=true ubuntu:14.04 $ docker exec -ti $(docker ps -lq) bash root@367763a677ad$ ip -4 a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 14: eth0@if15: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 inet 10.0.0.3/24 scope global eth0 valid_lft forever preferred_lft forever 接著進入到network-2節點，透過 docker 指令建立網路，這邊採用跟上一個同樣的網路： $ docker network create --driver=kuryr \\ --ipam-driver=kuryr \\ --subnet 10.0.0.0/16 \\ --gateway 10.0.0.1 \\ --ip-range 10.0.0.0/24 \\ -o neutron.net.uuid=8c069d2c-772c-47ae-90bb-d22148f37dc8 kuryr 這邊neutron.net.uuid可以透過以下在Controller方式取得： $ neutron net-list | awk &#39;/kuryr/ {print $2}&#39; 8c069d2c-772c-47ae-90bb-d22148f37dc8 接著一樣建立一個 container 來取得 IP： $ docker run -it -d --net kuryr --privileged=true ubuntu:14.04 $ docker exec -ti $(docker ps -lq) bash root@f2b48a802e92$ ip -4 a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 15: eth0@if16: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 inet 10.0.0.4/24 scope global eth0 valid_lft forever preferred_lft forever 透過 ping 來驗證網路是否有連接： root@f2b48a802e92$ ping -c 3 10.0.0.3 PING 10.0.0.3 (10.0.0.3) 56(84) bytes of data. 64 bytes from 10.0.0.3: icmp_seq=1 ttl=64 time=1.71 ms 64 bytes from 10.0.0.3: icmp_seq=2 ttl=64 time=1.37 ms 64 bytes from 10.0.0.3: icmp_seq=3 ttl=64 time=1.23 ms","tags":[{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"OpenStack","slug":"OpenStack","permalink":"https://kairen.github.io/tags/OpenStack/"}]},{"title":"用 Flashcache 建立高容量與高效能儲存","date":"2016-05-27T08:23:01.000Z","path":"2016/05/27/linux/ubuntu/flashcache/","text":"Flashcache 是 Facebook 的一個開源專案，主要被用於資料庫加速。基本結構為在硬碟（HDD）前面加了一層快取，即採用固態硬碟（SSD）裝置，把熱資料保存於快取中，寫入的過程也是先寫到 SSD，然後由 SSD 同步到傳統硬碟，最後的資料將保存於硬碟中，這樣可以不用擔心 SSD 損壞造成資料遺失問題，同時又可以有大容量、高效能的儲存。 安裝本教學採用 Ubuntu 14.04 LTS 進行安裝，並建立快取。首先安裝相依套件： $ sudo apt-get install -y git build-essential dkms linux-headers-`uname -r` 完成後，透過 git 指令將專案下載至主機上： $ git clone https://github.com/facebook/flashcache.git $ cd flashcache 進入目錄編譯 flashcache 套件，並透過 make 進行安裝套件： $ make $ sudo make install 安裝完成後，就可以載入 flashcache 模組，透過以下指令： $ sudo modprobe flashcache 若要檢查是否載入成功的話，可以使用以下指令： $ dmesg | tail [24181.921706] flashcache: module verification failed: signature and/or required key missing - tainting kernel [24181.922785] flashcache: flashcache-3.1.1 initialized 設定開機時自動載入模組： $ echo &quot;flashcache&quot; | sudo tee -a /etc/modules 設定快取首先準備一顆 SSD 與 HDD，並安裝於同一台主機上，如以下硬碟結構： +-------+----------+ +--------+---------+ | [ 固態硬碟(SSD)] | | [ 傳統硬碟(HDD)] | | System disk +-------+ System disk + | (/dev/sdb) | | (/dev/sdc) | +------------------+ +------------------+ 在開始前，必須先將傳統硬碟進行格式化： $ sudo mkfs.ext4 /dev/sdc 接著要初始化 Flashcache，然後透過 Flashcache 指令來設定快取： $ sudo flashcache_create -p back -b 4k cachedev /dev/sdb /dev/sdc cachedev cachedev, ssd_devname /dev/sdb, disk_devname /dev/sdc cache mode WRITE_BACK block_size 8, md_block_size 8, cache_size 0 Flashcache metadata will use 614MB of your 7950MB main memory 完成後，就可以透過 mount 來使用快取： $ sudo mount /dev/mapper/cachedev /mnt 若要在開機時自動 mount 為 Flashcache 的快取固態硬碟，可以在rc.local加入以下內容： flashcache_load /dev/sdb mount /dev/mapper/cachedev /mnt 若想監控 Flashcache 資訊的話，可以使用以下工具： $ flashstat 最後，若想要刪除 Flashcache 的話，可以使用以下指令： $ sudo umount /mnt $ sudo flashcache_destroy /dev/sdb $ sudo dmsetup remove cachedev fio 測試這邊採用 fio 來進行測試，首先透過apt-get安裝套件： $ sudo apt-get install fio 完成後，即可透過 fio 指令進行效能測試： $ fio --filename=/dev/sdb --direct=1 \\ --rw=randrw --ioengine=libaio --bs=4k \\ --rwmixread=100 --iodepth=16 \\ --numjobs=16 --runtime=60 \\ --group_reporting --name=4ktest fio 測試工具 options 參數： --filename=/dev/sdb：指定要測試的磁碟。 --direct=1：預設值為 0 ,必須設定為 1 才會測試到真實的 non-buffered I/O。 --rw=randrw：可以設定的參數如下 randrw 代表 random(隨機) 的 read(讀) write(寫),其他的請參考下面說明。 read : Sequential reads. (循序讀) write : Sequential writes. (循序寫) randread : Random reads. (隨機讀) randwrite : Random writes. (隨機寫) rw : Mixed sequential reads and writes. (循序讀寫) randrw : Mixed random reads and writes. (隨機讀寫) --ioengine=libaio：定義如何跑 I/O 的方式, libaio 是 Linux 本身非同步(asynchronous) I/O 的方式. 其他還有 sync , psync , vsync , posixaio , mmap , splice , syslet-rw , sg , null , net , netsplice , cpuio , guasi , external。 --bs=4k：bs 或是 blocksize ,也就是檔案寫入大小,預設值為 4K。 --rwmixread=100： 當設定為 Mixed ,同一時間 read 的比例為多少,預設為 50%。 --refill_buffers：refill_buffers 為預設值,應該是跟 I/O Buffer 有關 (refill the IO buffers on every submit),把 Buffer 填滿就不會跑到 Buffer 的值。 --iodepth=16：同一時間有多少 I/O 在做存取,越多不代表存儲裝置表現會更好,通常是 RAID 時須要設大一點。 --numjobs=16：跟前面的 iodepth 類似,但不一樣,在 Linux 下每一個 job 可以生出不同的 processes/threads ,numjobs 就是在同一個 workload 同時提出多個 I/O 請求,通常負載這個會比較大.預設值為 1。 --runtime=60：這一測試所需的時間,單位為 秒。 --group_reporting：如果 numjobs 有指定,設定 group_reporting 報告會以 per-group 的顯示方式。 --name=4ktest：代表這是一個新的測試 Job。 參考資料 Fio – Flexible I/O Tester Flashcache Wiki Flashcache初次体验 Ubuntu bonnie++硬碟測速 (Linux 適用)","tags":[{"name":"Storage","slug":"Storage","permalink":"https://kairen.github.io/tags/Storage/"},{"name":"Linux","slug":"Linux","permalink":"https://kairen.github.io/tags/Linux/"},{"name":"SSD","slug":"SSD","permalink":"https://kairen.github.io/tags/SSD/"}]},{"title":"Pacemaker + Corosync 做服務 HA","date":"2016-05-26T08:23:01.000Z","path":"2016/05/26/linux/ubuntu/corosync-pacemaker/","text":"Pacemaker 與 Corosync 是 Linux 中現今較常用的高可靠性叢集系統組合。Pacemaker 自身提供了很多常用的應用管理功能，不過若要使用 Pacemaker 來管理自己實作的服務，或是一些特別的東西時，就必須要自己實作管理資源。 節點配置本安裝將使用三台實體主機與一台虛擬機器，主機規格如以下所示： Role IP Address pacemaker1 172.16.35.10 pacemaker2 172.16.35.11 作業系統皆為 Ubuntu 14.04 Server。 進行安裝與設定首先要在所有節點之間設定無密碼 ssh 登入，透過以下方式： $ ssh-keygen -t rsa $ ssh-copy-id pacemaker1 安裝相關套件軟體： $ sudo apt-get install -y corosync pacemaker heartbeat resource-agents fence-agents apache2 完成後，在pacemaker1進行以下步驟，首先編輯/etc/corosync/corosync.conf設定檔，修改一下內容： # Please read the openais.conf.5 manual page totem { version: 2 # How long before declaring a token lost (ms) token: 3000 # How many token retransmits before forming a new configuration token_retransmits_before_loss_const: 10 # How long to wait for join messages in the membership protocol (ms) join: 60 # How long to wait for consensus to be achieved before starting a new round of membership configuration (ms) consensus: 3600 # Turn off the virtual synchrony filter vsftype: none # Number of messages that may be sent by one processor on receipt of the token max_messages: 20 # Limit generated nodeids to 31-bits (positive signed integers) clear_node_high_bit: yes # Disable encryption secauth: off #啟動認證功能 # How many threads to use for encryption/decryption threads: 0 # Optionally assign a fixed node id (integer) # nodeid: 1234 # This specifies the mode of redundant ring, which may be none, active, or passive. rrp_mode: none interface { # The following values need to be set based on your environment ringnumber: 0 bindnetaddr: 10.11.8.0 # 主機所在網路位址 mcastaddr: 226.93.2.1 # 廣播地址，不要被佔用即可 P.S. 範圍:224.0.2.0～238.255.255.255 mcastport: 5405 # 廣播埠口 } } amf { mode: disabled } quorum { # Quorum for the Pacemaker Cluster Resource Manager provider: corosync_votequorum expected_votes: 1 } aisexec { user: root group: root } logging { fileline: off to_stderr: yes # 輸出到標準输出 to_logfile: yes # 輸出到日誌檔案 logfile: /var/log/corosync.log # 日誌檔案位置 to_syslog: no # 輸出到系统日誌 syslog_facility: daemon debug: off timestamp: on logger_subsys { subsys: AMF debug: off tags: enter|leave|trace1|trace2|trace3|trace4|trace6 } } # 新增 pacemaker 服務配置 service { ver: 1 name: pacemaker } 接著產生節點之間的溝通時的認證金鑰文件： $ corosync-keygen -l 然後將設定檔與金鑰複製到pacemaker2上： $ cd /etc/corosync/ $ scp -p corosync.conf authkey pacemaker2:/etc/corosync/ 接著分別在兩個節點上編輯/etc/default/corosync檔案，修改以下： # start corosync at boot [yes|no] START=yes 接著將 Corosync 與 Pacemaker 服務啟動： $ sudo service corosync start $ sudo service pacemaker start 完成後透過 crm 指令來查看狀態： $ crm status Last updated: Tue Dec 27 03:12:07 2016 Last change: Tue Dec 27 02:35:18 2016 via cibadmin on pacemaker1 Stack: corosync Current DC: pacemaker1 (739255050) - partition with quorum Version: 1.1.10-42f2063 2 Nodes configured 0 Resources configured Online: [ pacemaker1 pacemaker2 ] 關閉 corosync 預設啟動的 stonith 與 quorum 在兩台節點之問題： $ crm configure property stonith-enabled=false $ crm configure property no-quorum-policy=ignore 完成後，透過指令檢查： $ crm configure show node $id=&quot;739255050&quot; pacemaker1 node $id=&quot;739255051&quot; pacemaker2 property $id=&quot;cib-bootstrap-options&quot; \\ dc-version=&quot;1.1.10-42f2063&quot; \\ cluster-infrastructure=&quot;corosync&quot; \\ stonith-enabled=&quot;false&quot; \\ no-quorum-policy=&quot;ignore&quot; 設定資源Corosync 支援了多種資源代理，如 heartbeat、LSB(Linux Standard Base)與 OCF(Open Cluster Framework) 等。而 Corosync 也可以透過指令來查詢： $ crm ra classes lsb ocf / heartbeat pacemaker redhat service stonith upstart 而更細部的資訊可以透過以下查詢： $ crm ra list lsb $ crm ra list ocf heartbeat $ crm ra info ocf:heartbeat:IPaddr 首先新增一個 heartbeat 資源： $ crm configure # 設定 VIP crm(live)configure# primitive vip ocf:heartbeat:IPaddr params ip=172.16.35.20 nic=eth2 cidr_netmask=24 op monitor interval=10s timeout=20s on-fail=restart # 設定 httpd crm(live)configure# primitive httpd lsb:apache2 crm(live)configure# exit There are changes pending. Do you want to commit them? yes 設定 Group 來將 httpd 與 vip 資源放一起： crm(live)configure# group webservice vip httpd 完成後，透過 crm 指令查詢狀態： $ crm status Last updated: Tue Dec 27 03:52:21 2016 Last change: Tue Dec 27 03:52:20 2016 via cibadmin on pacemaker1 Stack: corosync Current DC: pacemaker1 (739255050) - partition with quorum Version: 1.1.10-42f2063 2 Nodes configured 2 Resources configured Online: [ pacemaker1 pacemaker2 ] Resource Group: webservice vip (ocf::heartbeat:IPaddr): Started pacemaker1 httpd (lsb:apache2): Started pacemaker2 最後就可以在pacemaker1或pacemaker2關閉服務來確認是否正常執行。","tags":[{"name":"Linux","slug":"Linux","permalink":"https://kairen.github.io/tags/Linux/"},{"name":"Load Balancer","slug":"Load-Balancer","permalink":"https://kairen.github.io/tags/Load-Balancer/"},{"name":"High Availability","slug":"High-Availability","permalink":"https://kairen.github.io/tags/High-Availability/"}]},{"title":"利用 rados-java 存取 Ceph","date":"2016-05-15T09:08:54.000Z","path":"2016/05/15/ceph/rados-java/","text":"rados-java 透過 JNA 來綁定 librados (C) 的 API 來提供給 Java 使用，並且實作了 RADOS 與 RBD 的 API，由於透過 JNA 的關析，故不用建構任何的 Header 檔案(.h)。因此我們可以在擁有 JNA 與 librados 的系統上使用本函式庫。 環境準備在開始進行之前，需要滿足以下幾項要求： 需要部署一個 Ceph 叢集，可以參考 Ceph Docker 部署。 執行 rados-java 程式的環境，要能夠與 Ceph 叢集溝通(ceph.conf、admin key)。 需要安裝 Ceph 相關 library。可以透過以下方式安裝： $ wget -q -O- &#39;https://download.ceph.com/keys/release.asc&#39; | sudo apt-key add - $ echo &quot;deb https://download.ceph.com/debian-kraken/ $(lsb_release -sc) main&quot; | sudo tee /etc/apt/sources.list.d/ceph.list $ sudo apt-get update &amp;&amp; sudo apt-get install -y ceph 建構 rados-java jar 檔首先需要安裝一些相關軟體來提供建構 rados-java 使用： $ sudo apt-get install -y software-properties-common $ sudo add-apt-repository -y ppa:webupd8team/java $ sudo apt-get update $ sudo apt-get -y install oracle-java8-installer git libjna-java $ sudo ln -s /usr/share/java/jna-4.2.2.jar /usr/lib/jvm/java-8-oracle/jre/lib/ext/ 接著安裝 maven 3.3.1 + 工具： wget &quot;http://ftp.tc.edu.tw/pub/Apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz&quot; tar -zxf apache-maven-3.3.9-bin.tar.gz sudo cp -R apache-maven-3.3.9 /usr/local/ sudo ln -s /usr/local/apache-maven-3.3.9/bin/mvn /usr/bin/mvn mvn --version 然後透過 Git 取得 rados-java 原始碼： $ git clone &quot;https://github.com/ceph/rados-java.git&quot; $ cd rados-java &amp;&amp; git checkout v0.3.0 $ mvn clean install -Dmaven.test.skip=true 完成後將 rados-java Jar 檔複製到/usr/share/java/底下，並設定 JAR 連結 JVM Class path： $ sudo cp target/rados-0.3.0.jar /usr/share/java $ sudo ln -s /usr/share/java/rados-0.3.0.jar /usr/lib/jvm/java-8-oracle/jre/lib/ext/ 這邊也可以直接透過下載 Jar 檔來完成： $ wget &quot;http://central.maven.org/maven2/com/ceph/rados/0.3.0/rados-0.3.0.jar&quot; $ sudo cp rados-0.3.0.jar /usr/share/java/ 最後就可以透過簡單範例程式存取 Ceph 了。 簡單測試程式這邊透過 Java 程式連結到 Ceph 叢集，並且存取data儲存池來寫入物件，建立與編輯Example.java檔，加入以下程式內容： import com.ceph.rados.Rados; import com.ceph.rados.exceptions.RadosException; import java.io.File; import com.ceph.rados.IoCTX; public class Example { public static void main (String args[]){ try { Rados cluster = new Rados(&quot;admin&quot;); File f = new File(&quot;/etc/ceph/ceph.conf&quot;); cluster.confReadFile(f); cluster.connect(); System.out.println(&quot;Connected to the cluster.&quot;); IoCTX io = cluster.ioCtxCreate(&quot;data&quot;); /* Pool Name */ String oidone = &quot;kyle-say&quot;; String contentone = &quot;Hello World!&quot;; io.write(oidone, contentone); String oidtwo = &quot;my-object&quot;; String contenttwo = &quot;This is my object.&quot;; io.write(oidtwo, contenttwo); String[] objects = io.listObjects(); for (String object: objects) System.out.println(&quot;Put &quot; + object); /* io.remove(oidone); io.remove(oidtwo); */ cluster.ioCtxDestroy(io); } catch (RadosException e) { System.out.println(e.getMessage() + &quot;: &quot; + e.getReturnValue()); } } } 撰寫完程式後，執行以下指令來看結果： $ javac Example.java $ sudo java Example Connected to the cluster. Put kyle-say Put my-object 透過 rados 指令檢查當程式正確執行後，就可以透過 rados 指令來確認物件是否正確被寫入： $ sudo rados -p data ls kyle-say my-object 透過 Get 指令來取得物件的內容： $ sudo rados -p data get kyle-say - Hello World! $ sudo rados -p data get my-object - This is my object.","tags":[{"name":"Ceph","slug":"Ceph","permalink":"https://kairen.github.io/tags/Ceph/"},{"name":"Storage","slug":"Storage","permalink":"https://kairen.github.io/tags/Storage/"},{"name":"Java","slug":"Java","permalink":"https://kairen.github.io/tags/Java/"}]},{"title":"Alluxio 分散式虛擬儲存系統","date":"2016-05-06T09:08:54.000Z","path":"2016/05/06/data-engineer/alluxio/","text":"Alluxio 是分散式虛擬儲存系統，早期名稱為 Tachyon ，而現在已正式改名 Alluxio，並發佈 1.0 版本 Aluxion 是一個記憶體虛擬分散式儲存系統，具有高效能、高容錯以及高可靠性的特色，它能夠統一資料的存取去串接機算框架和儲存系統的橋梁，像是同時可相容於 Hadoop MapReduce 和 Apache Spark 以及 Apache Flink 的計算框架和 Alibaba OSS、Amazon S3、OpenStack Swift,、GlusterFS 及 Ceph 的儲存系統 Install Java7首先安裝相關套件： $ sudo apt-get purge openjdk* $ sudo apt-get -y autoremove $ sudo add-apt-repository -y ppa:webupd8team/java $ sudo apt-get update $ echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections $ echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections $ sudo apt-get -y install oracle-java7-installer Download Alluxio 1.0.0接著下載 Alluxio： $ wget http://alluxio.org/downloads/files/1.0.0/alluxio-1.0.0-bin.tar.gz $ tar xvfz alluxio-1.0.0-bin.tar.gz $ cd alluxio-1.0.0 複製一個conf/alluxio-env.sh檔案 $ cp conf/alluxio-env.sh.template conf/alluxio-env.sh conf/alluxio-env.sh中加入ALLUXIO_UNDERFS_ADDRESS參數 export ALLUXIO_UNDERFS_ADDRESS=/tmp 確認 ssh localhost 可成功 $ssh-copy-id localhsot 格式化 Alluxio FileSystem 並開啟它 $ ./bin/alluxio format $ ./bin/alluxio-start.sh local 驗證 Alluxio 可於瀏覽器輸入http://localhost:19999，也可執行簡單的程式，如下: $ ./bin/alluxio runTest Basic CACHE THROUGH 執行後，如下: 2015-11-20 08:32:22,271 INFO (ClientBase.java:connect) - Alluxio client (version 1.0.0) is trying to connect with FileSystemMaster master @ localhost/127.0.0.1:19998 2015-11-20 08:32:22,294 INFO (ClientBase.java:connect) - Client registered with FileSystemMaster master @ localhost/127.0.0.1:19998 2015-11-20 08:32:22,387 INFO (BasicOperations.java:createFile) - createFile with fileId 33554431 took 127 ms. 2015-11-20 08:32:22,552 INFO (ClientBase.java:connect) - Alluxio client (version 1.0.0) is trying to connect with BlockMaster master @ localhost/127.0.0.1:19998 2015-11-20 08:32:22,553 INFO (ClientBase.java:connect) - Client registered with BlockMaster master @ localhost/127.0.0.1:19998 2015-11-20 08:32:22,604 INFO (WorkerClient.java:connect) - Connecting local worker @ /192.168.2.15:29998 2015-11-20 08:32:22,698 INFO (BasicOperations.java:writeFile) - writeFile to file /default_tests_files/BasicFile_CACHE_THROUGH took 311 ms. 2015-11-20 08:32:22,759 INFO (FileUtils.java:createStorageDirPath) - Folder /Volumes/ramdisk/alluxioworker/7226211928567857329 was created! 2015-11-20 08:32:22,809 INFO (LocalBlockOutStream.java:&lt;init&gt;) - LocalBlockOutStream created new file block, block path: /Volumes/ramdisk/alluxioworker/7226211928567857329/16777216 2015-11-20 08:32:22,886 INFO (BasicOperations.java:readFile) - readFile file /default_tests_files/BasicFile_CACHE_THROUGH took 187 ms. Passed the test! 執行更複雜的測試: $ ./bin/alluxio runTests 停止服務： $ ./bin/alluxio-stop.sh all","tags":[{"name":"Storage","slug":"Storage","permalink":"https://kairen.github.io/tags/Storage/"},{"name":"Java","slug":"Java","permalink":"https://kairen.github.io/tags/Java/"},{"name":"Spark","slug":"Spark","permalink":"https://kairen.github.io/tags/Spark/"}]},{"title":"DM-cache 建立混和區塊裝置","date":"2016-04-21T08:23:01.000Z","path":"2016/04/21/linux/ubuntu/dm-cache/","text":"DM-cache 是一種利用高速的儲存裝置給低速儲存裝置當作快取的技術，透過此一技術使儲存系統兼容容量與效能之間的平衡。DM-cache 目前是 Linunx 核心的一部份，透過裝置映射(Device Mapper)機制允許管理者建立混合的磁區(Volume)。 快取建立流程DM-cache 在比較新版本的 Linux Kernel 已經整合，以下為建置流程： $ sudo blockdev --getsize64 /dev/sdb 250059350016 # ssd-metadata : 4194304 + (250059350016 * 16 / 262144) / 512 = 38001 # ssd-blocks : 250059350016 / 512 - 38001 = 488359166 $ sudo dmsetup create ssd-metadata --table &#39;0 38001 linear /dev/sdb 0&#39; $ sudo dd if=/dev/zero of=/dev/mapper/ssd-metadata $ sudo dmsetup create ssd-blocks --table &#39;0 189008622 linear /dev/sdb 38001&#39; $ sudo blockdev --getsz /dev/sdc 1953525168 $ sudo dmsetup create home-cached --table &#39;0 1953525168 cache /dev/mapper/ssd-metadata /dev/mapper/ssd-blocks /dev/sdc 512 1 writeback default 0&#39; $ ls -l /dev/mapper/home-cached $ sudo mkdir /mnt/cache $ sudo mount /dev/mapper/home-cached /mnt/cache","tags":[{"name":"Storage","slug":"Storage","permalink":"https://kairen.github.io/tags/Storage/"},{"name":"Linux","slug":"Linux","permalink":"https://kairen.github.io/tags/Linux/"},{"name":"SSD","slug":"SSD","permalink":"https://kairen.github.io/tags/SSD/"}]},{"title":"Akka 基本介紹","date":"2016-04-06T09:08:54.000Z","path":"2016/04/06/data-engineer/akka-intro/","text":"AkkaAkka 是基於 Actor 模型以 Scala 程式語言開發而成的開源工具，被用在建置可擴展、彈性、高度並行、分散式與快速響應的 JVM 應用程式平台。現在許多高度並行（Concurrent） JVM 應用程式被廣泛應用，尤其以巨量資料處理框架為甚，諸如：Spark、Storm等，甚至可以基於 Akka 建置高平行的 Web 框架，更能建置分散式系統。Akka 最主要的目是要解決同步造成的效能問題，以及可能發生的死鎖問題。 Akka 目前擁有以下幾個特點： 高度並行與分散式 可擴展 擁有容錯機制 去中心化，且彈性 基於 Actors 模型 事務性 Actors 支援 JAVA 與 Scala API。 支援叢集 Akka 是基於 Actor 模型來開發，透過 Actor 能夠簡化死鎖與執行緒管理，可以非常容易開發正確並行化行程與系統，在 Akka 中 Actor 是最基本、最重要的元素，被用來完成工作。Actor 具有以下特性： 提供高級別的抽象，能簡化在並行（Concurrency）/平行（Parallelism）應用下的程式開發。 提供異步（Async）非阻塞、高效能的事件驅動程式模型。 非常輕量的事件處理（每 GB Heap 記憶體有幾百萬的 Actor） Actor 是一個運算實體，在開發程式中就是對實體之間所回應接受到的訊息做互動，同時並行的傳送有限數量的訊息給其他 Actor、建立有限數量的 Actor 以及設計指定接收到下一個訊息時的行為。 Actor 之間是獨立的，多個 Actor 進行互動只能透過自定的訊息（Message）來完成發送與接收處理。如果一個 Actor 在某一個時刻收到多個 Actor 發送的訊息，就會發生並行問題，這時就需要一個訊息佇列來進行訊息的儲存與分散。可參考 Akka 為範例，介紹 Actor 模型。 Akka 應用場景有以下幾個項目，當然這不是全部： 交易處理（Transaction Processing） 後端服務（Backend Service） 平行運算（Concurrency/Parallelism） 通訊 Hub（Communications Hub） 複雜事件串流處理（Complex Event Stream Processing） 安裝Akka 一般在 Java 有兩種安裝方式，如以下： 當作 Library 使用，就是直接 Import JAR 使用。可參考 Java Documentation。 將應用放到獨立的微核心（Microkernel）裡使用。可參考 Microkernel。","tags":[{"name":"Java","slug":"Java","permalink":"https://kairen.github.io/tags/Java/"},{"name":"Spark","slug":"Spark","permalink":"https://kairen.github.io/tags/Spark/"},{"name":"Concurrent","slug":"Concurrent","permalink":"https://kairen.github.io/tags/Concurrent/"}]},{"title":"Apache Flume 快速上手","date":"2016-04-04T09:08:54.000Z","path":"2016/04/04/data-engineer/apache-flume/","text":"Apache Flume 是一個分散式日誌收集系統，是由 Cloudera 公司開發的一款高效能、高可靠性和高恢復性的系統。它能從不同來源的大量日誌資料進行高效收集、聚合、移動，最後儲存到一個資料中心儲存系統當中。架構經過重構後，從原來的 Flume OG 到現在的 Flume NG。Flume NG 更像一個輕量化的小套件，簡單使用且容易適應不同方式收集日誌，且支援 Failover 和 Load Balancing 架構角色說明Flume 架構中主要有以下幾個核心: Event：一個資料單元，會附帶一個可選的訊息來源。ex:日誌紀錄、avro。 Client：操作位在原點的 Event 且將它傳送到 Flume Agent，主要是產生資料，運行在一個獨立程式。 Agent：一個獨立的 Flume 程式，包含 Source、Channel、Sink。 Source：用來消費從 Client 端收集資料到此的 Event，然後傳送到 Channel。 Channel：轉換 Event 的一個臨時儲存空間，保有從 Source 傳送過來的 Event。 Sink:從 Channel 中讀取並且移除 Event，將 Event 傳遞到 Flow Pipeline 的下一個 Agent（如果存在的話）。 安裝 Apache Flume本節將說明如何部署 Apache Flume，其中包含單機與多機部署方式。 單機首先節點需先安裝 Java，這邊採用 Oracle 的 Java8 來進行安裝： $ sudo add-apt-repository -y ppa:webupd8team/java $ sudo apt-get update $ sudo apt-get install -y oracle-java8-installer 完成後，在主機上安裝下載 Flume 套件，使用wget下載： $ wget &quot;ftp://ftp.twaren.net/Unix/Web/apache/flume/1.6.0/apache-flume-1.6.0-src.tar.gz&quot; $ wget &quot;ftp://ftp.twaren.net/Unix/Web/apache/flume/1.6.0/apache-flume-1.6.0-bin.tar.gz&quot; $ tar zxvf apache-flume-1.6.0-src.tar.gz $ tar zxvf apache-flume-1.6.0-bin.tar.gz 下載完後，將 src 覆蓋到 bin 底下，並解壓縮到/opt底下: $ sudo cp -ri apache-flume-1.6.0-src/* apache-flume-1.6.0-bin $ sudo mv /opt/apache-flume-1.5.0-bin /opt/flume 之後到/opt/flume/conf底下建立 example 配置檔: $ sudo vim example.conf 設定以下內容: # example.conf: A single-node Flume configuration # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = netcat a1.sources.r1.bind = localhost a1.sources.r1.port = 44444 # Describe the sink a1.sinks.k1.type = logger # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 之後啟動 Flume: $ bin/flume-ng agent -c conf -f example.conf -n a1 -Dflume.root.logger=INFO,console -c/--conf設定檔目錄，-f/--conf-file設定檔案路徑，-n/--name指定 agent 的名稱 驗證 Flume 開啟是否已開啟 $ jps 6760 Jps 6623 Application 最後開 shell 終端窗口，telnet 到配置監聽 port: $ telnet localhost 44444 # 輸入 HI! OK # 輸出 2016-02-24 11:40:30,389 INFO sink.LoggerSink: Event: { headers:{} body: 48 65 6C 6C 6F 20 77 6F 72 6C 64 21 0D HI!. } 多節點部署本節說明多機部署方式，流程為 Agent1 和 Agent2 主要是兩個來源蒐集端，本身會監聽且接收 Flume 本地端的訊息，然後將資料整合到 Collector 做資料日誌整理 部署節點角色規則如下: IP Address Role 192.168.100.94 Agent1 192.168.100.96 Agent2 192.168.100.97 Collector 一開始安裝配置與單機相同，從第一步驟到下載完後，將 src 覆蓋到 bin 底下，並解壓縮到/opt底下 然後到各自的/opt/flume/conf底下建立配置檔 Agent1和Agent2配置內容如下: # flume-client.properties: Agent1 Flume configuration #agent1 name agent1.sources = r1 agent1.sinks = k1 agent1.channels = c1 #set gruop agent1.sinkgroups = g1 #set channel agent1.channels.c1.type = memory agent1.channels.c1.capacity = 1000 agent1.channels.c1.transactionCapacity = 100 #set source agent1.sources.r1.channels = c1 agent1.sources.r1.type = netcat agent1.sources.r1.bind = localhost agent1.sources.r1.port = 52020 agent1.sources.r1.interceptors = i1 agent1.sources.r1.interceptors.i1.type = static agent1.sources.r1.interceptors.i1.key = Type agent1.sources.r1.interceptors.i1.value = LOGIN # set sink agent1.sinks.k1.channel = c1 agent1.sinks.k1.type = avro agent1.sinks.k1.hostname = 192.168.100.97 agent1.sinks.k1.port = 44444 #set sink group agent1.sinkgroups.g1.sinks = k1 #set failover agent1.sinkgroups.g1.processor.type = failover agent1.sinkgroups.g1.processor.priority.k1 = 10 agent1.sinkgroups.g1.processor.maxpenalty = 10000 Collector 配置內容如下: # flume-server.properties: Agent1 Flume configuration #set Agent name a1.sources = r1 a1.sinks = k1 a1.channels = c1 #set channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 #set source a1.sources.r1.type = avro a1.sources.r1.bind = 192.168.100.97 a1.sources.r1.port = 44444 a1.sources.r1.interceptors = i1 a1.sources.r1.interceptors.i1.type = static a1.sources.r1.interceptors.i1.key = Collector a1.sources.r1.interceptors.i1.value = NNA a1.sources.r1.channels = c1 # set sink a1.sinks.k1.type=logger a1.sinks.k1.channel=c1 最後分別啓動Agent和Collector的 Flume Agent: $ bin/flume-ng agent -n agent1 -c conf -f flume-client.properties -Dflume.root.logger=DEBUG,console Collector: $ bin/flume-ng agent -n a1 -c conf -f flume-server.properties -Dflume.root.logger=DEBUG,console","tags":[{"name":"Spark","slug":"Spark","permalink":"https://kairen.github.io/tags/Spark/"},{"name":"Logging","slug":"Logging","permalink":"https://kairen.github.io/tags/Logging/"}]},{"title":"DRBD 進行跨節點的區塊儲存備份","date":"2016-04-01T08:23:01.000Z","path":"2016/04/01/linux/ubuntu/drbd/","text":"DRBD（Distributed Replicated BlockDevice）是一個分散式區塊裝置備份系統，DRBD 是由 Kernel 模組與相關腳本組成，被用來建置高可靠的叢集服務。實現方式是透過網路來 mirror 整個區塊裝置，一般可作為是網路 RAID 的一類。DRBD 允許使用者在遠端機器上建立一個 Local 區塊裝置的即時 mirror。 安裝 DRBD本教學將使用以下主機數量與角色： IP Address Role Disk 172.16.1.184 master /dev/vdb 172.16.1.182 backup /dev/vdb 在 Ubuntu 14.04 LTS Server 可以直接透過apt-get來安裝 DRBD，指令如下： $ sudo apt-get install linux-image-extra-virtual $ sudo apt-get install -y drbd8-utils 完成後可以透過 lsmod 檢查： $ lsmod | grep drbd # 若沒有則使用以下指令 $ sudo modprobe drbd P.S 若出現錯誤請重新啟動主機。 DRBD 設定首先在各兩個節點透過fdisk來建立分區： $ fdisk /dev/vdb Command (m for help): n Partition type: p primary (0 primary, 0 extended, 4 free) e extended Select (default p): p Partition number (1-4, default 1): 1 First sector (2048-20971519, default 2048): 2048 Last sector, +sectors or +size{K,M,G} (2048-20971519, default 20971519): Using default value 20971519 Command (m for help): w 之後建立/etc/drbd.d/ha.res設定檔，並加入以下內容： resource ha { on drbd-master { device /dev/drbd0; disk /dev/vdb1; address 172.16.1.184:1166; meta-disk internal; } on drbd-backup { device /dev/drbd0; disk /dev/vdb1; address 172.16.1.182:1166; meta-disk internal; } } 上面都設定完成後，到master接著透過drbdadm指令建立： $ drbdadm create-md ha Writing meta data... md_offset 10736365568 al_offset 10736332800 bm_offset 10736005120 Found some data ==&gt; This might destroy existing data! &lt;== Do you want to proceed? [need to type &#39;yes&#39; to confirm] yes initializing activity log NOT initializing bitmap New drbd meta data block successfully created. 透過指令啟用： $ drbdadm up ha $ drbd-overview 0:ha/0 WFConnection Secondary/Unknown Inconsistent/DUnknown C r----s 設定某一節點為主節點： $ drbdadm -- --force primary ha $ drbd-overview 0:ha/0 WFConnection Primary/Unknown UpToDate/DUnknown C r----s 檢查是否有正確啟動： $ cd /dev/drbd $ ls by-disk by-res $ ls -al by-disk/ total 0 drwxr-xr-x 2 root root 60 Mar 24 16:46 . drwxr-xr-x 4 root root 80 Mar 24 16:46 .. lrwxrwxrwx 1 root root 11 Mar 24 16:49 vdb1 -&gt; ../../drbd0 $ ls -al by-res/ha/ lrwxrwxrwx 1 root root 11 Mar 24 16:49 by-res/ha -&gt; ../../drbd0 若沒問題後，即可 mount 使用： $ mount /dev/drbd0 /mnt/ 若出現mount: you must specify the filesystem type的話，記得格式化： $ mkfs.ext4 /dev/drbd0 這時候再透過指令查詢，可以看到已成功同步： $ drbd-overview 0:ha/0 WFConnection Primary/Unknown UpToDate/DUnknown C r----s /mnt ext4 9.8G 23M 9.2G 1% 接著到backup節點，執行類似上面做法： $ drbdadm create-md ha $ drbdadm up ha $ drbd-overview 0:ha/0 SyncTarget Secondary/Primary Inconsistent/UpToDate C r----- [========&gt;...........] sync&#39;ed: 47.1% (5420/10236)Mfinish: 0:02:10 speed: 42,600 (45,252) want: 0 K/se","tags":[{"name":"Storage","slug":"Storage","permalink":"https://kairen.github.io/tags/Storage/"},{"name":"Linux","slug":"Linux","permalink":"https://kairen.github.io/tags/Linux/"}]},{"title":"OpenStack 貢獻基本流程","date":"2016-04-01T08:23:01.000Z","path":"2016/04/01/openstack/how-to-contribute/","text":"本部分將說明如何貢獻程式碼到 OpenStack 社群上。 申請 OpenStack 帳號流程首先註冊 Launchpad.net 帳號： 到 Launchpad 註冊一個帳號，使用 inwinStack 信箱 完成申請後，進入 inwinSTACK Org 點選加入請求 確認自己出現在 inwinSTACK Menber List 之後註冊 OpenStack Foundation 的 Foundation Member，到 Register 申請 Foundation Member。並完成以下步驟： 填寫使用者資訊。 在 Affiliations 部分，點選 Add New Affiliations的組織輸入inwinSTACK，選擇開始時間，勾選 Is Current? 最後填寫住址與密碼資訊，住址翻譯網站 : http://goo.gl/qez9tt 然後使用 Launchpad.net 登入 OpenStack的 Gerrit 平台，到 Gerrit 點選 sign in，登入 Launchpad 帳號。當第一次登入成功後，會需要你設定唯一的 username（注意設定後就不能更改）。並完成以下步驟： 簽署 ICLA， 到 New Agreement 選擇ICLA（OpenStack Individual Contributor License Agreement） 上傳 SSH 公有金鑰， 到 Git Review SSH Keys 上傳 Key。 用ssh-keygen -t rsa -b 4096 -C &quot;your_email@example.com&quot; 指令產生金鑰，複製 ~/.ssh/id_rsa.pub到 review.openstack.org 上。產生 key 參考 generating-ssh-keys 完成後，設定 Git 資訊： git config --global user.name &quot;Firstname Lastname&quot; git config --global user.email &quot;your_email@youremail.com&quot; git config --global gitreview.username &quot;yourgerritusername&quot; 之後安裝 git-review，參考 Git Review install 進行安裝。 貢獻程式碼（已 openstack-manuals 為例）一個基本的貢獻流程如下圖所示： 首先透過 git clone 來下載程式專案，並設定 review： git clone https://github.com/openstack/openstack-manuals cd openstack-manuals git review -s 成功的話，會在目錄底下產生檔案.gitreview。若 auth 有問題請檢查 ssh key 是否正確。 並透過 git 來切換到最新版本： git checkout master git pull 新建一個 branch，在單獨的一行中撰寫 summary（小於50個字），然後第二段進行詳細的描述。如果是實現 bp 或修改 bug，需要註明： blueprint BP-NAME bug BUG-NUMBER 一個簡單範例： Adds some summary less than 50 characters ...Long multiline description of the change... Implements: blueprint authentication Fixes: bug # 123456 詳細的程式碼提交資訊，參考 GitCommitMessages。 修改完程式碼後，記得跑過UT的測試。然後提交程式碼，並申請 review： git commit -a git review 提交 review 之後，會出現在 Git review，可以查看狀態和資訊，並自動執行 CI，然後程式碼會由 review 人員進行程式碼的 review。 如果 jenkins 回報了 failure，可以查看 Logs 除錯。如果確認不是自己的 patch 導致，可以在 comment 上留言 recheck no bug，重新再跑 Test。 如果 review 過程中，發現程式碼需要修改，再次提交時直接使用已存在的 Change-Id： git commit -a --amend git review","tags":[{"name":"OpenStack","slug":"OpenStack","permalink":"https://kairen.github.io/tags/OpenStack/"},{"name":"Git","slug":"Git","permalink":"https://kairen.github.io/tags/Git/"},{"name":"Python","slug":"Python","permalink":"https://kairen.github.io/tags/Python/"}]},{"title":"利用 Keepalived 提供 VIP","date":"2016-03-29T04:23:01.000Z","path":"2016/03/29/linux/ubuntu/keepalived/","text":"Keepalived 是一種基於 VRRP 協定實現的高可靠 Web 服務方案，用於防止單點故障問題。因此一個 Web 服務運作至少會擁有兩台伺服器執行 Keepalived，一台作為 master，一台作為 backup，並提供一個虛擬 IP（VIP），master 會定期發送特定訊息給 backup 伺服器，當 backup 沒收到 master 訊息時，表示 master 已故障，這時候 backup 會接管 VIP，繼續提供服務，來確保服務的高可靠性。 VRRPVRRP（Virtual Router Redundancy Protocol，虛擬路由器備援協定），是一個提供備援路由器來解決單點故障問題的協定，該協定有兩個重要概念： VRRP 路由器與虛擬路由器：VRRP 路由器是表示運作 VRRP 的路由器，是一個實體裝置，而虛擬路由器是指由 VRRP 建立的邏輯路由器。一組 VRRP 路由器協同運作，並一起構成一台虛擬路由器，該虛擬路由對外提供一個唯一固定的 IP 與 MAC 位址的邏輯路由器。 主控制路由器（master）與備援路由器（backup）：主要是在一組 VRRP 中的兩種互斥角色。一個 VRRP 群組中只能擁有一台是 master，但可以有多個 backup 路由器。 VRRP 協定使用選擇策略從路由器群組挑選一台作為 master 來負責 ARP 與轉送 IP 封包，群組中其他路由器則作為 backup 的角色處理等待狀態。當由於某種原因造成 master 故障時，backup 會在幾秒內成為 master 繼續提供服務，該階段不用改變任何 IP 與 MAC 位址。 Keepalived 節點配置本教學將使用以下主機數量與角色： IP Address Role 172.16.1.101 vip 172.16.1.102 master 172.16.1.103 backup 安裝與設定這 ubuntu 14.04 LTS Server 中已經內建了 Keepalived 可以透過 apt-get 來安裝： $ sudo apt-get install -y keepalived 也可以透過 source code 進行安裝，流程如下： $ sudo apt-get install build-essential libssl-dev $ wget http://www.keepalived.org/software/keepalived-1.2.2.tar.gz $ tar -zxvf keepalived-1.2.2.tar.gz $ cd keepalived-1.2.2 $ ./configure --prefix=/usr/local/keepalived $ make &amp;&amp; make install 完成後，要將需要的設定檔進行複製到/etc/: $ cp /usr/local/keepalived/etc/rc.d/init.d/keepalived /etc/init.d/keepalived $ cp /usr/local/keepalived/sbin/keepalived /usr/sbin/ $ cp /usr/local/keepalived/etc/sysconfig/keepalived /etc/sysconfig/ $ mkdir -p /etc/keepalived/ $ cp /usr/local/etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf 安裝完成後編輯/etc/keepalived/keepalived.conf檔案進行設定，在master節點加入以下內容： global_defs { notification_email { user@example.com } notification_email_from mail@example.org smtp_server 172.16.1.100 smtp_connect_timeout 30 router_id LVS_DEVEL } vrrp_instance VI_1 { state MASTER # Tag 為 MASTER interface eth0 virtual_router_id 51 priority 101 # MASTER 權重高於 BACKUP advert_int 1 mcast_src_ip 172.16.1.102 # VRRP 實體主機的 IP authentication { auth_type PASS # Master 驗證方式 auth_pass 1111 } #VIP virtual_ipaddress { 172.16.1.101 # 虛擬 IP } } Master 完成後，接著編輯backup節點的/etc/keepalived/keepalived.conf，加入以下內容： global_defs { notification_email { user@example.com } notification_email_from mail@example.org smtp_server 172.16.1.100 smtp_connect_timeout 30 router_id LVS_DEVEL } vrrp_instance VI_1 { state BACKUP # Tag 為 BACKUP interface eth0 virtual_router_id 51 priority 100 # 權重要低於 MASTER advert_int 1 mcast_src_ip 172.16.1.103 # vrrp 實體主機 IP authentication { auth_type PASS auth_pass 1111 } # VIP virtual_ipaddress { 172.16.1.101 # 提供的 VIP } }","tags":[{"name":"Linux","slug":"Linux","permalink":"https://kairen.github.io/tags/Linux/"},{"name":"High Availability","slug":"High-Availability","permalink":"https://kairen.github.io/tags/High-Availability/"}]},{"title":"使用 HAProxy 進行負載平衡","date":"2016-03-28T08:23:01.000Z","path":"2016/03/28/linux/ubuntu/haproxy/","text":"HAProxy 提供了高可靠性、負載平衡（Load Balancing）、基於 TCP 以及 HTTP 的應用程式代理，更支援了虛擬機的使用。HAProxy 是一個開放式原始碼，免費、快速以及非常可靠，根據官方測試結果，該軟體最高能夠支援到 10G 的並行傳輸，因此特別適合使用在負載很大的 Web 伺服器，且這些伺服器通常需要保持 Session 或者 Layer 7 網路的處理，但這些都可以使用 HAProxy 來完成。 HAProxy 具有以下幾個優點： 開放式原始碼，因此免費，且穩定性高 能夠負荷 10G 網路的並行傳輸 支援連線拒絕功能 支援全透明化的代理 擁有內建的監控狀態儀表板 支援虛擬機的使用 HAProxy 安裝本教學會使用到一台 Proxy 節點與兩台 Web 節點，如下： IP Address Role 172.17.0.2 proxy 172.17.0.3 web-1 172.17.0.4 web-2 本篇採用 Ubuntu 作業系統，因此可透過 apt 直接安裝，以下範例是在 Ubuntu Server 環境中操作： $ sudo apt-get install software-properties-common python-software-properties $ sudo apt-add-repository ppa:vbernat/haproxy-1.5 $ sudo apt-get update $ sudo apt-get install haproxy 若要安裝其他版本，可以修改成以下： sudo apt-add-repository ppa:vbernat/haproxy-1.6 HAProxy 設定完成安裝後，要透過編輯/etc/haproxy/haproxy.cfg設定檔來配置 Proxy： global log /dev/log local0 log /dev/log local1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin stats timeout 30s user haproxy group haproxy daemon maxconn 1024 ca-base /etc/ssl/certs crt-base /etc/ssl/private ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS ssl-default-bind-options no-sslv3 defaults log global mode http option httplog option dontlognull timeout connect 5000 timeout client 50000 timeout server 50000 errorfile 400 /etc/haproxy/errors/400.http errorfile 403 /etc/haproxy/errors/403.http errorfile 408 /etc/haproxy/errors/408.http errorfile 500 /etc/haproxy/errors/500.http errorfile 502 /etc/haproxy/errors/502.http errorfile 503 /etc/haproxy/errors/503.http errorfile 504 /etc/haproxy/errors/504.http frontend nginxs_proxy bind 172.17.0.2:80 mode http default_backend nginx_servers backend nginx_servers mode http balance roundrobin option forwardfor http-request set-header X-Forwarded-Port %[dst_port] http-request add-header X-Forwarded-Proto https if { ssl_fc } option httpchk HEAD / HTTP/1.1\\r\\nHost:localhost server web1 172.17.0.3:80 check cookie s1 server web2 172.17.0.4:80 check cookie s2 listen haproxy_stats bind 0.0.0.0:8080 stats enable stats hide-version stats refresh 30s stats show-node stats auth username:password stats uri /stats 完成設定後，需重啟服務： $ sudo service haproxy restart","tags":[{"name":"Linux","slug":"Linux","permalink":"https://kairen.github.io/tags/Linux/"},{"name":"Load Balancer","slug":"Load-Balancer","permalink":"https://kairen.github.io/tags/Load-Balancer/"}]},{"title":"透過官方 Ansible 部署 Kubernetes(Unrecommended)","date":"2016-02-24T09:08:54.000Z","path":"2016/02/24/kubernetes/deploy/official-ansible/","text":"Kubernetes 提供了許多雲端平台與作業系統的安裝方式，本篇將使用官方 Ansible Playbook 來部署 Kubernetes 到 CentOS 7 系統上，其中 Kubernetes 將額外部署 Dashboard 與 DNS 等 Add-ons。其他更多平台的部署可以參考 Creating a Kubernetes Cluster。 本次安裝版本為： Kubernetes v1.5.2 Etcd v3.1.0 Flannel v0.5.5 Docker v1.12.6 節點資訊本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統採用CentOS 7.x： IP Address Role CPU Memory 172.16.35.12 master1 2 4G 172.16.35.10 node1 2 4G 172.16.35.11 node2 2 4G 這邊 master 為主要控制節點，node 為應用程式工作節點。 預先準備資訊首先安裝前要確認以下幾項都已將準備完成： 所有節點彼此網路互通，並且不需要 SSH 密碼即可登入。 所有主機擁有 Sudoer 權限。 所有節點需要設定/etc/host解析到所有主機。 master1或部署節點需要安裝 Ansible 與相關套件： $ sudo yum install -y epel-release $ sudo yum install -y ansible python-netaddr git 部署 Kubernetes首先透過 Git 工具來取得 Kubernetes 官方的 Ansible Playbook 專案，並進入到目錄： $ git clone &quot;https://github.com/kubernetes/contrib.git&quot; $ cd contrib/ansible 編輯inventory/hosts檔案(inventory)，並加入以下內容： [masters] master1 [etcd:children] masters [nodes] node[1:2] 然後利用 Ansible ping module 來檢查節點是否可以溝通： $ ansible -i inventory/hosts all -m ping master1 | SUCCESS =&gt; { &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot; } node2 | SUCCESS =&gt; { &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot; } node1 | SUCCESS =&gt; { &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot; } 編輯inventory/group_vars/all.yml檔案，並修改以下內容： source_type: packageManager cluster_name: cluster.kairen networking: flannel cluster_logging: true cluster_monitoring: true kube_dash: true dns_setup: true dns_replicas: 1 其他參數可自行選擇是否啟用。 (Option)編輯roles/flannel/defaults/main.yaml檔案，修改以下內容： flannel_options: --iface=enp0s8 這邊主要解決 Vagrant 預設抓 NAT 網卡問題。 完成後進入到scripts目錄，並執行以下指令進行部署： $ INVENTORY=../inventory/hosts ./deploy-cluster.sh ... PLAY RECAP ********************************************************************* master1 : ok=229 changed=93 unreachable=0 failed=0 node1 : ok=126 changed=58 unreachable=0 failed=0 node2 : ok=122 changed=58 unreachable=0 failed=0 經過一段時候就會完成，若沒有發生任何錯誤的話，就可以令用 kubectl 查看節點資訊： $ kubectl get nodes NAME STATUS AGE node1 Ready 3m node2 Ready 3m 查看系統命名空間的 pod 與 svc 資訊： $ kubectl get svc --all-namespaces NAMESPACE NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes 10.254.0.1 &lt;none&gt; 443/TCP 3h kube-system elasticsearch-logging 10.254.164.5 &lt;none&gt; 9200/TCP 3h kube-system heapster 10.254.213.162 &lt;none&gt; 80/TCP 3h kube-system kibana-logging 10.254.176.124 &lt;none&gt; 5601/TCP 3h kube-system kube-dns 10.254.0.10 &lt;none&gt; 53/UDP,53/TCP 3h kube-system kubedash 10.254.68.80 80/TCP 3h kube-system kubernetes-dashboard 10.254.84.138 &lt;none&gt; 80/TCP 3h kube-system monitoring-grafana 10.254.193.233 &lt;none&gt; 80/TCP 3h kube-system monitoring-influxdb 10.254.135.115 &lt;none&gt; 8083/TCP,8086/TCP 3h 完成後，透過瀏覽器進入 Dashboard。 Targeted runsAnsible 提供 Tag 來指定執行或者忽略，這邊腳本也提供了該功能，如以下只部署 Etcd： $ ./deploy-cluster.sh --tags=etcd","tags":[{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"},{"name":"Ansible","slug":"Ansible","permalink":"https://kairen.github.io/tags/Ansible/"}]},{"title":"Vagrant CoreOS 部署 Kubernetes 測試叢集(Unrecommended)","date":"2016-02-23T09:08:54.000Z","path":"2016/02/23/kubernetes/deploy/vagrant-multi-install/","text":"本節將透過 Vagrant 與 CoreOS 來部署單機多節點的 Kubernetes 虛擬叢集，並使用 Kubernetest CLI 工具與 API 進行溝通。 本次安裝版本為： CoreOS alpha. Kubernetes v1.5.4. 事前準備首先必須在主機上安裝Vagrant工具，點選該 Vagrant downloads 頁面抓取當前系統的版本，並完成安裝。 接著在主機上安裝kubectl，該程式是主要與 Kubernetes API 進行溝通的工具，透過 Curl 工具來下載。如果是 Linux 作業系統，請下載以下： $ curl -O &quot;https://storage.googleapis.com/kubernetes-release/release/v1.5.4/bin/linux/amd64/kubectl&quot; $ chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/ 如果是 OS X，請取代 URL 為以下： $ curl -O &quot;https://storage.googleapis.com/kubernetes-release/release/v1.5.4/bin/darwin/amd64/kubectl&quot; $ chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/ 安裝 Kubernetes首先透過 Git 工具來下載 CoreOS 的 Kubernetes 專案，裡面包含了描述 Vagrant 要建立的檔案： $ git clone https://github.com/coreos/coreos-kubernetes.git $ cd coreos-kubernetes/multi-node/vagrant 接著複製config.rb.sample並改成config.rb檔案： $ cp config.rb.sample config.rb 編輯config.rb設定檔，並修改成以下內容： $update_channel=&quot;alpha&quot; $controller_count=1 $controller_vm_memory=1024 $worker_count=2 $worker_vm_memory=1024 $etcd_count=1 $etcd_vm_memory=512 (Option)若 CNI 想使用 Calico 網路與安裝不同版本 Kubernetes 的話，需要修改../generic/controller-install.sh與./generic/worker-install.sh檔案以下內容： export K8S_VER=v1.5.4_coreos.0 export USE_CALICO=true 設定好後，即可透過以下指令來建立 SSL CA Key 與更新 Box 資訊： $ sudo ln -sf /usr/local/bin/openssl /opt/vagrant/embedded/bin/openssl $ vagrant box update 確認完成後，執行以下指令開始建立叢集： $ vagrant up P.S. 這邊建置起來裡面虛擬機還要下載一些東西，要等一下子才會真正完成。 設定 Kubernetes Config當完成部署後，需要配置 kubectl 連接 API，這邊可以選擇以下兩種的其中一種進行： 使用一個 Custom Kubernetes Config$ export KUBECONFIG=&quot;${KUBECONFIG}:$(pwd)/kubeconfig&quot; $ kubectl config use-context vagrant-multi 更新與使用本地的 Config$ kubectl config set-cluster vagrant-multi-cluster --server=&quot;https://172.17.4.101:443&quot; --certificate-authority=${PWD}/ssl/ca.pem $ kubectl config set-credentials vagrant-multi-admin --certificate-authority=${PWD}/ssl/ca.pem --client-key=${PWD}/ssl/admin-key.pem --client-certificate=${PWD}/ssl/admin.pem $ kubectl config set-context vagrant-multi --cluster=vagrant-multi-cluster --user=vagrant-multi-admin $ kubectl config use-context vagrant-multi Kubernetes 系統驗證完成設定後，即可使用 kubectl 來查看節點資訊： $ kubectl get nodes NAME STATUS AGE 172.17.4.101 Ready,SchedulingDisabled 3m 172.17.4.201 Ready 3m 172.17.4.202 Ready 3m 查看系統命名空間的 pod 與 svc 資訊： $ kubectl get po --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system heapster-v1.2.0-4088228293-4vv12 2/2 Running 0 28m kube-system kube-apiserver-172.17.4.101 1/1 Running 0 29m kube-system kube-controller-manager-172.17.4.101 1/1 Running 0 29m kube-system kube-dns-782804071-w6w12 4/4 Running 0 29m kube-system kube-dns-autoscaler-2715466192-q1k18 1/1 Running 0 29m kube-system kube-proxy-172.17.4.101 1/1 Running 0 28m kube-system kube-proxy-172.17.4.201 1/1 Running 0 29m kube-system kube-proxy-172.17.4.202 1/1 Running 0 29m kube-system kube-scheduler-172.17.4.101 1/1 Running 0 28m kube-system kubernetes-dashboard-3543765157-vk0mt 1/1 Running 0 29m","tags":[{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"},{"name":"CoreOS","slug":"CoreOS","permalink":"https://kairen.github.io/tags/CoreOS/"},{"name":"Vagrant","slug":"Vagrant","permalink":"https://kairen.github.io/tags/Vagrant/"}]},{"title":"Ansible Playbooks","date":"2016-02-18T04:23:01.000Z","path":"2016/02/18/devops/ansible/ansible-playbook/","text":"Playbooks 是 Ansible 的設定、部署與編配語言等。可以被用來描述一個被遠端的主機要執行的指令方案，或是一組 IT 行程執行的指令集合。 在基礎層面上，Playbooks 可以被用來管理部署到遠端主機的組態檔案，在更高階層上 Playbooks 可以循序對多層式架構上的伺服器執行線上的 Polling 更新內部的操作，並將操作委派給其他主機，包含過程中發生的監視器服務、負載平衡伺服器等。 Playbooks 被設計成易懂與基於 Text Language 的二次開發，有許多方式可以組合 Playbooks 與其附屬的檔案。建議在閱讀 Playbooks 時，同步閱讀 Example Playbooks。 Playbooks 與 ad-hoc 相比是一種完全不同的 Ansible 應用方式，該方式也是 Ansible 強大之處。簡單來說 Playbooks 是一種組態管理系統與多機器部署系統基礎，與現有系統不同之處在於非常適合複雜的部署。若想參考範例，可以參閱 ansible-examples repository。 Playbook Language ExamplePlaybook 採用 YAML 語法來表示。playbook 由一或多個plays組成的內容為元素的列表。在play中一組機器會被映射成定義好的角色，在 Ansible 中play內容也被稱為tasks。 以下是一個簡單的範例： --- - name: Configure cluster with apache hosts: cluster sudo: yes remote_user: ubuntu tasks: - name: install apache2 apt: name=apache2 update_cache=yes state=latest - name: enabled mod_rewrite apache2_module: name=rewrite state=present notify: - restart apache2 - name: apache2 listen on port 8081 lineinfile: dest=/etc/apache2/ports.conf regexp=&quot;^Listen 80&quot; line=&quot;Listen 8081&quot; state=present notify: - restart apache2 - name: apache2 virtualhost on port 8081 lineinfile: dest=/etc/apache2/sites-available/000-default.conf regexp=&quot;^&lt;VirtualHost \\*:80&gt;&quot; line=&quot;&lt;VirtualHost *:8081&gt;&quot; state=present notify: - restart apache2 handlers: - name: restart apache2 service: name=apache2 state=restarted 從以上範例中，可以由上往下大概知道結構如何，但我們還是要依序講解一下。","tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://kairen.github.io/tags/DevOps/"},{"name":"Automation Engine","slug":"Automation-Engine","permalink":"https://kairen.github.io/tags/Automation-Engine/"},{"name":"Ansible","slug":"Ansible","permalink":"https://kairen.github.io/tags/Ansible/"}]},{"title":"Ansible Dynamic Inventory","date":"2016-02-17T06:23:01.000Z","path":"2016/02/17/devops/ansible/ansible-dynamic-inventory/","text":"在預設情況下，我們所使用的都是一個靜態的 Inventory 檔案，編輯主機、群組以及變數時都需要固定手動編輯完成。 Ansible 提供了 Dynamic Inventory 檔案，這個檔案是透過呼叫外部腳本或程式來產生指定的格式的 JSON 字串。這樣做的好處就是可以透過這個外部腳本與程式來管理系統（如 API）抓取最新資源訊息。 Ansible 使用者通常會互動於大多數的物理硬體，因此會有許多人可能也是Cobbler的使用者。 Cobbler 是一個透過網路部署 Linux 的服務，而且經過調整更能夠進行 Windows 部署。該工具是使用 Python 開發，因此輕巧便利，使用簡單指令就可以完成 PXE 網路安裝環境。 比如說以下這個範例就是透過腳本程式產生的： { &quot;production&quot;: [&quot;delaware.example.com&quot;, &quot;georgia.example.com&quot;, &quot;maryland.example.com&quot;, &quot;newhampshire.example.com&quot;, &quot;newjersey.example.com&quot;, &quot;newyork.example.com&quot;, &quot;northcarolina.example.com&quot;, &quot;pennsylvania.example.com&quot;, &quot;rhodeisland.example.com&quot;, &quot;virginia.example.com&quot; ], &quot;staging&quot;: [&quot;ontario.example.com&quot;, &quot;quebec.example.com&quot;], &quot;vagrant&quot;: [&quot;vagrant1&quot;, &quot;vagrant2&quot;, &quot;vagrant3&quot;], &quot;lb&quot;: [&quot;delaware.example.com&quot;], &quot;web&quot;: [&quot;georgia.example.com&quot;, &quot;newhampshire.example.com&quot;, &quot;newjersey.example.com&quot;, &quot;ontario.example.com&quot;, &quot;vagrant1&quot; ] &quot;task&quot;: [&quot;newyork.example.com&quot;, &quot;northcarolina.example.com&quot;, &quot;ontario.example.com&quot;, &quot;vagrant2&quot; ], &quot;redis&quot;: [&quot;pennsylvania.example.com&quot;, &quot;quebec.example.com&quot;, &quot;vagrant3&quot;], &quot;db&quot;: [&quot;rhodeisland.example.com&quot;, &quot;virginia.example.com&quot;, &quot;vagrant3&quot;] } 使用方式如下： 加上執行(x)的權限給 script 將 script 與 inventory file 放在同一目錄 如此一來 ansible 就會自動讀取 inventory file 取得靜態的 inventory 資訊，並執行 script 取得動態的 inventory 資訊，將兩者 merge 後並使用。 目前官方已有提供幾個 Dynamic Inventory 的範例教學，如以下： Cobbler External Inventory Script AWS EC2 External Inventory Script OpenStack External Inventory Script","tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://kairen.github.io/tags/DevOps/"},{"name":"Automation Engine","slug":"Automation-Engine","permalink":"https://kairen.github.io/tags/Automation-Engine/"},{"name":"Ansible","slug":"Ansible","permalink":"https://kairen.github.io/tags/Ansible/"}]},{"title":"Ansible Ad-Hoc 指令與 Modules","date":"2016-02-17T04:23:01.000Z","path":"2016/02/17/devops/ansible/ansible-adhoc/","text":"ad-hoc command（特設指令）簡單說就是直接執行指令，這些指令不需要要被保存在日後使用。在進行 Ansible 的 Playbook 語言之前，了解 ad-hoc 指令也可以幫助我們做一些快速的事情，不一定要寫出一個完整的 Playbooks 指令。 模組（也被稱為Task plugins或是Library plugins）是 Ansible 中實際執行的功能，它們會在每個 Playbook 任務中被執行，也可以透過 ansible 直接呼叫使用。目前 Ansible 已經擁有許多模組，可參閱 Module Index。 首先我們先編輯/etc/ansible/hosts，加入以下內容： [cluster] ansible-slave-1 ansible_host=172.16.1.206 ansible-slave-2 ansible_host=172.16.1.207 ansible-slave-3 ansible_host=172.16.1.208 Parallelism and Shell Commands接下來我們將透過範例來說明 Ansible 的平行性與 Shell 指令，一開始我們需要將 ssh-agent 加入私有金鑰管理： $ ssh-agent bash $ ssh-add ~/.ssh/id_rsa 如果不想要透過 ssh-agent 的金鑰登入，可以在 ansible 指令使用--ask-pass（-k）參數，但是建議使用 ssh-agent。 剛剛我們在 Inventroy 檔案建立了一個群組（Cluster），裡面擁有三台主機，接下來我們透過執行一個簡單的指令與參數來實現並行執行： $ ansible cluster -a &quot;sleep 2&quot; -f 1 上面的指令會隨機執行一台主機，完成後接下執行下一台，然而-f參數可以改變一次執行的 bash，好比改成： $ ansible cluster -a &quot;sleep 2&quot; -f 3 會發現 bash 是平行執行的。 我們除了使用預設的 user 登入以外，也可以指定要登入的使用者： $ ansible cluster -a &quot;echo $USER&quot; -u ubuntu 如果想透過特權（sudo）執行指令，可以透過以下方式： $ ansible cluster -a &quot;apt-get update&quot; -u ubuntu --become 若該使用者沒有設定 sudo 不需要密碼的話，可以加入--ask-sudo-pass（-k）來驗證密碼。也可以使用--become-method來改變權限使用方法（預設為 sudo）。 也可以透過--become-user來切換使用者： $ ansible cluster -a &quot;echo $USER&quot; -u ubuntu --become-user root 若有密碼，可以使用--ask-sudo-pass。 以上是基本的幾個指令，但當使用 ansible ad-hoc 指令時，會發現無法使用shell 變數以及pipeline 等相關，這是因為預設的 ansible ad-hoc 指令不支援，故要改用 shell 模組來執行： $ ansible cluster -m shell -a &#39;echo $(hostname) | grep -o &quot;[0-9]&quot;&#39; 以上指令的-m表示要使用的模組。但要注意！使用 ansible 指令時要留意&quot;cmd&quot;與&#39;comd&#39;的差別，比如使用&quot;cmd&quot;會是抓取當前系統的資訊。 File TransferAnsible 能夠以平行的方式同時scp大量的檔案到多台主機上，如以下範例： $ ansible cluster -m copy -a &quot;src=/etc/hosts dest=~/hosts&quot; 也可以使用file模組做到修改檔案的權限與屬性（這邊可以將copy替換成file）： $ ansible cluster -m file -a &quot;dest=~/hosts mode=600&quot; $ ansible cluster -m file -a &quot;dest=~/hosts mode=600 owner=ubuntu group=ubuntu&quot; file模組也能夠建立目錄： $ ansible cluster -m file -a &quot;dest=~/data mode=755 owner=ubuntu group=ubuntu state=directory&quot; 若要刪除可以使用以下方式： $ ansible cluster -m file -a &quot;dest=~/data state=absent&quot; Managing Packages目前 Ansible 已經支援了yum與apt的模組，以下是一個apt 確認指定軟體名稱是否已安裝，並且不升級： $ ansible cluster -m apt -a &quot;name=ntp state=present&quot; 也可以在name=ntp後面加版本號，如name=ntp-{version}。 若要確認是否為最新版本，可以使用以下指令： $ ansible cluster -m apt -a &quot;name=ntp state=latest&quot; 若要確認一個軟體套件沒有安裝，可以使用以下指令： $ ansible cluster -m apt -a &quot;name=ntp state=absent&quot; --become 更多的指令資訊可以查看 About Modules。 Users and Groups若想要建立系統使用者與群組，可以使用user模組，如以下範例： $ ansible all -m user -a &quot;name=food password=food&quot; --become 刪除則如以下： $ ansible all -m user -a &quot;name=food state=absent&quot; -b --become與-b是等效的。 Deploying From Source ControlAnsible 不只可以透過apt與ad-hoc 指令來安裝與部署應用程式，也能用git模組來安裝： $ ansible cluster -m git -a &quot;repo=https://github.com/imac-cloud/Spark-tutorial.git dest=~/spark-tutorial&quot; -f 3 Managing ServicesAnsible 也可以透過service模組來確認指定主機是否已啟動服務： $ ansible cluster -m service -a &quot;name=ssh state=started&quot; 也可以改變state來執行對應動作，如state=restarted就會重新啟動服務。 Time Limited Background Operations有些操作需要長時間執行於後台，在指令開始執行後，可以持續檢查執行狀態，但是若不想要獲取該資訊可以使用以下指令： $ ansible ansible-slave-1 -B 3600 -P 0 -a &quot;/usr/bin/long_running_operation --do-stuff&quot; 若要檢查執行狀態的話，可以使用async_status來傳入一個jid查看： $ ansible cluster -m async_status -a &quot;jid=488359678239.2844&quot; 獲取狀態指令如下： $ ansible ansible-slave-1 -B 1800 -P 60 -a &quot;/usr/bin/long_running_operation --do-stuff&quot; -B表示最常執行時間，-P表示每隔60秒回傳狀態。 Gathering Facts在 Playboooks 中有對 Facts 做一些描述，他表示的是一些系統已知的變數，若要查看所有 Facts，可以使用以下指令： $ ansible cluster[0] -m setup 接下來可以針對 Playbooks 與 Variables 進行研究。","tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://kairen.github.io/tags/DevOps/"},{"name":"Automation Engine","slug":"Automation-Engine","permalink":"https://kairen.github.io/tags/Automation-Engine/"},{"name":"Ansible","slug":"Ansible","permalink":"https://kairen.github.io/tags/Ansible/"}]},{"title":"Ansible Inventory","date":"2016-02-17T04:23:01.000Z","path":"2016/02/17/devops/ansible/ansible-inventory/","text":"Ansible 在同一時間能夠工作於多個系統，透過在 inventory file 所列舉的主機與群組來執行對應的指令，該檔案預設存於/etc/ansible/hosts。 IT 人員不只能夠使用預設的檔案，也能夠在同一時間使用多個檔案，甚至來抓取來至雲端的 inventory 檔案，這是一個是動態的 inventory ，這部分可以參考 Dynamic Inventory。 Hosts and GroupsInventory 是一個INI-like格式的檔案，如以下範例所示： mail.example.com [webservers] foo.example.com bar.example.com [dbservers] one.example.com two.example.com three.example.com 如果 SSH 不是標準 Port 的話，可以使用:來對應要使用的 Port。但在 SSH config 檔案所列出來的主機將不會與 paramiko 進行連線，但是會與 OpenSSH 進行連接使用。 badwolf.example.com:5309 雖然可以使用以上方式達到不同 Port 連接，但是還是建議使用預設 Port。 假設只有靜態 IP，但又希望透過一些別名（aliases）來表示主機，或透過不同 Port 連接的話，可以表示如以下： jumper ansible_port=5555 ansible_host=192.168.1.50 若要一次列出多個主機可以使用以下 Pattern： [webservers] www[01:50].example.com 在數字 Pattern，前導的 0 可以根據需求刪除或加入。不只可以定義數字型，還能定義英文字母範圍： [databases] db-[a:f].example.com 也可以為每台主機的設定基礎連線類型與使用者資訊： [targets] localhost ansible_connection=local other1.example.com ansible_connection=ssh ansible_user=mpdehaan other2.example.com ansible_connection=ssh ansible_user=mdehaan Host Variables如上述範例，我們可以很容易將變數分配給將在 Playbooks 使用的主機： [atlanta] host1 http_port=80 maxRequestsPerChild=808 host2 http_port=303 maxRequestsPerChild=909 Group Variables變數也能夠被應用到整個群組裡： [atlanta] host1 host2 [atlanta:vars] ntp_server=ntp.atlanta.example.com proxy=proxy.atlanta.example.com Groups of Groups, and Group Variables另外，也可以用:children 來建立群組中的群組，並使用:vars來設定變數： [atlanta] host1 host2 [raleigh] host2 host3 [southeast:children] atlanta raleigh [southeast:vars] some_server=foo.southeast.example.com halon_system_timeout=30 self_destruct_countdown=60 escape_pods=2 [usa:children] southeast northeast southwest northwest Splitting Out Host and Group Specific Data該部分說明想要儲存 list 與 hash table 資料，或者從 Inventory 檔案保持分離主機與群組的特定變數。在 Ansible 的第一優先作法實際上是不儲存變數於主 Inventort 檔案。 除了直接在 INI 檔案儲存變數外，主機與群組變數也可以儲存在個人相對的 Inventory 檔案。這些變數檔案格式為 YAML。有效的副檔名如.yml、.yaml，以及.json或沒有副檔名。 一般當 remote host 數量不多時，把變數定義在 inventory 中是 ok 的；但若 remote host 的數量越來越多時，將變數的宣告定義在外部的檔案中會是比較好的方式。 假設 Inventory 檔案路徑為： /etc/ansible/hosts 如果主機被命名為foosball以及在raleigh與webservers的群組，以下位置的 YAML 檔案變數將提供給主機使用： # can optionally end in &#39;.yml&#39;, &#39;.yaml&#39;, or &#39;.json&#39; /etc/ansible/group_vars/raleigh /etc/ansible/group_vars/webservers /etc/ansible/host_vars/foosball ansible 會自動尋找 playbook 所在的目錄中的host_vars目錄 以及group_vars目錄 中所包含的檔案，並使用定義在這兩個目錄中的變數資訊。 舉例來說，inventory / playbook / host_vars / group_vars 可以用類似以下的方式進行配置： inventory：/home/vagrant/ansible/playbooks/inventory playbook：/home/vagrant/ansible/playbooks/myplaybook host_vars：/home/vagrant/ansible/playbooks/host_vars/prod1.example.com.tw group_vars：/home/vagrant/ansible/playbooks/group_vars/production 變數定義的方式有兩種方式： db_primary_host: prod1.example.com.tw db_replica_host: prod2.example.com.tw db_name: widget_production db_user: widgetuser db_password: lastpassword redis_host: redis_stag.example.com.tw 也可以用 YAML 的方式定義： --- db: user: widgetuser password: lastpassword name: widget_production primary: host: prod1.example.com.tw port: 5432 replica: host: prod2.example.com.tw port: 5432 redis: host: redis_stag.example.com.tw port: 6379 甚至可以在繼續細分，定義檔案../playbooks/group_vars/production/db： --- db: user: widgetuser password: lastpassword name: widget_production primary: host: prod1.example.com.tw port: 5432 replica: host: prod2.example.com.tw port: 5432 List of Behavioral Inventory Parameters正如上述提到，設定以下變數可以定義 Ansible 該如何控制以及遠端主機。如主機連線： ansible_connection Connection type to the host. Candidates are local, smart, ssh or paramiko. The default is smart. SSH connection： ansible_host The name of the host to connect to, if different from the alias you wish to give to it. ansible_port The ssh port number, if not 22 ansible_user The default ssh user name to use. ansible_ssh_pass The ssh password to use (this is insecure, we strongly recommend using --ask-pass or SSH keys) ansible_ssh_private_key_file Private key file used by ssh. Useful if using multiple keys and you don&#39;t want to use SSH agent. ansible_ssh_common_args This setting is always appended to the default command line for sftp, scp, and ssh. Useful to configure a ``ProxyCommand`` for a certain host (or group). ansible_sftp_extra_args This setting is always appended to the default sftp command line. ansible_scp_extra_args This setting is always appended to the default scp command line. ansible_ssh_extra_args This setting is always appended to the default ssh command line. ansible_ssh_pipelining Determines whether or not to use SSH pipelining. This can override the ``pipelining`` setting in ``ansible.cfg``. 權限提升（可參閱Ansible Privilege Escalation）： ansible_become Equivalent to ansible_sudo or ansible_su, allows to force privilege escalation ansible_become_method Allows to set privilege escalation method ansible_become_user Equivalent to ansible_sudo_user or ansible_su_user, allows to set the user you become through privilege escalation ansible_become_pass Equivalent to ansible_sudo_pass or ansible_su_pass, allows you to set the privilege escalation password 遠端主機環境參數： ansible_shell_type The shell type of the target system. Commands are formatted using &#39;sh&#39;-style syntax by default. Setting this to &#39;csh&#39; or &#39;fish&#39; will cause commands executed on target systems to follow those shell&#39;s syntax instead. ansible_python_interpreter The target host python path. This is useful for systems with more than one Python or not located at &quot;/usr/bin/python&quot; such as \\*BSD, or where /usr/bin/python is not a 2.X series Python. We do not use the &quot;/usr/bin/env&quot; mechanism as that requires the remote user&#39;s path to be set right and also assumes the &quot;python&quot; executable is named python, where the executable might be named something like &quot;python26&quot;. ansible\\_\\*\\_interpreter Works for anything such as ruby or perl and works just like ansible_python_interpreter. This replaces shebang of modules which will run on that host. 一個主機檔案範例： some_host ansible_port=2222 ansible_user=manager aws_host ansible_ssh_private_key_file=/home/example/.ssh/aws.pem freebsd_host ansible_python_interpreter=/usr/local/bin/python ruby_module_host ansible_ruby_interpreter=/usr/bin/ruby.1.9.3","tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://kairen.github.io/tags/DevOps/"},{"name":"Automation Engine","slug":"Automation-Engine","permalink":"https://kairen.github.io/tags/Automation-Engine/"},{"name":"Ansible","slug":"Ansible","permalink":"https://kairen.github.io/tags/Ansible/"}]},{"title":"Ansible 介紹與使用","date":"2016-02-16T04:23:01.000Z","path":"2016/02/16/devops/ansible/ansible-basic/","text":"Ansible 是最近越來越夯多 DevOps 自動化組態管理軟體，從 2013 年發起的專案，由於該架構為無 agent 程式的架構，以部署靈活與程式碼易讀而受到矚目。Ansible 除了有開源版本之外，還針對企業用戶推出 Ansible Tower 版本，已有許多知名企業採用，如 Apple、Twitter 等。 Ansible 架構圖如下所示，使用者透過 Ansible 編配操控公有與私有雲或 CMDB（組態管理資料庫）中的主機，其中 Ansible 編排是由Inventory(主機與群組規則)、API、Modules(模組)與Plugins(插件)組合而成。 Ansible 與其他管理工具最大差異在於不需要任何 Agent，預設使用 SSH 來做遠端操控與配置，並採用 YAML 格式來描述配置資訊。 Ansible 提供了一個 Playbook 分享平台，可以讓管理與開發者上傳自己的功能與角色配置的 Playbook，該網址為 Ansible Galaxy。 優點： 開發社群活躍。 playbook 使用的 yaml 語言，很簡潔。 社群相關文件容易理解。。 沒有 Agent 端。 安裝與執行的速度快 配置簡單、功能強大、擴展性強 可透過 Python 擴展功能 提供用好的 Web 管理介面與 REST API 介面（AWX 平台） 缺點： Web UI 需要收費。 官方資料都比較淺顯。 Ansible 安裝與基本操作Ansible 有許多種安裝方式，如使用 Github 來透過 Source Code 安裝，也可以透過 python-pip 來安裝，甚至是使用作業系統的套件管理系統安裝，以下使用 Ubuntu APT 來進行安裝： $ sudo apt-get install software-properties-common $ sudo apt-add-repository ppa:ansible/ansible $ sudo apt-get update $ sudo apt-get install ansible 也可以使用 Python-pip 來進行安裝： $ sudo easy_install pip $ sudo pip install -U pip $ sudo pip install ansible 節點準備首先我們要在各節點先安裝 SSH Server ，並配置需要的相關環境： $ sudo apt-get install openssh-server 設定特權模式不需要輸入密碼： $ echo &quot;ubuntu ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/ubuntu $ sudo chmod 440 /etc/sudoers.d/ubuntu 這邊 User 為ubuntu，若使用者不一樣請更換。 建立 SSH Key，並複製 Key 使之不用密碼登入： $ ssh-keygen -t rsa $ ssh-copy-id localhost 新增各節點 Domain name 至/etc/hosts檔案： 172.16.1.205 ansible-master 172.16.1.206 ansible-slave-1 172.16.1.207 ansible-slave-2 172.16.1.208 ansible-slave-3 並在 Master 節點複製所有 Slave 的 SSH Key： $ ssh-copy-id ubuntu@ansible-slave-1 $ ssh-copy-id ubuntu@ansible-slave-2 ... 設定 Invetory FileAnsible 能夠在同一時間工作於多個基礎設施的系統中。透過作用於 Ansible 的 Inventory 檔案所列出的主機與群組，該檔案預設被存在/etc/ansible/hosts。 /etc/ansible/hosts 是一個 INI-like 的檔案格式，如以下內容： ansible-slave-1 ansible-slave-2 ansible-slave-3 也可以建立成 Groups，如以下內容： [openstack] ansible-slave-1 ansible-slave-2 ansible-slave-3 若要參考更多資訊，可看 Invetory File。 基本功能操作Ansible 基本操作如以下指令： $ ansible &lt;pattern_goes_here&gt; -m &lt;module_name&gt; -a &lt;arguments&gt; &lt;pattern_goes_here&gt;部分可以參考 Patterns。 比如我們可以用 Ping 模組來測試是否連線成功： $ ansible all -m ping ansible-slave-2 | SUCCESS =&gt; { &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot; } ansible-slave-3 | SUCCESS =&gt; { &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot; } ansible-slave-1 | SUCCESS =&gt; { &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot; } 其中all為所有 Invetory 的主機，-m為使用的模組。若使用指定的 Inventory 檔案可以使用-i。 也可以執行指定指令： $ ansible all -a &quot;/bin/echo hello&quot; -a 後面為要執行的指令。 若要指定登入的使用者，且執行特權模式，可以使用以下指令： $ ansible all -a &quot;apt-get update&quot; -u vagrant -b -u為登入使用者，-b 為切換成特權模式（root），早期版本為--sudo。 主機的 SSH Key 檢查在 Ansible 1.2.1 與之後的版本預設都需要做主機 SSH key 檢查。 如果一台主機重新安裝或者在 ‘known_hosts’ 有不同的 SSH Key 的話，將會導致錯誤發生，但不希望這樣的問題影響 Ansible 使用，可以在 /etc/ansible/ansible.cfg 或者~/.ansible.cfg檔案關閉檢查。 [defaults] host_key_checking = False 也可以代替為設定環境變數： $ export ANSIBLE_HOST_KEY_CHECKING=False 還要注意在 paramiko 模式主機金鑰檢查緩慢是合理的，因此建議切換使用 SSH。 Ansible 會在遠端系統上記錄有關模組參數的一些資訊存於 syslog，除非該執行任務有標示 ‘no_log: True’。","tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://kairen.github.io/tags/DevOps/"},{"name":"Automation Engine","slug":"Automation-Engine","permalink":"https://kairen.github.io/tags/Automation-Engine/"}]},{"title":"Ceph Ansible 自動化建立 Ceph 叢集","date":"2016-02-15T09:08:54.000Z","path":"2016/02/15/ceph/deploy/ceph-ansible/","text":"本節將介紹如何透過 ceph-ansible 工具安裝一個測試的 Ceph 環境，一個最簡單的 Ceph 儲存叢集至少要一台 Monitor與三台 OSD。而 MDS 則是當使用到 CephFS 的時候才需要部署。 節點資訊本安裝將使用四台虛擬機器作為部署主機，虛擬機器採用 OpenStack，其規格為以下： Role RAM CPUs Disk IP Address mon 2 GB 1vCPU 20 GB 172.16.1.200 osd1 2 GB 1vCPU 20 GB 172.16.1.201 osd2 2 GB 1vCPU 20 GB 172.16.1.202 osd3 2 GB 1vCPU 20 GB 172.16.1.203 其中若是虛擬機，要額外建立 3 顆虛擬硬碟來作為 OSD 使用，如以下： Dev path Disk Size Description /dev/vdb 25 GB osd1 掛載 /dev/vdb 25 GB osd2 掛載 /dev/vdb 25 GB osd3 掛載 事前準備首先在mon節點設定 SSH 到其他節點不需要密碼，請依照以下執行： $ ssh-keygen -t rsa $ ssh-copy-id osd1 ... 若虛擬機的話，建立金鑰後可以直接上傳公有金鑰提供給其他節點。 接著在每一個節點設定 sudo 不需要密碼： $ echo &quot;ubuntu ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/ubuntu &amp;&amp; sudo chmod 440 /etc/sudoers.d/ubuntu 一般虛擬機映像檔預設就有設定。 然後在每一台節點新增以下內容到/etc/hosts： 127.0.0.1 localhost 10.21.20.99 ceph-deploy 172.16.1.200 mon 172.16.1.201 osd1 172.16.1.202 osd2 172.16.1.203 osd3 回到mon節點安裝部署將使用到的 ansible 工具： $ sudo apt-get install -y software-properties-common git cowsay $ sudo apt-add-repository -y ppa:ansible/ansible $ sudo apt-get update &amp;&amp; sudo apt-get install -y ansible 在mon節點編輯/etc/ansible/hosts，加入以下內容： [mons] mon [osds] osd[1:3] (option)若要安裝 rgw 與 mds 的話，可再添加以下： [rgws] mon [mdss] mon 完成後透過以下指令檢查節點是否可以溝通： $ ansible all -m ping 172.16.1.200 | success &gt;&gt; { &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot; } ... 部署 Ansible Ceph 叢集首先在mon節點透過 git 來下載 ceph-ansible 專案： $ git clone &quot;https://github.com/ceph/ceph-ansible.git&quot; $ cd ceph-ansible $ cp site.yml.sample site.yml $ cp group_vars/all.sample group_vars/all $ cp group_vars/mons.sample group_vars/mons $ cp group_vars/osds.sample group_vars/osds 若要部署 rgw 與 mds 的話，需再執行以下指令： $ cp group_vars/mdss.sample group_vars/mdss $ cp group_vars/rgws.sample group_vars/rgws 接著編輯group_vars/all檔案，修改以下內容： ceph_origin: &#39;upstream&#39; ceph_stable: true monitor_interface: eth0 journal_size: 5000 public_network: 172.16.1.0/24 其他版本可以參考官方的說明 ceph-ansible。 完成後再編輯group_vars/osds檔案，修改以下內容： journal_collocation: true devices: - /dev/vdb 這邊使用 journal，也可以選擇其他使用。若有多顆 OSD則修改devices。 上述都確認無誤後，編輯site.yml檔案，並修改一下內容： --- - hosts: mons become: True roles: - ceph-mon - hosts: osds become: True roles: - ceph-osd (option)若要部署 rgw 與 mds 的話，需再加入以下內容： - hosts: mdss become: True roles: - ceph-mds - hosts: rgws become: True roles: - ceph-rgw 完成後就可以透過以下指令來進行部署： $ ansible-playbook site.yml","tags":[{"name":"Ceph","slug":"Ceph","permalink":"https://kairen.github.io/tags/Ceph/"},{"name":"Storage","slug":"Storage","permalink":"https://kairen.github.io/tags/Storage/"},{"name":"Ansible","slug":"Ansible","permalink":"https://kairen.github.io/tags/Ansible/"}]},{"title":"Foreman 管理 Puppet","date":"2016-02-14T04:23:01.000Z","path":"2016/02/14/devops/puppet-foreman/","text":"Foreman 是一個 Puppet 的生命周期管理系統，類似 puppet-dashboard，通過它可以很直觀的查看 Puppet 所有客戶端的同步狀態與 facter 參數。 事前準備由於 foreman 是取決於 puppet 執行主機的組態管理，他需要部署一個 puppet master 與 agent 環境。下面的列表為在安裝之前需要備設定的項目： Root 權限：所有伺服器能夠使用sudo。 私人網路 DNS：Forward 與 reverse 的 DNS 必須被設定，可參考How To Configure BIND as a Private Network DNS Server on Ubuntu 14.04。 防火牆有開啟使用的 port： Puppet master 必須可以被存取8140埠口。 安裝 Foreman安裝 Foreman 最簡單的方法是使用 Foreman 安裝程式。Foreman 安裝程式與配置必要的元件來執行 Foreman，包含以下內容： Foreman Puppet master and agent Apache Web Server with SSL and Passenger module* 下載 Foreman 可以依照以下指令進行： $ sudo sh -c &#39;echo &quot;deb http://deb.theforeman.org/ trusty 1.5&quot; &gt; /etc/apt/sources.list.d/foreman.list&#39; $ sudo sh -c &#39;echo &quot;deb http://deb.theforeman.org/ plugins 1.5&quot; &gt;&gt; /etc/apt/sources.list.d/foreman.list&#39; $ wget -q http://deb.theforeman.org/pubkey.gpg -O- | sudo apt-key add - $ sudo apt-get update &amp;&amp; sudo apt-get install foreman-installer 安裝完成後，要執行 Foreman Installer 可以使用以下指令： $ sudo foreman-installer 完成後會看到以下資訊： Success! * Foreman is running at https://puppet-master.com Default credentials are &#39;admin:changeme&#39; * Foreman Proxy is running at https://puppet-master.com:8443 * Puppetmaster is running at port 8140 The full log is at /var/log/foreman-installer/foreman-installer.log 之後修改puppet.conf檔案，開啟diff選項： $ sudo vim /etc/puppet/puppet.conf show_diff = true 新增 Foreman Host 到 Foreman 資料庫要新增 Host 可以使用以下指令： $ sudo puppet agent --test 完成後登入 Web，並輸入admin/changeme。 驗證 Foreman$ sudo puppet module install -i /etc/puppet/environments/production/modules puppetlabs/ntp Notice: Preparing to install into /etc/puppet/environments/production/modules ... Notice: Downloading from https://forge.puppetlabs.com ... Notice: Installing -- do not interrupt ... /etc/puppet/environments/production/modules └─┬ puppetlabs-ntp (v4.1.2) └── puppetlabs-stdlib (v4.10.0) 參考資源 How To Use Foreman To Manage Puppet Nodes on Ubuntu 14.04","tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://kairen.github.io/tags/DevOps/"},{"name":"Automation Engine","slug":"Automation-Engine","permalink":"https://kairen.github.io/tags/Automation-Engine/"},{"name":"Puppet","slug":"Puppet","permalink":"https://kairen.github.io/tags/Puppet/"}]},{"title":"Puppet 介紹與使用","date":"2016-02-13T04:23:01.000Z","path":"2016/02/13/devops/puppet-basic/","text":"Puppet 是一個開放原始碼專案，基於 Ruby 的系統組態管理工具，採用 Client/Server 的部署架構。是一個為了實現資料中心自動化管理，而被設計的組態管理軟體，它使用跨平台語言規範，管理組態檔案、使用者、軟體套件與系統服務等。用戶端預設每個半小時會與伺服器溝通一次，來確定是否有更新。當然也可以配置主動觸發來強制用戶端更新。這樣可以把平常的系統管理工作程式碼化，透過程式碼化的好處是可以分享、保存與避免重複勞動，也可以快速恢復以及快速的大規模環境部署伺服器。 優點： 成熟的組態管理軟體。 應用廣泛。 功能很完善。 提供許多資源可以配置 擁有許多的支持者。 缺點： 無法批次處理。 語言採用 DSL 與 Ruby。 缺少錯誤回報與檢查。 要透過程式定義先後順序。 基本概念介紹基礎設施即程式碼(Infrastructure as Code)在官方可以了解到 puppet 是一個概念為Infrastructure as Code的工具。Infrastructure as Code 與一般撰寫的 shell scrip 類似，但是比後者更高一個層次，將這一層虛擬化，使管理者只需要定義 Infrastructure 的狀況即可。這樣除了可以模組化、reuse外，也可以清楚透過 code 了解環境安裝了什麼與設定了什麼，因此 code 就是一個 infrastructure。 資源(Resource)Puppet 中一個基礎元素為resource，一個 resource 可以是file、package或者是service等，透過 resource 我們可以查看環境上檔案、套件、服務狀態等。更多資訊可以參考 Resource 列表與使用方式。 P.S resource type 要注意大小寫，當作 metaparameters 的時候寫作 Type[title] Type 要大寫。 相依性(Dependencies)在使用 Puppet 時，通常會撰寫 manifest 檔案來定義 resource。而這些 resource 在執行時會以同步的方式完成。 P.S 因為是同步(Sync)執行，故會有相依性的問題產生。這時候就可以用 Puppet 提供的 before / require 關鍵字來配置先後順序。 Puppet 安裝與基本操作環境建置我們將使用兩台 Ubuntu 14.04 主機來進行操作，一台為主控節點，另一台為Agent 節點。下面是我們將用到的伺服器的基礎資訊： puupet 主控節點 IP：10.21.20.10 主機名稱：puppetmaster 完整主機名稱：puppetmaster.example.com puupet agent 節點 IP：10.21.20.8 主機名稱：puppetslave 完整主機名稱：puppetslave.example.com 在每台節點完成以下步驟： $ sudo apt-get update &amp;&amp; sudo apt-get -y install ntp $ sudo vim /etc/ntp.conf server 1.tw.pool.ntp.org iburst server 3.asia.pool.ntp.org iburst server 2.asia.pool.ntp.org iburst Puppet 主控節點部署首先我們要先安裝 puppet 套件，透過wget下載puppetlabs-release.deb資源庫套件： $ wget https://apt.puppetlabs.com/puppetlabs-release-trusty.deb $ sudo dpkg -i puppetlabs-release-trusty.deb $ sudo apt-get update 完成後，我們就可以下載puppetmaster-passenger： $ sudo apt-get install puppetmaster-passenger 安裝過程中會發現錯誤，這部分可以忽略： Warning: Setting templatedir is deprecated. See http://links.puppetlabs.com/env-settings-deprecations (at /usr/lib/ruby/vendor_ruby/puppet/settings.rb:1139:in `issue_deprecation_warning&#39;) 安裝完後，可以透過以下指令查看版本： $ puppet --version 3.8.4 這時我們可以透過resource指令來查看可用資源： $ puppet resource [type] $ puppet resource service service { &#39;acpid&#39;: ensure =&gt; &#39;running&#39;, enable =&gt; &#39;true&#39;, } service { &#39;apache2&#39;: ensure =&gt; &#39;running&#39;, } 更多的 resource 可以查看 Type Reference。 在開始之前，我們先將 apache2 關閉，來讓 puppet 主控伺服器關閉： $ sudo service apache2 stop 接著我們要建立一個檔案/etc/apt/preferences.d/00-puppet.pref來鎖定 APT 自動更新套件，因為套件更新會造成組態檔的混亂： $ sudo vim /etc/apt/preferences.d/00-puppet.pref Package: puppet puppet-common puppetmaster-passenger Pin: version 3.8* Pin-Priority: 501 Puppet 主控伺服器是一個認證推送機構，需要產生自己的認證，用於簽署所有 agent 的認證要求。首先要刪除所有該套件安裝過程建立的 ssl 憑證。預設憑證放在 /var/lib/puppet/ssl底下。 $ sudo rm -rf /var/lib/puppet/ssl 接著我們要修改puppet.conf 檔案，來配置節點之前認證溝通，這邊要註解templatedir這行。然後在檔案的[main]增加以下資訊。 $ sudo vim /etc/puppet/puppet.conf [main] ... server = puppetmaster environment = production runinterval = 1h strict_variables = true certname = puppetmaster dns_alt_names = puppetmaster, puppetmaster.example.com 詳細的檔案可以參閱Main Config File (puppet.conf) 修改完後，透過puppet指令建立新的憑證： $ puppet master --verbose --no-daemonize Info: Creating a new certificate revocation list Info: Creating a new SSL key for puppetmaster Info: csr_attributes file loading from /etc/puppet/csr_attributes.yaml Info: Creating a new SSL certificate request for puppetmaster Info: Certificate Request fingerprint (SHA256): 9B:C5:45:F8:C5:8F:C2:B1:4D:15:E3:64:5F:DB:19:AB:06:C4:60:99:48:F3:BA:8F:D3:03:7E:35:BE:BC:4E:B1 Notice: puppetmaster has a waiting certificate request Notice: Signed certificate request for puppetmaster Notice: Removing file Puppet::SSL::CertificateRequest puppetmaster at &#39;/var/lib/puppet/ssl/ca/requests/puppetmaster.pem&#39; Notice: Removing file Puppet::SSL::CertificateRequest puppetmaster at &#39;/var/lib/puppet/ssl/certificate_requests/puppetmaster.pem&#39; Notice: Starting Puppet master version 3.8.4 當看到Notice: Starting Puppet master version 3.8.4代表完成，這時候可用 CTRL-C離開。 檢查新產生的 SSL 憑證，可以使用以下指令： $ puppet cert list -all + &quot;puppetmaster&quot; (SHA256) 8C:5E:39:A7:81:94:2B:09:7E:20:B8:F2:46:59:60:D9:FA:5D:4A:9E:BF:27:D7:C1:1A:A4:3E:97:12:D3:BE:21 (alt names: &quot;DNS:puppet-master&quot;, &quot;DNS:puppet-master.example.com&quot;, &quot;DNS:puppetmaster&quot;) 設定一個 Puppet manifests預設的 manifests 為/etc/puppet/manifests/site.pp。這個主要 manifests 檔案包括了用於在 Agent 節點執行的組態定義： $ sudo vim /etc/puppet/manifests/site.pp # execute &#39;apt-get update&#39; exec { &#39;apt-update&#39;: # exec resource named &#39;apt-update&#39; command =&gt; &#39;/usr/bin/apt-get update&#39; # command this resource will run } # install apache2 package package { &#39;apache2&#39;: require =&gt; Exec[&#39;apt-update&#39;], # require &#39;apt-update&#39; before installing ensure =&gt; installed, } # ensure apache2 service is running service { &#39;apache2&#39;: ensure =&gt; running, } 上面幾行用來部署 apache2 到 agent 節點。 完成後，修改/etc/apache2/sites-enabled/puppetmaster.conf檔，修改SSLCertificateFile與SSLCertificateKeyFile對應到新的憑證： SSLCertificateFile /var/lib/puppet/ssl/certs/puppetmaster.pem SSLCertificateKeyFile /var/lib/puppet/ssl/private_keys/puppetmaster.pem 然後重新開啟服務： $ sudo service apache2 restart Puppet agent 節點部署首先在 agent 節點上使用以下指令下載 puppet labs 的套件，並安裝： $ wget https://apt.puppetlabs.com/puppetlabs-release-trusty.deb $ sudo dpkg -i puppetlabs-release-trusty.deb $ sudo apt-get update $ sudo apt-get install -y puppet 由於 puppet 預設是不會啟動的，所以要編輯/etc/default/puppet檔案來設定： $ sudo vim /etc/default/puppet START=yes 之後一樣設定防止 APT 更新到 puppet，修改/etc/apt/preferences.d/00-puppet.pref檔案： $ sudo vim /etc/apt/preferences.d/00-puppet.pref Package: puppet puppet-common Pin: version 3.8* Pin-Priority: 501 設定 puppet agent編輯/etc/puppet/puppet.conf檔案，將templatedir這行註解掉，並移除[master]部分的相關設定： $ sudo vim /etc/puppet/puppet.conf [main] logdir=/var/log/puppet vardir=/var/lib/puppet ssldir=/var/lib/puppet/ssl rundir=/var/run/puppet factpath=$vardir/lib/facter # templatedir=$confdir/templates [agent] server = puppetmaster.example.com certname = puppetslave.example.com 完成後啟動 puppet： $ sudo service puppet start 在主控伺服器上對憑證要求進行簽證當完成 master 節點與 slave 節點後，可以在主控伺服器上使用以下指令來列出當前憑證請求： $ puppet cert list &quot;puppetnode.example.com&quot; (SHA256) 52:43:4C:ED:16:34:A3:EA:E7:5D:B0:97:FF:66:4F:C8:E0:51:AD:80:E6:32:95:53:FC:24:AE:15:17:17:3A:C0 接著使用以下指令進行簽證： $ puppet cert sign puppetnode.example.com Notice: Signed certificate request for puppetnode.example.com Notice: Removing file Puppet::SSL::CertificateRequest puppetnode.example.com at &#39;/var/lib/puppet/ssl/ca/requests/puppetnode.example.com.pem&#39; 也可以使用puppet cert sign --all來一次簽署多個。 若想要移除可以使用puppet cert clean hostname。 簽署成功後，可以用以下指令查看： $ puppet cert list --all + &quot;puppetmaster&quot; (SHA256) 8C:5E:39:A7:81:94:2B:09:7E:20:B8:F2:46:59:60:D9:FA:5D:4A:9E:BF:27:D7:C1:1A:A4:3E:97:12:D3:BE:21 (alt names: &quot;DNS:puppet-master&quot;, &quot;DNS:puppet-master.example.com&quot;, &quot;DNS:puppetmaster&quot;) + &quot;puppetnode.example.com&quot; (SHA256) EF:D6:E5:7E:45:B0:5D:EC:D4:17:E6:31:A2:97:F6:C2:31:2A:19:B9:0E:9D:31:77:9A:02:93:BC:73:B9:5E:58 部署主節點的 manifests當配置並完成 puppet manifests，現在需要部署 manifests 到 slave 節點上。要載入 puppet manifests 可以使用以下指令： $ puppet agent --test Info: Retrieving pluginfacts Info: Retrieving plugin Info: Caching catalog for puppetnode.example.com Info: Applying configuration version &#39;1452086629&#39; Notice: /Stage[main]/Main/Exec[apt-update]/returns: executed successfully Notice: Finished catalog run in 17.31 seconds 之後我們可以使用puppet apply來提交 manifests： $ puppet apply /etc/puppet/manifests/site.pp 若要指定節點，可以建立如以下的*.pp檔： $ sudo vim /etc/puppet/manifests/site-example.pp node &#39;puppetslave1&#39;, &#39;puppetslave2&#39; { # execute &#39;apt-get update&#39; exec { &#39;apt-update&#39;: # exec resource named &#39;apt-update&#39; command =&gt; &#39;/usr/bin/apt-get update&#39; # command this resource will run } # install apache2 package package { &#39;apache2&#39;: require =&gt; Exec[&#39;apt-update&#39;], # require &#39;apt-update&#39; before installing ensure =&gt; installed, } # ensure apache2 service is running service { &#39;apache2&#39;: ensure =&gt; running, } } Puppet 是一個很成熟的工具，已有許多模組被貢獻，我們可以透過以下方式下載模組： $ puppet module install puppetlabs-apache 注意，不要在一個已經部署 Apache 的環境上使用該模組，否則會清空為沒有被 puppet 管理的 apache 配置。 接著我們修改site.pp來配置 apache： $ sudo vim /etc/puppet/manifest/site.pp node &#39;puppetslave&#39; { class { &#39;apache&#39;: } # use apache module apache::vhost { &#39;example.com&#39;: # define vhost resource port =&gt; &#39;8080&#39;, docroot =&gt; &#39;/var/www/html&#39; } } 參考資源 Modules Search InfoQ Puppet 介紹 Puppet 學習 Puppet 筆記 puppet學習筆記：puppet資源file詳細介紹 How To Install Puppet To Manage Your Server Infrastructure","tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://kairen.github.io/tags/DevOps/"},{"name":"Automation Engine","slug":"Automation-Engine","permalink":"https://kairen.github.io/tags/Automation-Engine/"}]},{"title":"SaltStack 介紹","date":"2016-02-12T04:23:01.000Z","path":"2016/02/12/devops/saltstack-basic/","text":"Saltstack 是一套基礎設施管理開發套件、簡單易部署、可擴展到管理成千上萬的伺服器、控制速度佳(以 ms 為單位)。Saltstack 提供了動態基礎設施溝通總線用於編配、遠端執行、配置管理等等。Saltstack 是從 2011 年開始的專案，已經是很成熟的開源專案。該專案簡單的兩大基礎功能就是配置管理與遠端指令執行。 Saltstack 採用集中化管理，我們一般可以理解為 Puppet 的簡化版本與 Func的加強版本。Saltstack 是基於 Python 語言開發的，結合輕量級訊息佇列（ZeroMQ）以及 Python 第三方模組（Pyzmq、PyCrypto、Pyjinja2、python-msgpack與PyYAML等）。 優點： 部署簡單與方便。 支持大部分 UNIX/Liunx 及 Windows 環境。 主從集中化管理。 配置簡單、功能強大與擴展性強。 主控端（Master）與被控制端（Minion）基於憑證認證。 支援 API 以及自定義模組，透過 Python 輕鬆擴展。 社群活躍。 缺點： Web UI 雖然有，但是沒有報表功能。 需要 Agent 透過 Saltstack 環境，我們可在成千上萬的伺服器進行批次的指令執行，根據不同的集中化管理配置、分散檔案、收集伺服器資料、作業系統基礎環境以及軟體套件等。 參考資源 SaltStack介紹和架構解析","tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://kairen.github.io/tags/DevOps/"},{"name":"Automation Engine","slug":"Automation-Engine","permalink":"https://kairen.github.io/tags/Automation-Engine/"}]},{"title":"Docker 快速部署 Ceph 測試叢集","date":"2016-02-11T09:08:54.000Z","path":"2016/02/11/ceph/deploy/ceph-docker/","text":"本節將介紹如何透過 ceph-docker 工具安裝一個測試的 Ceph 環境，一個最簡單的 Ceph 儲存叢集至少要1 Monitor與3 OSD。另外部署 MDS 與 RGW 來進行簡單測試。 節點配置本安裝採用一台虛擬機器來提供部署，可使用 VBox 或 OpenStack 等建立，其環境資源大小如下： hostname CPUs RAM ceph-aio 2vCPU 4GB 若使用 Vagrant + VBox 的話，可以使用 Vagrantfile 腳本。 而該虛擬機要額外建立三顆虛擬區塊裝置，如下所示： Dev path Disk Description /dev/sdb 20 GB osd-1 使用 /dev/sdc 20 GB osd-2 使用 /dev/sdd 20 GB osd-3 使用 事前準備首先在主機安裝 Docker Engine，可以透過以下指令進行安裝： $ curl -fsSL https://get.docker.com/ | sh 部署 Ceph 測試叢集首先為了不與預設 Docker 網路共用，這邊額外建立一網路來提供給 Ceph 使用： $ docker network create --driver bridge ceph-net $ docker network inspect ceph-net { &quot;Subnet&quot;: &quot;172.18.0.0/16&quot;, &quot;Gateway&quot;: &quot;172.18.0.1/16&quot; } 建立 Monitor完成網路建立後，就可以開始部署 Ceph 叢集了。一開始我們必須先建立 Monitor Container： $ cd ~ &amp;&amp; DIR=$(pwd) $ sudo docker run -d --net=ceph-net \\ -v ${DIR}/ceph:/etc/ceph \\ -v ${DIR}/lib/ceph/:/var/lib/ceph/ \\ -e MON_IP=172.18.0.2 \\ -e CEPH_PUBLIC_NETWORK=172.18.0.0/16 \\ --name mon1 \\ ceph/daemon mon 若發生錯誤請刪除以下目錄。如以下指令： $ sudo rm -rf ${DIR}/etc/ceph/ $ sudo rm -rf ${DIR}/var/lib/ceph/ 檢查是否正確部署： $ docker exec -ti mon1 ceph -v ceph version 10.2.2 (45107e21c568dd033c2f0a3107dec8f0b0e58374) $ docker exec -ti mon1 ceph -s cluster 2c254496-e948-4abb-a6dc-9aea41bbb56a health HEALTH_ERR no osds monmap e1: 1 mons at {1068f41de69a=172.18.0.2:6789/0} election epoch 3, quorum 0 1068f41de69a osdmap e1: 0 osds: 0 up, 0 in flags sortbitwise pgmap v2: 64 pgs, 1 pools, 0 bytes data, 0 objects 0 kB used, 0 kB / 0 kB avail 64 creating 建立 OSD上面可以看到 Monitor 建立完成，但是會有錯誤，因為目前沒有 OSD。因此這邊將建立三個 OSD Container 來模擬叢集做實際儲存的功能，透過以下方式部署： $ cd ~ &amp;&amp; DIR=$(pwd) $ sudo docker run -d --net=ceph-net \\ --privileged=true --pid=host \\ -v ${DIR}/ceph:/etc/ceph \\ -v ${DIR}/lib/ceph/:/var/lib/ceph/ \\ -v /dev/:/dev/ \\ -e OSD_DEVICE=/dev/sdb \\ -e OSD_TYPE=disk \\ -e OSD_FORCE_ZAP=1 \\ --name osd1 \\ ceph/daemon osd 若要建立多個 OSD，只需要修改OSD_DEVICE與name即可，這邊建議建立三個 OSD。因為預設 pool 採用三份副本，若節點數過少需要自行修改副本數或 CRUSH Map。 完成後，可以透過以下指令檢查 Device 被使用： $ docker exec -ti osd1 df | grep &quot;osd&quot; /dev/sdb1 20857836 34924 20822912 1% /var/lib/ceph/osd/ceph-0 也可以直接透過 Monitor 來查看叢集安全狀態，如 PG 是否有誤等： $ docker exec -ti mon1 ceph -s cluster 23fa3f2c-a401-46e0-abc1-d71b4625b348 health HEALTH_OK monmap e2: 1 mons at {0b7ff674673f=172.18.0.2:6789/0} election epoch 4, quorum 0 0b7ff674673f mgr no daemons active osdmap e15: 3 osds: 3 up, 3 in flags sortbitwise,require_jewel_osds,require_kraken_osds pgmap v29: 64 pgs, 1 pools, 0 bytes data, 0 objects 101 MB used, 61005 MB / 61106 MB avail 64 active+clean 建立 RGW當完成一個 RAODS(MON+OSD)叢集後，即可建立物件儲存閘道(RAODS Gateway)提供 S3 與 Swift 相容的 API，來儲存檔案到叢集中，一個 RGW Container 建立如下所示： $ cd ~ &amp;&amp; DIR=$(pwd) $ sudo docker run -d --net=ceph-net \\ -v ${DIR}/lib/ceph/:/var/lib/ceph/ \\ -v ${DIR}/ceph:/etc/ceph \\ -p 8080:8080 \\ --name rgw1 \\ ceph/daemon rgw 完成後，透過 curl 工具來測試是否正確部署： $ curl -H &quot;Content-Type: application/json&quot; &quot;http://127.0.0.1:8080&quot; &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;ListAllMyBucketsResult xmlns=&quot;http://s3.amazonaws.com/doc/2006-03-01/&quot;&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt; 透過 Python Client 進行檔案儲存，首先下載程式： $ wget &quot;https://gist.githubusercontent.com/kairen/e0dec164fa6664f40784f303076233a5/raw/33add5a18cb7d6f18531d8d481562d017557747c/s3client&quot; $ chmod u+x s3client $ sudo pip install boto 接著透過以下指令建立一個使用者： $ docker exec -ti rgw1 radosgw-admin user create --uid=&quot;test&quot; --display-name=&quot;I&#39;m Test account&quot; --email=&quot;test@example.com&quot; &quot;keys&quot;: [ { &quot;user&quot;: &quot;test&quot;, &quot;access_key&quot;: &quot;PFMKGXCFD77L8X4CF0T4&quot;, &quot;secret_key&quot;: &quot;SA8RpGO7SoN4TIdRxYtxloc5kRSLQvhOihJdDGG3&quot; } ], 建立一個放置環境參數的檔案s3key.sh： export S3_ACCESS_KEY=&quot;PFMKGXCFD77L8X4CF0T4&quot; export S3_SECRET_KEY=&quot;SA8RpGO7SoN4TIdRxYtxloc5kRSLQvhOihJdDGG3&quot; export S3_HOST=&quot;127.0.0.1&quot; export S3_PORT=&quot;8080&quot; 然後 source 檔案，並嘗試執行列出 bucket 指令： $ . s3key.sh $ ./s3client list ---------- Bucket List ---------- 建立一個 Bucket，並上傳檔案： $ ./s3client create files Create [files] success ... $ ./s3client upload files s3key.sh / Upload [s3key.sh] success ... 完成後，即可透過 list 與 download 來查看與下載： $ ./s3client list files ---------- [files] ---------- s3key.sh 157 2016-07-26T06:48:14.327Z $ ./s3client download files s3key.sh Download [s3key.sh] success ... 建立 MDS當系統需要使用到 CephFS 時，我們將必須建立 MDS(Metadata Server) 來提供詮釋資料的儲存，一個 MDS 容器部署如下： $ cd ~ &amp;&amp; DIR=$(pwd) $ sudo docker run -d --net=ceph-net \\ -v ${DIR}/lib/ceph/:/var/lib/ceph/ \\ -v ${DIR}/ceph:/etc/ceph \\ -e CEPHFS_CREATE=1 \\ --name mds1 \\ ceph/daemon mds 透過以下指令檢查是否建立無誤： $ docker exec -ti mds1 ceph mds stat e5: 1/1/1 up {0=mds-aea2f53de13a=up:active} $ docker exec -ti mds1 ceph fs ls name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]","tags":[{"name":"Ceph","slug":"Ceph","permalink":"https://kairen.github.io/tags/Ceph/"},{"name":"Storage","slug":"Storage","permalink":"https://kairen.github.io/tags/Storage/"},{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"}]},{"title":"利用 Graphite 監控系統資料","date":"2016-02-11T04:23:01.000Z","path":"2016/02/11/devops/graphite/","text":"Graphite 是一款開源的監控繪圖工具。Graphite 可以實時收集、存儲、顯示時間序列類型的數據（time series data）。它主要有三個部分構成： Carbon：基於 Twisted 的行程，用來接收資料。 Whisper：專門儲存時間序列類型資料的小型資料庫。 Graphite webapp：基於 Django 的網頁應用程式。 安裝 Graphite在開始配置 Graphite 之前，需要先安裝系統相依套件： $ sudo apt-get install build-essential graphite-web graphite-carbon python-dev apache2 libapache2-mod-wsgi libpq-dev python-psycopg2 在安裝期間graphite-carbon會詢問是否要刪除 whisper database files，這邊回答YES。 配置 Carbon透過增加[test]到 Carbon 的/etc/carbon/storage-schemas.conf 檔案，這部分單純用於測試使用，如果不需要可以直接跳過： [carbon] pattern = ^carbon\\. retentions = 60:90d [test] pattern = ^test\\. retentions = 5s:3h,1m:1d [default_1min_for_1day] pattern = .* retentions = 60s:1d 更多如何配置 Carbon storage 的資訊，可以參考 storage-schemas.con。 之後複製預設的聚合組態到/etc/carbon： $ sudo cp /usr/share/doc/graphite-carbon/examples/storage-aggregation.conf.example /etc/carbon/storage-aggregation.conf 設定在開機時，啟動 Carbon 快取，編輯/etc/default/graphite-carbon： CARBON_CACHE_ENABLED=true 啟動 Carbon 服務： $ sudo service carbon-cache start 安裝與配置 PostgreSQL安裝 PostgreSQL 讓 graphite-web 應用程式使用： $ sudo apt-get install postgresql 切換到postgres使用者，並建立資料庫使用者給 Graphite： $ sudo su - postgres postgres# createuser graphite --pwprompt 建立graphite與grafana資料庫： postgres# createdb -O graphite graphite postgres# createdb -O graphite grafana 切換graphite來檢查配置是否成功： $ sudo su - graphite 設定 Graphite更新 Graphite web 使用的後端資料庫與其他設定，編輯/etc/graphite/local_settings.py，加入以下： DATABASES = { &#39;default&#39;: { &#39;NAME&#39;: &#39;graphite&#39;, &#39;ENGINE&#39;: &#39;django.db.backends.postgresql_psycopg2&#39;, &#39;USER&#39;: &#39;graphite&#39;, &#39;PASSWORD&#39;: &#39;graphiteuserpassword&#39;, &#39;HOST&#39;: &#39;127.0.0.1&#39;, &#39;PORT&#39;: &#39;&#39; } } USE_REMOTE_USER_AUTHENTICATION = True TIME_ZONE = &#39;UTC&#39; SECRET_KEY = &#39;some-secret-key&#39; TIME_ZONE 可以查詢 Wikipedia’s timezone database SECRET_KEY可以使用openssl rand -hex 10指令來建立。 初始化資料庫： $ sudo graphite-manage syncdb 設定 Graphite 使用 Apache首先複製 Graphite 的 Apache 配置樣板到 Apache sites-available 目錄： $ sudo cp /usr/share/graphite-web/apache2-graphite.conf /etc/apache2/sites-available 編輯/etc/apache2/sites-available/apache2-graphite.conf，修改預設監聽的 port： &lt;VirtualHost *:8080&gt; 編輯/etc/apache2/ports.conf加入監聽的 port： Listen 80 Listen 8080 取消預設 Apache 的 site： $ sudo a2dissite 000-default 啟用 Graphite 的虛擬 site，並重新載入： $ sudo a2ensite apache2-graphite $ sudo service apache2 reload 重新啟動 apache 服務： $ sudo service apache2 restart 完成後，即可登入example_domain.com:8080。 測試一個簡單資料： $ for i in 4 6 8 16 2; do echo &quot;test.count $i `date +%s`&quot; | nc -q0 127.0.0.1 2003; sleep 6; done 參考連結 Deploy Graphite with Grafana on Ubuntu 14.04 How To Install and Use Graphite on an Ubuntu 14.04 Server Grafana＋collectd＋InfluxDB","tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://kairen.github.io/tags/DevOps/"},{"name":"Monitoring","slug":"Monitoring","permalink":"https://kairen.github.io/tags/Monitoring/"},{"name":"Data Collect","slug":"Data-Collect","permalink":"https://kairen.github.io/tags/Data-Collect/"}]},{"title":"kube-up 腳本部署 Kubernetes 叢集(Deprecated)","date":"2016-01-16T09:08:54.000Z","path":"2016/01/16/kubernetes/deploy/kubeup-deploy/","text":"Kubernetes 提供了許多雲端平台與作業系統的安裝方式，本篇將使用官方腳本kube-up.sh來部署 Kubernetes 到 Ubuntu 14.04 系統上。其他更多平台的部署可以參考 Creating a Kubernetes Cluster。 本環境安裝資訊： Kubernetes v1.5.4 Etcd v2.3.0 Flannel v0.5.5 Docker v1.13.1 節點資訊本次安裝作業系統採用Ubuntu 14.04 Server，測試環境為 OpenStack VM 與實體主機： IP Address Role CPU Memory 172.16.35.12 master1 2 4G 172.16.35.10 node1 2 4G 172.16.35.11 node2 2 4G 這邊 master 為主要控制節點，node 為應用程式工作節點。 事前準備安裝前需要確認叢集滿足以下幾點： 目前官方只測試過 Ubuntu 14.04，官方說法是 15.x 也沒問題，但 16.04 上我測試無法自動完成，要自己補上各種服務的 Systemd 腳本。 部署節點可以透過 SSH 與其他節點溝通，並且是無密碼登入，以及有 Sudoer 權限。 所有節點需要安裝Docker或rtk引擎。安裝方式為以下： $ curl -fsSL &quot;https://get.docker.com/&quot; | sh $ sudo iptables -P FORWARD ACCEPT 部署 Kubernetes 叢集首先下載官方 Release 的原始碼程式： $ curl -sSL &quot;https://github.com/kubernetes/kubernetes/archive/v1.5.4.tar.gz&quot; | tar zx $ mv kubernetes-1.5.4 kubernetes 接著編輯kubernetes/cluster/ubuntu/config-default.sh設定檔，修改以下內容： export nodes=${nodes:-&quot;ubuntu@172.16.35.12 ubuntu@172.16.35.10 ubuntu@172.16.35.11&quot;} export role=&quot;ai i i&quot; export NUM_NODES=${NUM_NODES:-3} export SERVICE_CLUSTER_IP_RANGE=192.168.3.0/24 export FLANNEL_NET=172.16.0.0/16 SERVICE_NODE_PORT_RANGE=${SERVICE_NODE_PORT_RANGE:-&quot;30000-32767&quot;} 設定要部署的 Kubernetes 版本環境參數： export KUBE_VERSION=1.5.4 export FLANNEL_VERSION=0.5.5 export ETCD_VERSION=2.3.0 export KUBERNETES_PROVIDER=ubuntu 然後進入到kubernetes/cluster目錄，並執行以下指令： $ sudo sed -i &#39;s/verify-kube-binaries//g&#39; kube-up.sh $ ./kube-up.sh ... Cluster validation succeeded Done, listing cluster services: Kubernetes master is running at http://172.16.35.12:8080 當看到上述資訊即表示成功部署，這時候進入到cluster/ubuntu/binaries目錄複製 kubectl 工具： $ sudo cp kubectl /usr/local/bin/ 最後透過 kubectl 工具來查看叢集節點是否成功加入： $ kubectl get nodes NAME STATUS AGE 172.16.35.12 Ready 2m 172.16.35.10 Ready 2m 172.16.35.11 Ready 2m (Option)部署 Add-ons若要部署 kubernetes Dashboard 與 DNS 等額外服務的話，要修改kubernetes/cluster/ubuntu/config-default.sh設定檔，修改一下內容： ENABLE_CLUSTER_MONITORING=&quot;${KUBE_ENABLE_CLUSTER_MONITORING:-true}&quot; ENABLE_CLUSTER_UI=&quot;${KUBE_ENABLE_CLUSTER_UI:-true}&quot; ENABLE_CLUSTER_DNS=&quot;${KUBE_ENABLE_CLUSTER_DNS:-true}&quot; DNS_SERVER_IP=${DNS_SERVER_IP:-&quot;192.168.3.10&quot;} DNS_DOMAIN=${DNS_DOMAIN:-&quot;cluster.local&quot;} 通常基本款大概為 Dashboard、DNS、Monitoring 與 Logging，。 修改完成後，進入到kubernetes/cluster/ubuntu目錄，並執行以下指令： $ KUBERNETES_PROVIDER=ubuntu ./deployAddons.sh 透過 kubectl 查看資訊，這邊服務屬於系統的，所以預設會被分到kube-system命名空間： $ kubectl get pods --namespace=kube-system 最後就可以透過瀏覽器查看 Dashboard。 建立 Nginx 應用程式Kubernetes 可以選擇使用指令直接建立應用程式與服務，或者撰寫 YAML 與 JSON 檔案來描述部署應用程式的配置，以下將使用兩種方式建立一個簡單的 Nginx 服務。 利用 ad-hoc 指令建立kubectl 提供了 run 指令來快速建立應用程式部署，如下建立 Nginx 應用程式： $ kubectl run nginx --image=nginx $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-701339712-w5wlq 1/1 Running 0 26m 172.16.86.2 172.16.35.11 而當應用程式(deploy)被建立後，我們還需要透過 Kubernetes Service 來提供給外部網路存取應用程式，如下指令： $ kubectl expose deploy nginx --port 80 --type NodePort $ kubectl get svc -o wide 完成後要接著建立 svc（Service）來提供外部網路存取應用程式，使用以下指令建立： $ kubectl expose rc nginx --port=80 --type=NodePort $ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 192.168.3.1 &lt;none&gt; 443/TCP 37m nginx 192.168.3.199 &lt;nodes&gt; 80:31764/TCP 30m 這邊採用NodePort，即表示任何節點 IP 位址的31764 Port 都會 Forward 到 Nginx container 的80 Port。 若想刪除應用程式與服務的話，可以透過以下指令： $ kubectl delete deploy nginx $ kubectl delete svc nginx 撰寫 YAML 檔案建立Kubernetes 支援了 JSON 與 YAML 來描述要部署的應用程式資訊，這邊撰寫nginx-dp.yaml來部署 Nginx 應用： apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx spec: replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 接著建立 Service 來提供存取服務，這邊撰寫nginx-svc.yaml來建立服務： apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: NodePort ports: - port: 80 targetPort: 80 protocol: TCP nodePort: 30000 selector: app: nginx 然後透過 kubectl 指令來指定檔案建立： $ kubectl create -f nginx-dp.yaml deployment &quot;nginx&quot; created $ kubectl create -f nginx-svc.yaml service &quot;nginx-service&quot; created 完成後，可以查看一下資訊： $ kubectl get svc,pods,rc -o wide NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR svc/kubernetes 192.168.3.1 &lt;none&gt; 443/TCP 51m &lt;none&gt; svc/nginx-service 192.168.3.155 &lt;nodes&gt; 80:30000/TCP 1m app=nginx NAME READY STATUS RESTARTS AGE IP NODE po/nginx-4087004473-0wrbs 1/1 Running 0 2m 172.16.101.2 172.16.35.10 最後要刪除的話，直接將 create 改成使用delete即可： $ kubectl delete -f nginx-dp.yaml $ kubectl delete -f nginx-svc.yaml 其他 Kubernetes 網路技術Kubernetes 支援多種網路整合，若 Flannel 用不爽可以改以下幾種： OpenVSwitch with GRE/VxLAN Linux Bridge L2 networks Weave Calico(使用 BGP Routing)","tags":[{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://kairen.github.io/tags/Ubuntu/"}]},{"title":"hyperkube 建立多節點 Kubernetes(Unrecommended)","date":"2016-01-14T09:08:54.000Z","path":"2016/01/14/kubernetes/deploy/docker-multi/","text":"本篇將說明如何透過 Docker 來部署一個多節點的 kubernetes 叢集。其架構圖如下所示： 本環境安裝資訊： Kubernetes v1.5.5 Docker v17.03.0-ce 節點資訊本次安裝作業系統採用Ubuntu 16.04 Server，測試環境為 Vagrant with Libvirt 或 Vbox： IP Address Role CPU Memory 172.16.35.12 master1 2 4G 172.16.35.10 node1 2 4G 172.16.35.11 node2 2 4G 這邊 master 為主要控制節點，node 為應用程式工作節點。 事前準備安裝前需要確認叢集滿足以下幾點： 所有節點需要安裝Docker或rtk引擎。安裝方式為以下： $ curl -fsSL &quot;https://get.docker.com/&quot; | sh $ sudo iptables -P FORWARD ACCEPT Kubernetes 部署這邊將分別部署 Master 與 Node(Worker) 節點。 建立 Master 節點首先下載官方 Release 的原始碼程式： $ git clone &quot;https://github.com/kubernetes/kube-deploy&quot; 接著進入部署目錄來進行部署動作，Master 執行以下指令： $ export IP_ADDRESS=&quot;172.16.35.12&quot; $ cd kube-deploy/docker-multinode $ ./master.sh ... Master done! 執行後，透過 Docker 指令查看是否成功： $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES bfb6461499fb gcr.io/google_containers/hyperkube-amd64:v1.5.5 &quot;/hyperkube kubele...&quot; 4 minutes ago Up 4 minutes kubelet ... 這邊會隨時間開啟其他 Component 的 Docker Container。 確認完成後，就可以下載 kubectl 來透過 API 管理叢集： $ curl -O &quot;https://storage.googleapis.com/kubernetes-release/release/v1.5.5/bin/linux/amd64/kubectl&quot; $ chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/ 安裝好 kubectl 後就可以透過以下指令來查看資訊： $ kubectl get nodes NAME STATUS AGE 172.16.35.12 Ready 11s 查看系統命名空間的 pod 與 svc 資訊： $ kubectl get po --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system k8s-proxy-v1-bfdml 1/1 Running 0 1m kube-system kube-dns-4101612645-fb1rn 4/4 Running 0 1m kube-system kubernetes-dashboard-3543765157-999p2 1/1 Running 0 1m 建立 Node(Worker) 節點首先下載官方 Release 的原始碼程式： $ git clone &quot;https://github.com/kubernetes/kube-deploy&quot; 接著進入部署目錄來進行部署動作，Node 執行以下指令： $ export MASTER_IP=&quot;172.16.35.12&quot;; export IP_ADDRESS=&quot;172.16.35.11&quot; $ cd kube-deploy/docker-multinode $ ./worker.sh ... +++ [0324 07:23:06] Done. After about a minute the node should be ready 驗證安裝完成後可以查看所有節點狀態，執行以下指令： $ kubectl get nodes NAME STATUS AGE 172.16.35.10 Ready 3m 172.16.35.11 Ready 4m 172.16.35.12 Ready 1m 接著我們透過部署簡單的 Nginx 應用程式來驗證系統是否正常： $ kubectl run nginx --image=nginx --port=80 deployment &quot;nginx&quot; created $ kubectl expose deploy nginx --port=80 service &quot;nginx&quot; exposed 透過指令檢查 Pods： $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-3449338310-ttqp2 1/1 Running 0 32s 10.1.1.2 172.16.35.11 透過指令檢查 Service： $ kubectl get svc -o wide NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes 10.0.0.1 &lt;none&gt; 443/TCP 47m &lt;none&gt; nginx 10.0.0.149 &lt;none&gt; 80/TCP 37s run=nginx 取得應用程式的 Service ip，並存取服務： $ IP=$(kubectl get svc nginx --template={{.spec.clusterIP}}) $ curl ${IP}","tags":[{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"}]},{"title":"hyperkube 建立單機 Kubernetes(Unrecommended)","date":"2016-01-13T09:08:54.000Z","path":"2016/01/13/kubernetes/deploy/docker-singal/","text":"本篇將說明如何透過 Docker 來部署一個單機的 kubernetes。其架構圖如下所示： 事前準備在開始安裝前，我們必須在部署的主機或虛擬機安裝與完成以下兩點： 確認安裝 Docker Engine 於主機作業系統。 $ curl -fsSL &quot;https://get.docker.com/&quot; | sh $ sudo iptables -P FORWARD ACCEPT 定義要使用的 Kubernetes 版本，目前支援 1.2.0+ 版本。 $ export K8S_VERSION=&quot;1.5.4&quot; 部署 Kuberentes 元件完成上述後，透過執行以下指令進行部署： $ sudo docker run -d \\ --volume=/:/rootfs:ro \\ --volume=/sys:/sys:ro \\ --volume=/var/lib/docker/:/var/lib/docker:rw \\ --volume=/var/lib/kubelet/:/var/lib/kubelet:rw \\ --volume=/var/run:/var/run:rw \\ --net=host \\ --pid=host \\ --privileged=true \\ --name=kubelet \\ gcr.io/google_containers/hyperkube-amd64:v${K8S_VERSION} \\ /hyperkube kubelet \\ --containerized \\ --hostname-override=&quot;127.0.0.1&quot; \\ --address=&quot;0.0.0.0&quot; \\ --api-servers=&quot;http://localhost:8080&quot; \\ --config=/etc/kubernetes/manifests \\ --cluster-dns=10.0.0.10 \\ --allow-privileged=true --v=2 執行後，透過 Docker 指令查看是否成功： $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES bfb6461499fb gcr.io/google_containers/hyperkube-amd64:v1.5.4 &quot;/hyperkube kubele...&quot; 4 minutes ago Up 4 minutes kubelet ... 這邊會隨時間開啟其他 Component 的 Docker Container。 確認完成後，就可以下載 kubectl 來透過 API 管理叢集： $ curl -O &quot;https://storage.googleapis.com/kubernetes-release/release/v${K8S_VERSION}/bin/linux/amd64/kubectl&quot; $ chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/ 接著設定 kubectl config 來使用測試叢集： $ kubectl config set-cluster test-doc --server=http://localhost:8080 $ kubectl config set-context test-doc --cluster=test-doc $ kubectl config use-context test-doc 驗證安裝當完成所有步驟後，就可以檢查節點狀態： $ kubectl get nodes NAME STATUS AGE 127.0.0.1 Ready 6m 查看系統命名空間的 pod 與 svc 資訊： $ kubectl get po --all-namespaces kubectl get po --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system k8s-etcd-127.0.0.1 1/1 Running 0 15m kube-system k8s-master-127.0.0.1 4/4 Running 2 15m kube-system k8s-proxy-127.0.0.1 1/1 Running 0 15m kube-system kube-addon-manager-127.0.0.1 2/2 Running 0 15m 接著我們透過部署簡單的 Nginx 應用程式來驗證系統是否正常： $ kubectl run nginx --image=nginx --port=80 deployment &quot;nginx&quot; created $ kubectl expose deploy nginx --port=80 service &quot;nginx&quot; exposed 透過指令檢查 Pods： $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE NODE nginx-198147104-u9lt6 1/1 Running 0 3m 127.0.0.1 透過指令檢查 Service： $ kubectl get svc -o wide NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes 10.0.0.1 &lt;none&gt; 443/TCP 11m &lt;none&gt; nginx 10.0.0.133 &lt;none&gt; 80/TCP 3m run=nginx 取得應用程式的 Service ip，並存取服務： $ IP=$(kubectl get svc nginx --template={{.spec.clusterIP}}) $ curl ${IP}","tags":[{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"}]},{"title":"學習 Docker Network 之間的差別","date":"2016-01-05T09:08:54.000Z","path":"2016/01/05/container/docker-network/","text":"Docker 的網路是透過 Linux 的網路命名空間與虛擬網路裝置(Veth pair)實現而成。然而 Docker 的網路支援了不同類型功能，每一種都有其用意，本篇將針對以下幾項 Docker network mode 進行實作與介紹： Bridge Mode (default) Host Mode None Mode Container Mode Bridge ModeBridge Mode 是 Docker 預設使用的 network mode 。若你在已安裝 Docker 的環境中使用 ifconfig 指令，你可以看到有一個名為 docker0 的 network interface： 此時，你可以在這個 Docker 環境中執行一個 docker container ： $ docker run -ti busybox sh 一樣使用 ifconfig 指令，你也會看到一個 eth0 network interface： 這就是為什麼在 host 上可以直接使用 ping 指令與 container 進行通訊，因為 veth796c087 讓 docker0(172.17.0.1) 與 eth0(172.17.0.2) 位於同一個區網。 我們用一張簡單的圖來表示這三者的關係： 由上圖你可以發現，Container 1 可透過 eth0 經過 veth4fd8759 與 docker0 進行溝通，Container 2 也是如此，且 Container 1 與 Container 2 也可以進行溝通。 Host ModeHost Mode 可以把他想像成建立一個與 Host 擁有同樣的 network interface 的 Container ，使用方式： $ docker run --net=host -ti busybox sh None ModeNone Mode 是建置最簡潔的 Container ，也就是沒有任何 network interface 的 Container。使用方式是在建立 Container 的同時給與 --net=none 的參數： $ docker run --net=none -ti busybox sh 此時，你若使用 ifconfig 指令，會發現這個 Container 沒有任何 network interface。 Container Mode首先，我們要先啟動一個 Container，並且使用這個 Container 的 Container ID 建立另外一個 Container： 建置第二個 Container 的方式，將第一個 Container ID 當參數進行建置： docker run -ti --net=container:d16d87a29be3 busybox sh 此時，你會發現兩個 Container 的 IP 都是 172.17.0.2 ，雖然他們是不同的 Container 但是被放置同一個 Namespace 內，三者的關係如下：","tags":[{"name":"Linux Container","slug":"Linux-Container","permalink":"https://kairen.github.io/tags/Linux-Container/"},{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"}]},{"title":"用 ELK 做監控系統","date":"2016-01-03T04:23:01.000Z","path":"2016/01/03/devops/elk/","text":"ELK 是由三個套件的開頭英文組成的縮寫，其 E 表示Elasticsearch，L 表示Logstash，K 表示Kibana，作為收集資料、資料索引以及資料視覺化的工具集合，以下分別簡單介紹三個套件。 LogstashLogstash 可以簡單、有效、快速的處理Log資料，不過Logstash的主要功能是處理時間類型的Log，也就是在Log檔中有時間戳記（TimeStamp）的資料，而分析Log資料主要就是分析事件發生的時間和內容 Logstash Forwarder可傳送所收集到的 Log 訊息到 Logstash。 ElasticsearchElasticsearch 是一個開源的資料搜尋分析系統，它可以解決現在 Web 去做資料庫的搜尋的種種問題，嚴格來說也不只是 web，(有可能是為了撈資料的效能，或是 schema free, real-time 等等)。 KibanaKibana 是一個開源和免費的工具，他可以幫助您匯總、分析和搜索重要數據日志並提供友好的web界面 系統與安裝版本 OS: Ubuntu 14.04 Elasticsearch 1.4.4 Logstash 1.5.0 Kibana 4 進行安裝首先安裝 Java Oraclesudo apt-get purge openjdk* sudo apt-get -y autoremove sudo add-apt-repository -y ppa:webupd8team/java sudo apt-get update echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections sudo apt-get -y install oracle-java7-installer 安裝 Elasticsearch匯入 Elasticsearch public GPG key 到 apt wget -O - http://packages.elasticsearch.org/GPG-KEY-elasticsearch | sudo apt-key add - 建立 Elasticsearch source list： echo &#39;deb http://packages.elasticsearch.org/elasticsearch/1.4/debian stable main&#39; | sudo tee /etc/apt/sources.list.d/elasticsearch.list 更新套件： sudo apt-get update 安裝 elasticsearch 1.4.4： sudo apt-get -y install elasticsearch=1.4.4 安裝完成，開啟配置檔： sudo vi /etc/elasticsearch/elasticsearch.yml 如果想限制給外界存取 Elasticsearch，可找到 network.host ，將內容取代成”localhost”，如下： network.host: localhost 開啟 Elasticsearch： sudo service elasticsearch restart 重開機立即啟動 Elasticsearch ： sudo update-rc.d elasticsearch defaults 95 10 安裝 Kibana下載 Kibana 4 到 opt 資料夾 cd /opt 使用wget下載 Kibana 套件壓縮檔： wget https://download.elasticsearch.org/kibana/kibana/kibana-4.0.1-linux-x64.tar.gz 解壓縮檔案： tar xvf kibana-*.tar.gz 開啟 Kibana 配置檔： vi ~/kibana-4*/config/kibana.yml 配置檔中找到host將 IP address “0.0.0.0” 取代成 “localhost”，此設定讓 Kibana 只能被 localhost 存取，如下： host: &quot;localhost&quot; 將下載完的 kibana 資料夾名稱改成 kibana： sudo mv kibana-4.0.1-linux-x64 kibana Kibana 執行 /opt/kibana/bin/kibana 來開啟，但我們想用 service 的方式開啟。 下載 Kibana 4 init 腳本: cd /etc/init.d &amp;&amp; sudo wget https://gist.githubusercontent.com/thisismitch/8b15ac909aed214ad04a/raw/bce61d85643c2dcdfbc2728c55a41dab444dca20/kibana4 開啟 Kibana service： sudo chmod +x /etc/init.d/kibana4 sudo update-rc.d kibana4 defaults 96 9 sudo service kibana4 start 安裝 Logstash建立 Logstash source list： echo &#39;deb http://packages.elasticsearch.org/logstash/1.5/debian stable main&#39; | sudo tee /etc/apt/sources.list.d/logstash.list 更新套件： sudo apt-get update 安裝 Logstash： sudo apt-get install logstash 產生 SSL 認證因為我們將使用 Logstash Forwarder 收集 logs並傳送到 Logstash Server ，所以我們必須建立一對SSL 認證的 key： sudo mkdir -p /etc/pki/tls/certs sudo mkdir /etc/pki/tls/private 設定 openssl 配置： sudo vi /etc/ssl/openssl.cnf 配置檔中找到 [ v3_ca ] ，並新增以下內容： subjectAltName = IP:logstash_server_private_ip 產生 SSL 認證和 private key 到 /etc/pki/tls/ ，如下： cd /etc/pki/tls 設定 SSL 驗證： sudo openssl req -config /etc/ssl/openssl.cnf -x509 -days 3650 -batch -nodes -newkey rsa:2048 -keyout private/logstash-forwarder.key -out certs/logstash-forwarder.crt 配置 Logstash新增配置檔 01-lumberjack-input.conf： sudo vi /etc/logstash/conf.d/01-lumberjack-input.conf 新增以下配置內容： input { lumberjack { port =&gt; 5000 type =&gt; &quot;logs&quot; ssl_certificate =&gt; &quot;/etc/pki/tls/certs/logstash-forwarder.crt&quot; ssl_key =&gt; &quot;/etc/pki/tls/private/logstash-forwarder.key&quot; } } 新增配置檔 10-syslog.conf： sudo vi /etc/logstash/conf.d/10-syslog.conf 新增以下配置內容： filter { if [type] == &quot;syslog&quot; { grok { match =&gt; { &quot;message&quot; =&gt; &quot;%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\\[%{POSINT:syslog_pid}\\])?: %{GREEDYDATA:syslog_message}&quot; } add_field =&gt; [ &quot;received_at&quot;, &quot;%{@timestamp}&quot; ] add_field =&gt; [ &quot;received_from&quot;, &quot;%{host}&quot; ] } syslog_pri { } date { match =&gt; [ &quot;syslog_timestamp&quot;, &quot;MMM d HH:mm:ss&quot;, &quot;MMM dd HH:mm:ss&quot; ] } } } 新增配置檔 30-lumberjack-output.conf： sudo vi /etc/logstash/conf.d/30-lumberjack-output.conf 新增以下配置內容： output { elasticsearch { host =&gt; localhost } stdout { codec =&gt; rubydebug } } 重啟 Logstash： sudo service logstash restart 完成後就可以設置Logstash Forwarder（簡單說就是加入 Client）。 複製 SSL Certificate 與 Logstash Forwarder 套件 (On Logstash Server)scp /etc/pki/tls/certs/logstash-forwarder.crt user@client_server_private_address:/tmp 安裝 Logstash Forwarder 套件 (On Client)Logstash Forwarder source list： echo &#39;deb http://packages.elasticsearch.org/logstashforwarder/debian stable main&#39; | sudo tee /etc/apt/sources.list.d/logstashforwarder.list 一樣可使用Elasticsearch的 GPG key 來安裝： wget -O - http://packages.elasticsearch.org/GPG-KEY-elasticsearch | sudo apt-key add - 安裝 Logstash Forwarder package： sudo apt-get update sudo apt-get install logstash-forwarder 複製 Logstash server’s SSL認證到 /etc/pki/tls/certs： sudo mkdir -p /etc/pki/tls/certs sudo cp /tmp/logstash-forwarder.crt /etc/pki/tls/certs/ 配置 Logstash Forwarder設定Logstash Forwarder 配置檔(On Client Server)： sudo vi /etc/logstash-forwarder.conf 配置檔中找到 network ，底下加入以下內容： &quot;servers&quot;: [ &quot;logstash_server_private_address:5000&quot; ], &quot;timeout&quot;: 15, ssl ca&quot;: &quot;/etc/pki/tls/certs/logstash-forwarder.crt&quot; 配置檔中找到 files ，底下加入以下內容： { &quot;paths&quot;: [ &quot;/var/log/syslog&quot;, &quot;/var/log/auth.log&quot; ], &quot;fields&quot;: { &quot;type&quot;: &quot;syslog&quot; } } 重啟 Logstash Forwarder： sudo service logstash-forwarder restart 完成後，即可開啟瀏覽器，網址列輸入locahost:5601。","tags":[{"name":"Logging","slug":"Logging","permalink":"https://kairen.github.io/tags/Logging/"},{"name":"DevOps","slug":"DevOps","permalink":"https://kairen.github.io/tags/DevOps/"},{"name":"Monitoring","slug":"Monitoring","permalink":"https://kairen.github.io/tags/Monitoring/"},{"name":"Visualization","slug":"Visualization","permalink":"https://kairen.github.io/tags/Visualization/"}]},{"title":"自己建立 Docker Registry","date":"2016-01-02T09:08:54.000Z","path":"2016/01/02/container/docker-registry/","text":"Docker Registry 是被用來儲存 Docker 所建立的映像檔的地方，我們可以把自己建立的映像檔透過上傳到 Registries 來分享給其他人。Registries 也被分為了公有與私有，一般公有的 Registries 是 Docker Hub、QUAY 與 GCP registry，提供了所有基礎的映像檔與全球使用者上傳的映像檔。私人的則是企業或者個人環境建置的，可參考 Deploying a registry server。 預先準備資訊本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統可採用Ubuntu 16.x與CentOS 7.x： IP Address Role CPU Memory 172.16.35.13 docker-registry 1 2G 安裝首先進入到docker-registry節點，安裝 Docker engine： $ curl -fsSL &quot;https://get.docker.com/&quot; | sh 完成安裝後，接著透過以下指令建立一個 Docker registry 容器： $ docker run -d -p 5000:5000 --restart=always --name registry \\ -v $(pwd)/data:/var/lib/registry \\ registry:2 -v 為 host 與 container 要進行同步的目錄，主要存放 docker images 資料 接著為了方便檢視 Docker image，這邊另外部署 Docker registry UI： $ docker run -d -p 5001:80 \\ -e ENV_DOCKER_REGISTRY_HOST=172.16.35.13 \\ -e ENV_DOCKER_REGISTRY_PORT=5000 \\ konradkleine/docker-registry-frontend:v2 完成後就可以透過瀏覽器進入 Docker registry UI 查看資訊。也可以透過以下指令檢查是否部署成功： $ docker pull ubuntu:14.04 $ docker tag ubuntu:14.04 localhost:5000/ubuntu:14.04 $ docker push localhost:5000/ubuntu:14.04 The push refers to a repository [localhost:5000/ubuntu] 447f88c8358f: Pushed df9a135a6949: Pushed ... 其他 Docker registry 列表： Portus Atomic Registry Private Registries in RancherOS VMware Harbor","tags":[{"name":"Linux Container","slug":"Linux-Container","permalink":"https://kairen.github.io/tags/Linux-Container/"},{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"Docker registry","slug":"Docker-registry","permalink":"https://kairen.github.io/tags/Docker-registry/"}]},{"title":"Docker Machine Driver 使用","date":"2015-12-06T09:08:54.000Z","path":"2015/12/06/container/docker-machine-driver/","text":"Docker machine 是 Docker 早期提供 Windows 與 Mac OS X 建立 Docker 環境的工具，其採用 VirtualBox 來提供一個 Container OS，再利用 Docker client 來進行操作。然而 Docker Machine 除了使用 VirtualBox 外，還可以使用Generic Driver與OpenStack Driver來建立雲端平台上的 Docker 環境。 使用 Generic Driver以下範例使用 Generic Driver 來連接一個已存在的 OpenStack instance： $ docker-machine create --driver generic \\ --generic-ssh-user ubuntu \\ --generic-ssh-key ~/.ssh/id_rsa \\ --generic-ip-address 10.26.1.82 \\ docker-engine-1 --generic-ssh-user為要遠端的使用者。 --generic-ssh-key為要使用的 SSH Key。 --generic-ip-address為要使用的虛擬機 IP。 使用 OpenStack DriverDocker Machine 提供了使用者可以建立 Docker 容器於 OpenStack 虛擬機。以下範例是使用 OpenStack Driver 來連接 OpenStack 並建立虛擬機： $ docker-machine create --driver openstack \\ --openstack-username &quot;&lt;KEYSTONE_USERNAME&gt;&quot; \\ --openstack-password &quot;&lt;KEYSTONE_PASSWD&gt;&quot; \\ --openstack-tenant-name &quot;&lt;KEYSTONE_PROJECT_NAME&gt;&quot; \\ --openstack-auth-url &quot;&lt;KEYSTONE_URL&gt;&quot; \\ --openstack-flavor-name &quot;m1.medium&quot; \\ --openstack-image-name &quot;Ubuntu-14.04-Server-Cloud&quot; \\ --openstack-net-name &quot;admin-net&quot; \\ --openstack-floatingip-pool &quot;internal-net&quot; \\ --openstack-ip-version 4 \\ --openstack-ssh-user &quot;ubuntu&quot; \\ --openstack-sec-groups &quot;ALL_PASS&quot; \\ openstack-docker --openstack-username為 Keystone 使用者帳號。 --openstack-password為 Keystone 使用者密碼。 --openstack-tenant-name為要使用的 project 名稱。 --openstack-auth-url為 Keystone URL。 --openstack-flavor-name為要使用的 Flavor。 --openstack-image-name為要使用的 Image 名稱。 --openstack-net-name為要使用的私有網路名稱。 --openstack-floatingip-pool為要使用的 Floating 網路名稱。 --openstack-ip-versionl為要使用的網路版本。 --openstack-ssh-user為要遠端的使用者名稱。 --openstack-sec-groups為要使用的 Security Grous 名稱，可多個如以下ALL_PASS, SSH。","tags":[{"name":"Linux Container","slug":"Linux-Container","permalink":"https://kairen.github.io/tags/Linux-Container/"},{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"OpenStack","slug":"OpenStack","permalink":"https://kairen.github.io/tags/OpenStack/"}]},{"title":"Ubuntu Metal as a Service 安裝","date":"2015-12-04T04:23:01.000Z","path":"2015/12/04/linux/ubuntu/ubuntu-maas-install/","text":"Ubuntu MAAS 提供了裸機服務，將雲端的語言帶到物理伺服器中，其功能可以讓企業一次大量部署伺服器硬體環境的作業系統與基本設定等功能。 安裝與設定首先為了方便系統操作，將sudo設置為不需要密碼： $ echo &quot;ubuntu ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/ubuntu &amp;&amp; sudo chmod 440 /etc/sudoers.d/ubuntu ubuntu 為 User Name。 安裝相關套件： $ sudo apt-get install software-properties-common 新增 MAAS 與 JuJu 的資源庫： sudo add-apt-repository -y ppa:juju/stable sudo add-apt-repository -y ppa:maas/stable sudo add-apt-repository -y ppa:cloud-installer/stable sudo apt-get update 安裝 MAAS接著安裝 MAAS： $ sudo apt-get install -y maas 安裝完成後，建立一個 admin 使用者帳號： $ sudo maas-region createadmin 這時候就可以登入 MAAS。 登入後，需要設定 PXE 網路與下載 Image。當完成後就可以開其他主機來測試 PXE-boot。 安裝 JuJu安裝 JuJu quick start： $ sudo apt-get update &amp;&amp; sudo apt-get install juju-quickstart 完成後，透過以下指令來部署 JuJu GUI： $ juju-quickstart --gui-port 4242 上傳客製化 Images首先安裝 Maas Image Builder 來建立映像檔： $ sudo add-apt-repository -y ppa:blake-rouse/mib-daily $ sudo apt-get install maas-image-builder 安裝完成後，我們可以使用以下指令建立映像檔，如以下為建立一個CentOS 7的映像檔： $ sudo maas-image-builder -a amd64 -o centos7-amd64-root-tgz centos --edition 7 目前 CentOS 支援了6.5與7.0版本。 當映像檔建立完成後，就可以上傳到 MAAS 了，在這之前需要登入 MAAS，才有權限上傳： $ maas login &lt;user_name&gt; &lt;maas_url&gt; &lt;api_key&gt; user_name&gt;為使用者帳號名稱。&lt;maas_url&gt;為 MAAS 網址，如 http://192.168.1.2/MAAS&lt;api_key&gt;為帳號 API Key。r00tme 登入後，就可以使用以下指令進行上傳客製化的映像檔了： $ maas &lt;user_name&gt; boot-resources create name=centos/centos7 architecture=amd64/generic content@=centos7-amd64-root-tgz 登入時，要輸入指令需加&lt;user_name&gt;。","tags":[{"name":"Linux","slug":"Linux","permalink":"https://kairen.github.io/tags/Linux/"},{"name":"PXE","slug":"PXE","permalink":"https://kairen.github.io/tags/PXE/"},{"name":"Bare Metal","slug":"Bare-Metal","permalink":"https://kairen.github.io/tags/Bare-Metal/"}]},{"title":"NFS 簡單安裝與使用","date":"2015-11-24T04:23:01.000Z","path":"2015/11/24/linux/ubuntu/nfs/","text":"網路檔案系統(Network FileSystem，NFS)是早期由 SUN 公司所開發出來的分散式檔案系統協定。主要透過 RPC Service 使檔案能夠共享於網路中，NFS 的好處是它支援了不同系統與機器的溝通能力，使資料能夠很輕易透過網路共享給別人。 安裝與設定首先在 NFS 節點安裝以下套件： $ sudo apt-get -y install nfs-kernel-server 編輯/etc/idmapd.conf設定檔，然後設定 Domain： Domain = kyle.bai.example 接著編輯/etc/exports檔案，加入以下內容： /var/nfs/images 10.0.0.0/24(rw,sync,no_root_squash,no_subtree_check) /var/nfs/vms 10.0.0.0/24(rw,sync,no_root_squash,no_subtree_check) /var/nfs/volumes 10.0.0.0/24(rw,sync,no_root_squash,no_subtree_check) 然後重新啟動 NFS Server，如以下指令： $ sudo /etc/init.d/nfs-kernel-server restart 接著到 Client 端，安裝 NFS 工具： $ sudo apt-get -y install nfs-common 編輯/etc/idmapd.conf設定檔，然後設定 Domain： Domain = kyle.bai.example 然後透過以下指令來掛載使用： $ sudo mount -t nfs kyle.bai.example:/var/nfs/images /var/nfs/images 完成後，透過以下指令來檢查： $ df -hT Filesystem Type Size Used Avail Use% Mounted on udev devtmpfs 7.9G 8.0K 7.9G 1% /dev tmpfs tmpfs 1.6G 776K 1.6G 1% /run /dev/sda1 ext4 459G 8.3G 427G 2% / none tmpfs 4.0K 0 4.0K 0% /sys/fs/cgroup none tmpfs 5.0M 0 5.0M 0% /run/lock none tmpfs 7.9G 0 7.9G 0% /run/shm none tmpfs 100M 0 100M 0% /run/user 10.0.0.61:/var/nfs/images nfs4 230G 5.1G 213G 3% /var/nfs/images 編輯/etc/fstab檔案來提供開機掛載： 10.0.0.61:/var/nfs/vms /var/lib/nova/instances nfs defaults 0 0 也可以安裝自動掛載工具，透過以下指令安裝： $ sudo apt-get -y install autofs 編輯/etc/auto.master檔案，加入以下內容到最後面： /- /etc/auto.mount 然後編輯/etc/auto.mount檔案，設定以下內容： # create new : [mount point] [option] [location] /mntdir -fstype=nfs,rw kyle.bai.example:/home 建立掛載用目錄： $ sudo mkdir /mntdir 啟動 auto-mount 服務： $ sudo initctl restart autofs 完成後透過以下方式檢查： $ cat /proc/mounts | grep mntdir Cinder 使用 NFSOpenStack Cinder 也支援了 NFS 的驅動，因此只需要在/etc/cinder/cinder.conf設定以下即可： [DEFAULT] ... enabled_backends = nfs [nfs] nfs_shares_config = /etc/cinder/nfs_shares volume_driver = cinder.volume.drivers.nfs.NfsDriver volume_backend_name = nfs-backend nfs_sparsed_volumes = True 建立 Cinder backend 來提供不同的 Backend 的使用： $ cinder type-create TYPE $ cinder type-key TYPE set volume_backend_name=BACKEND","tags":[{"name":"Storage","slug":"Storage","permalink":"https://kairen.github.io/tags/Storage/"},{"name":"File System","slug":"File-System","permalink":"https://kairen.github.io/tags/File-System/"},{"name":"OpenStack","slug":"OpenStack","permalink":"https://kairen.github.io/tags/OpenStack/"},{"name":"Linux","slug":"Linux","permalink":"https://kairen.github.io/tags/Linux/"}]},{"title":"Ceph FS 基本操作","date":"2015-11-21T09:08:54.000Z","path":"2015/11/21/ceph/cephfs/","text":"Ceph FS 底層的部分同樣是由 RADOS(OSDs + Monitors + MDSs) 提供，在上一層同樣與 librados 溝通，最上層則是有不同的 library 將其轉換成標準的 POSIX 檔案系統供使用。 建立一個 Ceph File System首先將一個叢集建立完成，並提供 Metadata Server Node 與 Client，建立 Client 可以透過以下指令： $ ceph-deploy install &lt;myceph-client&gt; 建立 MDS 節點可以透過以下指令： $ ceph-deploy mds create mds-node 當 Ceph 叢集已經提供了MDS後，可以建立 Data Pool 與 Metadata Pool： $ ceph osd pool create cephfs_data 128 $ ceph osd pool create cephfs_metadata 128 How to judge PG number： Less than 5 OSDs set pg_num to 128 Between 5 and 10 OSDs set pg_num to 512 Between 10 and 50 OSDs set pg_num to 4096 If you have more than 50 OSDs, you need to understand the tradeoffs and how to calculate the pg_num value by yourself 完成 Pool 建立後，我們將儲存池拿來給 File System 使用，並建立檔案系統： $ ceph fs new cephfs cephfs_metadata cephfs_data 取得 Client 驗證金鑰： $ cat /etc/ceph/ceph.client.admin.keyring [client.admin] key = AQC/mo9VxqsXDBAAQ/LQtTmR+GTPs65KBsEPrw== 建立，並儲存到檔案admin.secret： AQC/mo9VxqsXDBAAQ/LQtTmR+GTPs65KBsEPrw== 檢查 MDS 與 FS： $ ceph fs ls $ ceph mds stat 建立 Mount 用目錄，並且 Mount File System： $ sudo mkdir /mnt/mycephfs $ sudo mount -t ceph {ip-address-of-monitor}:6789:/ /mnt/mycephfs/ -o name=admin,secretfile=admin.secret 檢查系統 DF 與 Mount 結果： $ sudo df -l $ sudo mount 使用CEPH檔案系統時，要注意是否安裝了元資料伺服器(Metadata Server)。且請確認CEPH版本為是0.84之後的版本。 Ceph Filesystem FUSE (File System in User Space)首先在MDS節點上安裝ceph-fuse 套件： $ sudo apt-get install -y ceph-fuse 完成後，我們就可以Mount起來使用： $ sudo mkdir /mnt/myceph-fuse $ sudo ceph-fuse -m {ip-address-of-monitor}:6789 /mnt/myceph-fuse 當 Mount 成功後，就可以到該目錄檢查檔案。 FUSE：使用者空間檔案系統（Filesystem in Userspace，簡稱FUSE）是作業系統中的概念，指完全在使用者態實作的檔案系統。目前Linux通過內核模組對此進行支援。一些檔案系統如ZFS，glusterfs和lustre使用FUSE實作。","tags":[{"name":"Ceph","slug":"Ceph","permalink":"https://kairen.github.io/tags/Ceph/"},{"name":"Storage","slug":"Storage","permalink":"https://kairen.github.io/tags/Storage/"},{"name":"File System","slug":"File-System","permalink":"https://kairen.github.io/tags/File-System/"}]},{"title":"使用 ceph-deploy 工具部署 Ceph 叢集","date":"2015-11-20T09:08:54.000Z","path":"2015/11/20/ceph/deploy/ceph-deploy/","text":"本節將介紹如何透過 ceph-deploy 工具安裝一個測試的 Ceph 環境，一個最簡單的 Ceph 儲存叢集至少要一台Monitor與三台OSD。而 MDS 則是當使用到 CephFS 的時候才需要部署。 環境準備在開始部署 Ceph 叢集之前，我們需要在每個節點做一些基本的準備，來確保叢集安裝的過程是流暢的，本次安裝會擁有 6 台節點，叢集拓樸圖如下所示： +------------------+ | [ Deploy Node ] | | 10.21.20.99 | | Ceph deploy tool | +--------+---------+ | | +------------------+ | +-----------------+ | [ Admin Node ] | | |[ Monitor Node ]| | 10.21.20.100 |-----------+-----------| 10.21.20.101 | | Ceph admin ops | | | Ceph mon Node | +------------------+ | +-----------------+ | +---------------------------+--------------------------+ | | | | | | +-------+----------+ +--------+---------+ +--------+---------+ | [ OSD Node 1 ] | | [ OSD Node 2 ] | | [ OSD Node 3 ] | | 10.21.20.121 +-------+ 10.21.20.122 +-------+ 10.21.20.123 | | Object Storage | | Object Storage | | Object Storage | | Disk * 2 | | Disk * 2 | | Disk * 2 | +------------------+ +------------------+ +------------------+ P.S. 上面磁碟分為兩個，因為這邊教學不將 journal 分開來，故一顆當作系統使用，一顆為資料儲存與 journal 使用。P.S. 這邊網路建議設定為static，若有支援 Jumbo frame 也可以開啟。 首先在每一台節點新增以下內容到/etc/hosts： 127.0.0.1 localhost 10.21.20.99 ceph-deploy 10.21.20.100 ceph-admin 10.21.20.101 ceph-mon1 10.21.20.121 ceph-osd1 10.21.20.122 ceph-osd2 10.21.20.123 ceph-osd3 然後在每台節點執行以下指令來使sudo不需要輸入密碼： $ echo &quot;ubuntu ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/ubuntu &amp;&amp; sudo chmod 440 /etc/sudoers.d/ubuntu 上面 ubuntu 是節點的使用者名稱，這邊都是一樣。P.S. 當然若要注意安全考量，而不讓該使用者直接使用有權限的資源，可以使用 root user。 接著在設定Deploy節點能夠以無密碼方式進行 SSH 登入其他節點，請依照以下執行： $ ssh-keygen -t rsa $ ssh-copy-id ceph-mon1 $ ssh-copy-id ceph-mds ... 若 Deploy 節點的使用者與其他不同的話，編輯~/.ssh/config加入以下內容： Host ceph-admin Hostname ceph-admin User ubuntu Host ceph-mds Hostname ceph-mds User mds ... 之後在Deploy節點安裝部署工具，首先安裝基本相依套件，使用apt-get來進行安裝，再透過python-pip進行安裝部署工具： $ sudo apt-get install -y python-pip $ sudo pip install ceph-deploy P.S. ceph-deploy 所安裝的 ceph 版本，會受到 ceph-deply 工具版本不同而有所差異。 完成後即可開始部署 Ceph 環境。 環境部署首先建立一個名稱為mycluster的目錄，並進入該目錄： $ sudo mkdir mycluster $ cd mycluster 採用 ceph-deploy 工具部署環境時，需要依照以下步驟進行，首先建立要當任 Monitor 的節點，透過以下方式： $ ceph-deploy new ceph-mon1 &lt;other_nodes&gt; 當執行該指令時，不是直接讓 ceph-mon1 節點安裝成為 Monitor，而只是新增一個conf，並標示誰是 Monitor，當在初始化階段時，才將該設定檔給對應節點，讓它啟動是設定為 Monitor 角色。 接著我們需要先讓每個節點（ceph-deploy除外）安裝 ceph-common 套件，透過以下方式安裝： $ ceph-deploy install ceph-admin ceph-mds &lt;other_nodes&gt; 當完成安裝後，才能開始真正的部署節點的角色，第一先將 Monitor 都完成部署，才能讓叢集先正常被運作，透過以下指令來將 Monitors 初始化： $ ceph-deploy mon create-initial 上述沒有問題後，就可以開始部署實際作為儲存的 OSD 節點，我們可以透過以下指令進行： $ ceph-deploy osd prepare ceph-osd1:/dev/sdb &lt;other_nodes&gt;:&lt;data_disk&gt; 若要將 journal 分離，可以使用以下方式： $ ceph-deploy osd prepare ceph-osd1:/dev/sdb:/dev/sdc &lt;other_nodes&gt;:&lt;data_disk&gt;:&lt;journal_disk&gt; 部署沒有問題的話，即可啟用該 OSD： $ ceph-deploy osd activate ceph-osd1:/dev/sdb &lt;other_nodes&gt;:&lt;data_disk&gt; P.S. 較新的版本該指令可以省略，因為在準備期間就會幫你直接啟動。 這樣一個簡單的叢集就部署完成了，這時候我們可以隨需求加入admin與MDS節點，可以透過以下方式進行： $ ceph-deploy admin ceph-admin $ ceph-deploy mds create ceph-mds 完成後，可以透過以下指令檢查 ceph 叢集狀態： $ ceph health HEALTH_OK $ ceph status cluster e2432059-e219-4555-8d37-c32d5b16e4a4 health HEALTH_OK monmap e1: 1 mons at {ceph-mon1=10.21.20.101:6789/0} election epoch 6, quorum 0, ceph-mon1 osdmap e119: 3 osds: 3 up, 3 in flags sortbitwise pgmap v813: 128 pgs, 2 pools, 91289 kB data, 22856 objects 691 MB used, 2152 GB / 2152 GB avail 128 active+clean 如果出現ERROR: missing keyring, cannot use cephx for authentication，請注意這個檔案/etc/ceph/ceph.client.admin.keyring是否有權限讀取。 如果出現too few PGs per，修改pg_num與pgp_num。範例如下： $ ceph osd pool set rbd pg_num 128 $ ceph osd pool set rbd pgp_num 128 若要檢查 mds 可以使用以下指令： $ ceph mds stat 若想檢查 OSDs 的目前狀態可以使用以下幾個指令： $ ceph osd stat osdmap e119: 3 osds: 3 up, 3 in flags sortbitwise $ ceph osd dump $ ceph osd tree 最後如果進行多台 Monitor 的部署的話，要注意讓這些節點的時間同步。Ceph 使用多 Monitort 來避免單點故障問題，部署的比例可自行定義，比如 1 台、3:1 台、5:3 台等等。","tags":[{"name":"Ceph","slug":"Ceph","permalink":"https://kairen.github.io/tags/Ceph/"},{"name":"Storage","slug":"Storage","permalink":"https://kairen.github.io/tags/Storage/"}]},{"title":"Ceph 分散式儲存系統介紹","date":"2015-11-19T09:08:54.000Z","path":"2015/11/19/ceph/ceph-intro/","text":"Ceph 提供了Ceph 物件儲存以及Ceph 區塊儲存，除此之外 Ceph 也提供了自身的Ceph 檔案系統，所有的 Ceph 儲存叢集的部署都開始於 Ceph 各節點，透過網路與 Ceph 的叢集溝通。最簡單的 Ceph 儲存叢集至少要建立一個 Monitor 與兩個 OSD（Object storage daemon），但是當需要運行 Ceph 檔案系統時，就需要再加入Metadata伺服器。 Ceph 目標開發檔案系統是一種複雜的投入，但是如果能夠準確地解決問題的話，則擁有著不可估量的價值。我們可以把 Ceph 的目標可以簡單定義為以下： 容易擴充到 PB 級別的儲存容量 在不同負載情況下的高效能（每秒輸入/輸出操作數[IPOS]、帶寬） 高可靠性 但這些目標彼此間相互矛盾(例如:可擴充性會減少或阻礙效能，或影響可靠性)。 Ceph 開發了一些有趣的概念(例如動態 metadata 分區、資料分散、複製)。 Ceph 的設計也實作了容錯性來防止單一節點故障問題（SOPF），並假設，大規模（PB 級）中儲存的故障是一種常態，而非異常。最後，它的設計沒有假設特定的工作負荷，而是包含了可變的分散式工作負荷的適應能力，從而提供最佳的效能。它以POSIX 兼容為目標完成這些工作，允許它透明的部署於那些依賴於 POSIX 語義上現有的應用(通過Ceph增強功能)。最後，Ceph 是開源分散式儲存和 Linux 主流核心的一部分。 其中最底層的 RADOS 是由 OSD、Monitor 與 MDS 三種所組成。 Ceph 架構現在，讓我們先在上層探討 Ceph 架構及其核心元素。之後深入到其它層次，來解析 Ceph 的一些主要方面，從而進行更詳細的分析。 Ceph 生態系統可以大致劃分為四部分（圖1）： 客戶端 (資料使用者) metadata 伺服器 (快取及同步分散的metadata) 物件儲存叢集 (以物件方式儲存資料與 metadata，實現其它主要職責) 叢集監控 (實現監控功能) 如圖所示，客戶使用 metadata 伺服器，執行 metadata 操作(來確定資料位置)。metadata 伺服器管理資料位置，以及在何處儲存取新資料。值得注意的是，metadata 儲存在一個儲存叢集上（標記為 “metadata I/O”）。實際的檔案 I/O 發生在客戶和物件儲存叢集之間。這樣一來，提供了更高層次的 POSIX 功能(例如，開啟、關閉、重新命名)就由 metadata 伺服器管理，不過 POSIX 功能（例如讀和寫）則直接由物件儲存叢集管理。 上面分層視圖說明了，一系列伺服器透過一個客戶端介面存取 Ceph 生態圈系統，這就明白了 metadata 伺服器和物件級儲存之間的關係。分散式儲存系統可以在一些層面中查看，包括一個儲存設備的格式（Extent and B-tree-based Object File System [EBOFS]或者一個備選），還有一個設計用於管理資料複製、故障檢測、復原，以及隨後資料遷移的覆蓋管理層，叫做Reliable Autonomic Distributed Object Storage（RADOS）。最後，監視器用於區別元件中的故障，包括隨後的通知。 Ceph 元件成員簡單的 Ceph 生態系統了解 Ceph 概念架構後，我們來探討另一個層次，了解在 Ceph 中實現的主要元件。 Ceph 和傳統檔案系統之間的重要差異之一，就是智能部分都用在了生態環境，而不是檔案系統本身。 圖中顯示了一個簡單的Ceph生態系統。Ceph Client 是 Ceph 檔案系統的用戶。Ceph Metadata Daemon 提供了 metadata 伺服器，而 Ceph Object Storage Daemon 提供了實際儲存（對資料和 metadata 兩者）。最後 Ceph Monitor 提供了叢集管理。要注意的是 Ceph 客戶，物件儲存端點，metadata 伺服器（根據檔案系統的容量）可以有許多，而且至少有一對冗餘的監視器。那麼這個檔案系統是如何分散的呢？ Ceph Client因為Linux顯示檔案系統的一個共有介面（通過虛擬檔案系統交換機[VFS]），Ceph 的用戶透視圖就是透明的。然而管理者的透視圖肯定是不同的，考慮到很多伺服器會包含儲存系統這一潛在因素（要查看更多建立的 Ceph 叢集的資訊，可看參考資料部分）。從用戶的角度來看，存取大容量的儲存系統，卻不知道下面聚合成一個大容量的儲存池的 metadata 伺服器、監視器、還有獨立的對象儲存裝置。用戶只是簡單地看到一個安裝點，在這點上可以執行標准檔案 I/O。 P.S 核心或使用者空間 : 早期版本的 Ceph 利用在 User SpacE（FUSE）的 Filesystems，它把文件系統推入到用戶空間，還可以很大程度上簡化其開發。但是今天，Ceph 已經被集成到主線內核，使其更快速，因為用戶空間上下文交換機對文件系統 I/O 已經不再需要。 Ceph Metadata Server（MDS）作為處理 Ceph File System 的 metadata 之用，若僅使用 Block or Object storage，就不會用到這部分功能。 P.S 若只使用 Ceph Object Storage 與 Ceph Block Device 的話，將不需要部署 MDS 節點。 Ceph Monitor（MON）Ceph 包含實施叢集映射管理的監控者，但是故障管理的一些要素是在物件儲存本身執行的。當物件儲存裝置發生故障或者新裝置加入時，監控者就會檢測和維護一個有效的叢集映射。這個功能會按一種分散式的方法執行，這種方式中映射升級可以和當前的流量溝通。Ceph 使用Paxos，它是一系列分散式共識演算法。 在每一個 Ceph 儲存中，都有一到多個 Monitor 存在，為了有效的在分散式的環境下存取檔案，每一個 Ceph Monitor daemon 維護著很多份與 叢集相關的映射資料(map)，包含： Monitor map：包含叢集的 fsid 、位置、名稱、IP 位址和 Port，也包括目前 epoch、此狀態圖何時建立與最近修改時間。 OSD map：包含叢集 fsid 、此狀態圖何時建立、最近修改時間、儲存池（Pools）列表 、副本數量、放置群組（PG）數量、 OSD 列表與其狀態（如 up 、 in ） Placement Group map：包含放置群組版本、其時間戳記、最新的 OSD epoch、佔用率以及各放置群組詳細，如放置群組 ID 、 up set 、 acting set 、 PG 狀態（如 active+clean），和各儲存池的資料使用情況統計。 CRUSH map： 包含儲存裝置列表、故障域樹狀結構（如裝置、主機、機架、row、機房等等），以及儲存資料時如何利用此樹狀結構的規則。 MDS map：包含當前 MDS map 的 epoch、建立於何時與最近修改時間，還包含了儲存 metadata 的儲存池、metadata 伺服器列表、還有哪些 metadata 伺服器是 up 且in 的。 主要是用來監控 Ceph 儲存叢集上的即時狀態，確保 Ceph 可以運作無誤。 Ceph Object Storage Daemon（OSD）與傳統的物件儲存類似，Ceph 儲存節點不僅包括儲存，還包含了智能容錯與恢復。傳統的驅動是只響應來自啟動者下達的命令中簡單目標。但是物件儲存裝置是智能裝置，它能作為目標和啟動者，支持與其他物件儲存裝置的溝通與合作。 實際與 Client 溝通進行資料存取的即為 OSD。每個 object 被儲存到多個 PG（Placement Group）中，每個 PG 再被存放到多個 OSD 中。為了達到 Active + Clean 的狀態，確認每份資料都有兩份的儲存，每個 Ceph 儲存叢集中至少都必須要有兩個 OSD。 主要功能為實際資料（object）的儲存，處理資料複製、恢復、回填（backfilling, 通常發生於新 OSD 加入時）與重新平衡（發生於新 OSD 加入 CRUSH map 時），並向 Monitor 提供鄰近 OSD 的心跳檢查資訊。 CRUSHCeph 透過 CRUSH（Controlled, Scalable, Decentralized Placement of Replicated Data） 演算法來計算資料儲存位置，來確認如何儲存和檢索資料，CRUSH 授權 Ceph client 可以直接連接 OSD，而不再是透過集中式伺服器或者中介者來儲存與讀取資料。該演算法與架構特性使 Ceph 可以避免單一節點故障問題、效能瓶頸與擴展的限制。 CRUSH 是一種偽隨機資料分散算法，能夠再階層結構的儲存叢集很好地分散物件的副本，該演算法實作了偽隨機(確定性)的函式，會透過輸入o​​bject id、object group id等參數，來回傳一組儲存裝置（即儲存物件的副本）。CRUSH 有以下兩個重要的資訊： Cluster Map：被用來描述儲存叢集的階層結構。 CRUSH Rule：被用來描述副本的分散策略。 CRUSH 主要提供了以下功能： 將資料有效率的存放在不同的儲存裝置中。 即使移除整個 cluster中的儲存裝置，也不會影響到資料正常的存取。 不需要有任何主要的管理裝置(or 節點)來做為控管之用。 可依照使用者所定義的規則來處理資料分散的方式。 透過以上資訊，CRUSH 可以將資料分散存放在不同的儲存實體位置，並避免單點錯誤造成資料無法存取的狀況發生。 PoolsPool 是儲存物件邏輯分區。Ceph Client 從 Monitor 取得 Cluster map，並把物件寫入 Pool。Pool 的副本數、CRUSH Ruleset 和 PG 數量決定著 Ceph 該如何放置資料。 一個 Pool 可以設定許多參數，但最少需要正確設定以下資訊： 物件擁有權與存取權限 Placement Group 數量 CRUSH Ruleset 設定與使用 Placement GroupCeph 把物件映射到放置群組（PG），PG 是一種邏輯的物件 Pool 片段，這些物件會組成一個群組後，再存到 OSD 中。PG 減少了各物件存入對應的 OSD 時 metadata 的數量，更多的 PG（如：每個OSD 有 100 個群組）可以使負載平衡更好。 P.S 多個 PG 也可以對應到同一個 OSD，因此 PG 與 OSD 其實是種多對多的關係。 PG ID 計算當 Client 在綁定某個 Monitor 時，會先取得最新的 Cluster map 副本，該 map 可讓 Client 端知道叢集有多少 Monitor、OSD 與 MDS。但是無法知道要存取的物件位置。 在 Ceph 中，物件的位置是透過計算得知的。因此 Client 只需要傳入 Object id 與 Pool 即可知道物件位置。當 Client 需要有名稱物件（如 mysql、archive 等）時，Ceph 會用物件名稱來計算 PG（是一個 Hash value）、 OSD 編號與 Pool。流程如下： Client 輸入 pool id 與 object id（e.g., pool=’vms’, object id=’instance-1’） CRUSH 取得 object id，並進行 Hash 取得值 CRUSH 在以 OSD 數量對 Hash value 進行 mod 運算，來取得 pg id（e.g., 58） CRUSH 再透過取得 pool name 來取得 pool id（e.g., ‘vms’ = ‘4’）5* CRUSH 在把 pool id 加到 pg id 前面（e.g, 4.58） 透過計算物件位置的方式，改善了傳統查詢定位的效能問題。CRUSH 演算法讓 Client 計算物件應該存放到哪裡，並連接該 Primary OSD 進行物件儲存與檢索。 Ceph 三大儲存服務系統特性與功能Ceph 是一個統一的儲存系統，提供了物件、區塊與檔案系統功能，且擁有高可靠、高擴展。該三大儲存服務功能與特性如下： CEPH 物件儲存 CEPH 區塊裝置 CEPH 檔案系統 RESTful 介面 精簡空間配置 與 POSIX 語意相容 與 S3 和 Swift 相容的 API 映像檔大小最大支援 16 EB（exabytes） Metadata 與資料分離 S3 風格的子域名 可組態的等量化（Configurable striping） 動態重新平衡(Dynamic rebalancing) 統一的 S3/Swift 命名空間 記憶體快取 子目錄快照 使用者管理 快照 可組態的等量化（Configurable striping） 利用率追蹤 寫入時複製 cloning 核心驅動的支援 等量化物件（Striped objects） 支援 KVM 與 libvirt 支援使用者空間檔案系統（FUSE） 雲端解決方案整合 可作為雲端解決方案的後端 可作為 NFS/CIFS 部署 多站點部署 累積備份 可用於 Hadoop 上（替代 HDFS ） 災難復原 參考 Ceph中文文件 小信豬 Ceph Ceph 官方文件 Ceph Development Ceph 整合案例 Ceph Book","tags":[{"name":"Ceph","slug":"Ceph","permalink":"https://kairen.github.io/tags/Ceph/"},{"name":"Storage","slug":"Storage","permalink":"https://kairen.github.io/tags/Storage/"},{"name":"Distribution System","slug":"Distribution-System","permalink":"https://kairen.github.io/tags/Distribution-System/"}]},{"title":"Apache Cassandra 分散式資料庫","date":"2015-11-17T09:08:54.000Z","path":"2015/11/17/data-engineer/apache-cassandra/","text":"Cassandra 是最初由 Facebook 開發，之後貢獻給 Apache 基金會維護的分散式 NoSQL 資料庫系統，一般被認為 Amazon Dyname 與 Google BigTable 的結合體，主要是分散性像 Dynamo，然而資料模型卻如 BigTable。目前有多家公司採用，可運行上千台節點來提供超過 200 TB 的資料。 Cassandra 擁有幾個特點，也因為這些特點讓許多人選擇使用該資料庫，以下幾個項目簡單列出其特點： 完全去中心化，且不是主從架構的備份 統一類型的節點 以 P2P 協定串連起網路，清除 SPOF（Single Point Of Failure）問題 高擴展性，新增與刪除節點容易 可呼叫的一致性，並支援強一致性與弱一致性 支援跨區域的叢集架構 每個區域儲存一份完整資料，來提供存取局部性、容錯與災難復原 寫入效能理論上比讀取好（但近期有證實現在讀取也很不錯），適合串流資料儲存 比 HBase 的隨機存取效能要好上許多，但不擅長區間掃描 可作為 HBase 的即時查詢快取。 若要瞭解更多 Cassandra 可以閱讀 Cassandra Wiki 。 部署多節點叢集本節將安裝一個簡單的 Cassandra 叢集，來提供 NoSQL 資料庫以存取資料，以下為節點配置： IP Address HostName 172.17.0.2 cassandra-1 172.17.0.3 cassandra-2 172.17.0.4 cassandra-3 首先需在每個節點安裝 Java，這邊採用 Oracle 的 Java 來進行安裝： $ sudo apt-get install -y software-properties-common $ sudo add-apt-repository -y ppa:webupd8team/java $ sudo apt-get update $ echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections $ echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections $ sudo apt-get -y install oracle-java7-installer 若要安裝java8請修改成oracle-java8-installer。 接著在各節點安裝 Cassandra 套件，這邊採用apt-get安裝，首先加入 Repository： $ echo &quot;deb http://www.apache.org/dist/cassandra/debian 22x main&quot; | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list 這邊安裝2.2.x版本，若要其他版本則修改22x，如改為21x。 為了避免軟體套件更新軟體時有簽證警告，需加入 Apache 基金會與套件資源庫相關的三個公有金鑰： $ gpg --keyserver pgp.mit.edu --recv-keys F758CE318D77295D $ gpg --export --armor F758CE318D77295D | sudo apt-key add - $ gpg --keyserver pgp.mit.edu --recv-keys 2B5C1B00 $ gpg --export --armor 2B5C1B00 | sudo apt-key add - $ gpg --keyserver pgp.mit.edu --recv-keys 0353B12C $ gpg --export --armor 0353B12C | sudo apt-key add - 完成後更新 apt-get Repository： $ sudo apt-get update 安裝 Cassandra NoSQL 於每個節點上： $ sudo apt-get install -y cassandra 完成後，我們必須開始配置各節點來組成一個叢集，先把每個節點的 Cassandra 服務關閉： $ sudo service cassandra stop 關閉後，在每個節點編輯/etc/cassandra/cassandra.yaml檔案，並修改一下內容： cluster_name: &#39;examples&#39; num_tokens: 256 seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: - seeds: &quot;172.17.0.2, 172.17.0.3, 172.17.0.4&quot; listen_address: 172.17.0.2 broadcast_address: 172.17.0.2 rpc_address: 0.0.0.0 broadcast_rpc_address: 172.17.0.2 P.S. 這邊要注意不同節點會有不同，如listen_address與seeds等。 都完成設定檔後，就可以重啟每台的 Cassandra 服務： $ sudo service cassandra restart 當確認所有節點重新啟動後，在其中一個節點建立 keyspaces 來做 replication： $ cqlsh cqlsh&gt; create keyspace Spark_smack WITH REPLICATION = { &#39;class&#39; : &#39;SimpleStrategy&#39;, &#39;replication_factor&#39; : &#39;3&#39; }; exit Spark_smack 該值是可以依個人名稱修改。 建立完成後，可以使用nodetool指令來檢查是否成功： $ nodetool status spark_smack Datacenter: datacenter1 ======================= Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN 172.17.0.3 255.68 KB 256 100.0% 9c9eb117-f787-47bd-825f-3daf49eba489 rack1 UN 172.17.0.2 252.03 KB 256 100.0% 7a4adb77-42d1-402f-b57b-4a40ad013e2c rack1 UN 172.17.0.4 127.2 KB 256 100.0% 5823fd78-45f2-4328-9470-f1053bb3fc3b rack1 驗證系統我們透過 Client 程式來連到cassandra-1節點來驗證叢集是否建立成功： $ cqlsh cqlsh&gt; use &lt;keyspace name&gt;; 這邊&lt;keyspace name&gt;範例為 spark_smack。 然後建立一個資料表： CREATE TABLE emp( emp_id int PRIMARY KEY, emp_name text, emp_city text, emp_sal varint, emp_phone varint ); 接著插入一筆資料到資料表： INSERT INTO emp (emp_id, emp_name, emp_city ) VALUES(1, &#39;Kyle&#39;, &#39;Taichung&#39; ); 若都沒有任何錯誤的話，現在連接到其他節點來檢查是否有同步資料： $ cqlsh cqlsh&gt; use spark_smack; cqlsh&gt; select * from emp; emp_id | emp_city | emp_name | emp_phone | emp_sal --------+----------+----------+-----------+--------- 1 | Taichung | Kyle | null | null P.S 若發生如以下錯誤，請依照指示解決： Connection error: (&#39;Unable to connect to any servers&#39;, {&#39;127.0.0.1&#39;: error(111, &quot;Tried connecting to [(&#39;127.0.0.1&#39;, 9042)]. Last error: Connection refused&quot;)}) 首先檢查設定檔/etc/cassandra/cassandra.yaml裡面的rpc_address是否為0.0.0.0。 最後，若要復原與備份可以閱讀 Cassandra Backup and Recovery。 參考資源 Cassandra简介 Install and Configure a 3 node Cassandra Cluster on Ubuntu 14.04 關於 Cassandra 的錯誤觀點","tags":[{"name":"Spark","slug":"Spark","permalink":"https://kairen.github.io/tags/Spark/"},{"name":"Database","slug":"Database","permalink":"https://kairen.github.io/tags/Database/"},{"name":"NoSQL","slug":"NoSQL","permalink":"https://kairen.github.io/tags/NoSQL/"}]},{"title":"Kuberentes 是什麼？","date":"2015-11-10T09:08:54.000Z","path":"2015/11/10/kubernetes/k8s-intro/","text":"Kubernetes 是 Google 的開源專案，主要用於管理跨主機容器化叢集系統。該專案脫胎於 Google 內部叢集管理工具 Borg，早期主要貢獻者更是參與 Borg 專案的人員，大概基本上都認為 Kubernetes 許多概念與架構是來至 Google 十餘年的設計、部署、管理大規模容器的經驗。 Kubernetes 在 Docker 叢集管理中是很重要的一員，其實作了許多功能，包含應用程式部署、叢集節點擴展、自動容錯機制等，並且能夠統一管理跨主機的管理。其架構如下圖所示。 使用 Kubernetes 的好處，網路上已有許多相關資訊，這邊列舉幾個： 輕易的 Scale out/in 容器 自動化容錯與擴展，如容器的部署與副本 以多個容器組成群組，並提供容器負載平衡 容易擴展節點 以叢集的方式來管理跨機器的容器。 基於 Docker 的應用程式封裝、實例化與執行。 兩種節點角色： Node：是運行 k8s worker 的實體或者虛擬機器，一般稱為 Minion 節點。該節點會運作幾個 k8s 關鍵元件： kubelet：為 master 節點的 agent。 kube-proxy： Sevice 使用其將服務請求連接路由到 Pod 的容器中。 docker engine（或 rocket）：主要建立容器的引擎。 Master：每一個 k8s 叢集都會有一個或多個 Master，主要提供 REST APIs、管理 k8s 工具、運行 Etcd 、Scheduler 以及 Pod 的 Replication Controller 等服務。 如架構圖所示，Kubernetes 由多個元件組合而成，然而在了解 Kubernetes 元件之前，我們需要先知道 Kubernetes 的生態圈中的一些重要概念，這些概念將影響對於 Kubernetes 進一步的探索，其主要概念如下： Pods（po）：是 k8s 的最基本執行單元，即包含一組容器與 Volumes。在同一個 Pod 裡的容器會共享使用一個網路命名空間，並利用 localhost 溝通，Pod 生命週期是短暫的。 Services（svc）：由於 k8s 的 Pods 中的容器 IP 會隨著排程、故障等因素而改變，因此 k8s 利用 Service 來定義一系列存取這些 Pod 應用程式服務的抽象層。Service 透過 Proxy 的 port 與 Selector 來決定服務請求要傳送給後端哪些 Pods 中的容器，因此對外是單一的存取介面，。 Replication Controllers（rc）：主要確保 k8s 叢集所指定 Pod 中的容器被正常運作的副本數量，當正在執行的容器發生故障，而少於副本數時，Replication Controller 會在任一節點啟動一個新的容器，然後若是多於副本數量則會自動移除一個容器。 Lable：被用來區分 Pod、Service 與 Replication Controllers 的 Key/Vaule 標籤，用來傳遞使用者定義的屬性。Label 是 rc 與 svc 的執行基礎，rc 透過標示容器 label 來讓 svc 服務請求正確選擇要存取容器。","tags":[{"name":"Linux Container","slug":"Linux-Container","permalink":"https://kairen.github.io/tags/Linux-Container/"},{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://kairen.github.io/tags/Kubernetes/"}]},{"title":"Docker 基礎介紹","date":"2015-11-09T09:08:54.000Z","path":"2015/11/09/container/docker/","text":"Docker 是一個開源的專案，主要提供容器化應用程式的部署與自動化的管理，透過部署 Docker engine 於 Linux 作業系統上，提供軟體抽象層以及作業系統層虛擬化自動管理機制。Docker 使用一些 Linux Kernel 中的軟體，如 Control Groups（cgroups）、namespaces（核心命名空間）、Another UnionFS 等來達到建置獨立的環境與存取 CPU、Memory 以及網路資源等，最終提供一個輕量級的虛擬化。Docker 的發展非常迅速，從 0.9.0 版本後，採用自己開發的 Libcontainer 函式庫作為直接存取由 Linux Kernel 提供的虛擬化基礎設施。Docker 也支援了多平台，從個人電腦到私有、公有雲都能夠進行使用與部署輕量級虛擬化。 Docker 並非一個新興技術，而是過往的 Linux Container 技術的基礎上進行封裝。因此使用者只需要關注應用程式如何被部署與執行就好，而不用管容器是如何被建置出來。 使用 Docker 有以下幾項好處： 更快速的交付和部署 高效能的虛擬化環境 易遷移和擴展服務 管理簡單化 更有效的使用實體主機資源 可建置任何語言的 Dockerized 應用程式 跨平台的管理、部署與使用 微軟過往許多需要相依於 Windows 語言應用程式也慢慢被支援到 Docker 了。 Docker Container vs 傳統虛擬化傳統虛擬化技術一般是透過在 Host OS 上安裝 Hypervisor（VMM），然後由 Hypervisor 來管理不同虛擬主機，每個虛擬主機都需要安裝不同的作業系統，因此每個環境被獨立的完全隔離。 然而 Docker 提供應用程式在獨立的 Container 中執行，這些Container 並不需要像虛擬化一樣額外依附在 Hypervisor 甚至 Guest OS 上，而是透過 Docker engine 來進行管理。 Docker 基本概念Docker 擁有幾個基本概念，其中包含 Docker 三大重要部分與元件。我們將針對這些概念進行概述，也會在其他章節進一步說明。 Docker images：Image（映像檔）被用來啟動容器的實際執行得應用程式環境。這概念類似 VM 的映像檔，VM 透過映像檔來啟動作業系統，並執行許多服務，但 Docker 的映像檔則只是檔案系統的儲存狀態，是一個唯讀的板模。 Docker containers：Container（容器）是一個應用程式執行的實例，Docker 會提供獨立、安全的環境給應用程式執行，容器是從映像檔建立，並運作於主機上。 P.S 盡量不要在一個 Container 執行過多的服務。 Docker registries：Registries 是被用來儲存 Docker 所建立的映像檔的地方，我們可以把自己建立的映像檔透過上傳到 Registries 來分享給其他人。Registries 也被分為了公有與私有，一般公有的 Registries 是 Docker Hub，提供了所有基礎的映像檔與全球使用者上傳的映像檔。私人的則是企業或者個人環境建置的，可參考 Deploying a registry server。 Docker 的推出與發展非常迅速，相關的部署工具與資源相繼出現，更因此讓原名為 dotcloud 變成 Docker, Inc。Docker 也在 2014 - 2015 年推出了以下三大工具： docker-machine：Docker machine 是可以透過指令來安裝 Docker engine 的工具。該工具可以讓使用者不需要學習一堆安裝指令來部署容器環境，目前已經支援了許多驅動程式，例如：OpenStack、Amazon EC2、Google Cloud Engine 與 Microsoft Azure等，更可以被用來建立混合環境。 docker-compose：Compose 是 Docker 的編配工具，可以用來建置 Swarm 上的多節點容器化叢集與單一節點的應用程式。Compose 的前身是 Fig，使用者可以透過定義 YAML 檔案來描述與維護所有應用程式服務定義與部署，如多個服務之間如何連接等，使用 Compose 部署的應用程式可以在不影響其他服務情況下自動更新。 docker-swarm：Swarm 是 Docker 的原生叢集與調度工具，它基於應用程式生命週期、容器使用、效能需求自動優化分散式應用程式的基礎架構。且 Swarm 可以透過許多服務發現（Service Discovery）套件來打造 HA 的叢集，Swarm 也提供很高的靈活性，使應用程式可以簡單的分散部署在多主機環境上。 使用者操作 Docker 的方法Docker 主機上會執行一個 Docker daemon，就能夠開啟許多 Container。如果要對 Docker 進行操作的話，可以使用 Docker client 軟體，如 docker client、docker-py、Kitematic，這些工具會分別採用以下兩種方式來對部署 Docker daemon 進行管理： UNIX Sockets RESTful API 溝通方式如下圖所示，其中 Docker daemon 可以同時安裝 Docker client 來直接進行 Docker 使用（一般安裝都會有），詳細資訊可以參閱 Docker Remote API - Docker Documentation。 最後有興趣看每週 Docker 的新聞可以訂閱 Docker Weekly，參閱 Docker Newsletter 參考資源 Docker 官方 Docker Blog InfoQ Docker 深入淺出 Docker —— 從入門到實踐","tags":[{"name":"Linux Container","slug":"Linux-Container","permalink":"https://kairen.github.io/tags/Linux-Container/"},{"name":"Docker","slug":"Docker","permalink":"https://kairen.github.io/tags/Docker/"}]},{"title":"簡單設定 Apache2 Proxy 與 VirtualHost","date":"2015-11-04T09:08:54.000Z","path":"2015/11/04/linux/ubuntu/apache2-proxy/","text":"Apache2 是一套經過測試與用於生產環境的 HTTP 伺服器，在許多網頁伺服器中被廣泛的採用，Apache2 除了本身能力強大外，其也整合了許多的額外模組來提供更多的擴展功能。 Apache2 安裝與設定要安裝 Apache 伺服器很簡單，只需要透過 APT 進行安裝即可： $ sudo apt-get update $ sudo apt-get install -y libapache2-mod-proxy-html libxml2-dev apache2 build-essential 啟用 Proxy Modules這邊可以透過以下指令來逐一啟動模組： a2enmod proxy a2enmod proxy_http a2enmod proxy_ajp a2enmod rewrite a2enmod deflate a2enmod headers a2enmod proxy_balancer a2enmod proxy_connect a2enmod proxy_html 設定 Default conf 來啟用編輯/etc/apache2/sites-available/000-default.conf設定檔，加入 Proxy 與 VirtualHost 資訊： # 簡單 Proxypass 範例 &lt;VirtualHost *:80&gt; ErrorLog ${APACHE_LOG_DIR}/laravel-error.log CustomLog ${APACHE_LOG_DIR}/laravel-access.log combined ProxyPass / http://192.168.20.10/ ProxyPassReverse / http://192.168.20.10/ ProxyPreserveHost On ServerName laravel.kairen.com ServerAlias laravel.kairen.com ServerAlias *.laravel.kairen.com &lt;/VirtualHost&gt; # 簡單 Load balancer 範例 &lt;Proxy balancer://api-gateways&gt; # Server 1 BalancerMember http://192.168.20.11:8080/ # Server 2 BalancerMember http://192.168.20.12:8080/ &lt;/Proxy&gt; &lt;VirtualHost *:*&gt; ProxyPass / balancer://api-gateways &lt;/VirtualHost&gt; 完成後重新啟動服務即可： $ sudo service apache2 restart 使用 SSL Reverse-Proxy如果需要設定 SSL 連線與認證的話，可以透過以下設定方式來提供： Listen 443 NameVirtualHost *:443 &lt;VirtualHost *:443&gt; SSLEngine On SSLCertificateFile /etc/apache2/ssl/file.pem ProxyPass / http://192.168.20.11:8080/ ProxyPassReverse / http://192.168.20.11:8080/ &lt;/VirtualHost&gt; 完成後重新啟動服務即可： $ sudo service apache2 restart","tags":[{"name":"Linux","slug":"Linux","permalink":"https://kairen.github.io/tags/Linux/"},{"name":"HTTP Server","slug":"HTTP-Server","permalink":"https://kairen.github.io/tags/HTTP-Server/"}]},{"title":"Apache Kafka 叢集","date":"2015-10-13T09:08:54.000Z","path":"2015/10/13/data-engineer/kafka-install/","text":"Apache Kafka 是一個分散式的訊息佇列框架，是由 LinkedIn 公司使用 Scala 語言開發的系統，被廣泛用來處理高吞吐量與容易水平擴展，目前許多巨量資料運算框架以都有整合 Kafka，諸如：Spark、Cloudera、Apache Storm等， Kafka 是基於Publish/Subscribe的訊息系統，主要設計由以下特點： 在 TB 級以上資料也能確保常數時間複雜度的存取效能，且時間複雜度為O(1)的訊息持久化。 高吞吐量，在低階的商業電腦上也能提供單機100k/sec條以上的訊息傳輸。 支援 Kafka Server 之間的訊息分區(Partition)以及分散式發送，並保證每個分區內的訊息循序傳輸。 同時支援離線資料處理與即時資料處理 容易的服務不中斷水平擴展。 Kafka 從架構上來看，Kafka 會擁有以下幾個角色： Producer：主要為 Publish 訊息到 Topic。 Consumer：主要為 Subscribe Topic 來取得訊息。 Broker：訊息的中介者，可看錯是一台訊息 Server，可部署單機至多台叢集。 Topic：拿來做訊息的分類。 Zookeeper：Zookeeper 不算是 Kafka 一員，但 Kafka 依賴 Zookeeper 來做到 Sync。 Apache Kafka 的一個簡單應用架構可以參考下圖，透過 Spark Streaming 來進行串接做快速的串流資料收集，並利用 Spark 框架進行分析後取得結果存於 Cassandra 資料庫叢集，最後在由應用程式或前端網頁來顯示處理過的資料。 安裝 Apache Kafka一個簡單的節點配置如下： IP Address Role zookeeper id broker.id 172.17.0.2 kafka-1 1 0 172.17.0.3 kafka-2 2 1 172.17.0.4 kafka-3 3 2 首先要在每台節點安裝 Java，這邊採用 Oracle 的 Java 來進行安裝： $ sudo apt-get install -y software-properties-common $ sudo add-apt-repository -y ppa:webupd8team/java $ sudo apt-get update $ echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections $ echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections $ sudo apt-get -y install oracle-java7-installer 完成後，接著在每台節點安裝 Zookeeper 服務，這邊採用apt-get來進行安裝： $ sudo apt-get install -y zookeeperd 完成安裝後，zookeeper 會自動開啟服務於 port 2181。若沒啟動使用以下指令： $ sudo service zookeeper restart 若想部署多節點 Zookeeper，請修改每台節點的/etc/zookeeper/conf/zoo.cfg檔案，加入以下內容：(Option) server.1=172.17.0.2:2888:3888 server.2=172.17.0.3:2888:3888 server.3=172.17.0.4:2888:3888 並設定 ID，如下指令： $ echo &quot;1&quot; | sudo tee /etc/zookeeper/conf/myid 測試 Zookeeper 是否啟動，可以透過 Telnet 來進行： $ telnet localhost 2181 當節點上述完成後就可以下載 Kafka 套件，並解壓縮到/opt/底下： $ sudo curl -s &quot;http://ftp.tc.edu.tw/pub/Apache/kafka/0.9.0.1/kafka_2.10-0.9.0.1.tgz&quot; | sudo tar -xz -C /opt/ $ sudo mv /opt/kafka_2.10-0.9.0.1 /opt/kafka 下載完成後，編輯/opt/kafka/config/server.properties，並加入以下內容： broker.id=0 host.name=172.17.0.2 zookeeper.connect=172.17.0.2:2181,172.17.0.3:2181,172.17.0.4:2181 這邊的broker.id需跟著節點數變動，從 0 開始計數。若使用OpenStack或者Docker這些虛擬化的話，需在設定檔加入： advertised.host.name=&lt;Advertised IP&gt; advertised.port=9092 編輯完以後就分別啟動這三台 Broker： $ cd /opt/kafka $ bin/kafka-server-start.sh config/server.properties &amp; 驗證服務當所有 Server 啟動完成後，就可以透過建立 Topic 來確認是否成功部署完成： $ /opt/kafka/bin/kafka-topics.sh --create \\ --zookeeper localhost:2181 \\ --replication-factor 3 \\ --partitions 1 \\ --topic test 可以試著將--replication-factor改為 4，若成功會看到以下錯誤訊息： replication factor: 4 larger than available brokers: 3 原因是我們只有建立 3 台叢集。 建立完成後，可以用以下指令查看： $ /opt/kafka/bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test # 會看到類似以下資訊 Topic:test PartitionCount:1 ReplicationFactor:3 Configs: Topic: test Partition: 0 Leader: 2 Replicas: 2,0,1 Isr: 2,0,1 接下來透過 Publish 來傳送訊息： $ /opt/kafka/bin/kafka-console-producer.sh \\ --broker-list localhost:9092 \\ --topic test # 輸入 ggeeder ggeeder 接著就要讀取訊息，透過 Subscribe 來訂閱收取資料： $ /opt/kafka/bin/kafka-console-consumer.sh \\ --zookeeper 172.17.0.2:2181,172.17.0.3:2181,172.17.0.4:2181 \\ --topic test \\ --from-beginning 這時透過手動方式關閉該 broker，來測試 replication 是否有正確運作： $ jps 83 Kafka $ sudo kill -9 83 接著可以先去看該 Topic 的 Leader 是否有變化： $ /opt/kafka/bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test 此時看 Consume message，會發現訊息應該還是會保存的完整無缺： $ /opt/kafka/bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic test","tags":[{"name":"Spark","slug":"Spark","permalink":"https://kairen.github.io/tags/Spark/"},{"name":"Kafka","slug":"Kafka","permalink":"https://kairen.github.io/tags/Kafka/"},{"name":"Message Queue","slug":"Message-Queue","permalink":"https://kairen.github.io/tags/Message-Queue/"}]},{"title":"Spark on Mesos 多節點部署","date":"2015-10-12T09:08:54.000Z","path":"2015/10/12/data-engineer/spark-mesos/","text":"Spark + Mesos 叢集是由多個主節點與工作節點組合而成，它實作了兩層的排程（Scheduler）來提供粗/細粒度的排程。在 Mesos 中主節點（Master）主要負責資料的分配與排程，然而從節點（Slave）則是主要執行任務負載的角色。Mesos 也提供了高可靠的部署模式，可利用多個主節點的 ZooKeeper 來做服務發現。 在 Mesos 上所執行的應用程式都被稱為框架（Framework），該框架會被 Mesos 以 API 方式處理資源的提供，並將任務提交給 Mesos。其任務執行流程有以下幾個步驟構成： Slave 提供可用資源給 Master Master 向 Framework 的資源供應，並說明 Slave 資源 Framework Scheduler 回應任務以及每個任務的資源需求 Master 將任務發送到適當的 Slave 執行器（Executor） 事前準備以下為節點配置： IP Address HostName 192.168.1.10 mesos-master 192.168.1.11 mesos-slave-1 192.168.1.12 mesos-slave-2 首先我們要在各節點先安裝 ssh-server 與 Java JDK，並配置需要的相關環境： $ sudo apt-get install openssh-server 設定(hadoop)不用需打 sudo 密碼： $ echo &quot;hadoop ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/hadoop &amp;&amp; sudo chmod 440 /etc/sudoers.d/hadoop P.S 要注意 hadoop 要隨著現在使用的 User 變動。 建立ssh key，並複製 key 使之不用密碼登入： $ ssh-keygen -t rsa $ ssh-copy-id localhost 安裝Java 1.8 JDK： $ sudo apt-get purge openjdk* $ sudo apt-get -y autoremove $ sudo apt-get install -y software-properties-common $ sudo add-apt-repository -y ppa:webupd8team/java $ sudo apt-get update $ echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections $ echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections $ sudo apt-get -y install oracle-java8-installer 新增各節點 Hostname 至/etc/hosts檔案： 127.0.0.1 localhost 192.168.1.10 mesos-master 192.168.1.11 mesos-slave-1 192.168.1.12 mesos-slave-2 並在Master節點複製所有Slave的 ssh key： $ ssh-copy-id ubuntu@mesos-slave-1 $ ssh-copy-id ubuntu@mesos-slave-2 Mesos 安裝首先要安裝 Mesos 於系統上，可以採用以下方式獲取最新版本的 Respository： $ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv E56151BF $ DISTRO=$(lsb_release -is | tr &#39;[:upper:]&#39; &#39;[:lower:]&#39;) $ CODENAME=$(lsb_release -cs) $ echo &quot;deb http://repos.mesosphere.com/${DISTRO} ${CODENAME} main&quot; | sudo tee /etc/apt/sources.list.d/mesosphere.list 加入 key 與 repository 後，即可透過apt-get安裝： sudo apt-get update sudo apt-get -y install mesos P.S Master需要再安裝 Marathon Master 節點設定首先設定 Zookeeper ID： $ echo 1 | sudo tee /etc/zookeeper/conf/myid 設定 Zookeeper configuration： HOST_IP=$(ip route get 8.8.8.8 | awk &#39;{print $NF; exit}&#39;) echo server.1=$HOST_IP:2888:3888 | sudo tee -a /etc/zookeeper/conf/zoo.cfg 若要部署 HA 需要加入多個 Master 節點的 Zookeeper。 完成後，重新啟動 Zookeeper 服務： $ sudo service zookeeper restart 接著設定 Mesos zk configuration： $ echo zk://$HOST_IP:2181/mesos | sudo tee /etc/mesos/zk 設定 Mesos quorum 參數： $ echo 1 | sudo tee /etc/mesos-master/quorum 若是 OpenStack VM 需要設定 Host IP 和 EXENTAL_IP 為 區網 IP 而非 Flaot IP：（Optional） EXENTAL_IP=&#39;192.168.1.10&#39; echo $EXENTAL_IP | sudo tee /etc/mesos-master/hostname echo $HOST_IP | sudo tee /etc/mesos-master/ip echo &#39;mesos-cluster&#39; | sudo tee /etc/mesos-master/cluster 接著設定advertise_ip： $ echo $HOST_IP | sudo tee /etc/mesos-master/advertise_ip 當設定完成，要接著設定 Marathon，首先建立組態目錄： sudo mkdir /etc/marathon/ sudo mkdir /etc/marathon/conf 設定 hostname： $ echo $EXENTAL_IP | sudo tee /etc/marathon/conf/hostname 設定 master ip ： $ echo zk://$HOST_IP:2181/mesos | sudo tee /etc/marathon/conf/master 設定 master zookeeper ： $ echo zk://$HOST_IP:2181/marathon | sudo tee /etc/marathon/conf/zk 關閉 Master 節點的mesos-slave service： sudo service mesos-slave stop sudo sh -c &quot;echo manual &gt; /etc/init/mesos-slave.override&quot; 重新啟動 Mesos 與 Marathon 服務： sudo service mesos-master restart sudo service marathon restart Slave 節點設定由於我們是使用 ubuntu 套件，Zookeeper 會以相依套件被自動下載至環境上，故我們要手動停止服務： sudo service zookeeper stop sudo sh -c &quot;echo manual &gt; /etc/init/zookeeper.override&quot; 設定 Mesos 與 Marathon： 若使用 OpenStack VM，需要將 MASTER_IP 和 PUBlIC_IP 設定為區網 IP MASTER_IP=&quot;192.168.1.10&quot; PUBlIC_IP=&quot;192.168.1.11&quot; HOST_IP=$(ip route get 8.8.8.8 | awk &#39;{print $NF; exit}&#39;) echo zk://$MASTER_IP:2181/mesos | sudo tee /etc/mesos/zk 設定 Hostname 可以使用 OpenStack Float IP（Optional）： echo $PUBlIC_IP | sudo tee /etc/mesos-slave/hostname 設定 slave ip： $ echo $HOST_IP | sudo tee /etc/mesos-slave/ip 關閉 mesos-master 服務，並取消自動開機啟動： sudo service mesos-master stop sudo sh -c &quot;echo manual &gt; /etc/init/mesos-master.override&quot; 重新啟動 Mesos slave 服務： $ sudo service mesos-slave restart 驗證安裝結果當安裝完成，我們要驗證系統是否正常運行，可以透過以下指令運行： MASTER=$(mesos-resolve `cat /etc/mesos/zk`) mesos-execute --master=$MASTER --name=&quot;cluster-test&quot; --command=&quot;sleep 5&quot; 若要查看細節資訊，可以用瀏覽器開啟 Mesos Console、Marathon console 安裝 Spark Driver首先下載 Spark，並修改權限： $ curl -s &quot;http://archive.apache.org/dist/spark/spark-1.5.2/spark-1.5.2-bin-hadoop2.6.tgz&quot; | sudo tar -xz -C /opt/ $ sudo mv /opt/spark-1.5.2-bin-hadoop2.6 /opt/spark $ sudo chown $USER:$USER -R /opt/spark 之後到spark/conf目錄，將spark-env.sh.template複製為park-env.sh： $ cp spark-env.sh.template spark-env.sh 在spark-env.sh這內容最下方增加這幾筆環境參數： export MESOS_NATIVE_JAVA_LIBRARY=&quot;/usr/lib/libmesos.so&quot; export MASTER=&quot;mesos://192.168.1.10:5050&quot; export SPARK_EXECUTOR_URI=&quot;/opt/spark-1.5.2.tgz&quot; export JAVA_HOME=$(readlink -f /usr/bin/java | sed &quot;s:jre/bin/java::&quot;) export SPARK_LOCAL_IP=$(ifconfig eth0 | awk &#39;/inet addr/{print substr($2,6)}&#39;) export SPARK_LOCAL_HOSTNAME=$(ifconfig eth0 | awk &#39;/inet addr/{print substr($2,6)}&#39;) 接著下載一個新的spark-1.5.2-bin-hadoop2.6.tgz，並解壓縮： $ cd ~/ $ wget &quot;http://archive.apache.org/dist/spark/spark-1.5.2/spark-1.5.2-bin-hadoop2.6.tgz&quot; $ tar -xvf spark-1.5.2-bin-hadoop2.6.tgz $ sudo mv spark-1.5.2-bin-hadoop2.6 spark-1.5.2 $ sudo vim spark-1.5.2/conf/spark-env.sh export MESOS_NATIVE_LIBRARY=/usr/local/lib/libmesos.so export SPARK_EXECUTOR_URI=&quot;/opt/spark-1.5.2.tgz&quot; export MASTER=mesos://192.168.1.10:5050 export JAVA_HOME=$(readlink -f /usr/bin/java | sed &quot;s:jre/bin/java::&quot;) 完成後壓縮資料夾： $ sudo tar -czvf spark-1.5.2.tgz spark-1.5.2/ 並在Master節點複製到所有Slave並解壓縮： $ scp spark-1.5.2.tgz mesos-slave-1:~/ &amp;&amp; ssh mesos-slave-1 sudo mv ~/spark-1.5.2.tgz /opt $ scp spark-1.5.2.tgz mesos-slave-2:~/ &amp;&amp; ssh mesos-slave-2 sudo mv ~/spark-1.5.2.tgz /opt $ sudo tar -xvf /opt/spark-1.5.2.tgz 設定使用者環境參數： $ echo &quot;export SPARK_HOME=/opt/spark&quot; | sudo tee -a ~/.bashrc $ echo &quot;export PATH=\\$SPARK_HOME/bin:\\$PATH&quot; | sudo tee -a ~/.bashrc 執行spark-shell，來驗證 Spark 可否正常執行： $ spark-shell --master mesos://192.168.1.34:5050 val data = 1 to 10000 val distData = sc.parallelize(data) distData.filter(_&lt; 10).collect() 或使用範例程式提交 Job： $ spark-submit --class org.apache.spark.examples.SparkPi \\ --master mesos://192.168.1.10:5050 \\ --num-executors 1 \\ --executor-memory 1g \\ --executor-cores 1 \\ lib/spark-examples*.jar \\ 1","tags":[{"name":"Spark","slug":"Spark","permalink":"https://kairen.github.io/tags/Spark/"},{"name":"Mesos","slug":"Mesos","permalink":"https://kairen.github.io/tags/Mesos/"},{"name":"HDFS","slug":"HDFS","permalink":"https://kairen.github.io/tags/HDFS/"}]},{"title":"Ubuntu PXE 安裝與設定","date":"2015-10-11T04:23:01.000Z","path":"2015/10/11/linux/ubuntu/ubuntu-pxe/","text":"預啟動執行環境（Preboot eXecution Environment，PXE，也被稱為預執行環境)提供了一種使用網路介面（Network Interface）啟動電腦的機制。這種機制讓電腦的啟動可以不依賴本地資料儲存裝置（如硬碟）或本地已安裝的作業系統。 PXE 伺服器必須要提供至少含有 DHCP 以及 TFTP : DHCP 服務必須要能夠提供用戶端的網路參數之外，還得要告知用戶端 TFTP 所在的位置為何才行 TFTP 則是提供用戶端 boot loader 及 kernel file 下載點的重要服務 Kickstart我們在手動安裝作業系統時，會針對需求安裝作業系統的相關套件、設定、disk切割等，當我們重複的輸入這些資訊時，隨著需安裝的電腦越多會越裝越阿雜，如果有人可以幫你完成這樣一套輸入資訊的話，就可以快速的自動化部署多台電腦，除了方便外，心情也格外爽快。 kickstart是Red Hat公司針對自動化安裝Red Had、Fedora、CentOS而制定的問題回覆規範，透過這個套件可以指定回覆設定問題，更能夠指定作業系統安裝其他套裝軟體，也可以執行Script(sh, bash)，通常kickstart設定檔(.cfg)是透過system-config-kickstart產生。也可以利用GUI的CentOS下產生安裝用的cfg檔案。 Preseed相對於kickstart，preseed是Debain/Ubuntu的自動化安裝回覆套件。 其他工具 Stacki 3 Ubuntu MAAS Foreman LinMin OpenStack Ironic Crowbar PXE 安裝與設定首先安裝相關軟體，如 TFTP、DHCP等： sudo apt-get install -y tftpd-hpa isc-dhcp-server lftp openbsd-inetd DHCP 設定首先編輯 /etc/dhcp/dhcpd.conf檔案，在下面配置 DHCP： ddns-update-style none; default-lease-time 600; max-lease-time 7200; log-facility local7; subnet 10.21.10.0 netmask 255.255.255.0 { range 10.21.10.200 10.21.10.250; option subnet-mask 255.255.255.0; option routers 10.21.10.254; option broadcast-address 10.21.10.255; filename &quot;pxelinux.0&quot;; next-server 10.21.10.240; } 完成後，重新啟動 DHCP 服務： $ sudo service isc-dhcp-server restart * Stopping ISC DHCP server dhcpd [fail] * Starting ISC DHCP server dhcpd [ OK ] 檢查 DHCP 是否正確被啟動： $ sudo netstat -lu | grep boot udp 0 0 *:bootps *:* TFTP Server 設定編輯/etc/inetd.conf檔案，在最下面加入以下內容： tftp dgram udp wait root /usr/sbin/in.tftpd /usr/sbin/in.tftpd -s /var/lib/tftpboot 接著設定 Boot 時啟動服務，以及重新啟動相關服務： $ sudo update-inetd --enable BOOT $ sudo service openbsd-inetd restart * Restarting internet superserver inetd [ OK ] $ sudo service tftpd-hpa restart 檢查 TFTP Server 是否正確啟動： $ netstat -lu | grep tftp udp 0 0 *:tftp *:* 建立開機選單完成後安裝 syslinux: sudo apt-get -y install syslinux 複製 syslinux 設定檔至/var/lib/tftpboot目錄中： sudo cp /usr/lib/syslinux/menu.c32 /var/lib/tftpboot sudo cp /usr/lib/syslinux/vesamenu.c32 /var/lib/tftpboot sudo cp /usr/lib/syslinux/pxelinux.0 /var/lib/tftpboot sudo cp /usr/lib/syslinux/memdisk /var/lib/tftpboot sudo cp /usr/lib/syslinux/mboot.c32 /var/lib/tftpboot sudo cp /usr/lib/syslinux/chain.c32 /var/lib/tftpboot 建立/var/lib/tftpboot/pxelinux.cfg目錄： $ sudo mkdir /var/lib/tftpboot/pxelinux.cfg 接著編輯/var/lib/tftpboot/pxelinux.cfg/default檔案，設定開機選單，以下為簡單設定範例： UI vesamenu.c32 TIMEOUT 100 MENU TITLE Welcom to KaiRen.Lab PXE Server System LABEL local MENU LABEL Boot from local drive MENU DEFAULT localboot 0 LABEL Custom CentOS 6.5 MENU LABEL Install Custom CentOS 6.5 kernel ./centos/vmlinuz append initrd=./centos/initrd.img ksdevice=bootif ip=dhcp ks=http://10.21.10.240/centos-ks/default_ks.cfg LABEL Hadoop CentOS 6.5 MENU LABEL Install Hadoop CentOS 6.5 kernel ./centos/vmlinuz append initrd=./centos/initrd.img ksdevice=bootif ip=dhcp ks=http://10.21.10.240/centos-ks/hdp_ks.cfg LABEL Ubuntu Server 14.04 MENU LABEL Install Ubuntu Server 14.04 kernel ./ubuntu/server/14.04/linux append initrd=./ubuntu/server/14.04/initrd.gz method=http://10.21.10.240/ubuntu/server/14.04/","tags":[{"name":"Linux","slug":"Linux","permalink":"https://kairen.github.io/tags/Linux/"},{"name":"PXE","slug":"PXE","permalink":"https://kairen.github.io/tags/PXE/"},{"name":"Bare Metal","slug":"Bare-Metal","permalink":"https://kairen.github.io/tags/Bare-Metal/"}]},{"title":"CentOS 6.5 PXE 安裝與設定","date":"2015-10-03T04:23:01.000Z","path":"2015/10/03/linux/centos/centos-pxe/","text":"預啟動執行環境（Preboot eXecution Environment，PXE，也被稱為預執行環境)提供了一種使用網路介面（Network Interface）啟動電腦的機制。這種機制讓電腦的啟動可以不依賴本地資料儲存裝置（如硬碟）或本地已安裝的作業系統。 安裝環境 CentOS 6.5 Minimal Install Intel(R) Core(TM)2 Quad CPU Q8400 @ 2.66GHz 500 GB 4G RAM Two Eth Card Inner eth = PEX DHCP Outer eth = Public network PXE 安裝與設定首先安装 Setuptool 於 CentOS 上 $ sudo yum install -y setuptool ntsysv iptables system-config-network-tui 關閉防火牆與 SElinux，避免驗證時被阻擋： $ sudo service iptables stop $ sudo setenforce 0 接著編輯/etc/selinux/config，修改以下內容: SELINUX=disabled 然後編輯/etc/sysconfig/network-scripts/ifcfg-ethx設定與確認 IP Address 是否正確： DEVICE=ethx HWADDR=C4:6E:1F:04:60:24 #依照個人eth TYPE=Ethernet UUID=ada7e5dc-a2e9-4a89-9c93-e1f559cd05f2 ONBOOT=yes NM_CONTROLLED=yes BOOTPROTO=none IPADDR=192.168.28.130 #依照網路 NETMASK=255.255.255.0 USERCTL=no DHCP Server 安裝與設定DHCP是「 動態主機配置協定」(Dynamic Host Configuration Protocol)。DHCP是可自動將IP位址指派給登入TCP/IP網路的用戶端的一種軟體(這種IP位址稱為「動態IP位址」)。這邊安裝方式為以下： $ sudo yum -y install dhcp 完成後編輯/etc/dhcp/dhcpd.conf，並修改以下設定: ddns-update-style none; ignore client-updates; allow booting; allow bootp; option ip-forwarding false; option mask-supplier false; option broadcast-address 192.168.28.255; subnet 192.168.28.0 netmask 255.255.255.0 { option routers 192.168.28.130 range 192.168.28.50 192.168.28.60; #option subnet-mask 255.255.255.0; #option domain-name &quot;i4502.dic.ksu&quot;; option domain-name-servers 10.21.20.1; next-server 192.168.28.130; filename &quot;pxelinux.0&quot;; } 設定完後，重新啟動 DHCP 服務： $ sudo service dhcpd start $ sudo chkconfig dhcpd on TFTP Server 安裝與設定簡單文件傳輸協議或稱小型文件傳輸協議（英文：Trivial File Transfer Protocol，縮寫TFTP），是一種簡化的文件傳輸協議。小型文件傳輸協議非常簡單，通過少量存儲器就能輕鬆實現——這在當時是很重要的考慮因素。所以TFTP被用於引導計算機，例如沒有大容量存儲器的路由器。安裝方式為以下： $ sudo yum -y install tftp-server tftp 安裝完成後編輯/etc/xinetd.d/tftp，修改以下內容： service tftp { socket_type = dgram protocol = udp wait = yes user = root server = /usr/sbin/in.tftpd server_args = -s /install/tftpboot disable = yes per_source = 11 cps = 100 2 flags = IPv4 } P.S 如果不修改 server_args，預設為 /var/lib/tftpboot/。 接著建立/install/tftpboot來存放 Boot 映像檔： sudo mkdir -p /install/tftpboot sudochcon --reference /var /install sudo service xinetd restart sudo chkconfig xinetd on sudo chkconfig tftp on 安裝 syslinu如果要使用 PXE 的開機管理程式與開機選單的話，那就得要安裝 CentOS 內建提供的 syslinux 軟體，從裡面撈出兩個檔案即可。當然啦，這兩個檔案得要放置在 TFTP 的根目錄下才好！整個實作的過程如下。 yum -y install syslinux cp /usr/share/syslinux/menu.c32 /install/tftpboot/ cp /usr/share/syslinux/vesamenu.c32 /install/tftpboot/ cp /usr/share/syslinux/pxelinux.0 /install/tftpboot/ mkdir /install/tftpboot/pxelinux.cfg ll /install/tftpboot/ 掛載CentOS 映像檔已CentOS 6.5 Minimal為範例。 mount -o loop CentOS-6.5-x86_64-minimal.iso /mnt mkdir -p /install/tftpboot/kernel/centos6.5 cp /mnt/isolinux/vmlinuz /install/tftpboot/kernel/centos6.5 cp /mnt/isolinux/initrd.img /install/tftpboot/kernel/centos6.5 cp /mnt/isolinux/isolinux.cfg /install/tftpboot/pxelinux.cfg/demo umount /mnt vmlinuz：就是安裝軟體的核心檔案 (kernel file) initrd.img：就是開機過程中所需要的核心模組參數 isolinux.cfg –&gt; demo：作為未來 PXE 所需要的開機選單之參考 設定開機選單vim /install/tftpboot/pxelinux.cfg/default 修改： UI vesamenu.c32 TIMEOUT 300 DISPLAY ./boot.msg MENU TITLE Welcome to KAIREN&#39;s PXE Server System LABEL local MENU LABEL Boot from local drive MENU DEFAULT localboot 0 LABEL ubuntu MENU LABEL Install CentOS 6.5 kernel ./kernel/centos6.5/vmlinuz append initrd=./kernel/centos6.5/initrd.img 修改額外開機選單訊息vim /install/tftpboot/boot.msg 訊息： Welcome to KAI-REN&#39;s PXE Server System. The 1st menu can let you system goto hard disk menu. The 2nd menu can goto interactive installation step. 提供NFS Server 提供映像檔NFS 就是 Network FileSystem 的縮寫，最早之前是由 Sun 這家公司所發展出來的。 它最大的功能就是可以透過網路，讓不同的機器、不同的作業系統、可以彼此分享個別的檔案 (share files)。這個 NFS 伺服器可以讓你的 PC 來將網路遠端的 NFS 伺服器分享的目錄，掛載到本地端的機器當中， 在本地端的機器看起來，那個遠端主機的目錄就好像是自己的一個磁碟分割槽一樣 (partition)。 mkdir -p /install/nfs_share/centos6.5 vim /etc/fstab 在最底下加入： /root/CentOS-6.5-x86_64-minimal.iso /install/nfs_share/centos6.5 iso9660 defaults,loop 0 0 安裝並提供分享目錄 mount -a df yum -y install nfs-utils vim /etc/exports 加入： /install/nfs_share/ 192.168.28.0/24(ro,async,nohide,crossmnt) localhost(ro,async,nohide,crossmnt) 修改System nfs conf vim /etc/sysconfig/nfs 如下(P.S 找到上面這幾個設定值，我們得要設定好固定的 port 來開放防火牆給用戶處理)： RQUOTAD_PORT=901 LOCKD_TCPPORT=902 LOCKD_UDPPORT=902 MOUNTD_PORT=903 STATD_PORT=904 修改NFS 不需要對映帳號 vim /etc/idmapd.conf 如下： [General] Domain = &quot;kairen.pxe.com&quot; [Mapping] Nobody-User = nfsnobody Nobody-Group = nfsnobody 重開服務 service rpcbind restart service nfs restart service rpcidmapd restart service nfslock restart chkconfig rpcbind on chkconfig nfs on chkconfig rpcidmapd on chkconfig nfslock on rpcinfo -p showmount -e localhost 如果看到Export list for localhost:/install/nfs_share 192.168.28.0/24,localhost就是成功了。 提供 HTTP ServerApache HTTP Server（簡稱Apache）是Apache軟體基金會的一個開放原始碼的網頁伺服器軟體，可以在大多數電腦作業系統中運行，由於其跨平台和安全性。 yum -y install httpd service httpd start chkconfig httpd on 建立CentOS 6.5目錄 mkdir -p /var/www/html/install/centos6.5 vim /etc/fstab 加入到最下方： /root/CentOS-6.5-x86_64-minimal.iso /var/www/html/install/centos6.5 iso9660 defaults,loop 0 0 掛載起來 mount -a df 提供 FTP Serveryum -y install vsftpd service vsftpd start chkconfig vsftpd on mkdir -p /var/ftp/install/centos6.5 vim /etc/fstab 一樣加入Mount : /root/CentOS-6.5-x86_64-minimal.iso /var/ftp/install/centos6.5 iso9660 defaults,loop,context=system_u:object_r:public_content_t:s0 0 0 掛載起來 mount -a df HTTP FTP","tags":[{"name":"Linux","slug":"Linux","permalink":"https://kairen.github.io/tags/Linux/"},{"name":"PXE","slug":"PXE","permalink":"https://kairen.github.io/tags/PXE/"},{"name":"Bare Metal","slug":"Bare-Metal","permalink":"https://kairen.github.io/tags/Bare-Metal/"}]},{"title":"Spark on Hadoop YARN 單機安裝","date":"2015-09-19T09:08:54.000Z","path":"2015/09/19/data-engineer/spark-yarn/","text":"本教學為安裝 Spark on Hadoop YARN 的 all-in-one 版本，將 Spark 應用程式執行於 YARN 上，來讓應用程式執行於不同的工作節點上。 事前準備首先我們要先安裝 ssh-server 與 Java JDK，並配置需要的相關環境： $ sudo apt-get install openssh-server 設定(hadoop)不用需打 sudo 密碼： $ echo &quot;hadoop ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/hadoop &amp;&amp; sudo chmod 440 /etc/sudoers.d/hadoop P.S 要注意 hadoop 要隨著現在使用的 User 變動。 建立ssh key，並複製 key 使之不用密碼登入： $ ssh-keygen -t rsa $ ssh-copy-id localhost 安裝Java 1.7 JDK： $ sudo apt-get purge openjdk* $ sudo apt-get -y autoremove $ sudo apt-get install -y software-properties-common $ sudo add-apt-repository -y ppa:webupd8team/java $ sudo apt-get update $ echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections $ echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections $ sudo apt-get -y install oracle-java7-installer 安裝 Hadoop YARN首先我們須先將 Hadoop YARN 安裝完成，詳細步驟如下所示。下載Hadoop 2.6.0 or laster version： $ curl -s &quot;https://archive.apache.org/dist/hadoop/core/hadoop-2.6.0/hadoop-2.6.0.tar.gz&quot; | sudo tar -xz -C /opt/ $ sudo mv /opt/hadoop-2.6.0 /opt/hadoop 若要下載不同版本可以到官方查看。 到 hadoop 底下的 /etc/hadoop 設定所有conf檔與evn.sh檔： $ cd /opt/hadoop/etc/hadoop $ sudo vim hadoop-env.sh # 修改裡面的Java_Home export JAVA_HOME=/usr/lib/jvm/java-7-oracle 修改mapred-site.xml.template檔案： $ sudo mv mapred-site.xml.template mapred-site.xml $ sudo vim mapred-site.xml # 修改以下放置到&lt;configuration&gt;&lt;/configuration&gt;裡面 &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 修改hdfs-site.xml檔案： $ sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode $ sudo mkdir -p /usr/local/hadoop_store/hdfs/datanode $ sudo chown -R $USER:$USER /usr/local/hadoop_store $ sudo vim hdfs-site.xml # 修改以下放置到&lt;configuration&gt;&lt;/configuration&gt;裡面 &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop_store/hdfs/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop_store/hdfs/datanode&lt;/value&gt; &lt;/property&gt; 修改core-site.xml檔案： $ sudo mkdir -p /app/hadoop/tmp $ sudo chown $USER_NAME:$USER_NAME /app/hadoop/tmp $ sudo vim core-site.xml # 修改以下放置到&lt;configuration&gt;&lt;/configuration&gt;裡面 &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/app/hadoop/tmp&lt;/value&gt; &lt;description&gt;A base for other temporary directories.&lt;/description&gt; &lt;/property&gt; 修改yarn-site.xml檔案： $ sudo vim yarn-site.xml # 修改以下放置到&lt;configuration&gt;&lt;/configuration&gt;裡面 &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 進行 Namenode 格式化： $ cd /opt/hadoop/bin $ ./hadoop namenode -format 沒出錯的話，就可以開啟Hadoop對應服務： $ cd /opt/hadoop/sbin $ ./start-yarn.sh $ ./start-dfs.sh 檢查是否開啟以下服務： $ jps 3457 ResourceManager 7087 Jps 3593 NodeManager 4190 DataNode 4025 NameNode 4383 SecondaryNameNode 開啟 Website YARN Dashboard 與 HDFS Dashboard 設定環境變數： $ cd $ sudo vim .bashrc # 加入以下到最後一行 export HADOOP_HOME=&quot;/opt/hadoop&quot; export PATH=PATH:$HADOOP_HOME export HADOOP_BIN=&quot;/opt/hadoop/bin&quot; export PATH=$PATH:$HADOOP_BIN 透過source指令引用環境變數： $ source .bashrc 驗證系統為了驗證系統是否建置成功，可執行一個範例程式來看看是否能夠正常執行，如下所示：首先上傳資料到HDFS上： $ sudo vim words.txt # 加入以下，可以自行在多加 AA CD BB DE AA AA # 加入以上 $ hadoop fs -mkdir /example $ hadoop fs -put words.txt /example 執行範例程式： $ cd /opt/hadoop/share/hadoop/mapreduce $ hadoop jar hadoop-mapreduce-examples-2.6.0.jar wordcount /example/words.txt /example/output Spark 安裝不管單機或叢集，安裝 Spark 只需要在 Master 節點上進行即可，步驟如下： 首先下載 Spark，並修改權限： $ curl -s https://d3kbcqa49mib13.cloudfront.net/spark-1.5.2-bin-hadoop2.6.tgz | sudo tar -xz -C /opt/ $ sudo mv /opt/spark-1.5.2-bin-hadoop2.6 /opt/spark $ sudo chown $USER:$USER -R /opt/spark 其他 Hadoop 版本可以到這邊Spark-Hadoop查看。 設定 Spark 環境參數： $ echo &quot;export HADOOP_CONF_DIR=\\$HADOOP_HOME/etc/hadoop&quot; | sudo tee -a /opt/spark/conf/spark-env.sh $ echo &quot;export YARN_CONF_DIR=\\$HADOOP_HOME/etc/hadoop&quot; | sudo tee -a /opt/spark/conf/spark-env.sh $ echo &quot;export SPARK_HOME=/opt/spark&quot; | sudo tee -a /opt/spark/conf/spark-env.sh $ echo &quot;export SPARK_JAR=/opt/spark/lib/spark-assembly-1.5.2-hadoop2.6.0.jar&quot; | sudo tee -a /opt/spark/conf/spark-env.sh $ echo &quot;export PATH=\\$SPARK_HOME/bin:\\$PATH&quot; | sudo tee -a /opt/spark/conf/spark-env.sh 設定使用者環境參數： $ echo &quot;export SPARK_HOME=/opt/spark&quot; | sudo tee -a ~/.bashrc $ echo &quot;export PATH=\\$SPARK_HOME/bin:\\$PATH&quot; | sudo tee -a ~/.bashrc 驗證系統為了驗證 Spark 是否成功安裝，可以透過執行一個範例程式來看看結果，如下所示： $ cd /opt/spark $ spark-submit --class org.apache.spark.examples.SparkPi \\ --master yarn-cluster \\ --num-executors 1 \\ --executor-memory 1g \\ --executor-cores 1 \\ lib/spark-examples*.jar \\ 1","tags":[{"name":"Spark","slug":"Spark","permalink":"https://kairen.github.io/tags/Spark/"},{"name":"Mesos","slug":"Mesos","permalink":"https://kairen.github.io/tags/Mesos/"}]},{"title":"Spark Standalone 模擬分散式運算","date":"2015-09-18T09:08:54.000Z","path":"2015/09/18/data-engineer/spark-standalone/","text":"本教學為安裝 Spark Standalone 的叢集版本，將 Spark 應用程式執行於自己的分散式機制與各台機器連結上，來讓應用程式執行於不同的工作節點上。 事前準備首先我們要在各節點先安裝 ssh-server 與 Java JDK，並配置需要的相關環境： $ sudo apt-get install openssh-server 設定(hadoop)不用需打 sudo 密碼： $ echo &quot;hadoop ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/hadoop &amp;&amp; sudo chmod 440 /etc/sudoers.d/hadoop P.S 要注意 hadoop 要隨著現在使用的 User 變動。 建立ssh key，並複製 key 使之不用密碼登入： $ ssh-keygen -t rsa $ ssh-copy-id localhost 安裝Java 1.7 JDK： $ sudo apt-get purge openjdk* $ sudo apt-get -y autoremove $ sudo apt-get install -y software-properties-common $ sudo add-apt-repository -y ppa:webupd8team/java $ sudo apt-get update $ echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections $ echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections $ sudo apt-get -y install oracle-java7-installer 新增各節點 Hostname 至 /etc/hosts 檔案： 127.0.0.1 localhost 192.168.1.10 hadoop-master 192.168.1.11 hadoop-slave1 192.168.1.11 hadoop-slave2 並在Master節點複製所有Slave的 ssh key： $ ssh-copy-id ubuntu@hadoop-slave1 $ ssh-copy-id ubuntu@hadoop-slave2 安裝 Spark首先下載 Spark，並修改權限： $ curl -s https://d3kbcqa49mib13.cloudfront.net/spark-1.5.2-bin-hadoop2.6.tgz | sudo tar -xz -C /opt/ $ sudo mv /opt/spark-1.5.2-bin-hadoop2.6 /opt/spark $ sudo chown $USER:$USER -R /opt/spark 之後到spark/conf目錄，將spark-env.sh.template複製為spark-env.sh： $ cp spark-env.sh.template spark-env.sh 在spark-env.sh這內容最下方增加這幾筆環境參數： export SPARK_MASTER_IP=&quot;hadoop-master&quot; export SPARK_MASTER_PORT=&quot;7077&quot; export SPARK_MASTER_WEBUI_PORT=&quot;8090&quot; SPARK_MASTER_IP為主節點（Master）的 IP。SPARK_MASTER_PORT為主節點（Master）的 Port。SPARK_MASTER_WEBUI_PORT為 WebUI 的 Port，預設為 8080。 接著複製slaves.template為slaves： $ cp slaves.template slaves 在最下方增加每台機器的 hostname： hadoop-slave1 hadoop-slave2 完成後將設定檔複製給其他台機器： scp -r /opt/spark ubuntu@hadoop-slave1:/opt scp -r /opt/spark ubuntu@hadoop-slave2:/opt 啟動 Spark ： /opt/spark/sbin/start-all.sh 這樣 Spark 就啟動完成了，開啟 Web UI 來檢查狀態。 設定使用者環境參數： $ echo &quot;export SPARK_HOME=/opt/spark&quot; | sudo tee -a ~/.bashrc $ echo &quot;export PATH=\\$SPARK_HOME/bin:\\$PATH&quot; | sudo tee -a ~/.bashrc 驗證系統為了驗證 Spark 是否成功安裝，可以透過執行一個範例程式來看看結果，如下所示： $ cd /opt/spark $ spark-submit --class org.apache.spark.examples.SparkPi \\ --master spark://hadoop-master:7077 \\ --num-executors 1 \\ --executor-memory 1g \\ --executor-cores 1 \\ lib/spark-examples*.jar \\ 1","tags":[{"name":"Spark","slug":"Spark","permalink":"https://kairen.github.io/tags/Spark/"}]}]