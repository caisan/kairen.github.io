<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[Prometheus Operator 介紹與安裝]]></title>
      <url>https://kairen.github.io/2018/06/23/devops/prometheus-operator/</url>
      <content type="html"><![CDATA[<p><a href="https://github.com/coreos/prometheus-operator" target="_blank" rel="noopener">Prometheus Operator</a> 是 CoreOS 開源的一套用於管理在 Kubernetes 上的 Prometheus 控制器，目標當然就是簡化部署與維護 Prometheus 上的事情，其架構如下所示：</p>
<p><img src="https://coreos.com/sites/default/files/inline-images/p1.png" alt=""></p>
<a id="more"></a>
<p>架構中的每一個部分都執行於 Kubernetes 的資源，這些資源分別負責不同作用與意義：</p>
<ul>
<li><strong><a href="https://coreos.com/operators/" target="_blank" rel="noopener">Operator</a></strong>：Operator 是整個系統的主要控制器，會以 Deployment 方式執行於 Kubernetes 叢集上，並根據自定義的資源(Custom Resource Definition，CRDs)來負責管理與部署 Prometheus Server。而 Operator 會透過監聽這些自定義資源的事件變化來做對應處理。</li>
<li><strong>Prometheus Server</strong>：由 Operator 依據一個自定義資源 Prometheus 類型中，所描述的內容而部署的 Prometheus Server 叢集，可以將這個自定義資源看作是一種特別用來管理 Prometheus Server 的 StatefulSets 資源。</li>
</ul>
<pre><code class="yaml=">apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: k8s
  labels:
    prometheus: k8s
spec:
  version: v2.3.0
  replicas: 2
  serviceMonitors:
  - selector:
      matchLabels:
        k8s-app: kubelet
...
</code></pre>
<ul>
<li><strong>ServiceMonitor</strong>：一個 Kubernetes 自定義資源，該資源描述了 Prometheus Server 的 Target 列表，Operator 會監聽這個資源的變化來動態的更新 Prometheus Server 的 Scrape targets。而該資源主要透過 Selector 來依據 Labels 選取對應的 Service Endpoint，並讓 Prometheus Server 透過 Service 進行拉取(Pull) Metrics 資料。</li>
</ul>
<pre><code class="yaml=">apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kubelet
  labels:
    k8s-app: kubelet
spec:
  jobLabel: k8s-app
  endpoints:
  - port: cadvisor
    interval: 30s # scrape the endpoint every 10 seconds
    honorLabels: true
  selector:
    matchLabels:
      k8s-app: kubelet
  namespaceSelector:
    matchNames:
    - kube-system
</code></pre>
<blockquote>
<p>這是一個抓取 Cadvisor metrics 的範例。</p>
</blockquote>
<ul>
<li><p><strong>Service</strong>：Kubernetes 中的 Service 資源，這邊主要用來對應 Kubernetes 中 Metrics Server Pod，然後提供給 ServiceMonitor 選取讓 Prometheus Server 拉取資料。在 Prometheus 術語中，可以稱為 Target，即被 Prometheus 監測的對象，如一個部署在 Kubernetes 上的 Node Exporter Service。</p>
</li>
<li><p><strong>Alertmanager</strong>：Prometheus Operator 不只提供 Prometheus Server 管理與部署，也包含了 AlertManager，並且一樣透過一個 Alertmanager 自定義資源來描述資訊，再由 Operator 依據描述內容部署 Alertmanager 叢集。</p>
</li>
</ul>
<pre><code class="yaml=">apiVersion: monitoring.coreos.com/v1
kind: Alertmanager
metadata:
  name: main
  labels:
    alertmanager: main
spec:
  replicas: 3
...
</code></pre>
<h2 id="部署-Prometheus-Operator"><a href="#部署-Prometheus-Operator" class="headerlink" title="部署 Prometheus Operator"></a>部署 Prometheus Operator</h2><p>本節將說明如何部署 Prometheus Operator 來管理 Kubernetes 上的 Prometheus 資源。</p>
<h3 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h3><p>測試環境將需要一套 Kubernetes 叢集，作業系統採用<code>Ubuntu 16.04 Server</code>，測試環境為實體機器：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Role</th>
<th>vCPU</th>
<th>RAM</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.22.132.10</td>
<td>k8s-m1</td>
<td>8</td>
<td>16G</td>
</tr>
<tr>
<td>172.22.132.11</td>
<td>k8s-n1</td>
<td>8</td>
<td>16G</td>
</tr>
<tr>
<td>172.22.132.12</td>
<td>k8s-n2</td>
<td>8</td>
<td>16G</td>
</tr>
</tbody>
</table>
<blockquote>
<p>這邊<code>m</code> 為 K8s master，<code>n</code>為 K8s node。</p>
</blockquote>
<h3 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h3><p>開始安裝前需要確保以下條件已達成：</p>
<ul>
<li><p>所有節點以 kubeadm 部署成 Kubernetes v1.9+ 叢集。請參考 <a href="https://kairen.github.io/2016/09/29/kubernetes/deploy/kubeadm/">用 kubeadm 部署 Kubernetes 叢集</a>。</p>
</li>
<li><p>在 Kubernetes 叢集部署 Helm 與 Tiller server。</p>
</li>
</ul>
<pre><code class="shell=">$ wget -qO- https://kubernetes-helm.storage.googleapis.com/helm-v2.8.1-linux-amd64.tar.gz | tar -zx
$ sudo mv linux-amd64/helm /usr/local/bin/
$ kubectl -n kube-system create sa tiller
$ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
$ helm init --service-account tiller
</code></pre>
<ul>
<li>在<code>k8s-m1</code>透過 kubectl 來建立 Ingress Controller 即可：</li>
</ul>
<pre><code class="shell=">$ kubectl create ns ingress-nginx
$ wget https://kairen.github.io/files/manual-v1.10/addon/ingress-controller.yml.conf -O ingress-controller.yml
$ sed -i ingress-controller.yml &#39;s/192.16.35.10/172.22.132.10/g&#39;
$ kubectl apply -f ingress-controller.yml.conf
</code></pre>
<h3 id="部署-Prometheus-Operator-1"><a href="#部署-Prometheus-Operator-1" class="headerlink" title="部署 Prometheus Operator"></a>部署 Prometheus Operator</h3><p>Prometheus Operator 提供了多種方式部署至 Kubernetes 上，一般會使用手動(or 腳本)與 Helm 來進行部署。</p>
<h4 id="手動-腳本-部署"><a href="#手動-腳本-部署" class="headerlink" title="手動(腳本)部署"></a>手動(腳本)部署</h4><p>透過 Git 取得最新版本腳本：</p>
<pre><code class="shell=">$ git clone https://github.com/camilb/prometheus-kubernetes.git
$ cd prometheus-kubernetes
</code></pre>
<p>接著執行<code>deploy</code>腳本來部署到 Kubernetes：</p>
<pre><code class="shell=">$ ./deploy
Check for uncommitted changes

OK! No uncommitted changes detected

Creating &#39;monitoring&#39; namespace.
Error from server (AlreadyExists): namespaces &quot;monitoring&quot; already exists

1) AWS
2) GCP
3) Azure
4) Custom
Please select your cloud provider:4
Deploying on custom providers without persistence
Setting components version
Enter Prometheus Operator version [v0.19.0]:

Enter Prometheus version [v2.2.1]:

Enter Prometheus storage retention period in hours [168h]:

Enter Prometheus storage volume size [40Gi]:

Enter Prometheus memory request in Gi or Mi [1Gi]:

Enter Grafana version [5.1.1]:

Enter Alert Manager version [v0.15.0-rc.1]:

Enter Node Exporter version [v0.16.0-rc.3]:

Enter Kube State Metrics version [v1.3.1]:

Enter Prometheus external Url [http://127.0.0.1:9090]:

Enter Alertmanager external Url [http://127.0.0.1:9093]:

Do you want to use NodeSelector  to assign monitoring components on dedicated nodes?
Y/N [N]:

Do you want to set up an SMTP relay?
Y/N [N]:

Do you want to set up slack alerts?
Y/N [N]:

# 這邊會跑一下部署階段，完成後要接著輸入一些資訊，如 Grafana username and passwd

Enter Grafana administrator username [admin]:
Enter Grafana administrator password: ******

...
Done
</code></pre>
<blockquote>
<p>沒有輸入部分請直接按<code>Enter</code>。</p>
</blockquote>
<p>當確認看到 Done 後就可以查看 <code>monitoring</code> namespace：</p>
<pre><code class="shell=">$ kubectl -n monitoring get po
NAME                                  READY     STATUS    RESTARTS   AGE
alertmanager-main-0                   2/2       Running   0          4m
alertmanager-main-1                   2/2       Running   0          3m
alertmanager-main-2                   2/2       Running   0          3m
grafana-568b569696-nltbh              2/2       Running   0          14s
kube-state-metrics-86467959c6-kxtl4   2/2       Running   0          3m
node-exporter-526nw                   1/1       Running   0          4m
node-exporter-c828w                   1/1       Running   0          4m
node-exporter-r2qq2                   1/1       Running   0          4m
node-exporter-s25x6                   1/1       Running   0          4m
node-exporter-xpgh7                   1/1       Running   0          4m
prometheus-k8s-0                      1/2       Running   0          10s
prometheus-k8s-1                      2/2       Running   0          10s
prometheus-operator-f596c68cf-wrpqc   1/1       Running   0          4m
</code></pre>
<p>查看 Kubernetes CRDs 與 SM：</p>
<pre><code class="shell=">$ kubectl -n monitoring get crd
NAME                                          AGE
alertmanagers.monitoring.coreos.com           4m
prometheuses.monitoring.coreos.com            4m
servicemonitors.monitoring.coreos.com         4m

$ kubectl -n monitoring get servicemonitors
NAME                      AGE
alertmanager              1m
kube-apiserver            1m
kube-controller-manager   1m
kube-dns                  1m
kube-scheduler            1m
kube-state-metrics        1m
kubelet                   1m
node-exporter             1m
prometheus                1m
prometheus-operator       1m
</code></pre>
<p>接著修改 Service 的 Grafana 的 Type：</p>
<pre><code class="shell=">$ kubectl -n monitoring edit svc grafana
# 修改成 NodePort
</code></pre>
<blockquote>
<p>也可以建立 Ingress 來存取 Grafana。</p>
<pre><code class="yaml=">apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  namespace: monitoring
  name: grfana-ingress
  annotations:
    ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: grafana.k8s-local.k2r2bai.com
    http:
      paths:
      - path: /
        backend:
          serviceName: grafana
          servicePort: 3000
</code></pre>
</blockquote>
<p>:::info<br>這邊也可以建立 Prometheus Ingress 來使用 Web-based console。</p>
<pre><code class="yaml=">apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  namespace: monitoring
  name: prometheus-ingress
  annotations:
    ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: prometheus.k8s-local.k2r2bai.com
    http:
      paths:
      - path: /
        backend:
          serviceName: prometheus-k8s
          servicePort: 9090
</code></pre>
<p>:::</p>
<p>最後就可以存取 Grafana 來查看 Metric 視覺化資訊了。</p>
<p><img src="https://i.imgur.com/39G6Zsm.png" alt=""></p>
<h4 id="Helm"><a href="#Helm" class="headerlink" title="Helm"></a>Helm</h4><p>首先透過 Helm 加入 coreos 的 repo：</p>
<pre><code class="shell=">$ helm repo add coreos https://s3-eu-west-1.amazonaws.com/coreos-charts/stable/
</code></pre>
<p>然後透過 kubectl 建立一個 Namespace 來管理 Prometheus，並用 Helm 部署 Prometheus Operator：</p>
<pre><code class="shell=">$ kubectl create namespace monitoring
$ helm install coreos/prometheus-operator \
    --name prometheus-operator \
    --set rbacEnable=true \
    --namespace=monitoring
</code></pre>
<p>接著部署 Prometheus、AlertManager 與 Grafana：</p>
<pre><code class="shell="># Prometheus
$ helm install coreos/prometheus --name prometheus \
    --set serviceMonitorsSelector.app=prometheus \
    --set ruleSelector.app=prometheus \
    --namespace=monitoring

# Alert Manager
$ helm install coreos/alertmanager --name alertmanager --namespace=monitoring

# Grafana
$ helm install coreos/grafana --name grafana --namespace=monitoring
</code></pre>
<p>部署 kube-prometheus 來提供 Kubernetes 監測的 Exporter 與 ServiceMonitor：</p>
<pre><code class="shell=">$ helm install coreos/kube-prometheus --name kube-prometheus --namespace=monitoring
</code></pre>
<p>完成後檢查安裝結果：</p>
<pre><code class="shell=">$ kubectl -n monitoring get po,svc
NAME                                                       READY     STATUS    RESTARTS   AGE
pod/alertmanager-alertmanager-0                            2/2       Running   0          1m
pod/alertmanager-kube-prometheus-0                         2/2       Running   0          31s
pod/grafana-grafana-77cfcdff66-jwxfp                       2/2       Running   0          1m
pod/kube-prometheus-exporter-kube-state-56857b596f-knt8q   1/2       Running   0          21s
pod/kube-prometheus-exporter-kube-state-844bb6f589-n7xfg   1/2       Running   0          31s
pod/kube-prometheus-exporter-node-665kc                    1/1       Running   0          31s
pod/kube-prometheus-exporter-node-bjvbx                    1/1       Running   0          31s
pod/kube-prometheus-exporter-node-j8jf8                    1/1       Running   0          31s
pod/kube-prometheus-exporter-node-pxn8p                    1/1       Running   0          31s
pod/kube-prometheus-exporter-node-vft8b                    1/1       Running   0          31s
pod/kube-prometheus-grafana-57d5b4d79f-lq5cr               1/2       Running   0          31s
pod/prometheus-kube-prometheus-0                           3/3       Running   1          29s
pod/prometheus-operator-d75587d6-qhz4h                     1/1       Running   0          2m
pod/prometheus-prometheus-0                                3/3       Running   1          1m

NAME                                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
service/alertmanager                          ClusterIP   10.99.170.79     &lt;none&gt;        9093/TCP            1m
service/alertmanager-operated                 ClusterIP   None             &lt;none&gt;        9093/TCP,6783/TCP   1m
service/grafana-grafana                       ClusterIP   10.100.217.27    &lt;none&gt;        80/TCP              1m
service/kube-prometheus                       ClusterIP   10.102.165.173   &lt;none&gt;        9090/TCP            31s
service/kube-prometheus-alertmanager          ClusterIP   10.99.221.122    &lt;none&gt;        9093/TCP            32s
service/kube-prometheus-exporter-kube-state   ClusterIP   10.100.233.129   &lt;none&gt;        80/TCP              32s
service/kube-prometheus-exporter-node         ClusterIP   10.97.183.222    &lt;none&gt;        9100/TCP            32s
service/kube-prometheus-grafana               ClusterIP   10.110.134.52    &lt;none&gt;        80/TCP              32s
service/prometheus                            ClusterIP   10.105.229.141   &lt;none&gt;        9090/TCP            1m
service/prometheus-operated                   ClusterIP   None             &lt;none&gt;        9090/TCP            1m
</code></pre>
]]></content>
      
        <categories>
            
            <category> DevOps </category>
            
        </categories>
        
        
        <tags>
            
            <tag> DevOps </tag>
            
            <tag> Monitoring </tag>
            
            <tag> CNCF </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Prometheus 介紹與基礎入門]]></title>
      <url>https://kairen.github.io/2018/06/10/devops/prometheus-intro/</url>
      <content type="html"><![CDATA[<p>Prometheus 是一套開放式原始碼的<code>系統監控警報框架</code>與<code>TSDB(Time Series Database)</code>，該專案是由 SoundCloud 的工程師(前 Google 工程師)建立，Prometheus 啟發於 Google 的 Borgmon 監控系統。目前 Prometheus 已貢獻到 CNCF 成為孵化專案(2016-)，其受歡迎程度僅次於 Kubernetes。</p>
<a id="more"></a>
<p>Prometheus 具備了以下特性：</p>
<ul>
<li>多維度資料模型<ul>
<li>時間序列資料透過 Metric 名稱與鍵值(Key-value)來區分。</li>
<li>所有 Metrics 可以設定任意的多維標籤。</li>
<li>資料模型彈性度高，不需要刻意設定為以特定符號(ex: ,)分割。</li>
<li>可對資料模型進行聚合、切割與切片操作。</li>
<li>支援雙精度浮點數類型，標籤可以設定成 Unicode。</li>
</ul>
</li>
<li>靈活的查詢語言(PromQL)，如可進行加減乘除等。</li>
<li>不依賴分散式儲存，因為 Prometheus Server 是一個二進制檔，可在單個服務節點自主運行。</li>
<li>基於 HTTP 的 Pull 方式收集時序資料。</li>
<li>可以透過 Push Gateway 進行資料推送。</li>
<li>支援多種視覺化儀表板呈現，如 Grafana。</li>
<li>能透過服務發現(Service discovery)或靜態組態去獲取監控的 Targets。</li>
</ul>
<h2 id="Prometheus-架構"><a href="#Prometheus-架構" class="headerlink" title="Prometheus 架構"></a>Prometheus 架構</h2><p><img src="https://i.imgur.com/iJKoxdD.png" alt=""></p>
<p>Prometheus 生態圈中是由多個元件組成，其中有些是選擇性的元件：</p>
<ul>
<li><strong>Prometheus Server</strong>：收集與儲存時間序列資料，並提供 PromQL 查詢語言支援。</li>
<li><strong>Client Library</strong>：客戶端函式庫，提供語言開發來開發產生 Metrics 並曝露 Prometheus Server。當 Prometheus Server 來 Pull 時，直接返回即時狀態的 Metrics。</li>
<li><strong>Pushgateway</strong>：主要用於臨時性 Job 推送。這類 Job 存在期間較短，有可能 Prometheus 來 Pull 時就消失，因此透過一個閘道來推送。適合用於服務層面的 Metrics。</li>
<li><strong>Exporter</strong>：用來曝露已有第三方服務的 Metrics 給 Prometheus Server，即以 Client Library 開發的 HTTP server。</li>
<li><strong>AlertManager</strong>：接收來至 Prometheus Server 的 Alert event，並依據定義的 Notification 組態發送警報，ex: E-mail、Pagerduty、OpenGenie 與 Webhook 等等。</li>
</ul>
<h2 id="Prometheus-運作流程"><a href="#Prometheus-運作流程" class="headerlink" title="Prometheus 運作流程"></a>Prometheus 運作流程</h2><ol>
<li>Prometheus Server 定期從組態好的 Jobs 或者 Exporters 中拉取 Metrics，或者接收來自 Pushgateway 發送的 Metrics，又或者從其他的 Prometheus Server 中拉取 Metrics。</li>
<li>Prometheus Server 在 Local 儲存收集到的 Metrics，並運行已定義好的 alert.rules，然後紀錄新時間序列或者像 AlertManager 發送警報。</li>
<li>AlertManager 根據組態檔案來對接受到的 Alert event 進行處理，然後發送警報。</li>
<li>在視覺化介面呈現採集資料。</li>
</ol>
<p>Prometheus Server 拉取 Exporter 資料，然後透過 PromQL 語法進行查詢，再將資料給 Web UI or Dashboard。<br><img src="https://i.imgur.com/QkwEVge.png" alt=""></p>
<p>Prometheus Server 觸發 Alert Definition 定義的事件，並發送給 AelertManager。<br><img src="https://i.imgur.com/6V3RJOh.png" alt=""></p>
<p>AlertManager 依據設定發送警報給 E-mail、Slack 等等。<br><img src="https://i.imgur.com/mB789G2.png" alt=""></p>
<h2 id="Prometheus-資料模型與-Metric-類型"><a href="#Prometheus-資料模型與-Metric-類型" class="headerlink" title="Prometheus 資料模型與 Metric 類型"></a>Prometheus 資料模型與 Metric 類型</h2><p>本節將介紹 Prometheus 的資料模型與 Metrics 類型。</p>
<h3 id="資料模型"><a href="#資料模型" class="headerlink" title="資料模型"></a>資料模型</h3><p>Prometheus 儲存的資料為時間序列，主要以 Metrics name 以及一系列的唯一標籤(key-value)組成，不同標籤表示不同時間序列。模型資訊如下：</p>
<ul>
<li><strong>Metrics Name</strong>：該名稱通常用來表示 Metric 功能，例如 <code>http_requests_total</code>，即表示 HTTP 請求的總數。而 Metrics Name 是以 ASCII 字元、數字、英文、底線與冒號組成，並且要滿足<code>[a-zA-Z_:][a-zA-Z0-9_:]*</code> 正規表示法。</li>
<li><strong>標籤</strong>：用來識別同一個時間序列不同維度。如 <code>http_request_total{method=&quot;Get&quot;}</code>表示所有 HTTP 的 Get Request 數量，因此當 <code>method=&quot;Post&quot;</code> 時又是另一個新的 Metric。標籤也需要滿足<code>[a-zA-Z_:][a-zA-Z0-9_:]*</code> 正規表示法。</li>
<li><strong>樣本</strong>：實際的時間序列，每個序列包含一個 float64 值與一個毫秒的時間戳。</li>
<li><strong>格式</strong>：一般為<code>&lt;metric name&gt;{&lt;label name&gt;=&lt;label value&gt;,...}</code>，例如：<code>http_requests_total{method=&quot;POST&quot;,endpoint=&quot;/api/tracks&quot;}</code>。</li>
</ul>
<h3 id="Metrics-類型"><a href="#Metrics-類型" class="headerlink" title="Metrics 類型"></a>Metrics 類型</h3><p>Prometheus Client 函式庫支援了四種主要 Metric 類型：</p>
<ul>
<li><strong>Counter</strong>: 可被累加的 Metric，比如一個 HTTP Get 錯誤的出現次數。</li>
<li><strong>Gauge</strong>: 屬於瞬時、與時間無關的任意更動 Metric，如記憶體使用率。</li>
<li><strong>Histogram</strong>: 主要使用在表示一段時間範圍內的資料採樣。</li>
<li><strong>Summary</strong>： 類似 Histogram，用來表示一端時間範圍內的資料採樣總結。</li>
</ul>
<h2 id="Job-與-Instance"><a href="#Job-與-Instance" class="headerlink" title="Job 與 Instance"></a>Job 與 Instance</h2><p>Prometheus 中會將任意獨立資料來源(Target)稱為 Instance。而包含多個相同 Instance 的集合稱為 Job。如以下範例：</p>
<pre><code class="yml">- job: api-server
    - instance 1: 1.2.3.4:5670
    - instance 2: 1.2.3.4:5671
    - instance 3: 5.6.7.8:5670
    - instance 4: 5.6.7.8:5671
</code></pre>
<ul>
<li><strong>Instance</strong>: 被抓取目標 URL 的<code>&lt;host&gt;:&lt;port&gt;</code>部分。</li>
<li><strong>Job</strong>: 一個同類型的 Instances 集合。(主要確保可靠性與擴展性)</li>
</ul>
<h2 id="Prometheus-簡單部署與使用"><a href="#Prometheus-簡單部署與使用" class="headerlink" title="Prometheus 簡單部署與使用"></a>Prometheus 簡單部署與使用</h2><p>Prometheus 官方提供了已建構完成的二進制執行檔可以下載，只需要至 <a href="https://prometheus.io/download/" target="_blank" rel="noopener">Download</a> 頁面下載即可。首先下載符合作業系統的檔案，這邊以 Linux 為例：</p>
<pre><code class="sh">$ wget https://github.com/prometheus/prometheus/releases/download/v2.3.0/prometheus-2.3.0.linux-amd64.tar.gz
$ tar xvfz prometheus-*.tar.gz
$ tree prometheus-2.3.0.linux-amd64
├── console_libraries # Web console templates
│   ├── menu.lib
│   └── prom.lib
├── consoles # Web console templates
│   ├── index.html.example
│   ├── node-cpu.html
│   ├── node-disk.html
│   ├── node.html
│   ├── node-overview.html
│   ├── prometheus.html
│   └── prometheus-overview.html
├── LICENSE
├── NOTICE
├── prometheus     # Prometheus 執行檔
├── prometheus.yml # Prometheus 設定檔
└── promtool       # 2.x+ 版本用來將一些 rules 格式轉成 YAML 用。
</code></pre>
<p>解壓縮完成後，編輯<code>prometheus.yml</code>檔案來調整設定：</p>
<pre><code class="yml">global:
  scrape_interval: 15s # 設定預設 scrape 的拉取間隔時間
  external_labels: # 外通溝通時標示在 time series 或 Alert 的 Labels。
    monitor: &#39;codelab-monitor&#39;

scrape_configs: # 設定 scrape jobs
  - job_name: &#39;prometheus&#39;
    scrape_interval: 5s # 若設定間隔時間，將會覆蓋 global 的預設時間。
    static_configs:
      - targets: [&#39;localhost:9090&#39;]
</code></pre>
<p>完成後，直接執行 prometheus 檔案來啟動伺服器：</p>
<pre><code class="sh">$ ./prometheus --config.file=prometheus.yml --storage.tsdb.path /tmp/data
...
level=info ts=2018-06-19T08:46:37.42756438Z caller=main.go:500 msg=&quot;Server is ready to receive web requests.&quot;
</code></pre>
<blockquote>
<p><code>--storage.tsdb.path</code> 預設會直接存放在<code>./data</code>底下。</p>
</blockquote>
<p>啟動後就可以瀏覽 <code>:9090</code> 來查看 Web-based console。</p>
<p><img src="https://i.imgur.com/qgi39CC.png" alt=""></p>
<p>另外也可以進入 <code>:9090/metrics</code> 查看 Export metrics 資訊，並且可以在 console 來查詢指定 Metrics，並以圖表呈現。</p>
<p><img src="https://i.imgur.com/Rv6XW6f.png" alt=""></p>
<p>Prometheus 提供了 <a href="https://prometheus.io/docs/prometheus/latest/querying/basics/" target="_blank" rel="noopener">Functional Expression Language</a> 進行查詢與聚合時間序列資料，比如用<code>sum(http_requests_total{method=&quot;GET&quot;} offset 5m)</code>來查看指定時間的資訊總和。</p>
<p>Prometheus 提供拉取第三方或者自己開發的 Exporter metrics 作為監測資料，這邊可以透過簡單的 <a href="https://github.com/prometheus/client_golang.git" target="_blank" rel="noopener">Go Client</a> 範例來簡單部署 Exporter：</p>
<pre><code class="sh">$ git clone https://github.com/prometheus/client_golang.git
$ cd client_golang/examples/random
$ go get -d
$ go build
</code></pre>
<p>完成後，開啟三個 Terminals 分別啟動以下 Exporter：</p>
<pre><code class="sh"># terminal 1
$ ./random -listen-address=:8081

# terminal 2
$ ./random -listen-address=:8082

# terminal 3
$ ./random -listen-address=:8083
</code></pre>
<blockquote>
<p>啟動後可以在<code>:8081</code>等 Ports 中查看 Metrics 資訊。</p>
</blockquote>
<p>確定沒問題後，修改<code>prometheus.yml</code>來新增 target，並重新啟動 Prometheus Server：</p>
<pre><code class="yaml">global:
  scrape_interval: 15s # 設定預設 scrape 的拉取間隔時間
  external_labels: # 外通溝通時標示在 time series 或 Alert 的 Labels。
    monitor: &#39;codelab-monitor&#39;

scrape_configs: # 設定 scrape jobs
  - job_name: &#39;prometheus&#39;
    scrape_interval: 5s # 若設定間隔時間，將會覆蓋 global 的預設時間。
    static_configs:
      - targets: [&#39;localhost:9090&#39;]
  - job_name: &#39;example-random&#39;
    scrape_interval: 5s
    static_configs:
      - targets: [&#39;localhost:8080&#39;, &#39;localhost:8081&#39;]
        labels:
          group: &#39;production&#39;
      - targets: [&#39;localhost:8082&#39;]
        labels:
          group: &#39;canary&#39;
</code></pre>
<p>啟動完成後，就可以 Web-console 的 Execute 執行以下來查詢：</p>
<pre><code class="sh">avg(rate(rpc_durations_seconds_count[5m])) by (job, service)
</code></pre>
<p><img src="https://i.imgur.com/Bo7YGo5.png" alt=""></p>
<p>另外 Prometheus 也提供自定義 Group rules 來將指定的 Expression query 當作一個 Metric，這邊建立一個檔案<code>prometheus.rules.yml</code>，並新增以下內容：</p>
<pre><code class="yaml">groups:
- name: example
  rules:
  - record: job_service:rpc_durations_seconds_count:avg_rate5m
    expr: avg(rate(rpc_durations_seconds_count[5m])) by (job, service)
</code></pre>
<p>接著修改<code>prometheus.yml</code>加入以下內容，並重新啟動 Prometheus Server：</p>
<pre><code class="yaml">global:
  ...
scrape_configs:
  ...
rule_files:
  - &#39;prometheus.rules.yml&#39;
</code></pre>
<blockquote>
<p><code>global</code> 與 <code>scrape_configs</code> 不做任何修改，只需加入<code>rule_files</code>即可，另外注意檔案路徑位置。</p>
</blockquote>
<p>正常啟動後，就可以看到新的 Metric 被加入。<br><img src="https://i.imgur.com/LhKcGVK.png" alt=""></p>
]]></content>
      
        <categories>
            
            <category> DevOps </category>
            
        </categories>
        
        
        <tags>
            
            <tag> DevOps </tag>
            
            <tag> Monitoring </tag>
            
            <tag> CNCF </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[以 Keystone 作為 Kubernetes 使用者認證]]></title>
      <url>https://kairen.github.io/2018/05/30/kubernetes/k8s-integration-keystone/</url>
      <content type="html"><![CDATA[<p>本文章將說明如何整合 Keystone 來提供給 Kubernetes 進行使用者認證。但由於 Keystone 整合 Kubernetes 認證在 1.10.x 版本已從原生移除(<code>--experimental-keystone-url</code>, <code>--experimental-keystone-ca-file</code>)，並轉而使用 <a href="https://github.com/kubernetes/cloud-provider-openstack" target="_blank" rel="noopener">cloud-provider-openstack</a> 中的 Webhook 來達成，而篇將說明如何建置與設定以整合該 Webhook。</p>
<a id="more"></a>
<h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統以<code>Ubuntu 16.x</code>進行測試：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Hostname</th>
<th>CPU</th>
<th>Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.22.132.20</td>
<td>k8s</td>
<td>4</td>
<td>8G</td>
</tr>
<tr>
<td>172.22.132.21</td>
<td>keystone</td>
<td>4</td>
<td>8G</td>
</tr>
</tbody>
</table>
<blockquote>
<ul>
<li><code>k8s</code>為 all-in-one Kubernetes 節點(就只是個執行 kubeadm init 的節點)。</li>
<li><code>keystone</code>利用 DevStack 部署一台 all-in-one OpenStack。</li>
</ul>
</blockquote>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>開始安裝前需要確保以下條件已達成：</p>
<ul>
<li><p><code>k8s</code>節點以 kubeadm 部署成 Kubernetes v1.9+ all-in-one 環境。請參考 <a href="https://kairen.github.io/2016/09/29/kubernetes/deploy/kubeadm/">用 kubeadm 部署 Kubernetes 叢集</a>。</p>
</li>
<li><p>在<code>k8s</code>節點安裝 openstack-client：</p>
</li>
</ul>
<pre><code class="sh">$ sudo apt-get update &amp;&amp; sudo  apt-get install -y python-pip
$ export LC_ALL=C; sudo pip install python-openstackclient
</code></pre>
<ul>
<li><code>keystone</code>節點部署成 OpenStack all-in-one 環境。請參考 <a href="https://docs.openstack.org/devstack/latest/" target="_blank" rel="noopener">DevStack</a>。</li>
</ul>
<h2 id="Kubernetes-與-Keystone-整合"><a href="#Kubernetes-與-Keystone-整合" class="headerlink" title="Kubernetes 與 Keystone 整合"></a>Kubernetes 與 Keystone 整合</h2><p>本節將逐節說明如何設定以整合 Keystone。</p>
<h3 id="建立-Keystone-User-與-Roles"><a href="#建立-Keystone-User-與-Roles" class="headerlink" title="建立 Keystone User 與 Roles"></a>建立 Keystone User 與 Roles</h3><p>當<code>keystone</code>節點的 OpenStack 部署完成後，進入到節點建立測試用 User 與 Roles：</p>
<pre><code class="sh">$ sudo su - stack
$ cd devstack
$ source openrc admin admin

# 建立 Roles
$ for role in &quot;k8s-admin&quot; &quot;k8s-viewer&quot; &quot;k8s-editor&quot;; do
    openstack role create $role;
  done

# 建立 User
$ openstack user create demo_editor --project demo --password secret
$ openstack user create demo_admin --project demo --password secret

# 加入 User 至 Roles
$ openstack role add --user demo --project demo k8s-viewer
$ openstack role add --user demo_editor --project demo k8s-editor
$ openstack role add --user demo_admin --project demo k8s-admin
</code></pre>
<h3 id="在-Kubernetes-安裝-Keystone-Webhook"><a href="#在-Kubernetes-安裝-Keystone-Webhook" class="headerlink" title="在 Kubernetes 安裝 Keystone Webhook"></a>在 Kubernetes 安裝 Keystone Webhook</h3><p>進入<code>k8s</code>節點，首先導入下載的檔案來源：</p>
<pre><code class="sh">$ export URL=&quot;https://kairen.github.io/files/openstack/keystone&quot;
</code></pre>
<p>新增一些腳本，來提供導入不同使用者環境變數給 OpenStack Client 使用：</p>
<pre><code class="sh">$ export KEYSTONE_HOST=&quot;172.22.132.21&quot;
$ export USER_PASSWORD=&quot;secret&quot;
$ for n in &quot;admin&quot; &quot;demo&quot; &quot;demoadmin&quot; &quot;demoeditor&quot; &quot;altdemo&quot;; do
    wget ${URL}/openrc-${n} -O ~/openrc-${n}
    sed -i &quot;s/KEYSTONE_HOST/${KEYSTONE_HOST}/g&quot; ~/openrc-${n}
    sed -i &quot;s/USER_PASSWORD/${USER_PASSWORD}/g&quot; ~/openrc-${n}
  done
</code></pre>
<p>下載 Keystone Webhook Policy 檔案，然後執行指令修改內容：</p>
<pre><code class="sh">$ sudo wget ${URL}/webhook-policy.json -O /etc/kubernetes/webhook-policy.json
$ source ~/openrc-demo
$ PROJECT_ID=$(openstack project list | awk &#39;/demo/ {print$2}&#39;)
$ sudo sed -i &quot;s/PROJECT_ID/${PROJECT_ID}/g&quot; /etc/kubernetes/webhook-policy.json
</code></pre>
<p>然後下載與部署 Keystone Webhook YAML 檔：</p>
<pre><code class="sh">$ wget ${URL}/keystone-webhook-ds.conf -O keystone-webhook-ds.yml
$ KEYSTONE_HOST=&quot;172.22.132.21&quot;
$ sed -i &quot;s/KEYSTONE_HOST/${KEYSTONE_HOST}/g&quot; keystone-webhook-ds.yml
$ kubectl create -f keystone-webhook-ds.yml
configmap &quot;keystone-webhook-kubeconfig&quot; created
daemonset.apps &quot;keystone-auth-webhook&quot; created
</code></pre>
<p>透過 kubectl 確認 Keystone Webhook 是否部署成功：</p>
<pre><code class="sh">$ kubectl -n kube-system get po -l component=k8s-keystone
NAME                          READY     STATUS    RESTARTS   AGE
keystone-auth-webhook-5qqwn   1/1       Running   0          1m
</code></pre>
<p>透過 cURL 確認是否能夠正確存取：</p>
<pre><code class="sh">$ source ~/openrc-demo
$ TOKEN=$(openstack token issue -f yaml -c id | awk &#39;{print $2}&#39;)
$ cat &lt;&lt; EOF | curl -kvs -XPOST -d @- https://localhost:8443/webhook | python -mjson.tool
{
  &quot;apiVersion&quot;: &quot;authentication.k8s.io/v1beta1&quot;,
  &quot;kind&quot;: &quot;TokenReview&quot;,
  &quot;metadata&quot;: {
    &quot;creationTimestamp&quot;: null
  },
  &quot;spec&quot;: {
    &quot;token&quot;: &quot;$TOKEN&quot;
  }
}
EOF

# output
{
    &quot;apiVersion&quot;: &quot;authentication.k8s.io/v1beta1&quot;,
    &quot;kind&quot;: &quot;TokenReview&quot;,
    &quot;metadata&quot;: {
        &quot;creationTimestamp&quot;: null
    },
    &quot;spec&quot;: {
        &quot;token&quot;: &quot;gAAAAABbFi1SacEPNstSuSuiBXiBG0Y_DikfbiR75j3P-CJ8CeaSKXa5kDQvun4LZUq8U6ehuW_RrQwi-N7j8t086uN6a4hLnPPGmvc6K_Iw0BZHZps7G1R5WniHZ8-WTUxtkMJROSz9eG7m33Bp18mvgx-P179QiwNYxLivf_rjnxePmvujNow&quot;
    },
    &quot;status&quot;: {
        &quot;authenticated&quot;: true,
        &quot;user&quot;: {
            &quot;extra&quot;: {
                &quot;alpha.kubernetes.io/identity/project/id&quot;: [
                    &quot;3ebcb1da142d427db04b8df43f6cb76a&quot;
                ],
                &quot;alpha.kubernetes.io/identity/project/name&quot;: [
                    &quot;demo&quot;
                ],
                &quot;alpha.kubernetes.io/identity/roles&quot;: [
                    &quot;k8s-viewer&quot;,
                    &quot;Member&quot;,
                    &quot;anotherrole&quot;
                ]
            },
            &quot;groups&quot;: [
                &quot;3ebcb1da142d427db04b8df43f6cb76a&quot;
            ],
            &quot;uid&quot;: &quot;19748c0131504b87a4117e49c67383c6&quot;,
            &quot;username&quot;: &quot;demo&quot;
        }
    }
}
</code></pre>
<h3 id="設定-kube-apiserver-使用-Webhook"><a href="#設定-kube-apiserver-使用-Webhook" class="headerlink" title="設定 kube-apiserver 使用 Webhook"></a>設定 kube-apiserver 使用 Webhook</h3><p>進入<code>k8s</code>節點，然後修改<code>/etc/kubernetes/manifests/kube-apiserver.yaml</code>檔案，加入以下內容：</p>
<pre><code class="yml">...
spec:
  containers:
  - command:
    ...
    # authorization-mode 加入 Webhook
    - --authorization-mode=Node,RBAC,Webhook
    - --runtime-config=authentication.k8s.io/v1beta1=true
    - --authentication-token-webhook-config-file=/srv/kubernetes/webhook-auth
    - --authorization-webhook-config-file=/srv/kubernetes/webhook-auth
    - --authentication-token-webhook-cache-ttl=5m
    volumeMounts:
    ...
    - mountPath: /srv/kubernetes/webhook-auth
      name: webhook-auth-file
      readOnly: true
  volumes:
  ...
  - hostPath:
      path: /srv/kubernetes/webhook-auth
      type: File
    name: webhook-auth-file
</code></pre>
<p>完成後重新啟動 kubelet(或者等待 static pod 自己更新)：</p>
<pre><code class="sh">$ sudo systemctl restart kubelet
</code></pre>
<h2 id="驗證部署結果"><a href="#驗證部署結果" class="headerlink" title="驗證部署結果"></a>驗證部署結果</h2><p>進入<code>k8s</code>節點，然後設定 kubectl context 並使用 openstack provider：</p>
<pre><code class="sh">$ kubectl config set-credentials openstack --auth-provider=openstack
$ kubectl config \
    set-context --cluster=kubernetes \
    --user=openstack \
    openstack@kubernetes \
    --namespace=default

$ kubectl config use-context openstack@kubernetes
</code></pre>
<p>測試 demo 使用者的存取權限是否有被限制：</p>
<pre><code class="sh">$ source ~/openrc-demo
$ kubectl get pods
No resources found.

$ cat &lt;&lt;EOF | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec:
  restartPolicy: Never
  containers:
  - image: nginx
    name: nginx-app
EOF
# output
Error from server (Forbidden): error when creating &quot;STDIN&quot;: pods is forbidden: User &quot;demo&quot; cannot create pods in the namespace &quot;default&quot;
</code></pre>
<blockquote>
<p>由於 demo 只擁有 k8s-viewer role，因此只能進行 get, list 與 watch API。</p>
</blockquote>
<p>測試 demo_editor 使用者是否能夠建立 Pod：</p>
<pre><code class="sh">$ source ~/openrc-demoeditor
$ cat &lt;&lt;EOF | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec:
  restartPolicy: Never
  containers:
  - image: nginx
    name: nginx-app
EOF
# output
pod &quot;nginx-pod&quot; created
</code></pre>
<blockquote>
<p>這邊可以看到 demo_editor 因為擁有 k8s-editor role，因此能夠執行 create API。</p>
</blockquote>
<p>測試 alt_demo 是否被禁止存取任何 API：</p>
<pre><code class="sh">$ source ~/openrc-altdemo
$ kubectl get po
Error from server (Forbidden): pods is forbidden: User &quot;alt_demo&quot; cannot list pods in the namespace &quot;default&quot;
</code></pre>
<blockquote>
<p>由於 alt_demo 不具備任何 roles，因此無法存取任何 API。</p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Kubernetes </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> Keystone </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[在 AWS 上建立跨地區的 Kubernetes Federation 叢集]]></title>
      <url>https://kairen.github.io/2018/04/21/kubernetes/aws-k8s-federation/</url>
      <content type="html"><![CDATA[<p>本篇延續先前 On-premises Federation 與 Kops 經驗來嘗試在 AWS 上建立 Federaion 叢集，這邊架構如下圖所示：</p>
<p><img src="/images/kops-fed/fed-clusters.png" alt=""></p>
<a id="more"></a>
<p>本次安裝的軟體版本：</p>
<ul>
<li>Kubernetes v1.9.3</li>
<li>kops v1.9.0</li>
<li>kubefed v1.10</li>
</ul>
<h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>測試環境為 AWS EC2 虛擬機器，共有三組叢集：</p>
<p>US West(Oregon) 叢集，也是 Federation 控制平面叢集：</p>
<table>
<thead>
<tr>
<th>Host</th>
<th>vCPU</th>
<th>RAM</th>
</tr>
</thead>
<tbody>
<tr>
<td>us-west-m1</td>
<td>1</td>
<td>2G</td>
</tr>
<tr>
<td>us-west-n1</td>
<td>1</td>
<td>2G</td>
</tr>
<tr>
<td>us-west-n2</td>
<td>1</td>
<td>2G</td>
</tr>
</tbody>
</table>
<p>US East(Ohio) 叢集:</p>
<table>
<thead>
<tr>
<th>Host</th>
<th>vCPU</th>
<th>RAM</th>
</tr>
</thead>
<tbody>
<tr>
<td>us-east-m1</td>
<td>1</td>
<td>2G</td>
</tr>
<tr>
<td>us-east-n1</td>
<td>1</td>
<td>2G</td>
</tr>
<tr>
<td>us-east-n2</td>
<td>1</td>
<td>2G</td>
</tr>
</tbody>
</table>
<p>Asia Pacific(Tokyo) 叢集:</p>
<table>
<thead>
<tr>
<th>Host</th>
<th>vCPU</th>
<th>RAM</th>
</tr>
</thead>
<tbody>
<tr>
<td>ap-northeast-m1</td>
<td>1</td>
<td>2G</td>
</tr>
<tr>
<td>ap-northeast-n1</td>
<td>1</td>
<td>2G</td>
</tr>
<tr>
<td>ap-northeast-n2</td>
<td>1</td>
<td>2G</td>
</tr>
</tbody>
</table>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>開始前，需要先安裝下列工具到操作機器上來提供使用：</p>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank" rel="noopener">kubectl</a>：用來操作部署完成的 Kubernetes 叢集。</li>
<li><a href="https://github.com/kubernetes/kops" target="_blank" rel="noopener">kops</a>：用來部署與管理公有雲上的 Kubernetes 叢集。</li>
</ul>
<p>Mac OS X：</p>
<pre><code class="sh">$ brew update &amp;&amp; brew install kops
</code></pre>
<p>Linux distro：</p>
<pre><code class="sh">$ curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d &#39;&quot;&#39; -f 4)/kops-linux-amd64
$ chmod +x kops-linux-amd64 &amp;&amp; sudo mv kops-linux-amd64 /usr/local/bin/kops
</code></pre>
<ul>
<li><a href="https://github.com/kubernetes/federation" target="_blank" rel="noopener">kubefed</a>：用來建立 Federation 控制平面與管理 Federation 叢集的工具。</li>
</ul>
<p>Mac OS X：</p>
<pre><code class="sh">$ git clone https://github.com/kubernetes/federation.git $GOPATH/src/k8s.io/federation
$ cd $GOPATH/src/k8s.io/federation
$ make quick-release
$ cp _output/dockerized/bin/linux/amd64/kubefed /usr/local/bin/kubefed
</code></pre>
<p>Linux distro：</p>
<pre><code class="sh">$ wget https://storage.googleapis.com/kubernetes-federation-release/release/v1.9.0-alpha.3/federation-client-linux-amd64.tar.gz
$ tar xvf federation-client-linux-amd64.tar.gz
$ cp federation/client/bin/kubefed /usr/local/bin/
$ kubefed version
Client Version: version.Info{Major:&quot;1&quot;, Minor:&quot;9+&quot;, GitVersion:&quot;v1.9.0-alpha.3&quot;, GitCommit:&quot;85c06145286da663755b140efa2b65f793cce9ec&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-02-14T12:54:40Z&quot;, GoVersion:&quot;go1.9.1&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}
Server Version: version.Info{Major:&quot;1&quot;, Minor:&quot;9&quot;, GitVersion:&quot;v1.9.6&quot;, GitCommit:&quot;9f8ebd171479bec0ada837d7ee641dec2f8c6dd1&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-03-21T15:13:31Z&quot;, GoVersion:&quot;go1.9.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}
</code></pre>
<ul>
<li><a href="https://aws.amazon.com/cli/?nc1=h_ls" target="_blank" rel="noopener">AWS CLI</a>：用來操作 AWS 服務的工具。</li>
</ul>
<pre><code class="sh">$ sudo pip install awscli
$ aws --version
aws-cli/1.15.4
</code></pre>
<p>上述工具完成後，我們還要準備一下資訊：</p>
<ul>
<li>申請 AWS 帳號，並在 IAM 服務新增一個 User 設定存取所有服務(AdministratorAccess)。另外這邊要記住 AccessKey 與 SecretKey。<blockquote>
<p>一般來說只需開啟 S3、Route53、EC2、EBS、ELB 與 VPC 就好，但由於偷懶就全開。以下為各 AWS 服務在本次安裝的用意：</p>
<ul>
<li>IAM: 提供身份認證與存取管理。</li>
<li>EC2: Kubernetes 叢集部署的虛擬機環境。</li>
<li>ELB: Kubernetes 元件與 Service 負載平衡。</li>
<li>Route53: 提供 Public domain 存取 Kubernetes 環境。</li>
<li>S3: 儲存 Kops 狀態。</li>
<li>VPC: 提供 Kubernetes 與 EC2 的網路環境。</li>
</ul>
</blockquote>
</li>
</ul>
<p><img src="/images/kops/iam-user2.png" alt=""></p>
<ul>
<li>擁有自己的 Domain Name，這邊可以在 AWS Route53 註冊，或者是到 GoDaddy 購買。</li>
</ul>
<h2 id="部署-Kubernetes-Federation-叢集"><a href="#部署-Kubernetes-Federation-叢集" class="headerlink" title="部署 Kubernetes Federation 叢集"></a>部署 Kubernetes Federation 叢集</h2><p>本節將說明如何利用自己撰寫好的腳本 <a href="https://github.com/kairen/aws-k8s-federation" target="_blank" rel="noopener">aws-k8s-federation</a> 來部署 Kubernetes 叢集與 Federation 叢集。首先在操作節點下載：</p>
<pre><code class="sh">$ git clone https://github.com/kairen/aws-k8s-federation
$ cd aws-k8s-federation
$ cp .env.sample .env
</code></pre>
<p>編輯<code>.env</code>檔案來提供後續腳本的環境變數：</p>
<pre><code class="sh"># 你的 Domain Name(這邊為 &lt;hoste_dzone_name&gt;.&lt;domain_name&gt;)
export DOMAIN_NAME=&quot;k8s.example.com&quot;

# Regions and zones
export US_WEST_REGION=&quot;us-west-2&quot;
export US_EAST_REGION=&quot;us-east-2&quot;
export AP_NORTHEAST_REGION=&quot;ap-northeast-1&quot;
export ZONE=&quot;a&quot;

# Cluster contexts name
export FED_CONTEXT=&quot;aws-fed&quot;
export US_WEST_CONTEXT=&quot;us-west.${DOMAIN_NAME}&quot;
export US_EAST_CONTEXT=&quot;us-east.${DOMAIN_NAME}&quot;
export AP_NORTHEAST_CONTEXT=&quot;ap-northeast.${DOMAIN_NAME}&quot;

# S3 buckets name
export US_WEST_BUCKET_NAME=&quot;us-west-k8s&quot;
export US_EAST_BUCKET_NAME=&quot;us-east-k8s&quot;
export AP_NORTHEAST_BUCKET_NAME=&quot;ap-northeast-k8s&quot;

# Get domain name id
export HOSTED_ZONE_ID=$(aws route53 list-hosted-zones \
       | jq -r &#39;.HostedZones[] | select(.Name==&quot;&#39;${DOMAIN_NAME}&#39;.&quot;) | .Id&#39; \
       | sed &#39;s/\/hostedzone\///&#39;)

# Kubernetes master and node size, and node count.
export MASTER_SIZE=&quot;t2.micro&quot;
export NODE_SIZE=&quot;t2.micro&quot;
export NODE_COUNT=&quot;2&quot;

# Federation simple apps deploy and service name
export DNS_RECORD_PREFIX=&quot;nginx&quot;
export SERVICE_NAME=&quot;nginx&quot;
</code></pre>
<h3 id="建立-Route53-Hosted-Zone"><a href="#建立-Route53-Hosted-Zone" class="headerlink" title="建立 Route53 Hosted Zone"></a>建立 Route53 Hosted Zone</h3><p>首先透過 aws 工具進行設定使用指定 AccessKey 與 SecretKey：</p>
<pre><code class="sh">$ aws configure
AWS Access Key ID [****************QGEA]:
AWS Secret Access Key [****************zJ+w]:
Default region name [None]:
Default output format [None]:
</code></pre>
<blockquote>
<p>設定的 Keys 可以在<code>~/.aws/credentials</code>找到。</p>
</blockquote>
<p>接著需要在 Route53 建立一個 Hosted Zone，並在 Domain Name 供應商上設定 <code>NameServers</code>：</p>
<pre><code class="sh">$ ./0-create-hosted-domain.sh
# output
...
{
    &quot;HostedZone&quot;: {
        &quot;ResourceRecordSetCount&quot;: 2,
        &quot;CallerReference&quot;: &quot;2018-04-25-16:16&quot;,
        &quot;Config&quot;: {
            &quot;PrivateZone&quot;: false
        },
        &quot;Id&quot;: &quot;/hostedzone/Z2JR49ADZ0P3WC&quot;,
        &quot;Name&quot;: &quot;k8s.example.com.&quot;
    },
    &quot;DelegationSet&quot;: {
        &quot;NameServers&quot;: [
            &quot;ns-1547.awsdns-01.co.uk&quot;,
            &quot;ns-1052.awsdns-03.org&quot;,
            &quot;ns-886.awsdns-46.net&quot;,
            &quot;ns-164.awsdns-20.com&quot;
        ]
    },
    &quot;Location&quot;: &quot;https://route53.amazonaws.com/2013-04-01/hostedzone/Z2JR49ADZ0P3WC&quot;,
    &quot;ChangeInfo&quot;: {
        &quot;Status&quot;: &quot;PENDING&quot;,
        &quot;SubmittedAt&quot;: &quot;2018-04-25T08:16:57.462Z&quot;,
        &quot;Id&quot;: &quot;/change/C3802PE0C1JVW2&quot;
    }
}
</code></pre>
<p>之後將上述<code>NameServers</code>新增至自己的 Domain name 的 record 中，如 Godaddy：</p>
<p><img src="/images/kops-fed/godday-ns.png" alt=""></p>
<h3 id="在每個-Region-建立-Kubernetes-叢集"><a href="#在每個-Region-建立-Kubernetes-叢集" class="headerlink" title="在每個 Region 建立 Kubernetes 叢集"></a>在每個 Region 建立 Kubernetes 叢集</h3><p>當 Hosted Zone 建立完成後，就可以接著建立每個 Region 的 Kubernetes 叢集，這邊腳本已包含建立叢集與 S3 Bucket 指令，因此只需要執行以下腳本即可：</p>
<pre><code class="sh">$ ./1-create-clusters.sh
....
Cluster is starting.  It should be ready in a few minutes.
...
</code></pre>
<blockquote>
<p>這邊會需要等待一點時間進行初始化與部署，也可以到 AWS Console 查看狀態。</p>
</blockquote>
<p>完成後，即可透過 kubectl 來操作叢集：</p>
<pre><code class="sh">$ ./us-east/kc get no
+ kubectl --context=us-east.k8s.example.com get no
NAME                                          STATUS    ROLES     AGE       VERSION
ip-172-20-43-26.us-east-2.compute.internal    Ready     node      1m        v1.9.3
ip-172-20-56-167.us-east-2.compute.internal   Ready     master    3m        v1.9.3
ip-172-20-63-133.us-east-2.compute.internal   Ready     node      2m        v1.9.3

$ ./ap-northeast/kc get no
+ kubectl --context=ap-northeast.k8s.example.com get no
NAME                                               STATUS    ROLES     AGE       VERSION
ip-172-20-42-184.ap-northeast-1.compute.internal   Ready     master    2m        v1.9.3
ip-172-20-52-176.ap-northeast-1.compute.internal   Ready     node      20s       v1.9.3
ip-172-20-56-88.ap-northeast-1.compute.internal    Ready     node      22s       v1.9.3

$ ./us-west/kc get no
+ kubectl --context=us-west.k8s.example.com get no
NAME                                          STATUS    ROLES     AGE       VERSION
ip-172-20-33-22.us-west-2.compute.internal    Ready     node      1m        v1.9.3
ip-172-20-55-237.us-west-2.compute.internal   Ready     master    2m        v1.9.3
ip-172-20-63-77.us-west-2.compute.internal    Ready     node      35s       v1.9.3
</code></pre>
<h3 id="建立-Kubernetes-Federation-叢集"><a href="#建立-Kubernetes-Federation-叢集" class="headerlink" title="建立 Kubernetes Federation 叢集"></a>建立 Kubernetes Federation 叢集</h3><p>當三個地區的叢集建立完成後，接著要在 US West 的叢集上部署 Federation 控制平面元件：</p>
<pre><code class="sh">$ ./2-init-federation.sh
...
Federation API server is running at: abba6864f490111e8b4bd028106a7a79-793027324.us-west-2.elb.amazonaws.com

$ ./us-west/kc -n federation-system get po
+ kubectl --context=us-west.k8s.example.com -n federation-system get po
NAME                                  READY     STATUS    RESTARTS   AGE
apiserver-5d46898995-tmzvl            2/2       Running   0          1m
controller-manager-6cc78c68d5-2pbg5   0/1       Error     3          1m
</code></pre>
<p>這邊會發現<code>controller-manager</code>會一直掛掉，這是因為它需要取得 AWS 相關權限，因此需要透過 Patch 方式來把 AccessKey 與 SecretKey 注入到 Deployment 中：</p>
<pre><code class="sh">$ ./3-path-federation.sh
Switched to context &quot;us-west.k8s.example.com&quot;.
deployment &quot;controller-manager&quot; patched

$ ./us-west/kc -n federation-system get po
+ kubectl --context=us-west.k8s.example.com -n federation-system get po
NAME                                  READY     STATUS        RESTARTS   AGE
apiserver-5d46898995-tmzvl            2/2       Running       0          3m
controller-manager-769bd95fbc-dkssr   1/1       Running       0          21s
</code></pre>
<p>確認上述沒問題後，透過 kubectl 確認 contexts：</p>
<pre><code class="sh">$ kubectl config get-contexts
CURRENT   NAME                           CLUSTER                        AUTHINFO                       NAMESPACE
          ap-northeast.k8s.example.com   ap-northeast.k8s.example.com   ap-northeast.k8s.example.com
          aws-fed                        aws-fed                        aws-fed
          us-east.k8s.example.com        us-east.k8s.example.com        us-east.k8s.example.com
*         us-west.k8s.example.com        us-west.k8s.example.com        us-west.k8s.example.com
</code></pre>
<p>接著透過以下腳本來加入<code>us-west</code>叢集至 aws-fed 的 Federation 中：</p>
<pre><code class="sh">$ ./4-join-us-west.sh
+ kubectl config use-context aws-fed
Switched to context &quot;aws-fed&quot;.
+ kubefed join us-west --host-cluster-context=us-west.k8s.example.com --cluster-context=us-west.k8s.example.com
cluster &quot;us-west&quot; created
</code></pre>
<p>加入<code>ap-northeast</code>叢集至 aws-fed 的 Federation 中：</p>
<pre><code class="sh">$ ./5-join-ap-northeast.sh
+ kubectl config use-context aws-fed
Switched to context &quot;aws-fed&quot;.
+ kubefed join ap-northeast --host-cluster-context=us-west.k8s.example.com --cluster-context=ap-northeast.k8s.example.com
cluster &quot;ap-northeast&quot; created
</code></pre>
<p>加入<code>us-east</code>叢集至 aws-fed 的 Federation 中：</p>
<pre><code class="sh">$ ./6-join-us-east.sh
+ kubectl config use-context aws-fed
Switched to context &quot;aws-fed&quot;.
+ kubefed join us-east --host-cluster-context=us-west.k8s.example.com --cluster-context=us-east.k8s.example.com
cluster &quot;us-east&quot; created
</code></pre>
<p>完成後，在 Federation 建立 Federated Namespace，並列出叢集：</p>
<pre><code class="sh">$ ./7-create-fed-ns.sh
+ kubectl --context=aws-fed create namespace default
namespace &quot;default&quot; created
+ kubectl --context=aws-fed get clusters
NAME           AGE
ap-northeast   2m
us-east        1m
us-west        2m
</code></pre>
<p>完成這些過程表示你已經建立了一套 Kubernetes Federation 叢集了，接下來就可以進行測試。</p>
<h2 id="測試叢集"><a href="#測試叢集" class="headerlink" title="測試叢集"></a>測試叢集</h2><p>首先建立一個簡單的 Nginx 來提供服務的測試，這邊可以透過以下腳本達成：</p>
<pre><code class="sh">$ ./8-deploy-fed-nginx.sh
+ cat
+ kubectl --context=aws-fed apply -f -
deployment &quot;nginx&quot; created
+ cat
+ kubectl --context=aws-fed apply -f -
service &quot;nginx&quot; created

$ kubectl get deploy,svc
NAME           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deploy/nginx   3         3         3            3           3m

NAME        TYPE           CLUSTER-IP   EXTERNAL-IP        PORT(S)   AGE
svc/nginx   LoadBalancer   &lt;none&gt;       a4d86547a4903...   80/TCP    2m
</code></pre>
<blockquote>
<p>這裡的 nginx deployment 有設定<code>deployment-preferences</code>，因此在 scale 時會依據下面資訊來分配：</p>
<pre><code class="sh">{
       &quot;rebalance&quot;: true,
       &quot;clusters&quot;: {
         &quot;us-west&quot;: {
           &quot;minReplicas&quot;: 2,
           &quot;maxReplicas&quot;: 10,
           &quot;weight&quot;: 200
         },
         &quot;us-east&quot;: {
           &quot;minReplicas&quot;: 0,
           &quot;maxReplicas&quot;: 2,
           &quot;weight&quot;: 150
         },
         &quot;ap-northeast&quot;: {
           &quot;minReplicas&quot;: 1,
           &quot;maxReplicas&quot;: 5,
           &quot;weight&quot;: 150
         }
       }
     }
</code></pre>
</blockquote>
<p>檢查每個叢集的 Pod：</p>
<pre><code class="sh"># us-west context(這邊策略為 2 - 10)
$ ./us-west/kc get po
+ kubectl --context=us-west.k8s.example.com get po
NAME                     READY     STATUS    RESTARTS   AGE
nginx-679dc9c764-4x78c   1/1       Running   0          3m
nginx-679dc9c764-fzv9z   1/1       Running   0          3m

# us-east context(這邊策略為 0 - 2)
$ ./us-east/kc get po
+ kubectl --context=us-east.k8s.example.com get po
No resources found.

# ap-northeast context(這邊策略為 1 - 5)
$ ./ap-northeast/kc get po
+ kubectl --context=ap-northeast.k8s.example.com get po
NAME                     READY     STATUS    RESTARTS   AGE
nginx-679dc9c764-hmwzq   1/1       Running   0          4m
</code></pre>
<p>透過擴展副本數來查看分配狀況：</p>
<pre><code class="sh">$ ./9-scale-fed-nginx.sh
+ kubectl --context=aws-fed scale deploy nginx --replicas=10
deployment &quot;nginx&quot; scaled

$ kubectl get deploy
NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx     10        10        10           10          8m
</code></pre>
<p>再次檢查每個叢集的 Pod：</p>
<pre><code class="sh"># us-west context(這邊策略為 2 - 10)
$ ./us-west/kc get po
+ kubectl --context=us-west.k8s.example.com get po
NAME                     READY     STATUS    RESTARTS   AGE
nginx-679dc9c764-4x78c   1/1       Running   0          8m
nginx-679dc9c764-7958k   1/1       Running   0          50s
nginx-679dc9c764-fzv9z   1/1       Running   0          8m
nginx-679dc9c764-j6kc9   1/1       Running   0          50s
nginx-679dc9c764-t6rvj   1/1       Running   0          50s

# us-east context(這邊策略為 0 - 2)
$ ./us-east/kc get po
+ kubectl --context=us-east.k8s.example.com get po
NAME                     READY     STATUS    RESTARTS   AGE
nginx-679dc9c764-8t7qz   1/1       Running   0          1m
nginx-679dc9c764-zvqmx   1/1       Running   0          1m

# ap-northeast context(這邊策略為 1 - 5)
$ ./ap-northeast/kc get po
+ kubectl --context=ap-northeast.k8s.example.com get po
NAME                     READY     STATUS    RESTARTS   AGE
nginx-679dc9c764-f79v7   1/1       Running   0          1m
nginx-679dc9c764-hmwzq   1/1       Running   0          9m
nginx-679dc9c764-vj7hb   1/1       Running   0          1m
</code></pre>
<blockquote>
<p>可以看到結果符合我們預期範圍內。</p>
</blockquote>
<p>最後因為服務是透過 ELB 來提供，為了統一透過 Domain name 存取相同服務，這邊更新 Hosted Zone Record 來轉發：</p>
<pre><code class="sh">$ ./10-update-fed-nginx-record.sh
</code></pre>
<p>完成後透過 cURL 工作來測試：</p>
<pre><code class="sh">$ curl nginx.k8s.example.com
...
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
...
</code></pre>
<p>最後透過該腳本來清楚叢集與 AWS 服務上建立的東西：</p>
<pre><code class="sh">$ ./99-purge.sh
</code></pre>
]]></content>
      
        <categories>
            
            <category> Kubernetes </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> AWS </tag>
            
            <tag> Kops </tag>
            
            <tag> Federation </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[使用 Kops 部署 Kubernetes 至公有雲(AWS)]]></title>
      <url>https://kairen.github.io/2018/04/18/kubernetes/deploy/kops-aws/</url>
      <content type="html"><![CDATA[<p><a href="https://github.com/kubernetes/kops" target="_blank" rel="noopener">Kops</a> 是 Kubernetes 官方維護的專案，是一套 Production ready 的 Kubernetes 部署、升級與管理工具，早期用於 AWS 公有雲上建置 Kubernetes 叢集使用，但隨著社群的推進已支援 GCP、vSphere(Alpha)，未來也會有更多公有雲平台慢慢被支援(Maybe)。本篇簡單撰寫使用 Kops 部署一個叢集，過去自己因為公司都是屬於建置 On-premises 的 Kubernetes，因此很少使用 Kops，剛好最近社群分享又再一次接觸的關析，所以就來寫個文章。</p>
<p>本次安裝的軟體版本：</p>
<ul>
<li>Kubernetes v1.9.3</li>
<li>Kops v1.9.0</li>
</ul>
<a id="more"></a>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>開始使用 Kops 前，需要先安裝下列工具到操作機器上來提供使用：</p>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank" rel="noopener">kubectl</a>：用來操作部署完成的 Kubernetes 叢集。</li>
<li><a href="https://github.com/kubernetes/kops" target="_blank" rel="noopener">kops</a>：本次使用工具，用來部署與管理公有雲上的 Kubernetes 叢集。</li>
</ul>
<p>Mac OS X：</p>
<pre><code class="sh">$ brew update &amp;&amp; brew install kops
</code></pre>
<p>Linux distro：</p>
<pre><code class="sh">$ curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d &#39;&quot;&#39; -f 4)/kops-linux-amd64
$ chmod +x kops-linux-amd64 &amp;&amp; sudo mv kops-linux-amd64 /usr/local/bin/kops
</code></pre>
<ul>
<li><a href="https://aws.amazon.com/cli/?nc1=h_ls" target="_blank" rel="noopener">AWS CLI</a>：用來操作 AWS 服務的工具。</li>
</ul>
<pre><code class="sh">$ sudo pip install awscli
$ aws --version
aws-cli/1.15.4
</code></pre>
<p>上述工具完成後，我們還要準備一下資訊：</p>
<ul>
<li>申請 AWS 帳號，並在 IAM 服務新增一個 User 設定存取所有服務(AdministratorAccess)。另外這邊要記住 AccessKey 與 SecretKey。<blockquote>
<p>一般來說只需開啟 S3、Route53、EC2、EBS 與 ELB 就好，但由於偷懶就全開。</p>
</blockquote>
</li>
</ul>
<p><img src="/images/kops/iam-user2.png" alt=""></p>
<ul>
<li>擁有自己的 Domain Name，這邊可以在 AWS Route53 註冊，或者是到 GoDaddy 購買。</li>
</ul>
<h2 id="建立-S3-Bucket-與-Route53-Hosted-Zone"><a href="#建立-S3-Bucket-與-Route53-Hosted-Zone" class="headerlink" title="建立 S3 Bucket 與 Route53 Hosted Zone"></a>建立 S3 Bucket 與 Route53 Hosted Zone</h2><p>首先透過 aws 工具進行設定使用指定 AccessKey 與 SecretKey：</p>
<pre><code class="sh">$ aws configure
AWS Access Key ID [****************QGEA]:
AWS Secret Access Key [****************zJ+w]:
Default region name [None]:
Default output format [None]:
</code></pre>
<blockquote>
<p>設定的 Keys 可以在<code>~/.aws/credentials</code>找到。</p>
</blockquote>
<p>完成後建立一個 S3 bucket 用來儲存 Kops 狀態：</p>
<pre><code class="sh">$ aws s3 mb s3://kops-k8s-1 --region us-west-2
make_bucket: kops-k8s-1
</code></pre>
<blockquote>
<p>這邊 region 可自行選擇，這邊選用 Oregon。</p>
</blockquote>
<p>接著建立一個 Route53 Hosted Zone：</p>
<pre><code class="sh">$ aws route53 create-hosted-zone \
    --name k8s.example.com \
    --caller-reference $(date &#39;+%Y-%m-%d-%H:%M&#39;)

# output
{
    &quot;HostedZone&quot;: {
        &quot;ResourceRecordSetCount&quot;: 2,
        &quot;CallerReference&quot;: &quot;2018-04-25-16:16&quot;,
        &quot;Config&quot;: {
            &quot;PrivateZone&quot;: false
        },
        &quot;Id&quot;: &quot;/hostedzone/Z2JR49ADZ0P3WC&quot;,
        &quot;Name&quot;: &quot;k8s.example.com.&quot;
    },
    &quot;DelegationSet&quot;: {
        &quot;NameServers&quot;: [
            &quot;ns-1547.awsdns-01.co.uk&quot;,
            &quot;ns-1052.awsdns-03.org&quot;,
            &quot;ns-886.awsdns-46.net&quot;,
            &quot;ns-164.awsdns-20.com&quot;
        ]
    },
    &quot;Location&quot;: &quot;https://route53.amazonaws.com/2013-04-01/hostedzone/Z2JR49ADZ0P3WC&quot;,
    &quot;ChangeInfo&quot;: {
        &quot;Status&quot;: &quot;PENDING&quot;,
        &quot;SubmittedAt&quot;: &quot;2018-04-25T08:16:57.462Z&quot;,
        &quot;Id&quot;: &quot;/change/C3802PE0C1JVW2&quot;
    }
}
</code></pre>
<blockquote>
<p>請修改<code>--name</code>為自己所擁有的 domain name。</p>
</blockquote>
<p>之後將上述<code>NameServers</code>新增至自己的 Domain name 的 record 中，如 Godaddy：</p>
<p><img src="/images/kops/route53-hostedzone.png" alt=""></p>
<h2 id="部署-Kubernetes-叢集"><a href="#部署-Kubernetes-叢集" class="headerlink" title="部署 Kubernetes 叢集"></a>部署 Kubernetes 叢集</h2><p>當上述階段完成後，在自己機器建立 SSH key，就可以使用 Kops 來建立 Kubernetes 叢集：</p>
<pre><code class="sh">$ ssh-keygen -t rsa
$ kops create cluster \
    --name=k8s.example.com \
    --state=s3://kops-k8s-1 \
    --zones=us-west-2a \
    --master-size=t2.micro \
    --node-size=t2.micro \
    --node-count=2 \
    --dns-zone=k8s.example.com

# output
...
Finally configure your cluster with: kops update cluster k8s.example.com --yes
</code></pre>
<p>若過程沒有發生錯誤的話，最後會提示再執行 update 來正式進行部署：</p>
<pre><code class="sh">$ kops update cluster k8s.example.com --state=s3://kops-k8s-1 --yes
# output
...
Cluster is starting.  It should be ready in a few minutes.
</code></pre>
<p>當看到上述資訊時，表示叢集已建立，這時候等待環境初始化完成後就可以使用 kubectl 來操作：</p>
<pre><code class="sh">$ kubectl get node
NAME                                          STATUS    ROLES     AGE       VERSION
ip-172-20-32-194.us-west-2.compute.internal   Ready     master    1m        v1.9.3
ip-172-20-32-21.us-west-2.compute.internal    Ready     node      22s       v1.9.3
ip-172-20-54-100.us-west-2.compute.internal   Ready     node      28s       v1.9.3
</code></pre>
<h2 id="測試"><a href="#測試" class="headerlink" title="測試"></a>測試</h2><p>完成後就可以進行功能測試，這邊簡單建立 Nginx app：</p>
<pre><code class="sh">$ kubectl run nginx --image nginx --port 80
$ kubectl expose deploy nginx --type=LoadBalancer --port 80
$ kubectl get po,svc
NAME                        READY     STATUS    RESTARTS   AGE
po/nginx-7587c6fdb6-7qtlr   1/1       Running   0          50s

NAME             TYPE           CLUSTER-IP    EXTERNAL-IP        PORT(S)        AGE
svc/kubernetes   ClusterIP      100.64.0.1    &lt;none&gt;             443/TCP        8m
svc/nginx        LoadBalancer   100.68.96.3   ad99f206f486e...   80:30174/TCP   28s
</code></pre>
<p>這邊會看到<code>EXTERNAL-IP</code>會直接透過 AWS ELB 建立一個 Load Balancer，這時只要更新 Route53 的 record set 就可以存取到服務：</p>
<pre><code class="sh">$ export DOMAIN_NAME=k8s.example.com
$ export NGINX_LB=$(kubectl get svc/nginx \
  --template=&quot;{{range .status.loadBalancer.ingress}} {{.hostname}} {{end}}&quot;)

$ cat &lt;&lt;EOF &gt; dns-record.json
{
  &quot;Comment&quot;: &quot;Create/Update a latency-based CNAME record for a federated Deployment&quot;,
  &quot;Changes&quot;: [
    {
      &quot;Action&quot;: &quot;UPSERT&quot;,
      &quot;ResourceRecordSet&quot;: {
        &quot;Name&quot;: &quot;nginx.${DOMAIN_NAME}&quot;,
        &quot;Type&quot;: &quot;CNAME&quot;,
        &quot;Region&quot;: &quot;us-west-2&quot;,
        &quot;TTL&quot;: 300,
        &quot;SetIdentifier&quot;: &quot;us-west-2&quot;,
        &quot;ResourceRecords&quot;: [
          {
            &quot;Value&quot;: &quot;${NGINX_LB}&quot;
          }
        ]
      }
    }
  ]
}
EOF

$ export HOSTED_ZONE_ID=$(aws route53 list-hosted-zones \
       | jq -r &#39;.HostedZones[] | select(.Name==&quot;&#39;${DOMAIN_NAME}&#39;.&quot;) | .Id&#39; \
       | sed &#39;s/\/hostedzone\///&#39;)

$ aws route53 change-resource-record-sets \
    --hosted-zone-id ${HOSTED_ZONE_ID} \
    --change-batch file://dns-record.json

# output
{
    &quot;ChangeInfo&quot;: {
        &quot;Status&quot;: &quot;PENDING&quot;,
        &quot;Comment&quot;: &quot;Create/Update a latency-based CNAME record for a federated Deployment&quot;,
        &quot;SubmittedAt&quot;: &quot;2018-04-25T10:06:02.545Z&quot;,
        &quot;Id&quot;: &quot;/change/C79MFJRHCF05R&quot;
    }
}
</code></pre>
<p>完成後透過 cURL 工作來測試：</p>
<pre><code class="sh">$ curl nginx.k8s.example.com
...
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
...
</code></pre>
<h2 id="刪除節點"><a href="#刪除節點" class="headerlink" title="刪除節點"></a>刪除節點</h2><p>當叢集測完後，可以利用以下指令來刪除：</p>
<pre><code class="sh">$ kops delete cluster \
 --name=k8s.example.com \
 --state=s3://kops-k8s-1 --yes

Deleted cluster: &quot;k8s.k2r2bai.com&quot;

$ aws s3 rb s3://kops-k8s-1 --force
remove_bucket: kops-k8s-1
</code></pre>
<p>接著清除 Route53 所有 record 並刪除 hosted zone：</p>
<pre><code class="sh">$ aws route53 list-resource-record-sets \
  --hosted-zone-id ${HOSTED_ZONE_ID} |
jq -c &#39;.ResourceRecordSets[]&#39; |
while read -r resourcerecordset; do
  read -r name type &lt;&lt;&lt;$(echo $(jq -r &#39;.Name,.Type&#39; &lt;&lt;&lt;&quot;$resourcerecordset&quot;))
  if [ $type != &quot;NS&quot; -a $type != &quot;SOA&quot; ]; then
    aws route53 change-resource-record-sets \
      --hosted-zone-id ${HOSTED_ZONE_ID} \
      --change-batch &#39;{&quot;Changes&quot;:[{&quot;Action&quot;:&quot;DELETE&quot;,&quot;ResourceRecordSet&quot;:
          &#39;&quot;$resourcerecordset&quot;&#39;
        }]}&#39; \
      --output text --query &#39;ChangeInfo.Id&#39;
  fi
done

$ aws route53 delete-hosted-zone --id ${HOSTED_ZONE_ID}
</code></pre>
]]></content>
      
        <categories>
            
            <category> Kubernetes </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> AWS </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[整合 OpenLDAP 進行 Kubernetes 身份認證]]></title>
      <url>https://kairen.github.io/2018/04/15/kubernetes/k8s-integration-ldap/</url>
      <content type="html"><![CDATA[<p>本文將說明如何整合 OpenLDAP 來提供給 Kubernetes 進行使用者認證。Kubernetes 官方並沒有提供針對 LDAP 與 AD 的整合，但是可以藉由 <a href="https://kubernetes.io/docs/admin/authentication/#webhook-token-authentication" target="_blank" rel="noopener">Webhook Token Authentication</a> 以及 <a href="https://kubernetes.io/docs/admin/authentication/#authenticating-proxy" target="_blank" rel="noopener">Authenticating Proxy</a> 來達到整合功能。概念是開發一個 HTTP Server 提供 POST Method 來塞入 Bearer Token，而該 HTTP Server 利用 LDAP library 檢索對應 Token 的 User 進行認證，成功後回傳該 User 的所有 Group 等資訊，而這時可以利用 Kubernetes 針對該 User 的 Group 設定對應的 RBAC role 進行權限控管。</p>
<a id="more"></a>
<h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統可採用<code>Ubuntu 16.x</code>與<code>CentOS 7.x</code>：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Hostname</th>
<th>CPU</th>
<th>Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>192.16.35.11</td>
<td>k8s-m1</td>
<td>1</td>
<td>2G</td>
</tr>
<tr>
<td>192.16.35.12</td>
<td>k8s-n1</td>
<td>1</td>
<td>2G</td>
</tr>
<tr>
<td>192.16.35.13</td>
<td>k8s-n2</td>
<td>1</td>
<td>2G</td>
</tr>
<tr>
<td>192.16.35.20</td>
<td>ldap-server</td>
<td>1</td>
<td>1G</td>
</tr>
</tbody>
</table>
<blockquote>
<ul>
<li>這邊<code>m</code>為 K8s master，<code>n</code>為 K8s node。</li>
<li>所有操作全部用<code>root</code>使用者進行(方便用)，以 SRE 來說不推薦。</li>
<li>可以下載 <a href="https://kairen.github.io/files/k8s-ldap/Vagrantfile">Vagrantfile</a> 來建立 Virtualbox 虛擬機叢集。不過需要注意機器資源是否足夠。</li>
</ul>
</blockquote>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>開始安裝前需要確保以下條件已達成：</p>
<ul>
<li><code>所有節點</code>需要安裝 Docker CE 版本的容器引擎：</li>
</ul>
<pre><code class="sh">$ curl -fsSL &quot;https://get.docker.com/&quot; | sh
</code></pre>
<blockquote>
<p>不管是在 <code>Ubuntu</code> 或 <code>CentOS</code> 都只需要執行該指令就會自動安裝最新版 Docker。<br>CentOS 安裝完成後需要再執行以下指令：</p>
<pre><code class="sh">$ systemctl enable docker &amp;&amp; systemctl start docker
</code></pre>
</blockquote>
<ul>
<li>所有節點以 kubeadm 部署成 Kubernetes v1.9+ 叢集。請參考 <a href="https://kairen.github.io/2016/09/29/kubernetes/deploy/kubeadm/">用 kubeadm 部署 Kubernetes 叢集</a>。</li>
</ul>
<h2 id="OpenLDAP-與-phpLDAPadmin"><a href="#OpenLDAP-與-phpLDAPadmin" class="headerlink" title="OpenLDAP 與 phpLDAPadmin"></a>OpenLDAP 與 phpLDAPadmin</h2><p>本節將說明如何部署、設定與操作 OpenLDAP。</p>
<h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><p>進入<code>ldap-server</code>節點透過 Docker 來進行部署：</p>
<pre><code class="sh">$ docker run -d \
    -p 389:389 -p 636:636 \
    --env LDAP_ORGANISATION=&quot;Kubernetes LDAP&quot; \
    --env LDAP_DOMAIN=&quot;k8s.com&quot; \
    --env LDAP_ADMIN_PASSWORD=&quot;password&quot; \
    --env LDAP_CONFIG_PASSWORD=&quot;password&quot; \
    --name openldap-server \
    osixia/openldap:1.2.0

$ docker run -d \
    -p 443:443 \
    --env PHPLDAPADMIN_LDAP_HOSTS=192.16.35.20 \
    --name phpldapadmin \
    osixia/phpldapadmin:0.7.1
</code></pre>
<blockquote>
<p>這邊為<code>cn=admin,dc=k8s,dc=com</code>為<code>admin</code> DN ，而<code>cn=admin,cn=config</code>為<code>config</code>。另外這邊僅做測試用，故不使用 Persistent Volumes，需要可以參考 <a href="https://github.com/osixia/docker-openldap" target="_blank" rel="noopener">Docker OpenLDAP</a>。</p>
</blockquote>
<p>完成後就可以透過瀏覽器來 <a href="https://192.16.35.20/" target="_blank" rel="noopener">phpLDAPadmin website</a>。這邊點選<code>Login</code>輸入 DN 與 Password。<br><img src="/images/k8s-ldap/ldap-login.png" alt=""></p>
<p>成功登入後畫面，這時可以自行新增其他資訊。<br><img src="/images/k8s-ldap/ldap-logined.png" alt=""></p>
<h3 id="建立-Kubenretes-Token-Schema"><a href="#建立-Kubenretes-Token-Schema" class="headerlink" title="建立 Kubenretes Token Schema"></a>建立 Kubenretes Token Schema</h3><p>進入<code>openldap-server 容器</code>，接著建立 Kubernetes token schema 物件的設定檔：</p>
<pre><code class="sh">$ docker exec -ti openldap-server sh
$ mkdir ~/kubernetes_tokens
$ cat &lt;&lt;EOF &gt; ~/kubernetes_tokens/kubernetesToken.schema
attributeType ( 1.3.6.1.4.1.18171.2.1.8
        NAME &#39;kubernetesToken&#39;
        DESC &#39;Kubernetes authentication token&#39;
        EQUALITY caseExactIA5Match
        SUBSTR caseExactIA5SubstringsMatch
        SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 SINGLE-VALUE )

objectClass ( 1.3.6.1.4.1.18171.2.3
        NAME &#39;kubernetesAuthenticationObject&#39;
        DESC &#39;Object that may authenticate to a Kubernetes cluster&#39;
        AUXILIARY
        MUST kubernetesToken )
EOF

$ echo &quot;include /root/kubernetes_tokens/kubernetesToken.schema&quot; &gt; ~/kubernetes_tokens/schema_convert.conf
$ slaptest -f ~/kubernetes_tokens/schema_convert.conf -F ~/kubernetes_tokens
config file testing succeeded
</code></pre>
<p>修改以下檔案內容，如以下所示：</p>
<pre><code class="sh">$ vim ~/kubernetes_tokens/cn=config/cn=schema/cn\=\{0\}kubernetestoken.ldif
# AUTO-GENERATED FILE - DO NOT EDIT!! Use ldapmodify.
# CRC32 e502306e
dn: cn=kubernetestoken,cn=schema,cn=config
objectClass: olcSchemaConfig
cn: kubernetestoken
olcAttributeTypes: {0}( 1.3.6.1.4.1.18171.2.1.8 NAME &#39;kubernetesToken&#39; DESC
 &#39;Kubernetes authentication token&#39; EQUALITY caseExactIA5Match SUBSTR caseExa
 ctIA5SubstringsMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 SINGLE-VALUE )
olcObjectClasses: {0}( 1.3.6.1.4.1.18171.2.3 NAME &#39;kubernetesAuthenticationO
 bject&#39; DESC &#39;Object that may authenticate to a Kubernetes cluster&#39; AUXILIAR
 Y MUST kubernetesToken )
</code></pre>
<p>新增 Schema 物件至 LDAP Server 中：</p>
<pre><code class="sh">$ cd ~/kubernetes_tokens/cn=config/cn=schema
$ ldapadd -c -Y EXTERNAL -H ldapi:/// -f cn\=\{0\}kubernetestoken.ldif
SASL/EXTERNAL authentication started
SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth
SASL SSF: 0
adding new entry &quot;cn=kubernetestoken,cn=schema,cn=config&quot;
</code></pre>
<p>完成後查詢是否成功新增 Entry：</p>
<pre><code class="sh">$ ldapsearch -x -H ldap:/// -LLL -D &quot;cn=admin,cn=config&quot; -w password -b &quot;cn=schema,cn=config&quot; &quot;(objectClass=olcSchemaConfig)&quot; dn -Z
Enter LDAP Password:
dn: cn=schema,cn=config
...
dn: cn={14}kubernetestoken,cn=schema,cn=config
</code></pre>
<h3 id="新增測試用-LDAP-Groups-與-Users"><a href="#新增測試用-LDAP-Groups-與-Users" class="headerlink" title="新增測試用 LDAP Groups 與 Users"></a>新增測試用 LDAP Groups 與 Users</h3><p>當上面 Schema 建立完成後，這邊需要新增一些測試用 Groups：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; groups.ldif
dn: ou=People,dc=k8s,dc=com
ou: People
objectClass: top
objectClass: organizationalUnit
description: Parent object of all UNIX accounts

dn: ou=Groups,dc=k8s,dc=com
ou: Groups
objectClass: top
objectClass: organizationalUnit
description: Parent object of all UNIX groups

dn: cn=kubernetes,ou=Groups,dc=k8s,dc=com
cn: kubernetes
gidnumber: 100
memberuid: user1
memberuid: user2
objectclass: posixGroup
objectclass: top
EOF

$ ldapmodify -x -a -H ldap:// -D &quot;cn=admin,dc=k8s,dc=com&quot; -w password -f groups.ldif
adding new entry &quot;ou=People,dc=k8s,dc=com&quot;

adding new entry &quot;ou=Groups,dc=k8s,dc=com&quot;

adding new entry &quot;cn=kubernetes,ou=Groups,dc=k8s,dc=com&quot;
</code></pre>
<p>Group 建立完成後再接著建立 User：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; users.ldif
dn: uid=user1,ou=People,dc=k8s,dc=com
cn: user1
gidnumber: 100
givenname: user1
homedirectory: /home/users/user1
loginshell: /bin/sh
objectclass: inetOrgPerson
objectclass: posixAccount
objectclass: top
objectClass: shadowAccount
objectClass: organizationalPerson
sn: user1
uid: user1
uidnumber: 1000
userpassword: user1

dn: uid=user2,ou=People,dc=k8s,dc=com
homedirectory: /home/users/user2
loginshell: /bin/sh
objectclass: inetOrgPerson
objectclass: posixAccount
objectclass: top
objectClass: shadowAccount
objectClass: organizationalPerson
cn: user2
givenname: user2
sn: user2
uid: user2
uidnumber: 1001
gidnumber: 100
userpassword: user2
EOF

$ ldapmodify -x -a -H ldap:// -D &quot;cn=admin,dc=k8s,dc=com&quot; -w password -f users.ldif
adding new entry &quot;uid=user1,ou=People,dc=k8s,dc=com&quot;

adding new entry &quot;uid=user2,ou=People,dc=k8s,dc=com&quot;
</code></pre>
<p>這邊可以登入 phpLDAPadmin 查看，結果如以下所示：<br><img src="/images/k8s-ldap/ldap-entry.png" alt=""></p>
<p>確認沒問題後，將 User dump 至一個文字檔案中：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; users.txt
dn: uid=user1,ou=People,dc=k8s,dc=com
dn: uid=user2,ou=People,dc=k8s,dc=com
EOF
</code></pre>
<blockquote>
<p>這邊偷懶直接用 cat。</p>
</blockquote>
<p>執行以下腳本來更新每個 LDAP User 的 kubernetesToken：</p>
<pre><code class="sh">$ while read -r user; do
fname=$(echo $user | grep -E -o &quot;uid=[a-z0-9]+&quot; | cut -d&quot;=&quot; -f2)
token=$(dd if=/dev/urandom bs=128 count=1 2&gt;/dev/null | base64 | tr -d &quot;=+/&quot; | dd bs=32 count=1 2&gt;/dev/null)
cat &lt;&lt; EOF &gt; &quot;${fname}.ldif&quot;
$user
changetype: modify
add: objectClass
objectclass: kubernetesAuthenticationObject
-
add: kubernetesToken
kubernetesToken: $token
EOF

ldapmodify -a -H ldapi:/// -D &quot;cn=admin,dc=k8s,dc=com&quot; -w password  -f &quot;${fname}.ldif&quot;
done &lt; users.txt

# output
Enter LDAP Password:
modifying entry &quot;uid=user1,ou=Users,dc=k8s,dc=com&quot;

Enter LDAP Password:
modifying entry &quot;uid=user2,ou=Users,dc=k8s,dc=com&quot;
</code></pre>
<h2 id="部署-Kubernetes-LDAP"><a href="#部署-Kubernetes-LDAP" class="headerlink" title="部署 Kubernetes LDAP"></a>部署 Kubernetes LDAP</h2><p>當 Kubernetes 環境建立完成後，首先進入<code>k8s-m1</code>節點，透過 git 取得 kube-ldap-authn 原始碼專案：</p>
<pre><code class="sh">$ git clone https://github.com/kairen/kube-ldap-authn.git
$ cd kube-ldap-authn
</code></pre>
<blockquote>
<p>若想使用 Go 語言實作的版本，可以參考 <a href="https://github.com/kairen/kube-ldap-webhook" target="_blank" rel="noopener">kube-ldap-webhook</a>.</p>
</blockquote>
<p>新增一個<code>config.py</code>檔案來提供相關設定內容：</p>
<pre><code class="sh">LDAP_URL=&#39;ldap://192.16.35.20/ ldap://192.16.35.20&#39;
LDAP_START_TLS = False
LDAP_BIND_DN = &#39;cn=admin,dc=k8s,dc=com&#39;
LDAP_BIND_PASSWORD = &#39;password&#39;
LDAP_USER_NAME_ATTRIBUTE = &#39;uid&#39;
LDAP_USER_UID_ATTRIBUTE = &#39;uidNumber&#39;
LDAP_USER_SEARCH_BASE = &#39;ou=People,dc=k8s,dc=com&#39;
LDAP_USER_SEARCH_FILTER = &quot;(&amp;(kubernetesToken={token}))&quot;
LDAP_GROUP_NAME_ATTRIBUTE = &#39;cn&#39;
LDAP_GROUP_SEARCH_BASE = &#39;ou=Groups,dc=k8s,dc=com&#39;
LDAP_GROUP_SEARCH_FILTER = &#39;(|(&amp;(objectClass=posixGroup)(memberUid={username}))(&amp;(member={dn})(objectClass=groupOfNames)))&#39;
</code></pre>
<blockquote>
<p>變數詳細說明可以參考 <a href="https://github.com/kairen/kube-ldap-authn/blob/master/config.py.example" target="_blank" rel="noopener">Config example</a></p>
</blockquote>
<p>建立 kube-ldap-authn secret 來提供給 pod 使用，並部署 kube-ldap-authn pod 到所有 master 節點上：</p>
<pre><code class="sh">$ kubectl -n kube-system create secret generic ldap-authn-config --from-file=config.py=config.py
$ kubectl create -f daemonset.yaml
$ kubectl -n kube-system get po -l app=kube-ldap-authn -o wide
NAME                    READY     STATUS    RESTARTS   AGE       IP             NODE
kube-ldap-authn-sx994   1/1       Running   0          13s       192.16.35.11   k8s-m1
</code></pre>
<p>這邊若成功部署的話，可以用 curl 進行測試：</p>
<pre><code class="sh">$ curl -X POST -H &quot;Content-Type: application/json&quot; \
    -d &#39;{&quot;apiVersion&quot;: &quot;authentication.k8s.io/v1beta1&quot;, &quot;kind&quot;: &quot;TokenReview&quot;,  &quot;spec&quot;: {&quot;token&quot;: &quot;&lt;LDAP_K8S_TOKEN&gt;&quot;}}&#39; \
    http://localhost:8087/authn

# output
{
  &quot;apiVersion&quot;: &quot;authentication.k8s.io/v1beta1&quot;,
  &quot;kind&quot;: &quot;TokenReview&quot;,
  &quot;status&quot;: {
    &quot;authenticated&quot;: true,
    &quot;user&quot;: {
      &quot;groups&quot;: [
        &quot;kubernetes&quot;
      ],
      &quot;uid&quot;: &quot;1000&quot;,
      &quot;username&quot;: &quot;user1&quot;
    }
  }
}
</code></pre>
<p>在所有<code>master</code>節點上新增一個名稱為<code>/srv/kubernetes/webhook-authn</code>的檔案，並加入以下內容：</p>
<pre><code class="sh">$ mkdir /srv/kubernetes
$ cat &lt;&lt;EOF &gt; /srv/kubernetes/webhook-authn
clusters:
  - name: ldap-authn
    cluster:
      server: http://localhost:8087/authn
users:
  - name: apiserver
current-context: webhook
contexts:
- context:
    cluster: ldap-authn
    user: apiserver
  name: webhook
EOF
</code></pre>
<p>修改所有<code>master</code>節點上的<code>kube-apiserver.yaml</code> Static Pod 檔案，該檔案會存在於<code>/etc/kubernetes/manifests</code>目錄中，請修改加入以下內容：</p>
<pre><code class="yaml">...
spec:
  containers:
  - command:
    ...
    - --runtime-config=authentication.k8s.io/v1beta1=true
    - --authentication-token-webhook-config-file=/srv/kubernetes/webhook-authn
    - --authentication-token-webhook-cache-ttl=5m
    volumeMounts:
      ...
    - mountPath: /srv/kubernetes/webhook-authn
      name: webhook-authn
      readOnly: true
  volumes:
    ...
  - hostPath:
      path: /srv/kubernetes/webhook-authn
      type: File
    name: webhook-authn
</code></pre>
<blockquote>
<p>這邊<code>...</code>表示已存在的內容，請不要刪除與變更。這邊也可以用 kubeadmconfig 來設定，請參考 <a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file" target="_blank" rel="noopener">Using kubeadm init with a configuration file</a>。</p>
</blockquote>
<h2 id="測試功能"><a href="#測試功能" class="headerlink" title="測試功能"></a>測試功能</h2><p>首先進入<code>k8s-m1</code>，建立一個綁定在 user1 namespace 的唯讀 Role 與 RoleBinding：</p>
<pre><code class="sh">$ kubectl create ns user1

# 建立 Role
$ cat &lt;&lt;EOF | kubectl create -f -
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: readonly-role
  namespace: user1
rules:
- apiGroups: [&quot;&quot;]
  resources: [&quot;pods&quot;]
  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]
EOF

# 建立 RoleBinding
$ cat &lt;&lt;EOF | kubectl create -f -
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: readonly-role-binding
  namespace: user1
subjects:
- kind: Group
  name: kubernetes
  apiGroup: &quot;&quot;
roleRef:
  kind: Role
  name: readonly-role
  apiGroup: &quot;&quot;
EOF
</code></pre>
<blockquote>
<p>注意!!這邊的<code>Group</code>是 LDAP 中的 Group。</p>
</blockquote>
<p>在任意台 Kubernetes client 端設定 Kubeconfig 來存取叢集，這邊直接在<code>k8s-m1</code>進行：</p>
<pre><code class="sh">$ cd
$ kubectl config set-credentials user1 --kubeconfig=.kube/config --token=&lt;user-ldap-token&gt;
$ kubectl config set-context user1-context \
    --kubeconfig=.kube/config \
    --cluster=kubernetes \
    --namespace=user1 --user=user1
</code></pre>
<p>接著透過 kubeclt 來測試權限是否正確設定：</p>
<pre><code class="sh">$ kubectl --context=user1-context get po
No resources found

$ kubectl --context=user1-context run nginx --image nginx --port 80
Error from server (Forbidden): deployments.extensions is forbidden: User &quot;user1&quot; cannot create deployments.extensions in the namespace &quot;user1&quot;

$ kubectl --context=user1-context get po -n default
Error from server (Forbidden): pods is forbidden: User &quot;user1&quot; cannot list pods in the namespace &quot;default&quot;
</code></pre>
<h2 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h2><ul>
<li><a href="https://github.com/osixia/docker-openldap" target="_blank" rel="noopener">https://github.com/osixia/docker-openldap</a></li>
<li><a href="https://icicimov.github.io/blog/virtualization/Kubernetes-LDAP-Authentication/" target="_blank" rel="noopener">https://icicimov.github.io/blog/virtualization/Kubernetes-LDAP-Authentication/</a></li>
<li><a href="https://github.com/torchbox/kube-ldap-authn" target="_blank" rel="noopener">https://github.com/torchbox/kube-ldap-authn</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Kubernetes </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> LDAP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes v1.10.x HA 全手動苦工安裝教學(TL;DR)]]></title>
      <url>https://kairen.github.io/2018/04/05/kubernetes/deploy/manual-v1.10/</url>
      <content type="html"><![CDATA[<p>本篇延續過往<code>手動安裝方式</code>來部署 Kubernetes v1.10.x 版本的 High Availability 叢集，主要目的是學習 Kubernetes 安裝的一些元件關析與流程。若不想這麼累的話，可以參考 <a href="https://kubernetes.io/docs/getting-started-guides/" target="_blank" rel="noopener">Picking the Right Solution</a> 來選擇自己最喜歡的方式。</p>
<p>本次安裝的軟體版本：</p>
<ul>
<li>Kubernetes v1.10.0</li>
<li>CNI v0.6.0</li>
<li>Etcd v3.1.13</li>
<li>Calico v3.0.4</li>
<li>Docker CE latest version</li>
</ul>
<a id="more"></a>
<p><img src="/images/kube/kubernetes-aa-ha.png" alt=""></p>
<h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統可採用<code>Ubuntu 16.x</code>與<code>CentOS 7.x</code>：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Hostname</th>
<th>CPU</th>
<th>Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>192.16.35.11</td>
<td>k8s-m1</td>
<td>1</td>
<td>4G</td>
</tr>
<tr>
<td>192.16.35.12</td>
<td>k8s-m2</td>
<td>1</td>
<td>4G</td>
</tr>
<tr>
<td>192.16.35.13</td>
<td>k8s-m3</td>
<td>1</td>
<td>4G</td>
</tr>
<tr>
<td>192.16.35.14</td>
<td>k8s-n1</td>
<td>1</td>
<td>4G</td>
</tr>
<tr>
<td>192.16.35.15</td>
<td>k8s-n2</td>
<td>1</td>
<td>4G</td>
</tr>
<tr>
<td>192.16.35.16</td>
<td>k8s-n2</td>
<td>1</td>
<td>4G</td>
</tr>
</tbody>
</table>
<p>另外由所有 master 節點提供一組 VIP <code>192.16.35.10</code>。</p>
<blockquote>
<ul>
<li>這邊<code>m</code>為主要控制節點，<code>n</code>為應用程式工作節點。</li>
<li>所有操作全部用<code>root</code>使用者進行(方便用)，以 SRE 來說不推薦。</li>
<li>可以下載 <a href="https://kairen.github.io/files/manual-v1.10/Vagrantfile">Vagrantfile</a> 來建立 Virtualbox 虛擬機叢集。不過需要注意機器資源是否足夠。</li>
</ul>
</blockquote>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>開始安裝前需要確保以下條件已達成：</p>
<ul>
<li><code>所有節點</code>彼此網路互通，並且<code>k8s-m1</code> SSH 登入其他節點為 passwdless。</li>
<li>所有防火牆與 SELinux 已關閉。如 CentOS：</li>
</ul>
<pre><code class="sh">$ systemctl stop firewalld &amp;&amp; systemctl disable firewalld
$ setenforce 0
$ vim /etc/selinux/config
SELINUX=disabled
</code></pre>
<ul>
<li><code>所有節點</code>需要設定<code>/etc/hosts</code>解析到所有叢集主機。</li>
</ul>
<pre><code>...
192.16.35.11 k8s-m1
192.16.35.12 k8s-m2
192.16.35.13 k8s-m3
192.16.35.14 k8s-n1
192.16.35.15 k8s-n2
192.16.35.16 k8s-n3
</code></pre><ul>
<li><code>所有節點</code>需要安裝 Docker CE 版本的容器引擎：</li>
</ul>
<pre><code class="sh">$ curl -fsSL &quot;https://get.docker.com/&quot; | sh
</code></pre>
<blockquote>
<p>不管是在 <code>Ubuntu</code> 或 <code>CentOS</code> 都只需要執行該指令就會自動安裝最新版 Docker。<br>CentOS 安裝完成後，需要再執行以下指令：</p>
<pre><code class="sh">$ systemctl enable docker &amp;&amp; systemctl start docker
</code></pre>
</blockquote>
<ul>
<li><code>所有節點</code>需要設定<code>/etc/sysctl.d/k8s.conf</code>的系統參數。</li>
</ul>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

$ sysctl -p /etc/sysctl.d/k8s.conf
</code></pre>
<ul>
<li>Kubernetes v1.8+ 要求關閉系統 Swap，若不關閉則需要修改 kubelet 設定參數，在<code>所有節點</code>利用以下指令關閉：</li>
</ul>
<pre><code class="sh">$ swapoff -a &amp;&amp; sysctl -w vm.swappiness=0
</code></pre>
<blockquote>
<p>記得<code>/etc/fstab</code>也要註解掉<code>SWAP</code>掛載。</p>
</blockquote>
<ul>
<li>在<code>所有節點</code>下載 Kubernetes 二進制執行檔：</li>
</ul>
<pre><code class="sh">$ export KUBE_URL=&quot;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64&quot;
$ wget &quot;${KUBE_URL}/kubelet&quot; -O /usr/local/bin/kubelet
$ chmod +x /usr/local/bin/kubelet

# node 請忽略下載 kubectl
$ wget &quot;${KUBE_URL}/kubectl&quot; -O /usr/local/bin/kubectl
$ chmod +x /usr/local/bin/kubectl
</code></pre>
<ul>
<li>在<code>所有節點</code>下載 Kubernetes CNI 二進制檔案：</li>
</ul>
<pre><code class="sh">$ mkdir -p /opt/cni/bin &amp;&amp; cd /opt/cni/bin
$ export CNI_URL=&quot;https://github.com/containernetworking/plugins/releases/download&quot;
$ wget -qO- --show-progress &quot;${CNI_URL}/v0.6.0/cni-plugins-amd64-v0.6.0.tgz&quot; | tar -zx
</code></pre>
<ul>
<li>在<code>k8s-m1</code>需要安裝<code>CFSSL</code>工具，這將會用來建立 TLS Certificates。</li>
</ul>
<pre><code class="sh">$ export CFSSL_URL=&quot;https://pkg.cfssl.org/R1.2&quot;
$ wget &quot;${CFSSL_URL}/cfssl_linux-amd64&quot; -O /usr/local/bin/cfssl
$ wget &quot;${CFSSL_URL}/cfssljson_linux-amd64&quot; -O /usr/local/bin/cfssljson
$ chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson
</code></pre>
<h2 id="建立叢集-CA-keys-與-Certificates"><a href="#建立叢集-CA-keys-與-Certificates" class="headerlink" title="建立叢集 CA keys 與 Certificates"></a>建立叢集 CA keys 與 Certificates</h2><p>在這個部分，將需要產生多個元件的 Certificates，這包含 Etcd、Kubernetes 元件等，並且每個叢集都會有一個根數位憑證認證機構(Root Certificate Authority)被用在認證 API Server 與 Kubelet 端的憑證。</p>
<blockquote>
<p>P.S. 這邊要注意 CA JSON 檔的<code>CN(Common Name)</code>與<code>O(Organization)</code>等內容是會影響 Kubernetes 元件認證的。</p>
</blockquote>
<h3 id="Etcd"><a href="#Etcd" class="headerlink" title="Etcd"></a>Etcd</h3><p>首先在<code>k8s-m1</code>建立<code>/etc/etcd/ssl</code>資料夾，然後進入目錄完成以下操作。</p>
<pre><code class="sh">$ mkdir -p /etc/etcd/ssl &amp;&amp; cd /etc/etcd/ssl
$ export PKI_URL=&quot;https://kairen.github.io/files/manual-v1.10/pki&quot;
</code></pre>
<p>下載<code>ca-config.json</code>與<code>etcd-ca-csr.json</code>檔案，並從 CSR json 產生 CA keys 與 Certificate：</p>
<pre><code class="sh">$ wget &quot;${PKI_URL}/ca-config.json&quot; &quot;${PKI_URL}/etcd-ca-csr.json&quot;
$ cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare etcd-ca
</code></pre>
<p>下載<code>etcd-csr.json</code>檔案，並產生 Etcd 證書：</p>
<pre><code class="sh">$ wget &quot;${PKI_URL}/etcd-csr.json&quot;
$ cfssl gencert \
  -ca=etcd-ca.pem \
  -ca-key=etcd-ca-key.pem \
  -config=ca-config.json \
  -hostname=127.0.0.1,192.16.35.11,192.16.35.12,192.16.35.13 \
  -profile=kubernetes \
  etcd-csr.json | cfssljson -bare etcd
</code></pre>
<blockquote>
<p><code>-hostname</code>需修改成所有 masters 節點。</p>
</blockquote>
<p>完成後刪除不必要檔案：</p>
<pre><code class="sh">$ rm -rf *.json *.csr
</code></pre>
<p>確認<code>/etc/etcd/ssl</code>有以下檔案：</p>
<pre><code class="sh">$ ls /etc/etcd/ssl
etcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem
</code></pre>
<p>複製相關檔案至其他 Etcd 節點，這邊為所有<code>master</code>節點：</p>
<pre><code class="sh">$ for NODE in k8s-m2 k8s-m3; do
    echo &quot;--- $NODE ---&quot;
    ssh ${NODE} &quot;mkdir -p /etc/etcd/ssl&quot;
    for FILE in etcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem; do
      scp /etc/etcd/ssl/${FILE} ${NODE}:/etc/etcd/ssl/${FILE}
    done
  done
</code></pre>
<h3 id="Kubernetes"><a href="#Kubernetes" class="headerlink" title="Kubernetes"></a>Kubernetes</h3><p>在<code>k8s-m1</code>建立<code>pki</code>資料夾，然後進入目錄完成以下章節操作。</p>
<pre><code class="sh">$ mkdir -p /etc/kubernetes/pki &amp;&amp; cd /etc/kubernetes/pki
$ export PKI_URL=&quot;https://kairen.github.io/files/manual-v1.10/pki&quot;
$ export KUBE_APISERVER=&quot;https://192.16.35.10:6443&quot;
</code></pre>
<p>下載<code>ca-config.json</code>與<code>ca-csr.json</code>檔案，並產生 CA 金鑰：</p>
<pre><code class="sh">$ wget &quot;${PKI_URL}/ca-config.json&quot; &quot;${PKI_URL}/ca-csr.json&quot;
$ cfssl gencert -initca ca-csr.json | cfssljson -bare ca
$ ls ca*.pem
ca-key.pem  ca.pem
</code></pre>
<h4 id="API-Server-Certificate"><a href="#API-Server-Certificate" class="headerlink" title="API Server Certificate"></a>API Server Certificate</h4><p>下載<code>apiserver-csr.json</code>檔案，並產生 kube-apiserver 憑證：</p>
<pre><code class="sh">$ wget &quot;${PKI_URL}/apiserver-csr.json&quot;
$ cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=10.96.0.1,192.16.35.10,127.0.0.1,kubernetes.default \
  -profile=kubernetes \
  apiserver-csr.json | cfssljson -bare apiserver

$ ls apiserver*.pem
apiserver-key.pem  apiserver.pem
</code></pre>
<blockquote>
<ul>
<li>這邊<code>-hostname</code>的<code>10.96.0.1</code>是 Cluster IP 的 Kubernetes 端點;</li>
<li><code>192.16.35.10</code>為虛擬 IP 位址(VIP);</li>
<li><code>kubernetes.default</code>為 Kubernetes DN。</li>
</ul>
</blockquote>
<h4 id="Front-Proxy-Certificate"><a href="#Front-Proxy-Certificate" class="headerlink" title="Front Proxy Certificate"></a>Front Proxy Certificate</h4><p>下載<code>front-proxy-ca-csr.json</code>檔案，並產生 Front Proxy CA 金鑰，Front Proxy 主要是用在 API aggregator 上:</p>
<pre><code class="sh">$ wget &quot;${PKI_URL}/front-proxy-ca-csr.json&quot;
$ cfssl gencert \
  -initca front-proxy-ca-csr.json | cfssljson -bare front-proxy-ca

$ ls front-proxy-ca*.pem
front-proxy-ca-key.pem  front-proxy-ca.pem
</code></pre>
<p>下載<code>front-proxy-client-csr.json</code>檔案，並產生 front-proxy-client 證書：</p>
<pre><code class="sh">$ wget &quot;${PKI_URL}/front-proxy-client-csr.json&quot;
$ cfssl gencert \
  -ca=front-proxy-ca.pem \
  -ca-key=front-proxy-ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  front-proxy-client-csr.json | cfssljson -bare front-proxy-client

$ ls front-proxy-client*.pem
front-proxy-client-key.pem  front-proxy-client.pem
</code></pre>
<h4 id="Admin-Certificate"><a href="#Admin-Certificate" class="headerlink" title="Admin Certificate"></a>Admin Certificate</h4><p>下載<code>admin-csr.json</code>檔案，並產生 admin certificate 憑證：</p>
<pre><code class="sh">$ wget &quot;${PKI_URL}/admin-csr.json&quot;
$ cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  admin-csr.json | cfssljson -bare admin

$ ls admin*.pem
admin-key.pem  admin.pem
</code></pre>
<p>接著透過以下指令產生名稱為 <code>admin.conf</code> 的 kubeconfig 檔：</p>
<pre><code class="sh"># admin set cluster
$ kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=${KUBE_APISERVER} \
    --kubeconfig=../admin.conf

# admin set credentials
$ kubectl config set-credentials kubernetes-admin \
    --client-certificate=admin.pem \
    --client-key=admin-key.pem \
    --embed-certs=true \
    --kubeconfig=../admin.conf

# admin set context
$ kubectl config set-context kubernetes-admin@kubernetes \
    --cluster=kubernetes \
    --user=kubernetes-admin \
    --kubeconfig=../admin.conf

# admin set default context
$ kubectl config use-context kubernetes-admin@kubernetes \
    --kubeconfig=../admin.conf
</code></pre>
<h4 id="Controller-Manager-Certificate"><a href="#Controller-Manager-Certificate" class="headerlink" title="Controller Manager Certificate"></a>Controller Manager Certificate</h4><p>下載<code>manager-csr.json</code>檔案，並產生 kube-controller-manager certificate 憑證：</p>
<pre><code class="sh">$ wget &quot;${PKI_URL}/manager-csr.json&quot;
$ cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  manager-csr.json | cfssljson -bare controller-manager

$ ls controller-manager*.pem
controller-manager-key.pem  controller-manager.pem
</code></pre>
<blockquote>
<p>若節點 IP 不同，需要修改<code>manager-csr.json</code>的<code>hosts</code>。</p>
</blockquote>
<p>接著透過以下指令產生名稱為<code>controller-manager.conf</code>的 kubeconfig 檔：</p>
<pre><code class="sh"># controller-manager set cluster
$ kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=${KUBE_APISERVER} \
    --kubeconfig=../controller-manager.conf

# controller-manager set credentials
$ kubectl config set-credentials system:kube-controller-manager \
    --client-certificate=controller-manager.pem \
    --client-key=controller-manager-key.pem \
    --embed-certs=true \
    --kubeconfig=../controller-manager.conf

# controller-manager set context
$ kubectl config set-context system:kube-controller-manager@kubernetes \
    --cluster=kubernetes \
    --user=system:kube-controller-manager \
    --kubeconfig=../controller-manager.conf

# controller-manager set default context
$ kubectl config use-context system:kube-controller-manager@kubernetes \
    --kubeconfig=../controller-manager.conf
</code></pre>
<h4 id="Scheduler-Certificate"><a href="#Scheduler-Certificate" class="headerlink" title="Scheduler Certificate"></a>Scheduler Certificate</h4><p>下載<code>scheduler-csr.json</code>檔案，並產生 kube-scheduler certificate 憑證：</p>
<pre><code class="sh">$ wget &quot;${PKI_URL}/scheduler-csr.json&quot;
$ cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  scheduler-csr.json | cfssljson -bare scheduler

$ ls scheduler*.pem
scheduler-key.pem  scheduler.pem
</code></pre>
<blockquote>
<p>若節點 IP 不同，需要修改<code>scheduler-csr.json</code>的<code>hosts</code>。</p>
</blockquote>
<p>接著透過以下指令產生名稱為 <code>scheduler.conf</code> 的 kubeconfig 檔：</p>
<pre><code class="sh"># scheduler set cluster
$ kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=${KUBE_APISERVER} \
    --kubeconfig=../scheduler.conf

# scheduler set credentials
$ kubectl config set-credentials system:kube-scheduler \
    --client-certificate=scheduler.pem \
    --client-key=scheduler-key.pem \
    --embed-certs=true \
    --kubeconfig=../scheduler.conf

# scheduler set context
$ kubectl config set-context system:kube-scheduler@kubernetes \
    --cluster=kubernetes \
    --user=system:kube-scheduler \
    --kubeconfig=../scheduler.conf

# scheduler use default context
$ kubectl config use-context system:kube-scheduler@kubernetes \
    --kubeconfig=../scheduler.conf
</code></pre>
<h4 id="Master-Kubelet-Certificate"><a href="#Master-Kubelet-Certificate" class="headerlink" title="Master Kubelet Certificate"></a>Master Kubelet Certificate</h4><p>接著在所有<code>k8s-m1</code>節點下載<code>kubelet-csr.json</code>檔案，並產生憑證：</p>
<pre><code class="sh">$ wget &quot;${PKI_URL}/kubelet-csr.json&quot;
$ for NODE in k8s-m1 k8s-m2 k8s-m3; do
    echo &quot;--- $NODE ---&quot;
    cp kubelet-csr.json kubelet-$NODE-csr.json;
    sed -i &quot;s/\$NODE/$NODE/g&quot; kubelet-$NODE-csr.json;
    cfssl gencert \
      -ca=ca.pem \
      -ca-key=ca-key.pem \
      -config=ca-config.json \
      -hostname=$NODE \
      -profile=kubernetes \
      kubelet-$NODE-csr.json | cfssljson -bare kubelet-$NODE
  done

$ ls kubelet*.pem
kubelet-k8s-m1-key.pem  kubelet-k8s-m1.pem  kubelet-k8s-m2-key.pem  kubelet-k8s-m2.pem  kubelet-k8s-m3-key.pem  kubelet-k8s-m3.pem
</code></pre>
<blockquote>
<p>這邊需要依據節點修改<code>-hostname</code>與<code>$NODE</code>。</p>
</blockquote>
<p>完成後複製 kubelet 憑證至其他<code>master</code>節點：</p>
<pre><code class="sh">$ for NODE in k8s-m2 k8s-m3; do
    echo &quot;--- $NODE ---&quot;
    ssh ${NODE} &quot;mkdir -p /etc/kubernetes/pki&quot;
    for FILE in kubelet-$NODE-key.pem kubelet-$NODE.pem ca.pem; do
      scp /etc/kubernetes/pki/${FILE} ${NODE}:/etc/kubernetes/pki/${FILE}
    done
  done
</code></pre>
<p>接著執行以下指令產生名稱為<code>kubelet.conf</code>的 kubeconfig 檔：</p>
<pre><code class="sh">$ for NODE in k8s-m1 k8s-m2 k8s-m3; do
    echo &quot;--- $NODE ---&quot;
    ssh ${NODE} &quot;cd /etc/kubernetes/pki &amp;&amp; \
      kubectl config set-cluster kubernetes \
        --certificate-authority=ca.pem \
        --embed-certs=true \
        --server=${KUBE_APISERVER} \
        --kubeconfig=../kubelet.conf &amp;&amp; \
      kubectl config set-cluster kubernetes \
        --certificate-authority=ca.pem \
        --embed-certs=true \
        --server=${KUBE_APISERVER} \
        --kubeconfig=../kubelet.conf &amp;&amp; \
      kubectl config set-credentials system:node:${NODE} \
        --client-certificate=kubelet-${NODE}.pem \
        --client-key=kubelet-${NODE}-key.pem \
        --embed-certs=true \
        --kubeconfig=../kubelet.conf &amp;&amp; \
      kubectl config set-context system:node:${NODE}@kubernetes \
        --cluster=kubernetes \
        --user=system:node:${NODE} \
        --kubeconfig=../kubelet.conf &amp;&amp; \
      kubectl config use-context system:node:${NODE}@kubernetes \
        --kubeconfig=../kubelet.conf &amp;&amp; \
      rm kubelet-${NODE}.pem kubelet-${NODE}-key.pem&quot;
  done
</code></pre>
<h4 id="Service-Account-Key"><a href="#Service-Account-Key" class="headerlink" title="Service Account Key"></a>Service Account Key</h4><p>Service account 不是透過 CA 進行認證，因此不要透過 CA 來做 Service account key 的檢查，這邊建立一組 Private 與 Public 金鑰提供給 Service account key 使用：</p>
<pre><code class="sh">$ openssl genrsa -out sa.key 2048
$ openssl rsa -in sa.key -pubout -out sa.pub
$ ls sa.*
sa.key  sa.pub
</code></pre>
<h4 id="刪除不必要檔案"><a href="#刪除不必要檔案" class="headerlink" title="刪除不必要檔案"></a>刪除不必要檔案</h4><p>所有資訊準備完成後，就可以將一些不必要檔案刪除：</p>
<pre><code class="sh">$ rm -rf *.json *.csr scheduler*.pem controller-manager*.pem admin*.pem kubelet*.pem
</code></pre>
<h4 id="複製檔案至其他節點"><a href="#複製檔案至其他節點" class="headerlink" title="複製檔案至其他節點"></a>複製檔案至其他節點</h4><p>複製憑證檔案至其他<code>master</code>節點：</p>
<pre><code class="sh">$ for NODE in k8s-m2 k8s-m3; do
    echo &quot;--- $NODE ---&quot;
    for FILE in $(ls /etc/kubernetes/pki/); do
      scp /etc/kubernetes/pki/${FILE} ${NODE}:/etc/kubernetes/pki/${FILE}
    done
  done
</code></pre>
<p>複製 Kubernetes config 檔案至其他<code>master</code>節點：</p>
<pre><code class="sh">$ for NODE in k8s-m2 k8s-m3; do
    echo &quot;--- $NODE ---&quot;
    for FILE in admin.conf controller-manager.conf scheduler.conf; do
      scp /etc/kubernetes/${FILE} ${NODE}:/etc/kubernetes/${FILE}
    done
  done
</code></pre>
<h2 id="Kubernetes-Masters"><a href="#Kubernetes-Masters" class="headerlink" title="Kubernetes Masters"></a>Kubernetes Masters</h2><p>本部分將說明如何建立與設定 Kubernetes Master 角色，過程中會部署以下元件：</p>
<ul>
<li><strong>kube-apiserver</strong>：提供 REST APIs，包含授權、認證與狀態儲存等。</li>
<li><strong>kube-controller-manager</strong>：負責維護叢集的狀態，如自動擴展，滾動更新等。</li>
<li><strong>kube-scheduler</strong>：負責資源排程，依據預定的排程策略將 Pod 分配到對應節點上。</li>
<li><strong>Etcd</strong>：儲存叢集所有狀態的 Key/Value 儲存系統。</li>
<li><strong>HAProxy</strong>：提供負載平衡器。</li>
<li><strong>Keepalived</strong>：提供虛擬網路位址(VIP)。</li>
</ul>
<h3 id="部署與設定"><a href="#部署與設定" class="headerlink" title="部署與設定"></a>部署與設定</h3><p>首先在<code>所有 master 節點</code>下載部署元件的 YAML 檔案，這邊不採用二進制執行檔與 Systemd 來管理這些元件，全部採用 <a href="https://kubernetes.io/docs/tasks/administer-cluster/static-pod/" target="_blank" rel="noopener">Static Pod</a> 來達成。這邊將檔案下載至<code>/etc/kubernetes/manifests</code>目錄：</p>
<pre><code class="sh">$ export CORE_URL=&quot;https://kairen.github.io/files/manual-v1.10/master&quot;
$ mkdir -p /etc/kubernetes/manifests &amp;&amp; cd /etc/kubernetes/manifests
$ for FILE in kube-apiserver kube-controller-manager kube-scheduler haproxy keepalived etcd etcd.config; do
    wget &quot;${CORE_URL}/${FILE}.yml.conf&quot; -O ${FILE}.yml
    if [ ${FILE} == &quot;etcd.config&quot; ]; then
      mv etcd.config.yml /etc/etcd/etcd.config.yml
      sed -i &quot;s/\${HOSTNAME}/${HOSTNAME}/g&quot; /etc/etcd/etcd.config.yml
      sed -i &quot;s/\${PUBLIC_IP}/$(hostname -i)/g&quot; /etc/etcd/etcd.config.yml
    fi
  done

$ ls /etc/kubernetes/manifests
etcd.yml  haproxy.yml  keepalived.yml  kube-apiserver.yml  kube-controller-manager.yml  kube-scheduler.yml
</code></pre>
<blockquote>
<ul>
<li>若<code>IP</code>與教學設定不同的話，請記得修改 YAML 檔案。</li>
<li>kube-apiserver 中的 <code>NodeRestriction</code> 請參考 <a href="https://kubernetes.io/docs/admin/authorization/node/" target="_blank" rel="noopener">Using Node Authorization</a>。</li>
</ul>
</blockquote>
<p>產生一個用來加密 Etcd 的 Key：</p>
<pre><code class="sh">$ head -c 32 /dev/urandom | base64
SUpbL4juUYyvxj3/gonV5xVEx8j769/99TSAf8YT/sQ=
</code></pre>
<blockquote>
<p>注意每台<code>master</code>節點需要用一樣的 Key。</p>
</blockquote>
<p>在<code>/etc/kubernetes/</code>目錄下，建立<code>encryption.yml</code>的加密 YAML 檔案：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/kubernetes/encryption.yml
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: SUpbL4juUYyvxj3/gonV5xVEx8j769/99TSAf8YT/sQ=
      - identity: {}
EOF
</code></pre>
<blockquote>
<p>Etcd 資料加密可參考這篇 <a href="https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/" target="_blank" rel="noopener">Encrypting data at rest</a>。</p>
</blockquote>
<p>在<code>/etc/kubernetes/</code>目錄下，建立<code>audit-policy.yml</code>的進階稽核策略 YAML 檔：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/kubernetes/audit-policy.yml
apiVersion: audit.k8s.io/v1beta1
kind: Policy
rules:
- level: Metadata
EOF
</code></pre>
<blockquote>
<p>Audit Policy 請參考這篇 <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/" target="_blank" rel="noopener">Auditing</a>。</p>
</blockquote>
<p>下載<code>haproxy.cfg</code>檔案來提供給 HAProxy 容器使用：</p>
<pre><code class="sh">$ mkdir -p /etc/haproxy/
$ wget &quot;${CORE_URL}/haproxy.cfg&quot; -O /etc/haproxy/haproxy.cfg
</code></pre>
<blockquote>
<p>若與本教學 IP 不同的話，請記得修改設定檔。</p>
</blockquote>
<p>下載<code>kubelet.service</code>相關檔案來管理 kubelet：</p>
<pre><code class="sh">$ mkdir -p /etc/systemd/system/kubelet.service.d
$ wget &quot;${CORE_URL}/kubelet.service&quot; -O /lib/systemd/system/kubelet.service
$ wget &quot;${CORE_URL}/10-kubelet.conf&quot; -O /etc/systemd/system/kubelet.service.d/10-kubelet.conf
</code></pre>
<blockquote>
<p>若 cluster <code>dns</code>或<code>domain</code>有改變的話，需要修改<code>10-kubelet.conf</code>。</p>
</blockquote>
<p>最後建立 var 存放資訊，然後啟動 kubelet 服務:</p>
<pre><code class="sh">$ mkdir -p /var/lib/kubelet /var/log/kubernetes /var/lib/etcd
$ systemctl enable kubelet.service &amp;&amp; systemctl start kubelet.service
</code></pre>
<p>完成後會需要一段時間來下載映像檔與啟動元件，可以利用該指令來監看：</p>
<pre><code class="sh">$ watch netstat -ntlp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      10344/kubelet
tcp        0      0 127.0.0.1:10251         0.0.0.0:*               LISTEN      11324/kube-schedule
tcp        0      0 0.0.0.0:6443            0.0.0.0:*               LISTEN      11416/haproxy
tcp        0      0 127.0.0.1:10252         0.0.0.0:*               LISTEN      11235/kube-controll
tcp        0      0 0.0.0.0:9090            0.0.0.0:*               LISTEN      11416/haproxy
tcp6       0      0 :::2379                 :::*                    LISTEN      10479/etcd
tcp6       0      0 :::2380                 :::*                    LISTEN      10479/etcd
tcp6       0      0 :::10255                :::*                    LISTEN      10344/kubelet
tcp6       0      0 :::5443                 :::*                    LISTEN      11295/kube-apiserve
</code></pre>
<blockquote>
<p>若看到以上資訊表示服務正常啟動，若發生問題可以用<code>docker</code>指令來查看。</p>
</blockquote>
<h3 id="驗證叢集"><a href="#驗證叢集" class="headerlink" title="驗證叢集"></a>驗證叢集</h3><p>完成後，在任意一台<code>master</code>節點複製 admin kubeconfig 檔案，並透過簡單指令驗證：</p>
<pre><code class="sh">$ cp /etc/kubernetes/admin.conf ~/.kube/config
$ kubectl get cs
NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-2               Healthy   {&quot;health&quot;: &quot;true&quot;}
etcd-1               Healthy   {&quot;health&quot;: &quot;true&quot;}
etcd-0               Healthy   {&quot;health&quot;: &quot;true&quot;}

$ kubectl get node
NAME      STATUS     ROLES     AGE       VERSION
k8s-m1    NotReady   master    52s       v1.10.0
k8s-m2    NotReady   master    51s       v1.10.0
k8s-m3    NotReady   master    50s       v1.10.0

$ kubectl -n kube-system get po
NAME                             READY     STATUS    RESTARTS   AGE
etcd-k8s-m1                      1/1       Running   0          7s
etcd-k8s-m2                      1/1       Running   0          57s
haproxy-k8s-m3                   1/1       Running   0          1m
...
</code></pre>
<p>接著確認服務能夠執行 logs 等指令：</p>
<pre><code class="sh">$ kubectl -n kube-system logs -f kube-scheduler-k8s-m2
Error from server (Forbidden): Forbidden (user=kube-apiserver, verb=get, resource=nodes, subresource=proxy) ( pods/log kube-scheduler-k8s-m2)
</code></pre>
<blockquote>
<p>這邊會發現出現 403 Forbidden 問題，這是因為 <code>kube-apiserver</code> user 並沒有 nodes 的資源存取權限，屬於正常。</p>
</blockquote>
<p>由於上述權限問題，必需建立一個<code>apiserver-to-kubelet-rbac.yml</code>來定義權限，以供對 Nodes 容器執行 logs、exec 等指令。在任意一台<code>master</code>節點執行以下指令：</p>
<pre><code class="sh">$ kubectl apply -f &quot;${CORE_URL}/apiserver-to-kubelet-rbac.yml.conf&quot;
clusterrole.rbac.authorization.k8s.io &quot;system:kube-apiserver-to-kubelet&quot; configured
clusterrolebinding.rbac.authorization.k8s.io &quot;system:kube-apiserver&quot; configured

# 測試 logs
$ kubectl -n kube-system logs -f kube-scheduler-k8s-m2
...
I0403 02:30:36.375935       1 server.go:555] Version: v1.10.0
I0403 02:30:36.378208       1 server.go:574] starting healthz server on 127.0.0.1:10251
</code></pre>
<p>設定<code>master</code>節點允許 Taint：</p>
<pre><code class="sh">$ kubectl taint nodes node-role.kubernetes.io/master=&quot;&quot;:NoSchedule --all
node &quot;k8s-m1&quot; tainted
node &quot;k8s-m2&quot; tainted
node &quot;k8s-m3&quot; tainted
</code></pre>
<blockquote>
<p><a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/" target="_blank" rel="noopener">Taints and Tolerations</a>。</p>
</blockquote>
<h3 id="建立-TLS-Bootstrapping-RBAC-與-Secret"><a href="#建立-TLS-Bootstrapping-RBAC-與-Secret" class="headerlink" title="建立 TLS Bootstrapping RBAC 與 Secret"></a>建立 TLS Bootstrapping RBAC 與 Secret</h3><p>由於本次安裝啟用了 TLS 認證，因此每個節點的 kubelet 都必須使用 kube-apiserver 的 CA 的憑證後，才能與 kube-apiserver 進行溝通，而該過程需要手動針對每台節點單獨簽署憑證是一件繁瑣的事情，且一旦節點增加會延伸出管理不易問題; 而 TLS bootstrapping 目標就是解決該問題，透過讓 kubelet 先使用一個預定低權限使用者連接到 kube-apiserver，然後在對 kube-apiserver 申請憑證簽署，當授權 Token 一致時，Node 節點的 kubelet 憑證將由 kube-apiserver 動態簽署提供。具體作法可以參考 <a href="https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/" target="_blank" rel="noopener">TLS Bootstrapping</a> 與 <a href="https://kubernetes.io/docs/admin/bootstrap-tokens/" target="_blank" rel="noopener">Authenticating with Bootstrap Tokens</a>。</p>
<p>首先在<code>k8s-m1</code>建立一個變數來產生<code>BOOTSTRAP_TOKEN</code>，並建立<code>bootstrap-kubelet.conf</code>的 Kubernetes config 檔：</p>
<pre><code class="sh">$ cd /etc/kubernetes/pki
$ export TOKEN_ID=$(openssl rand 3 -hex)
$ export TOKEN_SECRET=$(openssl rand 8 -hex)
$ export BOOTSTRAP_TOKEN=${TOKEN_ID}.${TOKEN_SECRET}
$ export KUBE_APISERVER=&quot;https://192.16.35.10:6443&quot;

# bootstrap set cluster
$ kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=${KUBE_APISERVER} \
    --kubeconfig=../bootstrap-kubelet.conf

# bootstrap set credentials
$ kubectl config set-credentials tls-bootstrap-token-user \
    --token=${BOOTSTRAP_TOKEN} \
    --kubeconfig=../bootstrap-kubelet.conf

# bootstrap set context
$ kubectl config set-context tls-bootstrap-token-user@kubernetes \
    --cluster=kubernetes \
    --user=tls-bootstrap-token-user \
    --kubeconfig=../bootstrap-kubelet.conf

# bootstrap use default context
$ kubectl config use-context tls-bootstrap-token-user@kubernetes \
    --kubeconfig=../bootstrap-kubelet.conf
</code></pre>
<blockquote>
<p>若想要用手動簽署憑證來進行授權的話，可以參考 <a href="https://kubernetes.io/docs/concepts/cluster-administration/certificates/" target="_blank" rel="noopener">Certificate</a>。</p>
</blockquote>
<p>接著在<code>k8s-m1</code>建立 TLS bootstrap secret 來提供自動簽證使用：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF | kubectl create -f -
apiVersion: v1
kind: Secret
metadata:
  name: bootstrap-token-${TOKEN_ID}
  namespace: kube-system
type: bootstrap.kubernetes.io/token
stringData:
  token-id: ${TOKEN_ID}
  token-secret: ${TOKEN_SECRET}
  usage-bootstrap-authentication: &quot;true&quot;
  usage-bootstrap-signing: &quot;true&quot;
  auth-extra-groups: system:bootstrappers:default-node-token
EOF

secret &quot;bootstrap-token-65a3a9&quot; created
</code></pre>
<p>在<code>k8s-m1</code>建立 TLS Bootstrap Autoapprove RBAC：</p>
<pre><code class="sh">$ kubectl apply -f &quot;${CORE_URL}/kubelet-bootstrap-rbac.yml.conf&quot;
clusterrolebinding.rbac.authorization.k8s.io &quot;kubelet-bootstrap&quot; created
clusterrolebinding.rbac.authorization.k8s.io &quot;node-autoapprove-bootstrap&quot; created
clusterrolebinding.rbac.authorization.k8s.io &quot;node-autoapprove-certificate-rotation&quot; created
</code></pre>
<h2 id="Kubernetes-Nodes"><a href="#Kubernetes-Nodes" class="headerlink" title="Kubernetes Nodes"></a>Kubernetes Nodes</h2><p>本部分將說明如何建立與設定 Kubernetes Node 角色，Node 是主要執行容器實例(Pod)的工作節點。</p>
<p>在開始部署前，先在<code>k8-m1</code>將需要用到的檔案複製到所有<code>node</code>節點上：</p>
<pre><code class="sh">$ cd /etc/kubernetes/pki
$ for NODE in k8s-n1 k8s-n2 k8s-n3; do
    echo &quot;--- $NODE ---&quot;
    ssh ${NODE} &quot;mkdir -p /etc/kubernetes/pki/&quot;
    ssh ${NODE} &quot;mkdir -p /etc/etcd/ssl&quot;
    # Etcd
    for FILE in etcd-ca.pem etcd.pem etcd-key.pem; do
      scp /etc/etcd/ssl/${FILE} ${NODE}:/etc/etcd/ssl/${FILE}
    done
    # Kubernetes
    for FILE in pki/ca.pem pki/ca-key.pem bootstrap-kubelet.conf; do
      scp /etc/kubernetes/${FILE} ${NODE}:/etc/kubernetes/${FILE}
    done
  done
</code></pre>
<h3 id="部署與設定-1"><a href="#部署與設定-1" class="headerlink" title="部署與設定"></a>部署與設定</h3><p>在每台<code>node</code>節點下載<code>kubelet.service</code>相關檔案來管理 kubelet：</p>
<pre><code class="sh">$ export CORE_URL=&quot;https://kairen.github.io/files/manual-v1.10/node&quot;
$ mkdir -p /etc/systemd/system/kubelet.service.d
$ wget &quot;${CORE_URL}/kubelet.service&quot; -O /lib/systemd/system/kubelet.service
$ wget &quot;${CORE_URL}/10-kubelet.conf&quot; -O /etc/systemd/system/kubelet.service.d/10-kubelet.conf
</code></pre>
<blockquote>
<p>若 cluster <code>dns</code>或<code>domain</code>有改變的話，需要修改<code>10-kubelet.conf</code>。</p>
</blockquote>
<p>最後建立 var 存放資訊，然後啟動 kubelet 服務:</p>
<pre><code class="sh">$ mkdir -p /var/lib/kubelet /var/log/kubernetes
$ systemctl enable kubelet.service &amp;&amp; systemctl start kubelet.service
</code></pre>
<h3 id="驗證叢集-1"><a href="#驗證叢集-1" class="headerlink" title="驗證叢集"></a>驗證叢集</h3><p>完成後，在任意一台<code>master</code>節點並透過簡單指令驗證：</p>
<pre><code class="sh">$ kubectl get csr
NAME                                                   AGE       REQUESTOR                 CONDITION
csr-bvz9l                                              11m       system:node:k8s-m1        Approved,Issued
csr-jwr8k                                              11m       system:node:k8s-m2        Approved,Issued
csr-q867w                                              11m       system:node:k8s-m3        Approved,Issued
node-csr-Y-FGvxZWJqI-8RIK_IrpgdsvjGQVGW0E4UJOuaU8ogk   17s       system:bootstrap:dca3e1   Approved,Issued
node-csr-cnX9T1xp1LdxVDc9QW43W0pYkhEigjwgceRshKuI82c   19s       system:bootstrap:dca3e1   Approved,Issued
node-csr-m7SBA9RAGCnsgYWJB-u2HoB2qLSfiQZeAxWFI2WYN7Y   18s       system:bootstrap:dca3e1   Approved,Issued

$ kubectl get nodes
NAME      STATUS     ROLES     AGE       VERSION
k8s-m1    NotReady   master    12m       v1.10.0
k8s-m2    NotReady   master    11m       v1.10.0
k8s-m3    NotReady   master    11m       v1.10.0
k8s-n1    NotReady   node      32s       v1.10.0
k8s-n2    NotReady   node      31s       v1.10.0
k8s-n3    NotReady   node      29s       v1.10.0
</code></pre>
<h2 id="Kubernetes-Core-Addons-部署"><a href="#Kubernetes-Core-Addons-部署" class="headerlink" title="Kubernetes Core Addons 部署"></a>Kubernetes Core Addons 部署</h2><p>當完成上面所有步驟後，接著需要部署一些插件，其中如<code>Kubernetes DNS</code>與<code>Kubernetes Proxy</code>等這種 Addons 是非常重要的。</p>
<h3 id="Kubernetes-Proxy"><a href="#Kubernetes-Proxy" class="headerlink" title="Kubernetes Proxy"></a>Kubernetes Proxy</h3><p><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/kube-proxy" target="_blank" rel="noopener">Kube-proxy</a> 是實現 Service 的關鍵插件，kube-proxy 會在每台節點上執行，然後監聽 API Server 的 Service 與 Endpoint 資源物件的改變，然後來依據變化執行 iptables 來實現網路的轉發。這邊我們會需要建議一個 DaemonSet 來執行，並且建立一些需要的 Certificates。</p>
<p>在<code>k8s-m1</code>下載<code>kube-proxy.yml</code>來建立 Kubernetes Proxy Addon：</p>
<pre><code class="sh">$ kubectl apply -f &quot;https://kairen.github.io/files/manual-v1.10/addon/kube-proxy.yml.conf&quot;
serviceaccount &quot;kube-proxy&quot; created
clusterrolebinding.rbac.authorization.k8s.io &quot;system:kube-proxy&quot; created
configmap &quot;kube-proxy&quot; created
daemonset.apps &quot;kube-proxy&quot; created

$ kubectl -n kube-system get po -o wide -l k8s-app=kube-proxy
NAME               READY     STATUS    RESTARTS   AGE       IP             NODE
kube-proxy-8j5w8   1/1       Running   0          29s       192.16.35.16   k8s-n3
kube-proxy-c4zvt   1/1       Running   0          29s       192.16.35.11   k8s-m1
kube-proxy-clpl6   1/1       Running   0          29s       192.16.35.12   k8s-m2
...
</code></pre>
<h3 id="Kubernetes-DNS"><a href="#Kubernetes-DNS" class="headerlink" title="Kubernetes DNS"></a>Kubernetes DNS</h3><p><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns" target="_blank" rel="noopener">Kube DNS</a> 是 Kubernetes 叢集內部 Pod 之間互相溝通的重要 Addon，它允許 Pod 可以透過 Domain Name 方式來連接 Service，其主要由 Kube DNS 與 Sky DNS 組合而成，透過 Kube DNS 監聽 Service 與 Endpoint 變化，來提供給 Sky DNS 資訊，已更新解析位址。</p>
<p>在<code>k8s-m1</code>下載<code>kube-proxy.yml</code>來建立 Kubernetes Proxy Addon：</p>
<pre><code class="sh">$ kubectl apply -f &quot;https://kairen.github.io/files/manual-v1.10/addon/kube-dns.yml.conf&quot;
serviceaccount &quot;kube-dns&quot; created
service &quot;kube-dns&quot; created
deployment.extensions &quot;kube-dns&quot; created

$ kubectl -n kube-system get po -l k8s-app=kube-dns
NAME                        READY     STATUS    RESTARTS   AGE
kube-dns-654684d656-zq5t8   0/3       Pending   0          1m
</code></pre>
<p>這邊會發現處於<code>Pending</code>狀態，是由於 Kubernetes Pod Network 還未建立完成，因此所有節點會處於<code>NotReady</code>狀態，而造成 Pod 無法被排程分配到指定節點上啟動，由於為了解決該問題，下節將說明如何建立 Pod Network。</p>
<h2 id="Calico-Network-安裝與設定"><a href="#Calico-Network-安裝與設定" class="headerlink" title="Calico Network 安裝與設定"></a>Calico Network 安裝與設定</h2><p>Calico 是一款純 Layer 3 的資料中心網路方案(不需要 Overlay 網路)，Calico 好處是它整合了各種雲原生平台，且 Calico 在每一個節點利用 Linux Kernel 實現高效的 vRouter 來負責資料的轉發，而當資料中心複雜度增加時，可以用 BGP route reflector 來達成。</p>
<blockquote>
<p>本次不採用手動方式來建立 Calico 網路，若想了解可以參考 <a href="https://docs.projectcalico.org/v3.0/getting-started/kubernetes/installation/integration" target="_blank" rel="noopener">Integration Guide</a>。</p>
</blockquote>
<p>在<code>k8s-m1</code>下載<code>calico.yaml</code>來建立 Calico Network：</p>
<pre><code class="sh">$ kubectl apply -f &quot;https://kairen.github.io/files/manual-v1.10/network/calico.yml.conf&quot;
configmap &quot;calico-config&quot; created
daemonset &quot;calico-node&quot; created
deployment &quot;calico-kube-controllers&quot; created
clusterrolebinding &quot;calico-cni-plugin&quot; created
clusterrole &quot;calico-cni-plugin&quot; created
serviceaccount &quot;calico-cni-plugin&quot; created
clusterrolebinding &quot;calico-kube-controllers&quot; created
clusterrole &quot;calico-kube-controllers&quot; created
serviceaccount &quot;calico-kube-controllers&quot; created

$ kubectl -n kube-system get po -l k8s-app=calico-node -o wide
NAME                READY     STATUS    RESTARTS   AGE       IP             NODE
calico-node-22mbb   2/2       Running   0          1m        192.16.35.12   k8s-m2
calico-node-2qwf5   2/2       Running   0          1m        192.16.35.11   k8s-m1
calico-node-g2sp8   2/2       Running   0          1m        192.16.35.13   k8s-m3
calico-node-hghp4   2/2       Running   0          1m        192.16.35.14   k8s-n1
calico-node-qp6gf   2/2       Running   0          1m        192.16.35.15   k8s-n2
calico-node-zfx4n   2/2       Running   0          1m        192.16.35.16   k8s-n3
</code></pre>
<blockquote>
<p>這邊若節點 IP 與網卡不同的話，請修改<code>calico.yml</code>檔案。</p>
</blockquote>
<p>在<code>k8s-m1</code>下載 Calico CLI 來查看 Calico nodes:</p>
<pre><code class="sh">$ wget https://github.com/projectcalico/calicoctl/releases/download/v3.1.0/calicoctl -O /usr/local/bin/calicoctl
$ chmod u+x /usr/local/bin/calicoctl
$ cat &lt;&lt;EOF &gt; ~/calico-rc
export ETCD_ENDPOINTS=&quot;https://192.16.35.11:2379,https://192.16.35.12:2379,https://192.16.35.13:2379&quot;
export ETCD_CA_CERT_FILE=&quot;/etc/etcd/ssl/etcd-ca.pem&quot;
export ETCD_CERT_FILE=&quot;/etc/etcd/ssl/etcd.pem&quot;
export ETCD_KEY_FILE=&quot;/etc/etcd/ssl/etcd-key.pem&quot;
EOF

$ . ~/calico-rc
$ calicoctl node status
Calico process is running.

IPv4 BGP status
+--------------+-------------------+-------+----------+-------------+
| PEER ADDRESS |     PEER TYPE     | STATE |  SINCE   |    INFO     |
+--------------+-------------------+-------+----------+-------------+
| 192.16.35.12 | node-to-node mesh | up    | 04:42:37 | Established |
| 192.16.35.13 | node-to-node mesh | up    | 04:42:42 | Established |
| 192.16.35.14 | node-to-node mesh | up    | 04:42:37 | Established |
| 192.16.35.15 | node-to-node mesh | up    | 04:42:41 | Established |
| 192.16.35.16 | node-to-node mesh | up    | 04:42:36 | Established |
+--------------+-------------------+-------+----------+-------------+
...
</code></pre>
<p>查看 pending 的 pod 是否已執行：</p>
<pre><code class="sh">$ kubectl -n kube-system get po -l k8s-app=kube-dns
kubectl -n kube-system get po -l k8s-app=kube-dns
NAME                        READY     STATUS    RESTARTS   AGE
kube-dns-654684d656-j8xzx   3/3       Running   0          10m
</code></pre>
<h2 id="Kubernetes-Extra-Addons-部署"><a href="#Kubernetes-Extra-Addons-部署" class="headerlink" title="Kubernetes Extra Addons 部署"></a>Kubernetes Extra Addons 部署</h2><p>本節說明如何部署一些官方常用的 Addons，如 Dashboard、Heapster 等。</p>
<h3 id="Dashboard"><a href="#Dashboard" class="headerlink" title="Dashboard"></a>Dashboard</h3><p><a href="https://github.com/kubernetes/dashboard" target="_blank" rel="noopener">Dashboard</a> 是 Kubernetes 社區官方開發的儀表板，有了儀表板後管理者就能夠透過 Web-based 方式來管理 Kubernetes 叢集，除了提升管理方便，也讓資源視覺化，讓人更直覺看見系統資訊的呈現結果。</p>
<p>在<code>k8s-m1</code>透過 kubectl 來建立 kubernetes dashboard 即可：</p>
<pre><code class="sh">$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
$ kubectl -n kube-system get po,svc -l k8s-app=kubernetes-dashboard
NAME                                    READY     STATUS    RESTARTS   AGE
kubernetes-dashboard-7d5dcdb6d9-j492l   1/1       Running   0          12s

NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes-dashboard   ClusterIP   10.111.22.111   &lt;none&gt;        443/TCP   12s
</code></pre>
<p>這邊會額外建立一個名稱為<code>open-api</code> Cluster Role Binding，這僅作為方便測試時使用，在一般情況下不要開啟，不然就會直接被存取所有 API:</p>
<pre><code class="sh">$ cat &lt;&lt;EOF | kubectl create -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: open-api
  namespace: &quot;&quot;
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: system:anonymous
EOF
</code></pre>
<blockquote>
<p>注意!管理者可以針對特定使用者來開放 API 存取權限，但這邊方便使用直接綁在 cluster-admin cluster role。</p>
</blockquote>
<p>完成後，就可以透過瀏覽器存取 <a href="https://192.16.35.10:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/" target="_blank" rel="noopener">Dashboard</a>。</p>
<p>在 1.7 版本以後的 Dashboard 將不再提供所有權限，因此需要建立一個 service account 來綁定 cluster-admin role：</p>
<pre><code class="sh">$ kubectl -n kube-system create sa dashboard
$ kubectl create clusterrolebinding dashboard --clusterrole cluster-admin --serviceaccount=kube-system:dashboard
$ SECRET=$(kubectl -n kube-system get sa dashboard -o yaml | awk &#39;/dashboard-token/ {print $3}&#39;)
$ kubectl -n kube-system describe secrets ${SECRET} | awk &#39;/token:/{print $2}&#39;
eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtdG9rZW4tdzVocmgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYWJmMTFjYzMtZjRlYi0xMWU3LTgzYWUtMDgwMDI3NjdkOWI5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZCJ9.Xuyq34ci7Mk8bI97o4IldDyKySOOqRXRsxVWIJkPNiVUxKT4wpQZtikNJe2mfUBBD-JvoXTzwqyeSSTsAy2CiKQhekW8QgPLYelkBPBibySjBhJpiCD38J1u7yru4P0Pww2ZQJDjIxY4vqT46ywBklReGVqY3ogtUQg-eXueBmz-o7lJYMjw8L14692OJuhBjzTRSaKW8U2MPluBVnD7M2SOekDff7KpSxgOwXHsLVQoMrVNbspUCvtIiEI1EiXkyCNRGwfnd2my3uzUABIHFhm0_RZSmGwExPbxflr8Fc6bxmuz-_jSdOtUidYkFIzvEWw2vRovPgs3MXTv59RwUw
</code></pre>
<blockquote>
<p>複製<code>token</code>，然後貼到 Kubernetes dashboard。注意這邊一般來說要針對不同 User 開啟特定存取權限。</p>
</blockquote>
<p><img src="/images/kube/kubernetes-dashboard.png" alt=""></p>
<h3 id="Heapster"><a href="#Heapster" class="headerlink" title="Heapster"></a>Heapster</h3><p><a href="https://github.com/kubernetes/heapster" target="_blank" rel="noopener">Heapster</a> 是 Kubernetes 社區維護的容器叢集監控與效能分析工具。Heapster 會從 Kubernetes apiserver 取得所有 Node 資訊，然後再透過這些 Node 來取得 kubelet 上的資料，最後再將所有收集到資料送到 Heapster 的後台儲存 InfluxDB，最後利用 Grafana 來抓取 InfluxDB 的資料源來進行視覺化。</p>
<p>在<code>k8s-m1</code>透過 kubectl 來建立 kubernetes monitor  即可：</p>
<pre><code class="sh">$ kubectl apply -f &quot;https://kairen.github.io/files/manual-v1.10/addon/kube-monitor.yml.conf&quot;
$ kubectl -n kube-system get po,svc
NAME                                           READY     STATUS    RESTARTS   AGE
...
po/heapster-74fb5c8cdc-62xzc                   4/4       Running   0          7m
po/influxdb-grafana-55bd7df44-nw4nc            2/2       Running   0          7m

NAME                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
...
svc/heapster               ClusterIP   10.100.242.225   &lt;none&gt;        80/TCP              7m
svc/monitoring-grafana     ClusterIP   10.101.106.180   &lt;none&gt;        80/TCP              7m
svc/monitoring-influxdb    ClusterIP   10.109.245.142   &lt;none&gt;        8083/TCP,8086/TCP   7m
···
</code></pre>
<p>完成後，就可以透過瀏覽器存取 <a href="https://192.16.35.10:6443/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy/" target="_blank" rel="noopener">Grafana Dashboard</a>。</p>
<p><img src="/images/kube/monitoring-grafana.png" alt=""></p>
<h3 id="Ingress-Controller"><a href="#Ingress-Controller" class="headerlink" title="Ingress Controller"></a>Ingress Controller</h3><p><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/" target="_blank" rel="noopener">Ingress</a>是利用 Nginx 或 HAProxy 等負載平衡器來曝露叢集內服務的元件，Ingress 主要透過設定 Ingress 規格來定義 Domain Name 映射 Kubernetes 內部 Service，這種方式可以避免掉使用過多的 NodePort 問題。</p>
<p>在<code>k8s-m1</code>透過 kubectl 來建立 Ingress Controller 即可：</p>
<pre><code class="sh">$ kubectl create ns ingress-nginx
$ kubectl apply -f &quot;https://kairen.github.io/files/manual-v1.10/addon/ingress-controller.yml.conf&quot;
$ kubectl -n ingress-nginx get po
NAME                                       READY     STATUS    RESTARTS   AGE
default-http-backend-5c6d95c48-rzxfb       1/1       Running   0          7m
nginx-ingress-controller-699cdf846-982n4   1/1       Running   0          7m
</code></pre>
<blockquote>
<p>這裡也可以選擇 <a href="https://github.com/containous/traefik" target="_blank" rel="noopener">Traefik</a> 的 Ingress Controller。</p>
</blockquote>
<h4 id="測試-Ingress-功能"><a href="#測試-Ingress-功能" class="headerlink" title="測試 Ingress 功能"></a>測試 Ingress 功能</h4><p>這邊先建立一個 Nginx HTTP server Deployment 與 Service：</p>
<pre><code class="sh">$ kubectl run nginx-dp --image nginx --port 80
$ kubectl expose deploy nginx-dp --port 80
$ kubectl get po,svc
$ cat &lt;&lt;EOF | kubectl create -f -
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-nginx-ingress
  annotations:
    ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: test.nginx.com
    http:
      paths:
      - path: /
        backend:
          serviceName: nginx-dp
          servicePort: 80
EOF
</code></pre>
<p>透過 curl 來進行測試：</p>
<pre><code class="sh">$ curl 192.16.35.10 -H &#39;Host: test.nginx.com&#39;
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
...

# 測試其他 domain name 是否會回傳 404
$ curl 192.16.35.10 -H &#39;Host: test.nginx.com1&#39;
default backend - 404
</code></pre>
<h3 id="Helm-Tiller-Server"><a href="#Helm-Tiller-Server" class="headerlink" title="Helm Tiller Server"></a>Helm Tiller Server</h3><p><a href="https://github.com/kubernetes/helm" target="_blank" rel="noopener">Helm</a> 是 Kubernetes Chart 的管理工具，Kubernetes Chart 是一套預先組態的 Kubernetes 資源套件。其中<code>Tiller Server</code>主要負責接收來至 Client 的指令，並透過 kube-apiserver 與 Kubernetes 叢集做溝通，根據 Chart 定義的內容，來產生與管理各種對應 API 物件的 Kubernetes 部署檔案(又稱為 <code>Release</code>)。</p>
<p>首先在<code>k8s-m1</code>安裝 Helm tool：</p>
<pre><code class="sh">$ wget -qO- https://kubernetes-helm.storage.googleapis.com/helm-v2.8.1-linux-amd64.tar.gz | tar -zx
$ sudo mv linux-amd64/helm /usr/local/bin/
</code></pre>
<p>另外在所有<code>node</code>節點安裝 socat：</p>
<pre><code class="sh">$ sudo apt-get install -y socat
</code></pre>
<p>接著初始化 Helm(這邊會安裝 Tiller Server)：</p>
<pre><code class="sh">$ kubectl -n kube-system create sa tiller
$ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
$ helm init --service-account tiller
...
Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.
Happy Helming!

$ kubectl -n kube-system get po -l app=helm
NAME                             READY     STATUS    RESTARTS   AGE
tiller-deploy-5f789bd9f7-tzss6   1/1       Running   0          29s

$ helm version
Client: &amp;version.Version{SemVer:&quot;v2.8.1&quot;, GitCommit:&quot;6af75a8fd72e2aa18a2b278cfe5c7a1c5feca7f2&quot;, GitTreeState:&quot;clean&quot;}
Server: &amp;version.Version{SemVer:&quot;v2.8.1&quot;, GitCommit:&quot;6af75a8fd72e2aa18a2b278cfe5c7a1c5feca7f2&quot;, GitTreeState:&quot;clean&quot;}
</code></pre>
<h4 id="測試-Helm-功能"><a href="#測試-Helm-功能" class="headerlink" title="測試 Helm 功能"></a>測試 Helm 功能</h4><p>這邊部署簡單 Jenkins 來進行功能測試：</p>
<pre><code class="sh">$ helm install --name demo --set Persistence.Enabled=false stable/jenkins
$ kubectl get po,svc  -l app=demo-jenkins
NAME                           READY     STATUS    RESTARTS   AGE
demo-jenkins-7bf4bfcff-q74nt   1/1       Running   0          2m

NAME                 TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
demo-jenkins         LoadBalancer   10.103.15.129    &lt;pending&gt;     8080:31161/TCP   2m
demo-jenkins-agent   ClusterIP      10.103.160.126   &lt;none&gt;        50000/TCP        2m

# 取得 admin 帳號的密碼
$ printf $(kubectl get secret --namespace default demo-jenkins -o jsonpath=&quot;{.data.jenkins-admin-password}&quot; | base64 --decode);echo
r6y9FMuF2u
</code></pre>
<p>完成後，就可以透過瀏覽器存取 <a href="http://192.16.35.10:31161" target="_blank" rel="noopener">Jenkins Web</a>。</p>
<p><img src="/images/kube/helm-jenkins-v1.10.png" alt=""></p>
<p>測試完成後，即可刪除：</p>
<pre><code class="sh">$ helm ls
NAME    REVISION    UPDATED                     STATUS      CHART             NAMESPACE
demo    1           Tue Apr 10 07:29:51 2018    DEPLOYED    jenkins-0.14.4    default

$ helm delete demo --purge
release &quot;demo&quot; deleted
</code></pre>
<p>更多 Helm Apps 可以到 <a href="https://hub.kubeapps.com/" target="_blank" rel="noopener">Kubeapps Hub</a> 尋找。</p>
<h2 id="測試叢集"><a href="#測試叢集" class="headerlink" title="測試叢集"></a>測試叢集</h2><p>SSH 進入<code>k8s-m1</code>節點，然後關閉該節點：</p>
<pre><code class="sh">$ sudo poweroff
</code></pre>
<p>接著進入到<code>k8s-m2</code>節點，透過 kubectl 來檢查叢集是否能夠正常執行：</p>
<pre><code class="sh"># 先檢查 etcd 狀態，可以發現 etcd-0 因為關機而中斷
$ kubectl get cs
NAME                 STATUS      MESSAGE                                                                                                                                          ERROR
scheduler            Healthy     ok
controller-manager   Healthy     ok
etcd-1               Healthy     {&quot;health&quot;: &quot;true&quot;}
etcd-2               Healthy     {&quot;health&quot;: &quot;true&quot;}
etcd-0               Unhealthy   Get https://192.16.35.11:2379/health: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)

# 測試是否可以建立 Pod
$ kubectl run nginx --image nginx --restart=Never --port 80
$ kubectl get po
NAME      READY     STATUS    RESTARTS   AGE
nginx     1/1       Running   0          22s
</code></pre>
]]></content>
      
        <categories>
            
            <category> Kubernetes </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Docker </tag>
            
            <tag> Kubernetes </tag>
            
            <tag> Calico </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[使用 kubefed 建立 Kubernetes Federation(On-premises)]]></title>
      <url>https://kairen.github.io/2018/03/21/kubernetes/k8s-federation/</url>
      <content type="html"><![CDATA[<p><a href="https://kubernetes.io/docs/concepts/cluster-administration/federation/" target="_blank" rel="noopener">Kubernetes Federation(聯邦)</a> 是實現跨地區與跨服務商多個 Kubernetes 叢集的管理機制。Kubernetes Federation 的架構非常類似純 Kubenretes 叢集，Federation 會擁有自己的 API Server 與 Controller Manager 來提供一個標準的 Kubernetes API，以及管理聯邦叢集，並利用 Etcd 來儲存所有狀態，不過差異在於 Kubenretes 只管理多個節點，而 Federation 是管理所有被註冊的 Kubernetes 叢集。</p>
<a id="more"></a>
<p>Federation 使管理多個叢集更為簡單，這主要是透過兩個模型來實現：</p>
<ol>
<li><strong>跨叢集的資源同步(Sync resources across clusters)</strong>：提供在多個叢集中保持資源同步的功能，如確保一個 Deployment 可以存在於多個叢集中。</li>
<li><strong>跨叢集的服務發現(Cross cluster discovery:)</strong>：提供自動配置 DNS 服務以及在所有叢集後端上進行負載平衡功能，如提供全域 VIP 或 DNS record，並透過此存取多個叢集後端。</li>
</ol>
<p><img src="/images/kube/federation-api.png" alt=""></p>
<p>Federation 有以下幾個好處：</p>
<ol>
<li>跨叢集的資源排程，能讓 Pod 分配至不同叢集的不同節點上執行，如果當前叢集超出負荷，能夠將額外附載分配到空閒叢集上。</li>
<li>叢集的高可靠，能夠做到 Pod 故障自動遷移。</li>
<li>可管理多個 Kubernetes 叢集。</li>
<li>跨叢集的服務發現。</li>
</ol>
<blockquote>
<p>雖然 Federation 能夠降低管理多叢集門檻，但是目前依據不建議放到生產環境。以下幾個原因：</p>
<ul>
<li><strong>成熟度問題</strong>，目前還處與 Alpha 階段，故很多功能都還處於實現性質，或者不太穩定。</li>
<li><strong>提升網路頻寬與成本</strong>，由於 Federation 需要監控所有叢集以確保當前狀態符合預期，因是會增加額外效能開銷。</li>
<li><strong>跨叢集隔離差</strong>，Federation 的子叢集git有可能因為 Bug 的引發而影響其他叢集運行狀況。</li>
<li>個人用起來不是很穩定，例如建立的 Deployment 刪除很常會 Timeout。</li>
<li>支援的物件資源有限，如不支援 StatefulSets。可參考 <a href="https://kubernetes.io/docs/concepts/cluster-administration/federation/#api-resources" target="_blank" rel="noopener">API resources</a>。</li>
</ul>
</blockquote>
<p>Federation 主要包含三個元件：</p>
<ul>
<li><strong>federation-apiserver</strong>：主要提供跨叢集的 REST API 伺服器，類似 kube-apiserver。</li>
<li><strong>federation-controller-manager</strong>：提供多個叢集之間的狀態同步，類似 kube-controller-manager。</li>
<li><strong>kubefed</strong>：Federation CLI 工具，用來初始化 Federation 元件與加入子叢集。</li>
</ul>
<h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本次安裝作業系統採用<code>Ubuntu 16.04 Server</code>，測試環境為實體機器，共有三組叢集：</p>
<p>Federation 控制平面叢集(簡稱 F):</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Host</th>
<th>vCPU</th>
<th>RAM</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.22.132.31</td>
<td>k8s-f-m1</td>
<td>4</td>
<td>16G</td>
</tr>
<tr>
<td>172.22.132.32</td>
<td>k8s-f-n1</td>
<td>4</td>
<td>16G</td>
</tr>
</tbody>
</table>
<p>叢集 A:</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Host</th>
<th>vCPU</th>
<th>RAM</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.22.132.41</td>
<td>k8s-a-m1</td>
<td>8</td>
<td>16G</td>
</tr>
<tr>
<td>172.22.132.42</td>
<td>k8s-a-n1</td>
<td>8</td>
<td>16G</td>
</tr>
</tbody>
</table>
<p>叢集 B:</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Host</th>
<th>vCPU</th>
<th>RAM</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.22.132.51</td>
<td>k8s-b-m1</td>
<td>8</td>
<td>16G</td>
</tr>
<tr>
<td>172.22.132.52</td>
<td>k8s-b-n1</td>
<td>8</td>
<td>16G</td>
</tr>
</tbody>
</table>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>安裝與進行 Federation 之前，需要確保以下條件達成：</p>
<ul>
<li>所有叢集的節點各自部署成一個 Kubernetes 叢集，請參考 <a href="https://kairen.github.io/2016/09/29/kubernetes/deploy/kubeadm/">用 kubeadm 部署 Kubernetes 叢集</a>。</li>
<li>修改 F、A 與 B 叢集的 Kubernetes config，並將 A 與 B 複製到 F 節點，如修改成以下：</li>
</ul>
<pre><code class="yaml">...
...
  name: k8s-a-cluster
contexts:
- context:
    cluster: k8s-a-cluster
    user: a-cluster-admin
  name: a-cluster-context
current-context: a-cluster-context
kind: Config
preferences: {}
users:
- name: a-cluster-admin
  user:
...
</code></pre>
<blockquote>
<p>這邊需要修改每個叢集 config。</p>
</blockquote>
<ul>
<li>接著在 F 叢集合併 F、A 與 B 三個 config，透過以下方式進行：</li>
</ul>
<pre><code class="sh">$ ls
a-cluster.conf  b-cluster.conf  f-cluster.conf

$ KUBECONFIG=f-cluster.conf:a-cluster.conf:b-cluster.conf kubectl config view --flatten &gt; ~/.kube/config
$ kubectl config get-contexts
CURRENT   NAME                CLUSTER         AUTHINFO          NAMESPACE
          a-cluster-context   k8s-a-cluster   a-cluster-admin
          b-cluster-context   k8s-b-cluster   b-cluster-admin
*         f-cluster-context   k8s-f-cluster   f-cluster-admin
</code></pre>
<ul>
<li>在 F 叢集安裝 kubefed 工具：</li>
</ul>
<pre><code class="sh">$ wget https://storage.googleapis.com/kubernetes-federation-release/release/v1.9.0-alpha.3/federation-client-linux-amd64.tar.gz
$ tar xvf federation-client-linux-amd64.tar.gz
$ cp federation/client/bin/kubefed /usr/local/bin/
$ kubefed version
Client Version: version.Info{Major:&quot;1&quot;, Minor:&quot;9+&quot;, GitVersion:&quot;v1.9.0-alpha.3&quot;, GitCommit:&quot;85c06145286da663755b140efa2b65f793cce9ec&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-02-14T12:54:40Z&quot;, GoVersion:&quot;go1.9.1&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}
Server Version: version.Info{Major:&quot;1&quot;, Minor:&quot;9&quot;, GitVersion:&quot;v1.9.6&quot;, GitCommit:&quot;9f8ebd171479bec0ada837d7ee641dec2f8c6dd1&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-03-21T15:13:31Z&quot;, GoVersion:&quot;go1.9.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}
</code></pre>
<ul>
<li>在 F 叢集安裝 Helm 工具，並進行初始化：</li>
</ul>
<pre><code class="sh">$ wget -qO- https://kubernetes-helm.storage.googleapis.com/helm-v2.8.1-linux-amd64.tar.gz | tar -zxf
$ sudo mv linux-amd64/helm /usr/local/bin/
$ kubectl -n kube-system create sa tiller
$ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
$ helm init --service-account tiller

# wait for a few minutes
$ helm version
Client: &amp;version.Version{SemVer:&quot;v2.8.1&quot;, GitCommit:&quot;6af75a8fd72e2aa18a2b278cfe5c7a1c5feca7f2&quot;, GitTreeState:&quot;clean&quot;}
Server: &amp;version.Version{SemVer:&quot;v2.8.1&quot;, GitCommit:&quot;6af75a8fd72e2aa18a2b278cfe5c7a1c5feca7f2&quot;, GitTreeState:&quot;clean&quot;}
</code></pre>
<h2 id="部署-Kubernetes-Federation"><a href="#部署-Kubernetes-Federation" class="headerlink" title="部署 Kubernetes Federation"></a>部署 Kubernetes Federation</h2><p>由於本篇是使用實體機器部署 Kubernetes 叢集，因此無法像是 GCP 可以提供 DNS 服務來給 Federation 使用，故這邊要用 CoreDNS 建立自定義 DNS 服務。</p>
<h3 id="CoreDNS-安裝"><a href="#CoreDNS-安裝" class="headerlink" title="CoreDNS 安裝"></a>CoreDNS 安裝</h3><p>首先透過 Helm 來安裝 CoreDNS 使用到的 Etcd：</p>
<pre><code class="sh">$ helm install --namespace federation --name etcd-operator stable/etcd-operator
$ helm upgrade --namespace federation --set cluster.enabled=true etcd-operator stable/etcd-operator
$ kubectl -n federation get po
NAME                                                              READY     STATUS    RESTARTS   AGE
etcd-operator-etcd-operator-etcd-backup-operator-577d56449zqkj2   1/1       Running   0          1m
etcd-operator-etcd-operator-etcd-operator-56679fb56-fpgmm         1/1       Running   0          1m
etcd-operator-etcd-operator-etcd-restore-operator-65b6cbccl7kzr   1/1       Running   0          1m
</code></pre>
<p>完成後就可以安裝 CoreDNS 來提供自定義 DNS 服務了：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; Values.yaml
isClusterService: false
serviceType: NodePort
middleware:
  kubernetes:
    enabled: false
  etcd:
    enabled: true
    zones:
    - &quot;kairen.com.&quot;
    endpoint: &quot;http://etcd-cluster.federation:2379&quot;
EOF

$ kubectl create clusterrolebinding federation-admin --clusterrole=cluster-admin --user=system:serviceaccount:federation:default
$ helm install --namespace federation --name coredns -f Values.yaml stable/coredns
# 測試 CoreDNS 可以查詢 Domain Name
$ kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools
dnstools# host kubernetes
kubernetes.default.svc.cluster.local has address 10.96.0.1
</code></pre>
<h3 id="安裝與初始化-Federation-控制平面元件"><a href="#安裝與初始化-Federation-控制平面元件" class="headerlink" title="安裝與初始化 Federation 控制平面元件"></a>安裝與初始化 Federation 控制平面元件</h3><p>完成 CoreDNS 後，接著透過 kubefed 安裝控制平面元件，由於使用到 CoreDNS，因此這邊要傳入相關 conf 檔，首先建立<code>coredns-provider.conf</code>檔案，加入以下內容：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; coredns-provider.conf
[Global]
etcd-endpoints = http://etcd-cluster.federation:2379
zones = kairen.com.
EOF
</code></pre>
<blockquote>
<p>請自行修改<code>etcd-endpoints</code>與<code>zones</code>。</p>
</blockquote>
<p>檔案建立並確認沒問題後，透過 kubefed 工具來初始化主叢集：</p>
<pre><code class="sh">$ kubefed init federation \
  --host-cluster-context=f-cluster-context \
  --dns-provider=&quot;coredns&quot; \
  --dns-zone-name=&quot;kairen.com.&quot; \
  --apiserver-enable-basic-auth=true \
  --apiserver-enable-token-auth=true \
  --dns-provider-config=&quot;coredns-provider.conf&quot; \
  --apiserver-arg-overrides=&quot;--anonymous-auth=false,--v=4&quot; \
  --api-server-service-type=&quot;NodePort&quot; \
  --api-server-advertise-address=&quot;172.22.132.31&quot; \
  --etcd-persistent-storage=true

$ kubectl -n federation-system get po
NAME                                  READY     STATUS    RESTARTS   AGE
apiserver-848d584b5d-cwxdh            2/2       Running   0          1m
controller-manager-5846c555c6-mw2jz   1/1       Running   1          1m
</code></pre>
<blockquote>
<p>這邊可以改變<code>--etcd-persistent-storage</code>來選擇使用或不使用 PV，若使用請先建立一個 PV 來提供給 Federation Pod 的 PVC 索取使用，可以參考 <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/" target="_blank" rel="noopener">Persistent Volumes</a>。</p>
</blockquote>
<h3 id="加入-Federation-的-Kubernetes-子叢集"><a href="#加入-Federation-的-Kubernetes-子叢集" class="headerlink" title="加入 Federation 的 Kubernetes 子叢集"></a>加入 Federation 的 Kubernetes 子叢集</h3><pre><code class="sh">$ kubectl config use-context federation

# 加入 k8s-a-cluster
$ kubefed join f-a-cluster \
  --cluster-context=a-cluster-context \
  --host-cluster-context=f-cluster-context

# 加入 k8s-b-cluster
$ kubefed join f-b-cluster \
  --cluster-context=b-cluster-context \
  --host-cluster-context=f-cluster-context

$ kubectl get cluster
NAME          AGE
f-a-cluster   57s
f-b-cluster   53s
</code></pre>
<h2 id="測試-Federation-叢集"><a href="#測試-Federation-叢集" class="headerlink" title="測試 Federation 叢集"></a>測試 Federation 叢集</h2><p>這邊利用 Nginx Deployment 來進行測試，先簡單建立一個副本為 4 的 Nginx：</p>
<pre><code class="sh">$ kubectl config use-context federation
$ kubectl create ns default
$ kubectl run nginx --image nginx --port 80 --replicas=4
</code></pre>
<p>查看 Cluster A：</p>
<pre><code class="sh">$ kubectl --context=a-cluster-context get po
NAME                     READY     STATUS    RESTARTS   AGE
nginx-7587c6fdb6-dpjv5   1/1       Running   0          25s
nginx-7587c6fdb6-sjv8v   1/1       Running   0          25s
</code></pre>
<p>查看 Cluster B：</p>
<pre><code class="sh">$ kubectl --context=b-cluster-context get po
NAME                     READY     STATUS    RESTARTS   AGE
nginx-7587c6fdb6-dv45v   1/1       Running   0          1m
nginx-7587c6fdb6-wxsmq   1/1       Running   0          1m
</code></pre>
<p>其他可測試功能：</p>
<ul>
<li>設定 Replica set preferences，參考 <a href="https://kubernetes.io/docs/tasks/administer-federation/replicaset/#spreading-replicas-in-underlying-clusters" target="_blank" rel="noopener">Spreading Replicas in Underlying Clusters</a>。</li>
<li>Federation 在 v1.7+ 加入了 <a href="https://kubernetes.io/docs/tasks/administer-federation/cluster/#clusterselector-annotation" target="_blank" rel="noopener">ClusterSelector Annotation</a></li>
<li><a href="https://kubernetes.io/docs/tasks/federation/set-up-placement-policies-federation/#deploying-federation-and-configuring-an-external-policy-engine" target="_blank" rel="noopener">Scheduling Policy</a>。</li>
</ul>
<h2 id="Refers"><a href="#Refers" class="headerlink" title="Refers"></a>Refers</h2><ul>
<li><a href="https://github.com/emaildanwilson/minikube-federation" target="_blank" rel="noopener">Minikube Federation</a></li>
<li><a href="http://cgrant.io/tutorials/gcp/compute/gke/global-kubernetes-three-steps/" target="_blank" rel="noopener">Global Kubernetes in 3 Steps</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Kubernetes </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> Federation </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[利用 Kubeflow 來管理 TensorFlow 應用程式]]></title>
      <url>https://kairen.github.io/2018/03/15/tensorflow/kubeflow/</url>
      <content type="html"><![CDATA[<p><a href="https://github.com/kubeflow/kubeflow" target="_blank" rel="noopener">Kubeflow</a> 是 Google 開源的機器學習工具，目標是簡化在 Kubernetes 上運行機器學習的過程，使之更簡單、可攜帶與可擴展。Kubeflow 目標不是在於重建其他服務，而是提供一個最佳開發系統來部署到各種基礎設施架構中，另外由於使用 Kubernetes 來做為基礎，因此只要有 Kubernetes 的地方，都能夠執行 Kubeflow。</p>
<center><img src="/images/kubeflow/logo.png" alt=""></center>

<a id="more"></a>
<p>該工具能夠建立以下幾項功能：</p>
<ul>
<li>用於建議與管理互動式 Jupyter notebook 的 JupyterHub。</li>
<li>可以設定使用 CPU 或 GPU，並透過單一設定調整單個叢集大小的 Tensorflow Training Controller。</li>
<li>用 TensorFlow Serving 容器來提供模型服務。</li>
</ul>
<p>Kubeflow 目標是透過 Kubernetes 的特性使機器學習更加簡單與快速：</p>
<ul>
<li>在不同基礎設施上實現簡單、可重複的攜帶性部署(Laptop &lt;-&gt; ML rig &lt;-&gt; Training cluster &lt;-&gt; Production cluster)。</li>
<li>部署與管理松耦合的微服務。</li>
<li>根據需求進行縮放。</li>
</ul>
<h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本次安裝作業系統採用<code>Ubuntu 16.04 Server</code>，測試環境為實體機器：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Role</th>
<th>vCPU</th>
<th>RAM</th>
<th>Extra Device</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.22.132.51</td>
<td>gpu-node1</td>
<td>8</td>
<td>16G</td>
<td>GTX 1060 3G</td>
</tr>
<tr>
<td>172.22.132.52</td>
<td>gpu-node2</td>
<td>8</td>
<td>16G</td>
<td>GTX 1060 3G</td>
</tr>
<tr>
<td>172.22.132.53</td>
<td>master1</td>
<td>8</td>
<td>16G</td>
<td>無</td>
</tr>
</tbody>
</table>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>使用 Kubeflow 之前，需要確保以下條件達成：</p>
<ul>
<li>所有節點正確安裝指定版本的 NVIDIA driver、CUDA、Docker、NVIDIA Docker，請參考 <a href="https://kairen.github.io/2018/02/17/container/docker-nvidia-install/">安裝 Nvidia Docker 2</a>。</li>
<li>(option)所有 GPU 節點安裝 cuDNN v7.1.2 for CUDA 9.1，請至 <a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener">NVIDIA cuDNN</a> 下載。</li>
</ul>
<pre><code class="sh">$ tar xvf cudnn-9.1-linux-x64-v7.1.tgz
$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include/
$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/
</code></pre>
<ul>
<li>所有節點以 kubeadm 部署成 Kubernetes v1.9+ 叢集，請參考 <a href="https://kairen.github.io/2016/09/29/kubernetes/deploy/kubeadm/">用 kubeadm 部署 Kubernetes 叢集</a>。</li>
<li>Kubernetes 叢集需要安裝 NVIDIA Device Plugins，請參考 <a href="https://kairen.github.io/2018/03/01/kubernetes/k8s-device-plugin/">安裝 Kubernetes NVIDIA Device Plugins</a>。</li>
<li>建立 NFS server 並在 Kubernetes 節點安裝 NFS common，然後利用 Kubernetes 建立 PV 提供給 Kubeflow 使用：</li>
</ul>
<pre><code class="sh"># 在 master 執行
$ sudo apt-get update &amp;&amp; sudo apt-get install -y nfs-server
$ sudo mkdir /nfs-data
$ echo &quot;/nfs-data *(rw,sync,no_root_squash,no_subtree_check)&quot; | sudo tee -a /etc/exports
$ sudo /etc/init.d/nfs-kernel-server restart

# 在 node 執行
$ sudo apt-get update &amp;&amp; sudo apt-get install -y nfs-common
</code></pre>
<ul>
<li>安裝<code>ksonnet 0.9.2</code>，請參考以下：</li>
</ul>
<pre><code class="sh">$ wget https://github.com/ksonnet/ksonnet/releases/download/v0.9.2/ks_0.9.2_linux_amd64.tar.gz
$ tar xvf ks_0.9.2_linux_amd64.tar.gz
$ sudo cp ks_0.9.2_linux_amd64/ks /usr/local/bin/
$ ks version
ksonnet version: 0.9.2
jsonnet version: v0.9.5
client-go version: 1.8
</code></pre>
<h2 id="部署-Kubeflow"><a href="#部署-Kubeflow" class="headerlink" title="部署 Kubeflow"></a>部署 Kubeflow</h2><p>本節將說明如何利用 ksonnet 來部署 Kubeflow 到 Kubernetes 叢集中。首先在<code>master</code>節點初始化 ksonnet 應用程式目錄：</p>
<pre><code class="sh">$ ks init my-kubeflow
</code></pre>
<blockquote>
<p>如果遇到以下問題的話，可以自己建立 GitHub Token 來存取 GitHub API，請參考 <a href="https://ksonnet.io/docs/tutorial#troubleshooting-github-rate-limiting-errors" target="_blank" rel="noopener">Github rate limiting errors</a>。</p>
<pre><code class="sh">ERROR GET https://api.github.com/repos/ksonnet/parts/commits/master: 403 API rate limit exceeded for 122.146.93.152.
</code></pre>
</blockquote>
<p>接著安裝 Kubeflow 套件至應用程式目錄：</p>
<pre><code class="sh">$ cd my-kubeflow
$ ks registry add kubeflow github.com/kubeflow/kubeflow/tree/master/kubeflow
$ ks pkg install kubeflow/core
$ ks pkg install kubeflow/tf-serving
$ ks pkg install kubeflow/tf-job
</code></pre>
<p>然後建立 Kubeflow 核心元件，該元件包含 JupyterHub 與 TensorFlow job controller：</p>
<pre><code class="sh">$ kubectl create namespace kubeflow
$ kubectl create clusterrolebinding tf-admin --clusterrole=cluster-admin --serviceaccount=default:tf-job-operator
$ ks generate core kubeflow-core --name=kubeflow-core --namespace=kubeflow

# 啟動收集匿名使用者使用量資訊，如果不想開啟則忽略
$ ks param set kubeflow-core reportUsage true
$ ks param set kubeflow-core usageId $(uuidgen)

# 部署 Kubeflow
$ ks param set kubeflow-core jupyterHubServiceType LoadBalancer
$ ks apply default -c kubeflow-core
</code></pre>
<blockquote>
<p>詳細使用量資訊請參考 <a href="https://github.com/kubeflow/kubeflow/blob/master/user_guide.md#usage-reporting" target="_blank" rel="noopener">Usage Reporting
</a>。</p>
</blockquote>
<p>完成後檢查 Kubeflow 元件部署結果：</p>
<pre><code class="sh">$ kubectl -n kubeflow get po -o wide
NAME                                  READY     STATUS    RESTARTS   AGE       IP               NODE
ambassador-7956cf5c7f-6hngq           2/2       Running   0          34m       10.244.41.132    kube-gpu-node1
ambassador-7956cf5c7f-jgxnd           2/2       Running   0          34m       10.244.152.134   kube-gpu-node2
ambassador-7956cf5c7f-jww2d           2/2       Running   0          34m       10.244.41.133    kube-gpu-node1
spartakus-volunteer-8c659d4f5-bg7kn   1/1       Running   0          34m       10.244.152.135   kube-gpu-node2
tf-hub-0                              1/1       Running   0          34m       10.244.152.133   kube-gpu-node2
tf-job-operator-78757955b-2jbdh       1/1       Running   0          34m       10.244.41.131    kube-gpu-node1
</code></pre>
<p>這時候就可以登入 Jupyter Notebook，但這邊需要修改 Kubernetes Service，透過以下指令進行：</p>
<pre><code class="sh">$ kubectl -n kubeflow get svc -o wide
NAME               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE       SELECTOR
ambassador         ClusterIP   10.101.157.91    &lt;none&gt;        80/TCP     45m       service=ambassador
ambassador-admin   ClusterIP   10.107.24.138    &lt;none&gt;        8877/TCP   45m       service=ambassador
k8s-dashboard      ClusterIP   10.111.128.104   &lt;none&gt;        443/TCP    45m       k8s-app=kubernetes-dashboard
tf-hub-0           ClusterIP   None             &lt;none&gt;        8000/TCP   45m       app=tf-hub
tf-hub-lb          ClusterIP   10.105.47.253    &lt;none&gt;        80/TCP     45m       app=tf-hub

# 修改 svc 將 Type 修改成 LoadBalancer，並且新增 externalIPs 指定為 Master IP。
$ kubectl -n kubeflow edit svc tf-hub-lb
...
spec:
  type: LoadBalancer
  externalIPs:
  - 172.22.132.41
...
</code></pre>
<h2 id="測試-Kubeflow"><a href="#測試-Kubeflow" class="headerlink" title="測試 Kubeflow"></a>測試 Kubeflow</h2><p>開始測試前先建立一個 NFS PV 來提供給 Kubeflow Jupyter 使用：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF | kubectl create -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  nfs:
    server: 172.22.132.41
    path: /nfs-data
EOF
</code></pre>
<p>完成後連接 <code>http://Master_IP</code>，並輸入<code>任意帳號密碼</code>進行登入。</p>
<p><img src="/images/kubeflow/1.png" alt=""></p>
<p>登入後點選<code>Start My Server</code>按鈕來建立 Server 的 Spawner options，預設會有多種映像檔可以使用：</p>
<ul>
<li>CPU：gcr.io/kubeflow-images-staging/tensorflow-notebook-cpu。</li>
<li>GPU：gcr.io/kubeflow-images-staging/tensorflow-notebook-gpu。</li>
</ul>
<blockquote>
<p>這邊也使用以下 GCP 建構的映像檔做測試使用(GPU 當前為 CUDA 8)：</p>
<ul>
<li>gcr.io/kubeflow/tensorflow-notebook-cpu:latest</li>
<li>gcr.io/kubeflow/tensorflow-notebook-gpu:latest</li>
</ul>
<p>若 CUDA 版本不同，請自行修改 <a href="https://github.com/GoogleCloudPlatform/container-engine-accelerators/blob/master/example/tensorflow-notebook-image" target="_blank" rel="noopener">GCP Tensorflow Notebook image</a> 或是 <a href="https://github.com/kubeflow/kubeflow/tree/master/components/k8s-model-server/images" target="_blank" rel="noopener">Kubeflow Tensorflow Notebook image </a>重新建構。</p>
<p>如果使用 GPU 請執行以下指令確認是否可被分配資源：</p>
<pre><code class="sh">$ kubectl get nodes &quot;-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\.com/gpu&quot;
NAME               GPU
kube-gpu-master1   &lt;none&gt;
kube-gpu-node1     1
kube-gpu-node2     1
</code></pre>
</blockquote>
<p>最後點選<code>Spawn</code>來完成建立 Server，如下圖所示：</p>
<p><img src="/images/kubeflow/2.png" alt=""></p>
<blockquote>
<p>這邊先用 CPU 進行測試，由於本篇是安裝 CUDA 9.1 + cuDNN 7，因此要自己建構映像檔。</p>
</blockquote>
<p>接著等 Kubernetes 下載映像檔後，就會正常啟動，如下圖所示：</p>
<p><img src="/images/kubeflow/3.png" alt=""></p>
<p>當正常啟動後，點選<code>New &gt; Python 3</code>建立一個 Notebook 並貼上以下範例程式：</p>
<pre><code class="python">from __future__ import print_function

import tensorflow as tf

hello = tf.constant(&#39;Hello TensorFlow!&#39;)
s = tf.Session()
print(s.run(hello))
</code></pre>
<p>正確執行會如以下圖所示：</p>
<p><img src="/images/kubeflow/4.png" alt=""></p>
<blockquote>
<p>若想關閉叢集的話，可以點選<code>Control Plane</code>。</p>
</blockquote>
<p>另外由於 Kubeflow 會安裝 TF Operator 來管理 TFJob，這邊可以透過 Kubernetes 來手動建立 Job：</p>
<pre><code class="sh">$ kubectl create -f https://raw.githubusercontent.com/kubeflow/tf-operator/master/examples/tf_job.yaml
$ kubectl get po
NAME                              READY     STATUS    RESTARTS   AGE
example-job-ps-qq6x-0-pdx7v       1/1       Running   0          5m
example-job-ps-qq6x-1-2mpfp       1/1       Running   0          5m
example-job-worker-qq6x-0-m5fm5   1/1       Running   0          5m
</code></pre>
<p>若想從 Kubernetes 叢集刪除 Kubeflow 相關元件的話，可執行下列指令達成：</p>
<pre><code class="sh">$ ks delete default -c kubeflow-core
</code></pre>
]]></content>
      
        <categories>
            
            <category> TensorFlow </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> GPU </tag>
            
            <tag> DL/ML </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes NVIDIA Device Plugins]]></title>
      <url>https://kairen.github.io/2018/03/01/kubernetes/nvidia-device-plugin/</url>
      <content type="html"><![CDATA[<p><a href="https://kubernetes.io/docs/concepts/cluster-administration/device-plugins/" target="_blank" rel="noopener">Device Plugins</a> 是 Kubernetes v1.8 版本開始加入的 Alpha 功能，目標是結合 Extended Resource 來支援 GPU、FPGA、高效能 NIC、InfiniBand 等硬體設備介接的插件，這樣好處在於硬體供應商不需要修改 Kubernetes 核心程式，只需要依據 <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md" target="_blank" rel="noopener">Device Plugins 介面</a>來實作特定硬體設備插件，就能夠提供給 Kubernetes Pod 使用。而本篇會稍微提及 Device Plugin 原理，並說明如何使用 NVIDIA device plugin。</p>
<p>P.S. 傳統的<code>alpha.kubernetes.io/nvidia-gpu</code>將於 1.11 版本移除，因此與 GPU 相關的排程與部署原始碼都將從 Kubernetes 核心移除。<br><a id="more"></a></p>
<h2 id="Device-Plugins-原理"><a href="#Device-Plugins-原理" class="headerlink" title="Device Plugins 原理"></a>Device Plugins 原理</h2><p>Device  Plugins 主要提供了一個 <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md" target="_blank" rel="noopener">gRPC 介面</a>來給廠商實現<code>ListAndWatch()</code>與<code>Allocate()</code>等 gRPC 方法，並監聽節點的<code>/var/lib/kubelet/device-plugins/</code>目錄中的 gRPC Server Unix Socket，這邊可以參考官方文件 <a href="https://kubernetes.io/docs/concepts/cluster-administration/device-plugins/" target="_blank" rel="noopener">Device Plugins</a>。一旦啟動 Device Plugins 時，透過 Kubelet Unix Socket 註冊，並提供該 plugin 的 Unix Socket 名稱、API 版本號與插件資源名稱(vendor-domain/resource，例如 nvidia.com/gpu)，接著 Kubelet 會將這些曝露到 Node 狀態以便 Scheduler 使用。</p>
<p>Unix Socket 範例：</p>
<pre><code class="sh">$ ls /var/lib/kubelet/device-plugins/
kubelet_internal_checkpoint  kubelet.sock  nvidia.sock
</code></pre>
<p>一些 Device Plugins 列表：</p>
<ul>
<li><a href="https://github.com/NVIDIA/k8s-device-plugin" target="_blank" rel="noopener">NVIDIA GPU</a></li>
<li><a href="https://github.com/hustcat/k8s-rdma-device-plugin" target="_blank" rel="noopener">RDMA</a></li>
<li><a href="https://github.com/kubevirt/kubernetes-device-plugins" target="_blank" rel="noopener">Kubevirt</a></li>
<li><a href="https://github.com/vikaschoudhary16/sfc-device-plugin" target="_blank" rel="noopener">SFC</a></li>
</ul>
<h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本次安裝作業系統採用<code>Ubuntu 16.04 Server</code>，測試環境為實體機器：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Role</th>
<th>vCPU</th>
<th>RAM</th>
<th>Extra Device</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.22.132.51</td>
<td>gpu-node1</td>
<td>8</td>
<td>16G</td>
<td>GTX 1060 3G</td>
</tr>
<tr>
<td>172.22.132.52</td>
<td>gpu-node2</td>
<td>8</td>
<td>16G</td>
<td>GTX 1060 3G</td>
</tr>
<tr>
<td>172.22.132.53</td>
<td>master1</td>
<td>8</td>
<td>16G</td>
<td>無</td>
</tr>
</tbody>
</table>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>安裝 Device Plugin 前，需要確保以下條件達成：</p>
<ul>
<li>所有節點正確安裝指定版本的 NVIDIA driver、CUDA、Docker、NVIDIA Docker。請參考 <a href="https://kairen.github.io/2018/02/17/container/docker-nvidia-install/">安裝 Nvidia Docker 2</a>。</li>
<li>所有節點以 kubeadm 部署成 Kubernetes v1.9+ 叢集。請參考 <a href="https://kairen.github.io/2016/09/29/kubernetes/deploy/kubeadm/">用 kubeadm 部署 Kubernetes 叢集</a>。</li>
</ul>
<h2 id="安裝-NVIDIA-Device-Plugin"><a href="#安裝-NVIDIA-Device-Plugin" class="headerlink" title="安裝 NVIDIA Device Plugin"></a>安裝 NVIDIA Device Plugin</h2><p>若上述要求以符合，再開始前需要在<code>每台 GPU worker 節點</code>修改<code>/lib/systemd/system/docker.service</code>檔案，將 Docker default runtime 改成 nvidia，依照以下內容來修改：</p>
<pre><code class="sh">...
ExecStart=/usr/bin/dockerd -H fd:// --default-runtime=nvidia
...
</code></pre>
<blockquote>
<p>這邊也可以修改<code>/etc/docker/daemon.json</code>檔案，請參考 <a href="https://docs.docker.com/config/daemon/" target="_blank" rel="noopener">Configure and troubleshoot the Docker daemon</a>。</p>
</blockquote>
<p>完成後儲存，並重新啟動 Docker：</p>
<pre><code class="sh">$ sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart docker
</code></pre>
<p>接著由於 v1.9 版本的 Device Plugins 還是處於 Alpha 中，因此需要手動修改<code>每台 GPU worker 節點</code>的 kubelet drop-in <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code>檔案，這邊在<code>KUBELET_CERTIFICATE_ARGS</code>加入一行 args：</p>
<pre><code class="sh">...
Environment=&quot;KUBELET_EXTRA_ARGS=--feature-gates=DevicePlugins=true&quot;
...
</code></pre>
<p>完成後儲存，並重新啟動 kubelet：</p>
<pre><code class="sh">$ sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart kubelet
</code></pre>
<p>確認上述完成，接著在<code>Master</code>節點安裝 NVIDIA Device Plugins，透過以下方式來進行：</p>
<pre><code class="sh">$ kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.9/nvidia-device-plugin.yml
daemonset &quot;nvidia-device-plugin-daemonset&quot; created

$ kubectl -n kube-system get po -o wide
NAME                                       READY     STATUS    RESTARTS   AGE       IP               NODE
...
nvidia-device-plugin-daemonset-bncw2       1/1       Running   0          2m        10.244.41.135    kube-gpu-node1
nvidia-device-plugin-daemonset-ddnhd       1/1       Running   0          2m        10.244.152.132   kube-gpu-node2
</code></pre>
<h2 id="測試-GPU"><a href="#測試-GPU" class="headerlink" title="測試 GPU"></a>測試 GPU</h2><p>首先執行以下指令確認是否可被分配資源：</p>
<pre><code class="sh">$ kubectl get nodes &quot;-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\.com/gpu&quot;
NAME               GPU
master1           &lt;none&gt;
gpu-node1          1
gpu-node2          1
</code></pre>
<p>當 NVIDIA Device Plugins 部署完成後，即可建立一個簡單範例來進行測試：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF | kubectl create -f -
apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  restartPolicy: Never
  containers:
  - image: nvidia/cuda
    name: cuda
    command: [&quot;nvidia-smi&quot;]
    resources:
      limits:
        nvidia.com/gpu: 1
EOF
pod &quot;gpu-pod&quot; created

$ kubectl get po -a -o wide
NAME      READY     STATUS      RESTARTS   AGE       IP              NODE
gpu-pod   0/1       Completed   0          50s       10.244.41.136   kube-gpu-node1

$ kubectl logs gpu-pod
Thu Mar 15 07:28:45 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.30                 Driver Version: 390.30                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 106...  Off  | 00000000:01:00.0 Off |                  N/A |
|  0%   41C    P8    10W / 120W |      0MiB /  3019MiB |      1%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre>
<p>從上面結果可以看到 Kubernetes Pod 正確的使用到 NVIDIA GPU，這邊也可以利用 TensorFlow 來進行測試，新增一個檔案<code>tf-gpu-dp.yml</code>加入以下內容：</p>
<pre><code class="yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: tf-gpu
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tf-gpu
  template:
    metadata:
     labels:
       app: tf-gpu
    spec:
      containers:
      - name: tensorflow
        image: tensorflow/tensorflow:latest-gpu
        ports:
        - containerPort: 8888
        resources:
          limits:
            nvidia.com/gpu: 1
</code></pre>
<p>利用 kubectl 建立 Deployment，並曝露 Jupyter port：</p>
<pre><code class="sh">$ kubectl create -f tf-gpu-dp.yml
deployment &quot;tf-gpu&quot; created

$ kubectl expose deploy tf-gpu --type LoadBalancer --external-ip=172.22.132.53 --port 8888 --target-port 8888
service &quot;tf-gpu&quot; exposed

$ kubectl get po,svc -o wide
NAME                         READY     STATUS    RESTARTS   AGE       IP               NODE
po/tf-gpu-6f9464f94b-pq8t9   1/1       Running   0          1m        10.244.152.133   kube-gpu-node2

NAME             TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)          AGE       SELECTOR
svc/kubernetes   ClusterIP      10.96.0.1        &lt;none&gt;          443/TCP          23h       &lt;none&gt;
svc/tf-gpu       LoadBalancer   10.105.104.183   172.22.132.53   8888:30093/TCP   12s       app=tf-gpu
</code></pre>
<blockquote>
<p>確認無誤後，透過 logs 指令取得 token，並登入<code>Jupyter Notebook</code>，這邊 IP 為 <master1_ip>:8888。</master1_ip></p>
</blockquote>
<p>這邊執行一個簡單範例，並在用 logs 指令查看就能看到 Pod 透過 NVIDIA Device Plugins 使用 GPU：</p>
<pre><code class="sh">$ kubectl logs -f tf-gpu-6f9464f94b-pq8t9
...
2018-03-15 07:37:22.022052: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-03-15 07:37:22.155254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-03-15 07:37:22.155565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties:
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7845
pciBusID: 0000:01:00.0
totalMemory: 2.95GiB freeMemory: 2.88GiB
2018-03-15 07:37:22.155586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0
2018-03-15 07:37:22.346590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2598 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
</code></pre>
<p>最後因為目前 Pod 會綁整張 GPU 來使用，因此當無多餘顯卡時就讓 Pod 處於 Pending：</p>
<pre><code class="sh">$ kubectl scale deploy tf-gpu --replicas=3
$ kubectl get po -o wide
NAME                      READY     STATUS    RESTARTS   AGE       IP               NODE
tf-gpu-6f9464f94b-42xcf   0/1       Pending   0          4s        &lt;none&gt;           &lt;none&gt;
tf-gpu-6f9464f94b-nxdw5   1/1       Running   0          12s       10.244.41.138    kube-gpu-node1
tf-gpu-6f9464f94b-pq8t9   1/1       Running   0          5m        10.244.152.133   kube-gpu-node2
</code></pre>
]]></content>
      
        <categories>
            
            <category> Kubernetes </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NVIDIA GPU </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[安裝 NVIDIA Docker 2 來讓容器使用 GPU]]></title>
      <url>https://kairen.github.io/2018/02/17/container/docker-nvidia-install/</url>
      <content type="html"><![CDATA[<p>本篇主要介紹如何使用 <a href="https://github.com/NVIDIA/nvidia-docker" target="_blank" rel="noopener">NVIDIA Docker v2</a> 來讓容器使用 GPU，過去 NVIDIA Docker v1 需要使用 nvidia-docker 來取代 Docker 執行 GPU image，或是透過手動掛載 NVIDIA driver 與 CUDA 來使 Docker 能夠編譯與執行 GPU 應用程式 image，而新版本的 Docker 則可以透過 –runtime 來選擇使用 NVIDIA Docker v2 的 Runtime 來執行 GPU 應用。</p>
<a id="more"></a>
<p>安裝前需要確認滿足以下幾點：</p>
<ul>
<li>GNU/Linux x86_64 with kernel version &gt; 3.10</li>
<li>Docker CE or EE == v18.03.1</li>
<li>NVIDIA GPU with Architecture &gt; Fermi (2.1)</li>
<li>NVIDIA drivers ~= 361.93 (untested on older versions)</li>
</ul>
<p>首先透過 APT 安裝 Docker CE or EE v17.12 版本：</p>
<pre><code class="sh">$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
$ echo &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial edge&quot; | sudo tee /etc/apt/sources.list.d/docker.list
$ sudo apt-get update &amp;&amp; sudo apt-get install -y docker-ce=18.03.1~ce-0~ubuntu
</code></pre>
<p>接著透過 APT 安裝 NVIDIA Driver(v390.30) 與 CUDA 9.1：</p>
<pre><code class="sh">$ wget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_9.1.85-1_amd64.deb
$ sudo dpkg -i cuda-repo-ubuntu1604_9.1.85-1_amd64.deb
$ sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub
$ sudo apt-get update &amp;&amp; sudo apt-get install -y cuda
</code></pre>
<p>測試 NVIDIA Dirver 與 CUDA 是否有安裝完成：</p>
<pre><code class="sh">$ cat /usr/local/cuda/version.txt
CUDA Version 9.1.85

$ sudo nvidia-smi
Tue Mar 13 06:10:39 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.30                 Driver Version: 390.30                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 106...  Off  | 00000000:01:00.0 Off |                  N/A |
|  0%   33C    P0    15W / 120W |      0MiB /  3019MiB |      2%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre>
<p>確認上述無誤後，接著安裝 NVIDIA Docker v2，這邊透過 APT 來進行安裝：</p>
<pre><code class="sh">$ curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
$ curl -s -L https://nvidia.github.io/nvidia-docker/ubuntu16.04/amd64/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
$ sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-docker2=2.0.3+docker18.03.1-1
$ sudo pkill -SIGHUP dockerd
</code></pre>
<p>測試 NVIDIA runtime，這邊下載 NVIDIA image 來進行測試：</p>
<pre><code class="sh">$ docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi
...
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.30                 Driver Version: 390.30                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 106...  Off  | 00000000:01:00.0 Off |                  N/A |
|  0%   35C    P0    15W / 120W |      0MiB /  3019MiB |      2%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre>
<p>透過 TensorFlow GPU image 來進行測試，這邊執行後登入 IP:8888 執行簡單範例程式：</p>
<pre><code class="sh">$ docker run --runtime=nvidia -it -p 8888:8888 tensorflow/tensorflow:latest-gpu
...
2018-03-13 06:44:21.719705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties:
name: GeForce GTX 1060 3GB major: 6 minor: 1 memoryClockRate(GHz): 1.7845
pciBusID: 0000:01:00.0
totalMemory: 2.95GiB freeMemory: 2.88GiB
2018-03-13 06:44:21.719728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0
2018-03-13 06:44:21.919097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2598 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1060 3GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
</code></pre>
]]></content>
      
        <categories>
            
            <category> Container </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Docker </tag>
            
            <tag> Container </tag>
            
            <tag> NVIDIA GPU </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Ceph Luminous CRUSH map 400000000000000 問題]]></title>
      <url>https://kairen.github.io/2018/02/11/ceph/luminous-crush-issue/</url>
      <content type="html"><![CDATA[<p>在 Ceph Luminous(v12) 版本中，預設開啟了一些 Kernel 特性，其中首先遇到的一般是 400000000000000 問題，即<code>CEPH_FEATURE_NEW_OSDOPREPLY_ENCODING</code>特性(可以從對照表得知<a href="http://cephnotes.ksperis.com/blog/2014/01/21/feature-set-mismatch-error-on-ceph-kernel-client/" target="_blank" rel="noopener">CEPH_FEATURE Table and Kernel Version</a>)，剛問題需要在 Kernel 4.5+ 才能夠被支援，但如果不想升級可以依據本篇方式解決。</p>
<a id="more"></a>
<p>在 L 版本中，當建立 RBD 並且想要 Map 時，會發生 timeout 問題，這時候可以透過 journalctl 來查看問題，如以下：</p>
<pre><code class="sh">$ journalctl -xe
Feb 12 08:36:57 kube-server2 kernel: libceph: mon0 172.22.132.51:6789 feature set mismatch, my 106b84a842a42 &lt; server&#39;s 40106b84a842a42, missing 400000000000000
</code></pre>
<p>查詢發現是 400000000000000 問題，這時可以選擇兩個解決方式：</p>
<ul>
<li>將作業系統更新到 Linux kernel v4.5+ 的版本。</li>
<li>修改 CRUSH 中的 tunables 參數。</li>
</ul>
<p>若想修改 CRUSH tunnables 參數，可以先到任一 Monitor 或者 Admin 節點中，執行以下指令：</p>
<pre><code class="sh">$ ceph osd crush tunables jewel
$ ceph osd crush reweight-all
</code></pre>
<p>只要執行以上指令即可。</p>
]]></content>
      
        <categories>
            
            <category> Ceph </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> Storage </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[利用 RBAC + SA 進行 Kubectl 權限控管]]></title>
      <url>https://kairen.github.io/2018/01/08/kubernetes/rbac-sa-kubectl/</url>
      <content type="html"><![CDATA[<p>這邊說明如何建立不同 Service account user，以及 RBAC 來定義存取規則，並綁定於指定 Service account ，以對指定 Namespace 中資源進行存取權限控制。</p>
<a id="more"></a>
<h2 id="Service-account"><a href="#Service-account" class="headerlink" title="Service account"></a>Service account</h2><p>Service account 一般使用情境方便是 Pod 中的行程呼叫 Kubernetes API 或者其他服務設計而成，這可能會跟 Kubernetes user account 有所混肴，但是由於 Service account 有別於 User account 是可以針對 Namespace 進行建立，因此這邊嘗試拿 Service account 來提供資訊給 kubectl 使用，並利用 RBAC 來設定存取規則，以限制該 Account 存取 API 的資源。</p>
<h2 id="RBAC"><a href="#RBAC" class="headerlink" title="RBAC"></a>RBAC</h2><p>RBAC(Role-Based Access Control)是從 Kubernetes 1.6 開始支援的存取控制機制，叢集管理者能夠對 User 或 Service account 的角色設定指定資源存取權限，在 RBAC 中，權限與角色相互關聯，其透過成為適當的角色成員，以獲取這些角色的存取權限，這比起過去 ABAC 來的方便使用、更簡化等好處。</p>
<h2 id="簡單範例"><a href="#簡單範例" class="headerlink" title="簡單範例"></a>簡單範例</h2><p>首先建立一個 Namespace 與 Service account：</p>
<pre><code class="sh">$ kubectl create ns dev
$ kubectl -n dev create sa dev

# 取得 secret 資訊
$ SECRET=$(kubectl -n dev get sa dev -o go-template=&#39;{{range .secrets}}{{.name}}{{end}}&#39;)
</code></pre>
<p>建立一個 dev.conf 設定檔，添加以下內容：</p>
<pre><code class="sh">$ API_SERVER=&quot;https://172.22.132.51:6443&quot;
$ CA_CERT=$(kubectl -n dev get secret ${SECRET} -o yaml | awk &#39;/ca.crt:/{print $2}&#39;)
$ cat &lt;&lt;EOF &gt; dev.conf
apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority-data: $CA_CERT
    server: $API_SERVER
  name: cluster
EOF

$ TOKEN=$(kubectl -n dev get secret ${SECRET} -o go-template=&#39;{{.data.token}}&#39;)
$ kubectl config set-credentials dev-user \
    --token=`echo ${TOKEN} | base64 -d` \
    --kubeconfig=dev.conf

$ kubectl config set-context default \
    --cluster=cluster \
    --user=dev-user \
    --kubeconfig=dev.conf

$ kubectl config use-context default \
    --kubeconfig=dev.conf
</code></pre>
<blockquote>
<ul>
<li>在不同作業系統中，<code>base64</code> 的 decode 指令不一樣，有些是 -D(OS X)。</li>
</ul>
</blockquote>
<p>新增 RBAC role 來限制 dev-user 存取權限:</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; dev-user-role.yml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: dev
  name: dev-user-pod
rules:
- apiGroups: [&quot;*&quot;]
  resources: [&quot;pods&quot;, &quot;pods/log&quot;]
  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;, &quot;update&quot;, &quot;create&quot;, &quot;delete&quot;]
EOF

$ kubectl create rolebinding dev-view-pod \
    --role=dev-user-pod \
    --serviceaccount=dev:dev \
    --namespace=dev
</code></pre>
<blockquote>
<ul>
<li>apiGroups 為不同 API 的群組，如 rbac.authorization.k8s.io，[“*”] 為允許存取全部。</li>
<li>resources 為 API 存取資源，如 pods、pods/log、pod/exec，[“*”] 為允許存取全部。</li>
<li>verbs 為 API 存取方法，如 get、list、watch、create、update、 delete、proxy，[“*”] 為允許存取全部。</li>
</ul>
</blockquote>
<p>透過 kubectl 確認權限設定沒問題：</p>
<pre><code class="shell=">$ kubectl --kubeconfig=dev.conf get po
Error from server (Forbidden): pods is forbidden: User &quot;system:serviceaccount:dev:dev&quot; cannot list pods in the namespace &quot;default&quot;

$ kubectl -n dev --kubeconfig=dev.conf run nginx --image nginx --port 80 --restart=Never
$ kubectl -n dev --kubeconfig=dev.conf get po
NAME      READY     STATUS    RESTARTS   AGE
nginx     1/1       Running   0          39s

$ kubectl -n dev --kubeconfig=dev.conf logs -f nginx
10.244.102.64 - - [04/Jan/2018:06:42:36 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;curl/7.47.0&quot; &quot;-&quot;

$ kubectl -n dev --kubeconfig=dev.conf exec -ti nginx sh
Error from server (Forbidden): pods &quot;nginx&quot; is forbidden: User &quot;system:serviceaccount:dev:dev&quot; cannot create pods/exec in the namespace &quot;dev&quot;
</code></pre>
<blockquote>
<ul>
<li>也可以用<code>export KUBECONFIG=dev.conf</code>來設定使用的 config。</li>
</ul>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Kubernetes </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Docker </tag>
            
            <tag> Kubernetes </tag>
            
            <tag> Kubernetes RBAC </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[多租戶 Kubernetes 部署方案 Stackube]]></title>
      <url>https://kairen.github.io/2017/12/20/openstack/stackube/</url>
      <content type="html"><![CDATA[<p><a href="https://github.com/openstack/stackube" target="_blank" rel="noopener">Stackube</a>是一個 Kubernetes-centric 的 OpenStack 發行版本(架構如下圖所示)，該專案結合 Kubernetes 與 OpenStack 的技術來達到真正的 Kubernetes 租戶隔離，如租戶實例採用 Frakti 來進行隔離、網路採用 Neutron OVS 達到每個 Namespace 擁有獨立的網路資源等。本篇會簡單介紹如何用 DevStack 建立測試用 Stackube。</p>
<a id="more"></a>
<p><img src="/images/openstack/stackube-arch.png" alt=""></p>
<blockquote>
<p>P.S. 目前 Stackube 已經不再維護，僅作為測試與研究程式碼使用。</p>
</blockquote>
<h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本次安裝作業系統採用<code>Ubuntu 16.04 Server</code>，測試環境為實體機器：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Host</th>
<th>vCPU</th>
<th>RAM</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.22.132.42</td>
<td>stackube1</td>
<td>8</td>
<td>32G</td>
</tr>
</tbody>
</table>
<h2 id="部署-Stackube"><a href="#部署-Stackube" class="headerlink" title="部署 Stackube"></a>部署 Stackube</h2><p>首先新增 Devstack 使用的 User：</p>
<pre><code class="sh">$ sudo useradd -s /bin/bash -d /opt/stack -m stack
$ echo &quot;stack ALL=(ALL) NOPASSWD: ALL&quot; | sudo tee /etc/sudoers.d/stack
$ sudo su - stack
</code></pre>
<p>透過 Git 取得 Ocata 版本的 Devstack：</p>
<pre><code class="sh">$ git clone https://git.openstack.org/openstack-dev/devstack -b stable/ocata
$ cd devstack
</code></pre>
<p>取得單節範例設定檔：</p>
<pre><code class="sh">$ curl -sSL https://raw.githubusercontent.com/kairen/stackube/master/devstack/local.conf.sample -o local.conf
</code></pre>
<p>完成後即可進行安裝：</p>
<pre><code class="sh">$ ./stack.sh
</code></pre>
<h2 id="測試基本功能"><a href="#測試基本功能" class="headerlink" title="測試基本功能"></a>測試基本功能</h2><p>完成後，就可以透過以下指令來引入 Kubernetes 與 OpenStack client 需要的環境變數：</p>
<pre><code class="sh">$ export KUBECONFIG=/opt/stack/admin.conf
$ source /opt/stack/devstack/openrc admin admin
</code></pre>
<p>Stackube 透過 CRD 新增了一個新抽象物件 Tenant，可以直接透過 Kubernetes API 來建立一個租戶，並將該租戶與 Kubernettes namespace 做綁定：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF | kubectl create -f -
apiVersion: &quot;stackube.kubernetes.io/v1&quot;
kind: Tenant
metadata:
  name: test
spec:
  username: &quot;test&quot;
  password: &quot;password&quot;
EOF

$ kubectl get namespace test
NAME      STATUS    AGE
test      Active    2h

$ kubectl -n test get network test -o yaml
apiVersion: stackube.kubernetes.io/v1
kind: Network
metadata:
  clusterName: &quot;&quot;
  creationTimestamp: 2017-12-20T06:03:33Z
  generation: 0
  name: test
  namespace: test
  resourceVersion: &quot;4631&quot;
  selfLink: /apis/stackube.kubernetes.io/v1/namespaces/test/networks/test
  uid: e9aef6fa-3316-11e8-8b66-448a5bd481f0
spec:
  cidr: 10.244.0.0/16
  gateway: 10.244.0.1
  networkID: &quot;&quot;
status:
  state: Active
</code></pre>
<p>檢查 Neutron 網路狀況：</p>
<pre><code class="sh">$ neutron net-list
+--------------------------------------+----------------------+----------------------------------+----------------------------------------------------------+
| id                                   | name                 | tenant_id                        | subnets                                                  |
+--------------------------------------+----------------------+----------------------------------+----------------------------------------------------------+
| 2a8e3b54-d76f-48a9-8380-7c2a5513b1fe | kube-test-test       | f2f25d24fd9a4616bff41b018e8725d2 | 625909a9-6abf-4661-b259-ffc625bdf681 10.244.0.0/16       |
</code></pre>
<blockquote>
<p>P.S. 這邊個人只是研究 Stackube CNI，故不針對其於進行測試，可自行參考 <a href="https://stackube.readthedocs.io/en/latest/user_guide.html" target="_blank" rel="noopener">Stackube</a>。</p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> OpenStack </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> Openstack </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Deploy OpenStack on Kubernetes using OpenStack-helm]]></title>
      <url>https://kairen.github.io/2017/11/29/openstack/openstack-helm/</url>
      <content type="html"><![CDATA[<p><a href="https://github.com/openstack/openstack-helm" target="_blank" rel="noopener">OpenStack Helm</a> 是一個提供部署建置的專案，其目的是為了推動 OpenStack 生產環境的解決方案，而這種部署方式採用容器化方式，並執行於 Kubernetes 系統上來提供 OpenStack 服務的管理與排程等使用。</p>
<p><img src="https://i.imgur.com/8sMjowM.png" alt=""></p>
<a id="more"></a>
<p>而本篇文章將說明如何建置多節點的 OpenStack Helm 環境來進行功能驗證。</p>
<h2 id="節點與安裝版本"><a href="#節點與安裝版本" class="headerlink" title="節點與安裝版本"></a>節點與安裝版本</h2><p>以下為各節點的硬體資訊。</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Role</th>
<th>CPU</th>
<th>Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.22.132.10</td>
<td>vip</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>172.22.132.101</td>
<td>master1</td>
<td>4</td>
<td>16G</td>
</tr>
<tr>
<td>172.22.132.22</td>
<td>node1</td>
<td>4</td>
<td>16G</td>
</tr>
<tr>
<td>172.22.132.24</td>
<td>node2</td>
<td>4</td>
<td>16G</td>
</tr>
<tr>
<td>172.22.132.28</td>
<td>node3</td>
<td>4</td>
<td>16G</td>
</tr>
</tbody>
</table>
<p>使用 Kernel、作業系統與軟體版本：</p>
<table>
<thead>
<tr>
<th></th>
<th>資訊描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>作業系統版本</td>
<td>16.04.3 LTS (Xenial Xerus)</td>
</tr>
<tr>
<td>Kernel 版本</td>
<td>4.4.0-101-generic</td>
</tr>
<tr>
<td>Kubernetes</td>
<td>v1.8.4</td>
</tr>
<tr>
<td>Docker</td>
<td>Docker 17.09.0-ce</td>
</tr>
<tr>
<td>Calico</td>
<td>v2.6.2</td>
</tr>
<tr>
<td>Etcd</td>
<td>v3.2.9</td>
</tr>
<tr>
<td>Ceph</td>
<td>v10.2.10</td>
</tr>
<tr>
<td>Helm</td>
<td>v2.7.0</td>
</tr>
</tbody>
</table>
<h2 id="Kubernetes-叢集"><a href="#Kubernetes-叢集" class="headerlink" title="Kubernetes 叢集"></a>Kubernetes 叢集</h2><p>本節說明如何建立 Kubernetes Cluster，這邊採用 <a href="https://github.com/kairen/kube-ansible" target="_blank" rel="noopener">kube-ansible</a> 工具來建立。</p>
<h3 id="初始化與設定基本需求"><a href="#初始化與設定基本需求" class="headerlink" title="初始化與設定基本需求"></a>初始化與設定基本需求</h3><p>安裝前需要確認以下幾個項目：</p>
<ul>
<li>所有節點的網路之間可以互相溝通。</li>
<li><code>部署節點</code>對其他節點不需要 SSH 密碼即可登入。</li>
<li>所有節點都擁有 Sudoer 權限，並且不需要輸入密碼。</li>
<li>所有節點需要安裝<code>Python</code>。</li>
<li>所有節點需要設定<code>/etc/host</code>解析到所有主機。</li>
<li><code>部署節點</code>需要安裝 <strong>Ansible &gt;= 2.4.0</strong>。</li>
</ul>
<pre><code class="shell"># Ubuntu install
$ sudo apt-get install -y software-properties-common
$ sudo apt-add-repository -y ppa:ansible/ansible
$ sudo apt-get update &amp;&amp; sudo apt-get install -y ansible git make

# CentOS install
$ sudo yum install -y epel-release
$ sudo yum -y install ansible cowsay
</code></pre>
<h3 id="安裝與設定-Kube-ansible"><a href="#安裝與設定-Kube-ansible" class="headerlink" title="安裝與設定 Kube-ansible"></a>安裝與設定 Kube-ansible</h3><p>首先取得最新穩定版本的 Kubernetes Ansible:</p>
<pre><code class="shell">$ git clone https://github.com/kairen/kube-ansible.git
$ cd kube-ansible
</code></pre>
<p>然後新增<code>inventory</code>檔案來描述要部屬的主機角色:</p>
<pre><code>[etcds]
172.22.132.101 ansible_user=ubuntu

[masters]
172.22.132.101 ansible_user=ubuntu

[nodes]
172.22.132.22 ansible_user=ubuntu
172.22.132.24 ansible_user=ubuntu
172.22.132.28 ansible_user=ubuntu

[kube-cluster:children]
masters
nodes

[kube-addon:children]
masters
</code></pre><p>接著編輯<code>group_vars/all.yml</code>檔案來添加與修改以下內容：</p>
<pre><code class="yaml"># Kubenrtes version, only support 1.8.0+.
kube_version: 1.8.4

# CNI plugin
# Support: flannel, calico, canal, weave or router.
network: calico
pod_network_cidr: 10.244.0.0/16
# CNI opts: flannel(--iface=enp0s8), calico(interface=enp0s8), canal(enp0s8).
cni_iface: &quot;&quot;

# Kubernetes cluster network.
cluster_subnet: 10.96.0
kubernetes_service_ip: &quot;{{ cluster_subnet }}.1&quot;
service_ip_range: &quot;{{ cluster_subnet }}.0/12&quot;
service_node_port_range: 30000-32767
api_secure_port: 5443

# Highly Available configuration.
haproxy: true
keepalived: true # set `lb_vip_address` as keepalived vip, if this enable.
keepalived_vip_interface: &quot;{{ ansible_default_ipv4.interface }}&quot;

lb_vip_address: 172.22.132.10
lb_secure_port: 6443
lb_api_url: &quot;https://{{ lb_vip_address }}:{{ lb_secure_port }}&quot;

etcd_iface: &quot;&quot;

insecure_registrys:
- &quot;172.22.132.253:5000&quot; # 有需要的話

ceph_cluster: true
</code></pre>
<blockquote>
<ul>
<li>這邊<code>insecure_registrys</code>為 deploy 節點的 Docker registry ip 與 port。</li>
<li>Extra addons 部分針對需求開啟，預設不會開啟。</li>
<li>若想把 Etcd, VIP 與 Network plugin 綁定在指定網路的話，請修改<code>etcd_iface</code>, <code>keepalived_vip_interface</code> 與 <code>cni_iface</code>。其中<code>cni_iface</code>需要針對不同 Plugin 來改變。</li>
<li>若想要修改部署版本的 Packages 的話，請編輯<code>roles/commons/packages/defaults/main.yml</code>來修改版本。</li>
</ul>
</blockquote>
<p>接著由於 OpenStack-helm 使用的 Kubernetes Controller Manager 不同，因此要修改<code>roles/commons/container-images/defaults/main.yml</code>的 Image 來源如下：</p>
<pre><code class="yaml">...
  manager:
  name: kube-controller-manager
  repos: kairen/
  tag: &quot;v{{ kube_version }}&quot;
...
</code></pre>
<p>完後成修改 storage roles 設定版本並進行安裝。</p>
<p>首先編輯<code>roles/storage/ceph/defaults/main.yml</code>修改版本為以下：</p>
<pre><code class="yaml">ceph_version: jewel
</code></pre>
<p>接著編輯<code>roles/storage/ceph/tasks/main.yml</code>修改成以下內容：</p>
<pre><code class="yaml">---

- name: Install Ceph dependency packages
  include_tasks: install-ceph.yml

# - name: Create and copy generator config file
#   include_tasks: gen-config.yml
#   delegate_to: &quot;{{ groups['masters'][0] }}&quot;
#   run_once: true
#
# - name: Deploy Ceph components on Kubernetes
#   include_tasks: ceph-on-k8s.yml
#   delegate_to: &quot;{{ groups['masters'][0] }}&quot;
#   run_once: true

# - name: Label all storage nodes
#   shell: &quot;kubectl label nodes node-type=storage&quot;
#   delegate_to: &quot;{{ groups['masters'][0] }}&quot;
#   run_once: true
#   ignore_errors: true
</code></pre>
<h3 id="部屬-Kubernetes-叢集"><a href="#部屬-Kubernetes-叢集" class="headerlink" title="部屬 Kubernetes 叢集"></a>部屬 Kubernetes 叢集</h3><p>確認<code>group_vars/all.yml</code>與其他設定都完成後，就透過 ansible ping 來檢查叢集狀態：</p>
<pre><code class="shell">$ ansible -i inventory all -m ping
...
172.22.132.101 | SUCCESS =&gt; {
    &quot;changed&quot;: false,
    &quot;failed&quot;: false,
    &quot;ping&quot;: &quot;pong&quot;
}
...
</code></pre>
<p>接著就可以透過以下指令進行部署叢集：</p>
<pre><code class="shell">$ ansible-playbook cluster.yml
...
TASK [cni : Apply calico network daemonset] *********************************************************************************************************************************
changed: [172.22.132.101 -&gt; 172.22.132.101]

PLAY RECAP ******************************************************************************************************************************************************************
172.22.132.101             : ok=155  changed=58   unreachable=0    failed=0
172.22.132.22              : ok=117  changed=28   unreachable=0    failed=0
172.22.132.24              : ok=50   changed=18   unreachable=0    failed=0
172.22.132.28              : ok=51   changed=19   unreachable=0    failed=0
</code></pre>
<p>完成後，進入<code>master</code>節點執行以下指令確認叢集：</p>
<pre><code class="shell">$ kubectl get node
NAME           STATUS    ROLES     AGE       VERSION
kube-master1   Ready     master    1h        v1.8.4
kube-node1     Ready     &lt;none&gt;    1h        v1.8.4
kube-node2     Ready     &lt;none&gt;    1h        v1.8.4
kube-node3     Ready     &lt;none&gt;    1h        v1.8.4

$ kubectl -n kube-system get po
NAME                                       READY     STATUS    RESTARTS   AGE
calico-node-js6qp                          2/2       Running   2          1h
calico-node-kx9xn                          2/2       Running   2          1h
calico-node-lxrjl                          2/2       Running   2          1h
calico-node-vwn5f                          2/2       Running   2          1h
calico-policy-controller-d549764f6-9kn9l   1/1       Running   1          1h
haproxy-kube-master1                       1/1       Running   1          1h
keepalived-kube-master1                    1/1       Running   1          1h
kube-apiserver-kube-master1                1/1       Running   1          1h
kube-controller-manager-kube-master1       1/1       Running   1          1h
kube-dns-7bd4879dc9-kxmx6                  3/3       Running   3          1h
kube-proxy-7tqkm                           1/1       Running   1          1h
kube-proxy-glzmm                           1/1       Running   1          1h
kube-proxy-krqxs                           1/1       Running   1          1h
kube-proxy-x9zdb                           1/1       Running   1          1h
kube-scheduler-kube-master1                1/1       Running   1          1h
</code></pre>
<p>檢查 kube-dns 是否連 host 都能夠解析:</p>
<pre><code class="shell">$ nslookup kubernetes
Server:        10.96.0.10
Address:    10.96.0.10#53

Non-authoritative answer:
Name:    kubernetes.default.svc.cluster.local
Address: 10.96.0.1
</code></pre>
<p>接著安裝 Ceph 套件：</p>
<pre><code class="sh">$ ansible-playbook storage.yml
</code></pre>
<h2 id="OpenStack-helm-叢集"><a href="#OpenStack-helm-叢集" class="headerlink" title="OpenStack-helm 叢集"></a>OpenStack-helm 叢集</h2><p>本節說明如何建立 OpenStack on Kubernetes 使用 Helm，部署是使用 <a href="https://github.com/openstack/openstack-helm" target="_blank" rel="noopener">openstack-helm</a>。過程將透過 OpenStack-helm 來在 Kubernetes 建置 OpenStack 叢集。以下所有操作都在<code>kube-master1</code>上進行。</p>
<h3 id="Helm-init"><a href="#Helm-init" class="headerlink" title="Helm init"></a>Helm init</h3><p>在開始前需要先將 Helm 進行初始化，以提供後續使用，然而這邊由於使用到 RBAC 的關係，因此需建立一個 Service account 來提供給 Helm 使用：</p>
<pre><code class="shell">$ kubectl -n kube-system create sa tiller
$ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
$ helm init --service-account tiller
</code></pre>
<blockquote>
<p>由於 <code>kube-ansible</code> 本身包含 Helm 工具, 因此不需要自己安裝，只需要依據上面指令進行 init 即可。</p>
</blockquote>
<p>新增一個檔案<code>openrc</code>來提供環境變數：</p>
<pre><code class="shell">export HELM_HOST=$(kubectl describe svc/tiller-deploy -n kube-system | awk &#39;/Endpoints/{print $2}&#39;)
export OSD_CLUSTER_NETWORK=172.22.132.0/24
export OSD_PUBLIC_NETWORK=172.22.132.0/24
export WORK_DIR=local
export CEPH_RGW_KEYSTONE_ENABLED=true
</code></pre>
<blockquote>
<ul>
<li><code>OSD_CLUSTER_NETWORK</code>與<code>OSD_PUBLIC_NETWORK</code>都是使用實體機器網路，這邊 daemonset 會使用 hostNetwork。</li>
<li><code>CEPH_RGW_KEYSTONE_ENABLED</code> 在 Kubernetes 版本有點不穩，可依需求關閉。</li>
</ul>
</blockquote>
<p>完成後，透過 source 指令引入:</p>
<pre><code class="shell">$ source openrc
$ helm version
Client: &amp;version.Version{SemVer:&quot;v2.7.0&quot;, GitCommit:&quot;08c1144f5eb3e3b636d9775617287cc26e53dba4&quot;, GitTreeState:&quot;clean&quot;}
Server: &amp;version.Version{SemVer:&quot;v2.7.0&quot;, GitCommit:&quot;08c1144f5eb3e3b636d9775617287cc26e53dba4&quot;, GitTreeState:&quot;clean&quot;}
</code></pre>
<h3 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h3><p>首先透過 Kubernetes label 來標示每個節點的角色：</p>
<pre><code class="shell">kubectl label nodes openstack-control-plane=enabled --all
kubectl label nodes ceph-mon=enabled --all
kubectl label nodes ceph-osd=enabled --all
kubectl label nodes ceph-mds=enabled --all
kubectl label nodes ceph-rgw=enabled --all
kubectl label nodes ceph-mgr=enabled --all
kubectl label nodes openvswitch=enabled --all
kubectl label nodes openstack-compute-node=enabled --all
</code></pre>
<blockquote>
<p>這邊為了避免過度的節點污染，因此不讓 masters 充當任何角色：</p>
<pre><code class="shell">kubectl label nodes kube-master1 openstack-control-plane-
kubectl label nodes kube-master1 ceph-mon-
kubectl label nodes kube-master1 ceph-osd-
kubectl label nodes kube-master1 ceph-mds-
kubectl label nodes kube-master1 ceph-rgw-
kubectl label nodes kube-master1 ceph-mgr-
kubectl label nodes kube-master1 openvswitch-
kubectl label nodes kube-master1 openstack-compute-node-
</code></pre>
</blockquote>
<p>由於使用 Kubernetes RBAC，而目前 openstack-helm 有 bug，不會正確建立 Service account 的 ClusterRoleBindings，因此要手動建立(這邊偷懶一下直接使用 Admin roles)：</p>
<pre><code class="shell">$ cat &lt;&lt;EOF | kubectl create -f -
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: ceph-sa-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: system:serviceaccount:ceph:default
EOF

$ cat &lt;&lt;EOF | kubectl create -f -
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: openstack-sa-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: system:serviceaccount:openstack:default
EOF
</code></pre>
<blockquote>
<p>若沒有建立的話，會有類似以下的錯誤資訊：</p>
<pre><code>Error from server (Forbidden): error when creating &quot;STDIN&quot;: secrets is forbidden: User &quot;system:serviceaccount:ceph:default&quot; cannot create secrets in the namespace &quot;ceph&quot;
</code></pre></blockquote>
<p>下載最新版本 openstack-helm 專案：</p>
<pre><code class="shell">$ git clone https://github.com/openstack/openstack-helm.git
$ cd openstack-helm
</code></pre>
<p>現在須建立 openstack-helm chart 來提供部署使用：</p>
<pre><code class="shell">$ helm serve &amp;
$ helm repo add local http://localhost:8879/charts
$ make
# output
...
1 chart(s) linted, no failures
if [ -d congress ]; then helm package congress; fi
Successfully packaged chart and saved it to: /root/openstack-helm/congress-0.1.0.tgz
make[1]: Leaving directory &#39;/root/openstack-helm&#39;
</code></pre>
<h3 id="Ceph-Chart"><a href="#Ceph-Chart" class="headerlink" title="Ceph Chart"></a>Ceph Chart</h3><p>在部署 OpenStack 前，需要先部署 Ceph 叢集，這邊透過以下指令建置：</p>
<pre><code class="shell">$ helm install --namespace=ceph ${WORK_DIR}/ceph --name=ceph \
  --set endpoints.identity.namespace=openstack \
  --set endpoints.object_store.namespace=ceph \
  --set endpoints.ceph_mon.namespace=ceph \
  --set ceph.rgw_keystone_auth=${CEPH_RGW_KEYSTONE_ENABLED} \
  --set network.public=${OSD_PUBLIC_NETWORK} \
  --set network.cluster=${OSD_CLUSTER_NETWORK} \
  --set deployment.storage_secrets=true \
  --set deployment.ceph=true \
  --set deployment.rbd_provisioner=true \
  --set deployment.client_secrets=false \
  --set deployment.rgw_keystone_user_and_endpoints=false \
  --set bootstrap.enabled=true
</code></pre>
<blockquote>
<ul>
<li><code>CEPH_RGW_KEYSTONE_ENABLED</code>是否啟動 Ceph RGW Keystone。</li>
<li><code>OSD_PUBLIC_NETWORK</code>與<code>OSD_PUBLIC_NETWORK</code>為 Ceph 叢集網路。</li>
</ul>
</blockquote>
<p>成功安裝 Ceph chart 後，就可以透過 kubectl 來查看結果：</p>
<pre><code class="shell">$ kubectl -n ceph get po
NAME                                   READY     STATUS    RESTARTS   AGE
ceph-mds-57798cc8f6-r898r              1/1       Running   2          10min
ceph-mon-96p9r                         1/1       Running   0          10min
ceph-mon-check-bd8875f87-whvhd         1/1       Running   0          10min
ceph-mon-qkj95                         1/1       Running   0          10min
ceph-mon-zx7tw                         1/1       Running   0          10min
ceph-osd-5fvfl                         1/1       Running   0          10min
ceph-osd-kvw9b                         1/1       Running   0          10min
ceph-osd-wcf5j                         1/1       Running   0          10min
ceph-rbd-provisioner-599ff9575-mdqnf   1/1       Running   0          10min
ceph-rbd-provisioner-599ff9575-vpcr6   1/1       Running   0          10min
ceph-rgw-7c8c5d4f6f-8fq9c              1/1       Running   3          10min
</code></pre>
<p>確認 Ceph 叢集建立正確：</p>
<pre><code class="shell">$ MON_POD=$(kubectl get pods \
  --namespace=ceph \
  --selector=&quot;application=ceph&quot; \
  --selector=&quot;component=mon&quot; \
  --no-headers | awk &#39;{ print $1; exit }&#39;)
$ kubectl exec -n ceph ${MON_POD} -- ceph -s

    cluster 02ad8724-dee0-4f55-829f-3cc24e2c7571
     health HEALTH_WARN
            too many PGs per OSD (856 &gt; max 300)
     monmap e2: 3 mons at {kube-node1=172.22.132.22:6789/0,kube-node2=172.22.132.24:6789/0,kube-node3=172.22.132.28:6789/0}
            election epoch 8, quorum 0,1,2 kube-node1,kube-node2,kube-node3
      fsmap e5: 1/1/1 up {0=mds-ceph-mds-57798cc8f6-r898r=up:active}
     osdmap e21: 3 osds: 3 up, 3 in
            flags sortbitwise,require_jewel_osds
      pgmap v6053: 856 pgs, 10 pools, 3656 bytes data, 191 objects
            43091 MB used, 2133 GB / 2291 GB avail
                 856 active+clean
</code></pre>
<blockquote>
<p>Warn 這邊忽略，OSD 機器太少….。</p>
</blockquote>
<p>接著為了讓 Ceph 可以在其他 Kubernetes namespace 中存取 PVC，這邊要產生 client secret key 於 openstack namespace 中來提供給 OpenStack 元件使用，這邊執行以下 Chart 來產生：</p>
<pre><code class="shell">$ helm install --namespace=openstack ${WORK_DIR}/ceph --name=ceph-openstack-config \
  --set endpoints.identity.namespace=openstack \
  --set endpoints.object_store.namespace=ceph \
  --set endpoints.ceph_mon.namespace=ceph \
  --set ceph.rgw_keystone_auth=${CEPH_RGW_KEYSTONE_ENABLED} \
  --set network.public=${OSD_PUBLIC_NETWORK} \
  --set network.cluster=${OSD_CLUSTER_NETWORK} \
  --set deployment.storage_secrets=false \
  --set deployment.ceph=false \
  --set deployment.rbd_provisioner=false \
  --set deployment.client_secrets=true \
  --set deployment.rgw_keystone_user_and_endpoints=false
</code></pre>
<p>檢查 pod 與 secret 是否建立成功：</p>
<pre><code class="shell">$ kubectl -n openstack get secret,po -a
NAME                          TYPE                                  DATA      AGE
secrets/default-token-q2r87   kubernetes.io/service-account-token   3         2m
secrets/pvc-ceph-client-key   kubernetes.io/rbd                     1         2m

NAME                                           READY     STATUS      RESTARTS   AGE
po/ceph-namespace-client-key-generator-w84n4   0/1       Completed   0          2m
</code></pre>
<h3 id="OpenStack-Chart"><a href="#OpenStack-Chart" class="headerlink" title="OpenStack Chart"></a>OpenStack Chart</h3><p>確認沒問題後，就可以開始部署 OpenStack chart 了。首先先安裝 Mariadb cluster:</p>
<pre><code class="shell">$ helm install --name=mariadb ./mariadb --namespace=openstack
</code></pre>
<blockquote>
<p>這邊跑超久…34mins…，原因可能是 Storage 效能問題。</p>
</blockquote>
<p>這邊正確執行後，會依序依據 StatefulSet 建立起 Pod 組成 Cluster：</p>
<pre><code class="shell">$ kubectl -n openstack get po
NAME        READY     STATUS    RESTARTS   AGE
mariadb-0   1/1       Running   0          37m
mariadb-1   1/1       Running   0          4m
mariadb-2   1/1       Running   0          2m
</code></pre>
<p>當 Mariadb cluster 完成後，就可以部署一些需要的服務，如 RabbitMQ, OVS 等：</p>
<pre><code class="shell">helm install --name=memcached ./memcached --namespace=openstack
helm install --name=etcd-rabbitmq ./etcd --namespace=openstack
helm install --name=rabbitmq ./rabbitmq --namespace=openstack
helm install --name=ingress ./ingress --namespace=openstack
helm install --name=libvirt ./libvirt --namespace=openstack
helm install --name=openvswitch ./openvswitch --namespace=openstack
</code></pre>
<p>上述指令若正確執行的話，會分別建立起以下服務：</p>
<pre><code class="shell">$ kubectl -n openstack get po
NAME                                   READY     STATUS    RESTARTS   AGE
etcd-5c9bc8c97f-jpm2k                  1/1       Running   0          4m
ingress-api-jhjjv                      1/1       Running   0          4m
ingress-api-nx5qm                      1/1       Running   0          4m
ingress-api-vr8xf                      1/1       Running   0          4m
ingress-error-pages-86b9db69cc-mmq4p   1/1       Running   0          4m
libvirt-94xq5                          1/1       Running   0          4m
libvirt-lzfzs                          1/1       Running   0          4m
libvirt-vswxb                          1/1       Running   0          4m
mariadb-0                              1/1       Running   0          42m
mariadb-1                              1/1       Running   0          9m
mariadb-2                              1/1       Running   0          7m
memcached-746fcc894-cwhpr              1/1       Running   0          4m
openvswitch-db-7fjr2                   1/1       Running   0          4m
openvswitch-db-gtmcr                   1/1       Running   0          4m
openvswitch-db-hqmbt                   1/1       Running   0          4m
openvswitch-vswitchd-gptp9             1/1       Running   0          4m
openvswitch-vswitchd-s4cwd             1/1       Running   0          4m
openvswitch-vswitchd-tvxlg             1/1       Running   0          4m
rabbitmq-6fdb8879df-6vmz8              1/1       Running   0          4m
rabbitmq-6fdb8879df-875zz              1/1       Running   0          4m
rabbitmq-6fdb8879df-h5wj6              1/1       Running   0          4m
</code></pre>
<p>一旦所有基礎服務與元件都建立完成後，就可以開始部署 OpenStack 的專案 Chart，首先建立 Keystone 來提供身份認證服務：</p>
<pre><code class="shell">$ helm install --namespace=openstack --name=keystone ./keystone \
  --set pod.replicas.api=1

$ kubectl -n openstack get po -l application=keystone
NAME                            READY     STATUS     RESTARTS   AGE
keystone-api-74c774d448-dkqmj   0/1       Init:0/1   0          4m
keystone-bootstrap-xpdtl        0/1       Init:0/1   0          4m
keystone-db-sync-2bxtp          1/1       Running    0          4m        0          29s
</code></pre>
<blockquote>
<p>這邊由於叢集規模問題，副本數都為一份。</p>
</blockquote>
<p>這時候會先建立 Keystone database tables，完成後將啟動 API pod，如以下結果：</p>
<pre><code class="shell">$ kubectl -n openstack get po -l application=keystone
NAME                            READY     STATUS    RESTARTS   AGE
keystone-api-74c774d448-dkqmj   1/1       Running   0          11m
</code></pre>
<p>如果安裝支援 RGW 的 Keystone endpoint 的話，可以使用以下方式建立：</p>
<pre><code class="shell">$ helm install --namespace=openstack ${WORK_DIR}/ceph --name=radosgw-openstack \
  --set endpoints.identity.namespace=openstack \
  --set endpoints.object_store.namespace=ceph \
  --set endpoints.ceph_mon.namespace=ceph \
  --set ceph.rgw_keystone_auth=${CEPH_RGW_KEYSTONE_ENABLED} \
  --set network.public=${OSD_PUBLIC_NETWORK} \
  --set network.cluster=${OSD_CLUSTER_NETWORK} \
  --set deployment.storage_secrets=false \
  --set deployment.ceph=false \
  --set deployment.rbd_provisioner=false \
  --set deployment.client_secrets=false \
  --set deployment.rgw_keystone_user_and_endpoints=true

$ kubectl -n openstack get po -a -l application=ceph
NAME                                        READY     STATUS      RESTARTS   AGE
ceph-ks-endpoints-vfg4l                     0/3       Completed   0          1m
ceph-ks-service-tr9xt                       0/1       Completed   0          1m
ceph-ks-user-z5tlt                          0/1       Completed   0          1m
</code></pre>
<p>完成後，安裝 Horizon chart 來提供 OpenStack dashbaord：</p>
<pre><code class="shell">$ helm install --namespace=openstack --name=horizon ./horizon \
  --set network.enable_node_port=true \
  --set network.node_port=31000

$ kubectl -n openstack get po -l application=horizon
NAME                       READY     STATUS    RESTARTS   AGE
horizon-7c54878549-45668   1/1       Running   0          3m
</code></pre>
<p>接著安裝 Glance chart 來提供 OpenStack image service。目前 Glance 支援幾個 backend storage:</p>
<ul>
<li><strong>pvc</strong>: 一個簡單的 Kubernetes PVCs 檔案後端。</li>
<li><strong>rbd</strong>: 使用 Ceph RBD 來儲存 images。</li>
<li><strong>radosgw</strong>: 使用 Ceph RGW 來儲存 images。</li>
<li><strong>swift</strong>: 另用 OpenStack switf 所提供的物件儲存服務來儲存 images.</li>
</ul>
<p>這邊可以利用以下方式來部署不同的儲存後端：</p>
<pre><code class="shell">$ export GLANCE_BACKEND=radosgw
$ helm install --namespace=openstack --name=glance ./glance \
  --set pod.replicas.api=1 \
  --set pod.replicas.registry=1 \
  --set storage=${GLANCE_BACKEND}

$ kubectl -n openstack get po -l application=glance
NAME                               READY     STATUS    RESTARTS   AGE
glance-api-6cd8b856d6-lhzfs        1/1       Running   0          14m
glance-registry-599f8b857b-gt4c6   1/1       Running   0          14m
</code></pre>
<p>接著安裝 Neutron chart 來提供 OpenStack 虛擬化網路服務：</p>
<pre><code class="shell">$ helm install --namespace=openstack --name=neutron ./neutron \
  --set pod.replicas.server=1

$ kubectl -n openstack get po -l application=neutron
NAME                              READY     STATUS    RESTARTS   AGE
neutron-dhcp-agent-2z49d          1/1       Running   0          9h
neutron-dhcp-agent-d2kn8          1/1       Running   0          9h
neutron-dhcp-agent-mrstl          1/1       Running   0          9h
neutron-l3-agent-9f9mw            1/1       Running   0          9h
neutron-l3-agent-cshzw            1/1       Running   0          9h
neutron-l3-agent-j5vb9            1/1       Running   0          9h
neutron-metadata-agent-6bfb2      1/1       Running   0          9h
neutron-metadata-agent-kxk9c      1/1       Running   0          9h
neutron-metadata-agent-w8cnl      1/1       Running   0          9h
neutron-ovs-agent-j2549           1/1       Running   0          9h
neutron-ovs-agent-plj9t           1/1       Running   0          9h
neutron-ovs-agent-xlx7z           1/1       Running   0          9h
neutron-server-6f45d74b87-6wmck   1/1       Running   0          9h
</code></pre>
<p>接著安裝 Nova chart 來提供 OpenStack 虛擬機運算服務:</p>
<pre><code class="shell">$ helm install --namespace=openstack --name=nova ./nova \
  --set pod.replicas.api_metadata=1 \
  --set pod.replicas.osapi=1 \
  --set pod.replicas.conductor=1 \
  --set pod.replicas.consoleauth=1 \
  --set pod.replicas.scheduler=1 \
  --set pod.replicas.novncproxy=1

$ kubectl -n openstack get po -l application=nova
NAME                                 READY     STATUS    RESTARTS   AGE
nova-api-metadata-84fdc84fd7-ldzrh   1/1       Running   1          9h
nova-api-osapi-57f599c6d6-pqrjv      1/1       Running   0          9h
nova-compute-8rvm9                   2/2       Running   0          9h
nova-compute-cbk7h                   2/2       Running   0          9h
nova-compute-tf2jb                   2/2       Running   0          9h
nova-conductor-7f5bc76d79-bxwnb      1/1       Running   0          9h
nova-consoleauth-6946b5884f-nss6n    1/1       Running   0          9h
nova-novncproxy-d789dccff-7ft9q      1/1       Running   0          9h
nova-placement-api-f7c79578f-hj2g9   1/1       Running   0          9h
nova-scheduler-778866f555-mmksg      1/1       Running   0          9h
</code></pre>
<p>接著安裝 Cinfer chart 來提供 OpenStack 區塊儲存服務:</p>
<pre><code class="shell">$ helm install --namespace=openstack --name=cinder ./cinder \
  --set pod.replicas.api=1

$ kubectl -n openstack get po -l application=cinder
NAME                                READY     STATUS    RESTARTS   AGE
cinder-api-5cc89f5467-ssm8k         1/1       Running   0          32m
cinder-backup-67c4d8dfdb-zfsq4      1/1       Running   0          32m
cinder-scheduler-65f9dd49bf-6htwg   1/1       Running   0          32m
cinder-volume-69bfb67b4-bmst2       1/1       Running   0          32m
</code></pre>
<p>(option)都完成後，將 Horizon 服務透過 NodePort 方式曝露出來(如果上面 Horizon chart 沒反應的話)，執行以下指令編輯：</p>
<pre><code class="shell">$ kubectl -n openstack edit svc horizon-int
# 修改 type:
  type: NodePort
</code></pre>
<p>最後連接 <a href="http://172.22.132.10:31000" target="_blank" rel="noopener">Horizon Dashboard</a>，預設使用者為<code>admin/password</code>。</p>
<p><img src="https://i.imgur.com/8yunUPy.png" alt=""></p>
<p>其他 Chart 可以利用以下方式來安裝，如 Heat chart：</p>
<pre><code class="shell">$ helm install --namespace=openstack --name=heat ./heat

$ kubectl -n openstack get po -l application=heat
NAME                              READY     STATUS    RESTARTS   AGE
heat-api-5cf45d9d44-qrt69         1/1       Running   0          13m
heat-cfn-79dbf55789-bq4wh         1/1       Running   0          13m
heat-cloudwatch-bcc4647f4-4c4ln   1/1       Running   0          13m
heat-engine-55cfcc86f8-cct4m      1/1       Running   0          13m
</code></pre>
<h2 id="測試-OpenStack-功能"><a href="#測試-OpenStack-功能" class="headerlink" title="測試 OpenStack 功能"></a>測試 OpenStack 功能</h2><p>在<code>kube-master1</code>安裝 openstack client:</p>
<pre><code class="shell">$ sudo pip install python-openstackclient
</code></pre>
<p>建立<code>adminrc</code>來提供 client 環境變數：</p>
<pre><code class="shell">export OS_PROJECT_DOMAIN_NAME=default
export OS_USER_DOMAIN_NAME=default
export OS_PROJECT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=password
export OS_AUTH_URL=http://keystone.openstack.svc.cluster.local:80/v3
export OS_IDENTITY_API_VERSION=3
export OS_IMAGE_API_VERSION=2
</code></pre>
<p>引入環境變數，並透過 openstack client 測試：</p>
<pre><code class="shell">$ source adminrc
$ openstack user list
+----------------------------------+-----------+
| ID                               | Name      |
+----------------------------------+-----------+
| 42f0d2e7823e413cb469f9cce731398a | glance    |
| 556a2744811f450098f64b37d34192d4 | nova      |
| a97ec73724aa4445b2d575be54f23240 | cinder    |
| b28a5dcfd18948419e14acba7ecf6f63 | swift     |
| d1f312b6bb7c460eb7d8d78c8bf350fc | admin     |
| dc326aace22c4314a0100865fe4f57c2 | neutron   |
| ec5d6d3c529847b29a1c9187599c8a6b | placement |
+----------------------------------+-----------+
</code></pre>
<p>接著需要設定對外網路來提供給 VM 存取，在有<code>neutron-l3-agent</code>節點上，新增一個腳本<code>setup-gateway.sh</code>：</p>
<pre><code class="shell">#!/bin/bash
set -x

# Assign IP address to br-ex
OSH_BR_EX_ADDR=&quot;172.24.4.1/24&quot;
OSH_EXT_SUBNET=&quot;172.24.4.0/24&quot;
sudo ip addr add ${OSH_BR_EX_ADDR} dev br-ex
sudo ip link set br-ex up

# Setup masquerading on default route dev to public subnet
DEFAULT_ROUTE_DEV=&quot;enp3s0&quot;
sudo iptables -t nat -A POSTROUTING -o ${DEFAULT_ROUTE_DEV} -s ${OSH_EXT_SUBNET} -j MASQUERADE
</code></pre>
<blockquote>
<ul>
<li>網卡記得修改<code>DEFAULT_ROUTE_DEV</code>。</li>
<li>這邊因為沒有額外提供其他張網卡，所以先用 bridge 處理。</li>
</ul>
</blockquote>
<p>然後透過執行該腳本建立一個 bridge 網路：</p>
<pre><code class="shell">$ chmod u+x setup-gateway.sh
$ ./setup-gateway.sh
</code></pre>
<p>確認完成後，接著建立 Neutron ext net，透過以下指令進行建立：</p>
<pre><code class="shell">$ openstack network create \
   --share --external \
   --provider-physical-network external \
   --provider-network-type flat ext-net

$ openstack subnet create --network ext-net \
    --allocation-pool start=172.24.4.10,end=172.24.4.100 \
    --dns-nameserver 8.8.8.8 --gateway 172.24.4.1 \
    --subnet-range 172.24.4.0/24 \
    --no-dhcp ext-subnet

$ openstack router create router1
$ neutron router-gateway-set router1 ext-net
</code></pre>
<p>直接進入 Dashboard 新增 Self-service Network:<br><img src="https://i.imgur.com/lqMrgqs.png" alt=""></p>
<p>加入到 router1:<br><img src="https://i.imgur.com/4aNnF3O.png" alt=""></p>
<p>完成後，就可以建立 instance，這邊都透過 Dashboard 來操作：<br><img src="https://i.imgur.com/fCYkxSC.png" alt=""></p>
<p>透過 SSH 進入 instance：<br><img src="https://i.imgur.com/Ijylo9X.png" alt=""></p>
<h2 id="Refers"><a href="#Refers" class="headerlink" title="Refers"></a>Refers</h2><ul>
<li><a href="https://github.com/portdirect/sydney-workshop" target="_blank" rel="noopener">sydney-workshop</a></li>
<li><a href="https://docs.openstack.org/openstack-helm/latest/install/multinode.html" target="_blank" rel="noopener">Multi Node</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> OpenStack </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> Helm </tag>
            
            <tag> Openstack </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes v1.8.x 全手動苦工安裝教學(TL;DR)]]></title>
      <url>https://kairen.github.io/2017/10/27/kubernetes/deploy/manual-v1.8/</url>
      <content type="html"><![CDATA[<p>Kubernetes 提供了許多雲端平台與作業系統的安裝方式，本章將以<code>全手動安裝方式</code>來部署 Kubernetes v1.8.x 版本，主要是學習與了解 Kubernetes 建置流程。若想要瞭解更多平台的部署可以參考 <a href="https://kubernetes.io/docs/getting-started-guides/" target="_blank" rel="noopener">Picking the Right Solution</a>來選擇自己最喜歡的方式。</p>
<p>本次安裝版本為：</p>
<ul>
<li>Kubernetes v1.8.6</li>
<li>CNI v0.6.0</li>
<li>Etcd v3.2.9</li>
<li>Calico v2.6.2</li>
<li>Docker v17.10.0-ce</li>
</ul>
<a id="more"></a>
<h2 id="預先準備資訊"><a href="#預先準備資訊" class="headerlink" title="預先準備資訊"></a>預先準備資訊</h2><p>本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統可採用<code>Ubuntu 16.x</code>與<code>CentOS 7.x</code>：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Role</th>
<th>CPU</th>
<th>Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.16.35.12</td>
<td>master1</td>
<td>1</td>
<td>2G</td>
</tr>
<tr>
<td>172.16.35.10</td>
<td>node1</td>
<td>1</td>
<td>2G</td>
</tr>
<tr>
<td>172.16.35.11</td>
<td>node2</td>
<td>1</td>
<td>2G</td>
</tr>
</tbody>
</table>
<blockquote>
<ul>
<li>這邊 master 為主要控制節點也是<code>部署節點</code>，node 為應用程式工作節點。</li>
<li>所有操作全部用<code>root</code>使用者進行(方便用)，以 SRE 來說不推薦。</li>
<li>可以下載 <a href="https://kairen.github.io/files/manual-v1.8/Vagrantfile">Vagrantfile</a> 來建立 Virtual box 虛擬機叢集。</li>
</ul>
</blockquote>
<p>首先安裝前要確認以下幾項都已經準備完成：</p>
<ul>
<li>所有節點彼此網路互通，並且<code>master1</code> SSH 登入其他節點為 passwdless。</li>
<li>所有防火牆與 SELinux 已關閉。如 CentOS：</li>
</ul>
<pre><code class="sh">$ systemctl stop firewalld &amp;&amp; systemctl disable firewalld
$ setenforce 0
$ vim /etc/selinux/config
SELINUX=disabled
</code></pre>
<ul>
<li>所有節點需要設定<code>/etc/host</code>解析到所有主機。</li>
</ul>
<pre><code>...
172.16.35.10 node1
172.16.35.11 node2
172.16.35.12 master1
</code></pre><ul>
<li>所有節點需要安裝<code>Docker</code>或<code>rtk</code>引擎。這邊採用<code>Docker</code>來當作容器引擎，安裝方式如下：</li>
</ul>
<pre><code class="sh">$ curl -fsSL &quot;https://get.docker.com/&quot; | sh
</code></pre>
<blockquote>
<p>不管是在 <code>Ubuntu</code> 或 <code>CentOS</code> 都只需要執行該指令就會自動安裝最新版 Docker。<br>CentOS 安裝完成後，需要再執行以下指令：</p>
<pre><code class="sh">$ systemctl enable docker &amp;&amp; systemctl start docker
</code></pre>
</blockquote>
<p>編輯<code>/lib/systemd/system/docker.service</code>，在<code>ExecStart=..</code>上面加入：</p>
<pre><code>ExecStartPost=/sbin/iptables -A FORWARD -s 0.0.0.0/0 -j ACCEPT
</code></pre><blockquote>
<p>完成後，重新啟動 docker 服務：</p>
<pre><code class="sh">$ systemctl daemon-reload &amp;&amp; systemctl restart docker
</code></pre>
</blockquote>
<ul>
<li>所有節點需要設定<code>/etc/sysctl.d/k8s.conf</code>的系統參數。</li>
</ul>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

$ sysctl -p /etc/sysctl.d/k8s.conf
</code></pre>
<ul>
<li>在<code>master1</code>需要安裝<code>CFSSL</code>工具，這將會用來建立 TLS certificates。</li>
</ul>
<pre><code class="sh">$ export CFSSL_URL=&quot;https://pkg.cfssl.org/R1.2&quot;
$ wget &quot;${CFSSL_URL}/cfssl_linux-amd64&quot; -O /usr/local/bin/cfssl
$ wget &quot;${CFSSL_URL}/cfssljson_linux-amd64&quot; -O /usr/local/bin/cfssljson
$ chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson
</code></pre>
<h2 id="Etcd"><a href="#Etcd" class="headerlink" title="Etcd"></a>Etcd</h2><p>在開始安裝 Kubernetes 之前，需要先將一些必要系統建置完成，其中 Etcd 就是 Kubernetes 最重要的一環，Kubernetes 會將大部分資訊儲存於 Etcd 上，來提供給其他節點索取，以確保整個叢集運作與溝通正常。</p>
<h3 id="建立叢集-CA-與-Certificates"><a href="#建立叢集-CA-與-Certificates" class="headerlink" title="建立叢集 CA 與 Certificates"></a>建立叢集 CA 與 Certificates</h3><p>在這部分，將會需要產生 client 與 server 的各元件 certificates，並且替 Kubernetes admin user 產生 client 證書。</p>
<p>首先在<code>master1</code>建立<code>/etc/etcd/ssl</code>資料夾，然後進入目錄完成以下操作。</p>
<pre><code class="sh">$ mkdir -p /etc/etcd/ssl &amp;&amp; cd /etc/etcd/ssl
$ export PKI_URL=&quot;https://kairen.github.io/files/manual-v1.8/pki&quot;
</code></pre>
<p>下載<code>ca-config.json</code>與<code>etcd-ca-csr.json</code>檔案，並從 CSR json 產生 CA 金鑰與 Certificate：</p>
<pre><code class="sh">$ wget &quot;${PKI_URL}/ca-config.json&quot; &quot;${PKI_URL}/etcd-ca-csr.json&quot;
$ cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare etcd-ca
$ ls etcd-ca*.pem
etcd-ca-key.pem  etcd-ca.pem
</code></pre>
<p>下載<code>etcd-csr.json</code>檔案，並產生 Etcd certificate 證書：</p>
<pre><code class="sh">$ wget &quot;${PKI_URL}/etcd-csr.json&quot;
$ cfssl gencert \
  -ca=etcd-ca.pem \
  -ca-key=etcd-ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  etcd-csr.json | cfssljson -bare etcd

$ ls etcd*.pem
etcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem
</code></pre>
<blockquote>
<p>若節點 IP 不同，需要修改<code>etcd-csr.json</code>的<code>hosts</code>。</p>
</blockquote>
<p>完成後刪除不必要檔案：</p>
<pre><code class="sh">$ rm -rf *.json
</code></pre>
<p>確認<code>/etc/etcd/ssl</code>有以下檔案：</p>
<pre><code class="sh">$ ls /etc/etcd/ssl
etcd-ca.csr  etcd-ca-key.pem  etcd-ca.pem  etcd.csr  etcd-key.pem  etcd.pem
</code></pre>
<h3 id="Etcd-安裝與設定"><a href="#Etcd-安裝與設定" class="headerlink" title="Etcd 安裝與設定"></a>Etcd 安裝與設定</h3><p>首先在<code>master1</code>節點下載 Etcd，並解壓縮放到 /opt 底下與安裝：</p>
<pre><code class="sh">$ export ETCD_URL=&quot;https://github.com/coreos/etcd/releases/download&quot;
$ cd &amp;&amp; wget -qO- --show-progress &quot;${ETCD_URL}/v3.2.9/etcd-v3.2.9-linux-amd64.tar.gz&quot; | tar -zx
$ mv etcd-v3.2.9-linux-amd64/etcd* /usr/local/bin/ &amp;&amp; rm -rf etcd-v3.2.9-linux-amd64
</code></pre>
<p>完成後新建 Etcd Group 與 User，並建立 Etcd 設定檔目錄：</p>
<pre><code class="sh">$ groupadd etcd &amp;&amp; useradd -c &quot;Etcd user&quot; -g etcd -s /sbin/nologin -r etcd
</code></pre>
<p>下載<code>etcd</code>相關檔案，我們將來管理 Etcd：</p>
<pre><code class="sh">$ export ETCD_CONF_URL=&quot;https://kairen.github.io/files/manual-v1.8/master&quot;
$ wget &quot;${ETCD_CONF_URL}/etcd.conf&quot; -O /etc/etcd/etcd.conf
$ wget &quot;${ETCD_CONF_URL}/etcd.service&quot; -O /lib/systemd/system/etcd.service
</code></pre>
<blockquote>
<p>若與該教學 IP 不同的話，請用自己 IP 取代<code>172.16.35.12</code>。</p>
</blockquote>
<p>建立 var 存放資訊，然後啟動 Etcd 服務:</p>
<pre><code class="sh">$ mkdir -p /var/lib/etcd &amp;&amp; chown etcd:etcd -R /var/lib/etcd /etc/etcd
$ systemctl enable etcd.service &amp;&amp; systemctl start etcd.service
</code></pre>
<p>透過簡單指令驗證：</p>
<pre><code class="sh">$ export CA=&quot;/etc/etcd/ssl&quot;
$ ETCDCTL_API=3 etcdctl \
    --cacert=${CA}/etcd-ca.pem \
    --cert=${CA}/etcd.pem \
    --key=${CA}/etcd-key.pem \
    --endpoints=&quot;https://172.16.35.12:2379&quot; \
    endpoint health
# output
https://172.16.35.12:2379 is healthy: successfully committed proposal: took = 641.36µs
</code></pre>
<h2 id="Kubernetes-Master"><a href="#Kubernetes-Master" class="headerlink" title="Kubernetes Master"></a>Kubernetes Master</h2><p>Master 是 Kubernetes 的大總管，主要建置<code>apiserver</code>、<code>Controller manager</code>與<code>Scheduler</code>來元件管理所有 Node。本步驟將下載 Kubernetes 並安裝至 <code>master1</code>上，然後產生相關 TLS Cert 與 CA 金鑰，提供給叢集元件認證使用。</p>
<h3 id="下載-Kubernetes-元件"><a href="#下載-Kubernetes-元件" class="headerlink" title="下載 Kubernetes 元件"></a>下載 Kubernetes 元件</h3><p>首先透過網路取得所有需要的執行檔案：</p>
<pre><code class="sh"># Download Kubernetes
$ export KUBE_URL=&quot;https://storage.googleapis.com/kubernetes-release/release/v1.8.6/bin/linux/amd64&quot;
$ wget &quot;${KUBE_URL}/kubelet&quot; -O /usr/local/bin/kubelet
$ wget &quot;${KUBE_URL}/kubectl&quot; -O /usr/local/bin/kubectl
$ chmod +x /usr/local/bin/kubelet /usr/local/bin/kubectl

# Download CNI
$ mkdir -p /opt/cni/bin &amp;&amp; cd /opt/cni/bin
$ export CNI_URL=&quot;https://github.com/containernetworking/plugins/releases/download&quot;
$ wget -qO- --show-progress &quot;${CNI_URL}/v0.6.0/cni-plugins-amd64-v0.6.0.tgz&quot; | tar -zx
</code></pre>
<h3 id="建立叢集-CA-與-Certificates-1"><a href="#建立叢集-CA-與-Certificates-1" class="headerlink" title="建立叢集 CA 與 Certificates"></a>建立叢集 CA 與 Certificates</h3><p>在這部分，將會需要產生 client 與 server 的各元件 certificates，並且替 Kubernetes admin user 產生 client 證書。</p>
<p>一樣在<code>master1</code>建立<code>pki</code>資料夾，然後進入目錄完成以下操作。</p>
<pre><code class="sh">$ mkdir -p /etc/kubernetes/pki &amp;&amp; cd /etc/kubernetes/pki
$ export PKI_URL=&quot;https://kairen.github.io/files/manual-v1.8/pki&quot;
$ export KUBE_APISERVER=&quot;https://172.16.35.12:6443&quot;
</code></pre>
<p>下載<code>ca-config.json</code>與<code>ca-csr.json</code>檔案，並產生 CA 金鑰：</p>
<pre><code class="sh">$ wget &quot;${PKI_URL}/ca-config.json&quot; &quot;${PKI_URL}/ca-csr.json&quot;
$ cfssl gencert -initca ca-csr.json | cfssljson -bare ca
$ ls ca*.pem
ca-key.pem  ca.pem
</code></pre>
<h4 id="API-server-certificate"><a href="#API-server-certificate" class="headerlink" title="API server certificate"></a>API server certificate</h4><p>下載<code>apiserver-csr.json</code>檔案，並產生 kube-apiserver certificate 證書：</p>
<pre><code class="sh">$ wget &quot;${PKI_URL}/apiserver-csr.json&quot;
$ cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=10.96.0.1,172.16.35.12,127.0.0.1,kubernetes.default \
  -profile=kubernetes \
  apiserver-csr.json | cfssljson -bare apiserver

$ ls apiserver*.pem
apiserver-key.pem  apiserver.pem
</code></pre>
<blockquote>
<p>若節點 IP 不同，需要修改<code>-hostname</code>。</p>
</blockquote>
<h4 id="Front-proxy-certificate"><a href="#Front-proxy-certificate" class="headerlink" title="Front proxy certificate"></a>Front proxy certificate</h4><p>下載<code>front-proxy-ca-csr.json</code>檔案，並產生 Front proxy CA 金鑰，Front proxy 主要是用在 API aggregator 上:</p>
<pre><code class="sh">$ wget &quot;${PKI_URL}/front-proxy-ca-csr.json&quot;
$ cfssl gencert \
  -initca front-proxy-ca-csr.json | cfssljson -bare front-proxy-ca

$ ls front-proxy-ca*.pem
front-proxy-ca-key.pem  front-proxy-ca.pem
</code></pre>
<p>下載<code>front-proxy-client-csr.json</code>檔案，並產生 front-proxy-client 證書：</p>
<pre><code class="sh">$ wget &quot;${PKI_URL}/front-proxy-client-csr.json&quot;
$ cfssl gencert \
  -ca=front-proxy-ca.pem \
  -ca-key=front-proxy-ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  front-proxy-client-csr.json | cfssljson -bare front-proxy-client

$ ls front-proxy-client*.pem
front-proxy-client-key.pem  front-proxy-client.pem
</code></pre>
<h4 id="Bootstrap-Token"><a href="#Bootstrap-Token" class="headerlink" title="Bootstrap Token"></a>Bootstrap Token</h4><p>由於透過手動建立 CA 方式太過繁雜，只適合少量機器，因為每次簽證時都需要綁定 Node IP，隨機器增加會帶來很多困擾，因此這邊使用 TLS Bootstrapping 方式進行授權，由 apiserver 自動給符合條件的 Node 發送證書來授權加入叢集。</p>
<p>主要做法是 kubelet 啟動時，向 kube-apiserver 傳送 TLS Bootstrapping 請求，而 kube-apiserver 驗證 kubelet 請求的 token 是否與設定的一樣，若一樣就自動產生 kubelet 證書與金鑰。具體作法可以參考 <a href="https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/" target="_blank" rel="noopener">TLS bootstrapping</a>。</p>
<p>首先建立一個變數來產生<code>BOOTSTRAP_TOKEN</code>，並建立 <code>bootstrap.conf</code> 的 kubeconfig 檔：</p>
<pre><code class="sh">$ export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d &#39; &#39;)
$ cat &lt;&lt;EOF &gt; /etc/kubernetes/token.csv
${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;
EOF

# bootstrap set-cluster
$ kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=${KUBE_APISERVER} \
    --kubeconfig=../bootstrap.conf

# bootstrap set-credentials
$ kubectl config set-credentials kubelet-bootstrap \
    --token=${BOOTSTRAP_TOKEN} \
    --kubeconfig=../bootstrap.conf

# bootstrap set-context
$ kubectl config set-context default \
    --cluster=kubernetes \
    --user=kubelet-bootstrap \
   --kubeconfig=../bootstrap.conf

# bootstrap set default context
$ kubectl config use-context default --kubeconfig=../bootstrap.conf
</code></pre>
<blockquote>
<p>若想要用 CA 方式來認證，可以參考 <a href="https://gist.github.com/kairen/60ad8545b79e8e7aa9bdc8a2893df7a0" target="_blank" rel="noopener">Kubelet certificate</a>。</p>
</blockquote>
<h4 id="Admin-certificate"><a href="#Admin-certificate" class="headerlink" title="Admin certificate"></a>Admin certificate</h4><p>下載<code>admin-csr.json</code>檔案，並產生 admin certificate 證書：</p>
<pre><code class="sh">$ wget &quot;${PKI_URL}/admin-csr.json&quot;
$ cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  admin-csr.json | cfssljson -bare admin

$ ls admin*.pem
admin-key.pem  admin.pem
</code></pre>
<p>接著透過以下指令產生名稱為 <code>admin.conf</code> 的 kubeconfig 檔：</p>
<pre><code class="sh"># admin set-cluster
$ kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=${KUBE_APISERVER} \
    --kubeconfig=../admin.conf

# admin set-credentials
$ kubectl config set-credentials kubernetes-admin \
    --client-certificate=admin.pem \
    --client-key=admin-key.pem \
    --embed-certs=true \
    --kubeconfig=../admin.conf

# admin set-context
$ kubectl config set-context kubernetes-admin@kubernetes \
    --cluster=kubernetes \
    --user=kubernetes-admin \
    --kubeconfig=../admin.conf

# admin set default context
$ kubectl config use-context kubernetes-admin@kubernetes \
    --kubeconfig=../admin.conf
</code></pre>
<h4 id="Controller-manager-certificate"><a href="#Controller-manager-certificate" class="headerlink" title="Controller manager certificate"></a>Controller manager certificate</h4><p>下載<code>manager-csr.json</code>檔案，並產生 kube-controller-manager certificate 證書：</p>
<pre><code class="sh">$ wget &quot;${PKI_URL}/manager-csr.json&quot;
$ cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  manager-csr.json | cfssljson -bare controller-manager

$ ls controller-manager*.pem
</code></pre>
<blockquote>
<p>若節點 IP 不同，需要修改<code>manager-csr.json</code>的<code>hosts</code>。</p>
</blockquote>
<p>接著透過以下指令產生名稱為<code>controller-manager.conf</code>的 kubeconfig 檔：</p>
<pre><code class="sh"># controller-manager set-cluster
$ kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=${KUBE_APISERVER} \
    --kubeconfig=../controller-manager.conf

# controller-manager set-credentials
$ kubectl config set-credentials system:kube-controller-manager \
    --client-certificate=controller-manager.pem \
    --client-key=controller-manager-key.pem \
    --embed-certs=true \
    --kubeconfig=../controller-manager.conf

# controller-manager set-context
$ kubectl config set-context system:kube-controller-manager@kubernetes \
    --cluster=kubernetes \
    --user=system:kube-controller-manager \
    --kubeconfig=../controller-manager.conf

# controller-manager set default context
$ kubectl config use-context system:kube-controller-manager@kubernetes \
    --kubeconfig=../controller-manager.conf
</code></pre>
<h4 id="Scheduler-certificate"><a href="#Scheduler-certificate" class="headerlink" title="Scheduler certificate"></a>Scheduler certificate</h4><p>下載<code>scheduler-csr.json</code>檔案，並產生 kube-scheduler certificate 證書：</p>
<pre><code class="sh">$ wget &quot;${PKI_URL}/scheduler-csr.json&quot;
$ cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  scheduler-csr.json | cfssljson -bare scheduler

$ ls scheduler*.pem
scheduler-key.pem  scheduler.pem
</code></pre>
<blockquote>
<p>若節點 IP 不同，需要修改<code>scheduler-csr.json</code>的<code>hosts</code>。</p>
</blockquote>
<p>接著透過以下指令產生名稱為 <code>scheduler.conf</code> 的 kubeconfig 檔：</p>
<pre><code class="sh"># scheduler set-cluster
$ kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=${KUBE_APISERVER} \
    --kubeconfig=../scheduler.conf

# scheduler set-credentials
$ kubectl config set-credentials system:kube-scheduler \
    --client-certificate=scheduler.pem \
    --client-key=scheduler-key.pem \
    --embed-certs=true \
    --kubeconfig=../scheduler.conf

# scheduler set-context
$ kubectl config set-context system:kube-scheduler@kubernetes \
    --cluster=kubernetes \
    --user=system:kube-scheduler \
    --kubeconfig=../scheduler.conf

# scheduler set default context
$ kubectl config use-context system:kube-scheduler@kubernetes \
    --kubeconfig=../scheduler.conf
</code></pre>
<h4 id="Kubelet-master-certificate"><a href="#Kubelet-master-certificate" class="headerlink" title="Kubelet master certificate"></a>Kubelet master certificate</h4><p>下載<code>kubelet-csr.json</code>檔案，並產生 master node certificate 證書：</p>
<pre><code class="sh">$ wget &quot;${PKI_URL}/kubelet-csr.json&quot;
$ sed -i &#39;s/$NODE/master1/g&#39; kubelet-csr.json
$ cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=master1,172.16.35.12 \
  -profile=kubernetes \
  kubelet-csr.json | cfssljson -bare kubelet

$ ls kubelet*.pem
kubelet-key.pem  kubelet.pem
</code></pre>
<blockquote>
<p>這邊<code>$NODE</code>需要隨節點名稱不同而改變。</p>
</blockquote>
<p>接著透過以下指令產生名稱為 <code>kubelet.conf</code> 的 kubeconfig 檔：</p>
<pre><code class="sh"># kubelet set-cluster
$ kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=${KUBE_APISERVER} \
    --kubeconfig=../kubelet.conf

# kubelet set-credentials
$ kubectl config set-credentials system:node:master1 \
    --client-certificate=kubelet.pem \
    --client-key=kubelet-key.pem \
    --embed-certs=true \
    --kubeconfig=../kubelet.conf

# kubelet set-context
$ kubectl config set-context system:node:master1@kubernetes \
    --cluster=kubernetes \
    --user=system:node:master1 \
    --kubeconfig=../kubelet.conf

# kubelet set default context
$ kubectl config use-context system:node:master1@kubernetes \
    --kubeconfig=../kubelet.conf
</code></pre>
<h4 id="Service-account-key"><a href="#Service-account-key" class="headerlink" title="Service account key"></a>Service account key</h4><p>Service account 不是透過 CA 進行認證，因此不要透過 CA 來做 Service account key 的檢查，這邊建立一組 Private 與 Public 金鑰提供給 Service account key 使用：</p>
<pre><code class="sh">$ openssl genrsa -out sa.key 2048
$ openssl rsa -in sa.key -pubout -out sa.pub
$ ls sa.*
sa.key  sa.pub
</code></pre>
<p>完成後刪除不必要檔案：</p>
<pre><code class="sh">$ rm -rf *.json *.csr
</code></pre>
<p>確認<code>/etc/kubernetes</code>與<code>/etc/kubernetes/pki</code>有以下檔案：</p>
<pre><code class="sh">$ ls /etc/kubernetes/
admin.conf  bootstrap.conf  controller-manager.conf  kubelet.conf  pki  scheduler.conf  token.csv

$ ls /etc/kubernetes/pki
admin-key.pem  apiserver-key.pem  ca-key.pem  controller-manager-key.pem  front-proxy-ca-key.pem  front-proxy-client-key.pem  kubelet-key.pem  sa.key  scheduler-key.pem
admin.pem      apiserver.pem      ca.pem      controller-manager.pem      front-proxy-ca.pem      front-proxy-client.pem      kubelet.pem      sa.pub  scheduler.pem
</code></pre>
<h3 id="安裝-Kubernetes-核心元件"><a href="#安裝-Kubernetes-核心元件" class="headerlink" title="安裝 Kubernetes 核心元件"></a>安裝 Kubernetes 核心元件</h3><p>首先下載 Kubernetes 核心元件 YAML 檔案，這邊我們不透過 Binary 方案來建立 Master 核心元件，而是利用 Kubernetes Static Pod 來達成，因此需下載所有核心元件的<code>Static Pod</code>檔案到<code>/etc/kubernetes/manifests</code>目錄：</p>
<pre><code class="sh">$ export CORE_URL=&quot;https://kairen.github.io/files/manual-v1.8/master&quot;
$ mkdir -p /etc/kubernetes/manifests &amp;&amp; cd /etc/kubernetes/manifests
$ for FILE in apiserver manager scheduler; do
    wget &quot;${CORE_URL}/${FILE}.yml.conf&quot; -O ${FILE}.yml
  done
</code></pre>
<blockquote>
<p>若<code>IP</code>與教學設定不同的話，請記得修改<code>apiserver.yml</code>、<code>manager.yml</code>、<code>scheduler.yml</code>。<br>apiserver 中的 <code>NodeRestriction</code> 請參考 <a href="https://kubernetes.io/docs/admin/authorization/node/" target="_blank" rel="noopener">Using Node Authorization</a>。</p>
</blockquote>
<p>產生一個用來加密 Etcd 的 Key：</p>
<pre><code class="sh">$ head -c 32 /dev/urandom | base64
SUpbL4juUYyvxj3/gonV5xVEx8j769/99TSAf8YT/sQ=
</code></pre>
<p>在<code>/etc/kubernetes/</code>目錄下，建立<code>encryption.yml</code>的加密 YAML 檔案：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/kubernetes/encryption.yml
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: SUpbL4juUYyvxj3/gonV5xVEx8j769/99TSAf8YT/sQ=
      - identity: {}
EOF
</code></pre>
<blockquote>
<p>Etcd 資料加密可參考這篇 <a href="https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/" target="_blank" rel="noopener">Encrypting data at rest</a>。</p>
</blockquote>
<p>在<code>/etc/kubernetes/</code>目錄下，建立<code>audit-policy.yml</code>的進階稽核策略 YAML 檔：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/kubernetes/audit-policy.yml
apiVersion: audit.k8s.io/v1beta1
kind: Policy
rules:
- level: Metadata
EOF
</code></pre>
<blockquote>
<p>Audit Policy 請參考這篇 <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/" target="_blank" rel="noopener">Auditing</a>。</p>
</blockquote>
<p>下載<code>kubelet.service</code>相關檔案來管理 kubelet：</p>
<pre><code class="sh">$ export KUBELET_URL=&quot;https://kairen.github.io/files/manual-v1.8/master&quot;
$ mkdir -p /etc/systemd/system/kubelet.service.d
$ wget &quot;${KUBELET_URL}/kubelet.service&quot; -O /lib/systemd/system/kubelet.service
$ wget &quot;${KUBELET_URL}/10-kubelet.conf&quot; -O /etc/systemd/system/kubelet.service.d/10-kubelet.conf
</code></pre>
<blockquote>
<p>若<code>cluster-dns</code>或<code>cluster-domain</code>有改變的話，需要修改<code>10-kubelet.conf</code>。</p>
</blockquote>
<p>最後建立 var 存放資訊，然後啟動 kubelet 服務:</p>
<pre><code class="sh">$ mkdir -p /var/lib/kubelet /var/log/kubernetes
$ systemctl enable kubelet.service &amp;&amp; systemctl start kubelet.service
</code></pre>
<p>完成後會需要一段時間來下載映像檔與啟動元件，可以利用該指令來監看：</p>
<pre><code class="sh">$ watch netstat -ntlp
tcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      23012/kubelet
tcp        0      0 127.0.0.1:10251         0.0.0.0:*               LISTEN      22305/kube-schedule
tcp        0      0 127.0.0.1:10252         0.0.0.0:*               LISTEN      22529/kube-controll
tcp6       0      0 :::6443                 :::*                    LISTEN      22956/kube-apiserve
</code></pre>
<blockquote>
<p>若看到以上資訊表示服務正常啟動，若發生問題可以用<code>docker cli</code>來查看。</p>
</blockquote>
<p>完成後，複製 admin kubeconfig 檔案，並透過簡單指令驗證：</p>
<pre><code class="sh">$ cp /etc/kubernetes/admin.conf ~/.kube/config
$ kubectl get cs
NAME                 STATUS    MESSAGE              ERROR
etcd-0               Healthy   {&quot;health&quot;: &quot;true&quot;}
scheduler            Healthy   ok
controller-manager   Healthy   ok

$ kubectl get node
NAME      STATUS     ROLES     AGE       VERSION
master1   NotReady   master    1m        v1.8.6

$ kubectl -n kube-system get po
NAME                              READY     STATUS    RESTARTS   AGE
kube-apiserver-master1            1/1       Running   0          4m
kube-controller-manager-master1   1/1       Running   0          4m
kube-scheduler-master1            1/1       Running   0          4m
</code></pre>
<p>確認服務能夠執行 logs 等指令：</p>
<pre><code class="sh">$ kubectl -n kube-system logs -f kube-scheduler-master1
Error from server (Forbidden): Forbidden (user=kube-apiserver, verb=get, resource=nodes, subresource=proxy) ( pods/log kube-apiserver-master1)
</code></pre>
<blockquote>
<p>這邊會發現出現 403 Forbidden 問題，這是因為 <code>kube-apiserver</code> user 並沒有 nodes 的資源權限，屬於正常。</p>
</blockquote>
<p>由於上述權限問題，我們必需建立一個 <code>apiserver-to-kubelet-rbac.yml</code> 來定義權限，以供我們執行 logs、exec 等指令：</p>
<pre><code class="sh">$ cd /etc/kubernetes/
$ export URL=&quot;https://kairen.github.io/files/manual-v1.8/master&quot;
$ wget &quot;${URL}/apiserver-to-kubelet-rbac.yml.conf&quot; -O apiserver-to-kubelet-rbac.yml
$ kubectl apply -f apiserver-to-kubelet-rbac.yml

# 測試 logs
$ kubectl -n kube-system logs -f kube-scheduler-master1
...
I1031 03:22:42.527697       1 leaderelection.go:184] successfully acquired lease kube-system/kube-scheduler
</code></pre>
<h2 id="Kubernetes-Node"><a href="#Kubernetes-Node" class="headerlink" title="Kubernetes Node"></a>Kubernetes Node</h2><p>Node 是主要執行容器實例的節點，可視為工作節點。在這步驟我們會下載 Kubernetes binary 檔，並建立 node 的 certificate 來提供給節點註冊認證用。Kubernetes 使用<code>Node Authorizer</code>來提供<a href="https://kubernetes.io/docs/admin/authorization/node/" target="_blank" rel="noopener">Authorization mode</a>，這種授權模式會替 Kubelet 生成 API request。</p>
<p>在開始前，我們先在<code>master1</code>將需要的 ca 與 cert 複製到 Node 節點上：</p>
<pre><code class="sh">$ for NODE in node1 node2; do
    ssh ${NODE} &quot;mkdir -p /etc/kubernetes/pki/&quot;
    ssh ${NODE} &quot;mkdir -p /etc/etcd/ssl&quot;
    # Etcd ca and cert
    for FILE in etcd-ca.pem etcd.pem etcd-key.pem; do
      scp /etc/etcd/ssl/${FILE} ${NODE}:/etc/etcd/ssl/${FILE}
    done
    # Kubernetes ca and cert
    for FILE in pki/ca.pem pki/ca-key.pem bootstrap.conf; do
      scp /etc/kubernetes/${FILE} ${NODE}:/etc/kubernetes/${FILE}
    done
  done
</code></pre>
<h3 id="下載-Kubernetes-元件-1"><a href="#下載-Kubernetes-元件-1" class="headerlink" title="下載 Kubernetes 元件"></a>下載 Kubernetes 元件</h3><p>首先透過網路取得所有需要的執行檔案：</p>
<pre><code class="sh"># Download Kubernetes
$ export KUBE_URL=&quot;https://storage.googleapis.com/kubernetes-release/release/v1.8.6/bin/linux/amd64&quot;
$ wget &quot;${KUBE_URL}/kubelet&quot; -O /usr/local/bin/kubelet
$ chmod +x /usr/local/bin/kubelet

# Download CNI
$ mkdir -p /opt/cni/bin &amp;&amp; cd /opt/cni/bin
$ export CNI_URL=&quot;https://github.com/containernetworking/plugins/releases/download&quot;
$ wget -qO- --show-progress &quot;${CNI_URL}/v0.6.0/cni-plugins-amd64-v0.6.0.tgz&quot; | tar -zx
</code></pre>
<h3 id="設定-Kubernetes-node"><a href="#設定-Kubernetes-node" class="headerlink" title="設定 Kubernetes node"></a>設定 Kubernetes node</h3><p>接著下載 Kubernetes 相關檔案，包含 drop-in file、systemd service 檔案等：</p>
<pre><code class="sh">$ export KUBELET_URL=&quot;https://kairen.github.io/files/manual-v1.8/node&quot;
$ mkdir -p /etc/systemd/system/kubelet.service.d
$ wget &quot;${KUBELET_URL}/kubelet.service&quot; -O /lib/systemd/system/kubelet.service
$ wget &quot;${KUBELET_URL}/10-kubelet.conf&quot; -O /etc/systemd/system/kubelet.service.d/10-kubelet.conf
</code></pre>
<blockquote>
<p>若<code>cluster-dns</code>或<code>cluster-domain</code>有改變的話，需要修改<code>10-kubelet.conf</code>。</p>
</blockquote>
<p>接著在所有<code>node</code>建立 var 存放資訊，然後啟動 kubelet 服務:</p>
<pre><code class="sh">$ mkdir -p /var/lib/kubelet /var/log/kubernetes /etc/kubernetes/manifests
$ systemctl enable kubelet.service &amp;&amp; systemctl start kubelet.service
</code></pre>
<blockquote>
<p>P.S. 重複一樣動作來完成其他節點。</p>
</blockquote>
<h3 id="授權-Kubernetes-Node"><a href="#授權-Kubernetes-Node" class="headerlink" title="授權 Kubernetes Node"></a>授權 Kubernetes Node</h3><p>當所有節點都完成後，在<code>master1</code>節點，因為我們採用 TLS Bootstrapping，所需要建立一個 ClusterRoleBinding：</p>
<pre><code class="sh">$ kubectl create clusterrolebinding kubelet-bootstrap \
    --clusterrole=system:node-bootstrapper \
    --user=kubelet-bootstrap
</code></pre>
<p>在<code>master</code>透過簡單指令驗證，會看到節點處於<code>pending</code>：</p>
<pre><code class="sh">$ kubectl get csr
NAME                                                   AGE       REQUESTOR           CONDITION
node-csr-YWf97ZrLCTlr2hmXsNLfjVLwaLfZRsu52FRKOYjpcBE   2s        kubelet-bootstrap   Pending
node-csr-eq4q6ffOwT4yqYQNU6sT7mphPOQdFN6yulMVZeu6pkE   2s        kubelet-bootstrap   Pending
</code></pre>
<p>透過 kubectl 來允許節點加入叢集：</p>
<pre><code class="sh">$ kubectl get csr | awk &#39;/Pending/ {print $1}&#39; | xargs kubectl certificate approve
certificatesigningrequest &quot;node-csr-YWf97ZrLCTlr2hmXsNLfjVLwaLfZRsu52FRKOYjpcBE&quot; approved
certificatesigningrequest &quot;node-csr-eq4q6ffOwT4yqYQNU6sT7mphPOQdFN6yulMVZeu6pkE&quot; approved

$ kubectl get csr
NAME                                                   AGE       REQUESTOR           CONDITION
node-csr-YWf97ZrLCTlr2hmXsNLfjVLwaLfZRsu52FRKOYjpcBE   30s       kubelet-bootstrap   Approved,Issued
node-csr-eq4q6ffOwT4yqYQNU6sT7mphPOQdFN6yulMVZeu6pkE   30s       kubelet-bootstrap   Approved,Issued

$ kubectl get no
NAME      STATUS     ROLES     AGE       VERSION
master1   NotReady   master    21m       v1.8.6
node1     NotReady   node      8s        v1.8.6
node2     NotReady   node      8s        v1.8.6
</code></pre>
<h2 id="Kubernetes-Core-Addons-部署"><a href="#Kubernetes-Core-Addons-部署" class="headerlink" title="Kubernetes Core Addons 部署"></a>Kubernetes Core Addons 部署</h2><p>當完成上面所有步驟後，接著我們需要安裝一些插件，而這些有部分是非常重要跟好用的，如<code>Kube-dns</code>與<code>Kube-proxy</code>等。</p>
<h3 id="Kube-proxy-addon"><a href="#Kube-proxy-addon" class="headerlink" title="Kube-proxy addon"></a>Kube-proxy addon</h3><p><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/kube-proxy" target="_blank" rel="noopener">Kube-proxy</a> 是實現 Service 的關鍵元件，kube-proxy 會在每台節點上執行，然後監聽 API Server 的 Service 與 Endpoint 資源物件的改變，然後來依據變化執行 iptables 來實現網路的轉發。這邊我們會需要建議一個 DaemonSet 來執行，並且建立一些需要的 certificate。</p>
<p>首先在<code>master1</code>下載<code>kube-proxy-csr.json</code>檔案，並產生 kube-proxy certificate 證書：</p>
<pre><code class="sh">$ export PKI_URL=&quot;https://kairen.github.io/files/manual-v1.8/pki&quot;
$ cd /etc/kubernetes/pki
$ wget &quot;${PKI_URL}/kube-proxy-csr.json&quot; &quot;${PKI_URL}/ca-config.json&quot;
$ cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-proxy-csr.json | cfssljson -bare kube-proxy

$ ls kube-proxy*.pem
kube-proxy-key.pem  kube-proxy.pem
</code></pre>
<p>接著透過以下指令產生名稱為 <code>kube-proxy.conf</code> 的 kubeconfig 檔：</p>
<pre><code class="sh"># kube-proxy set-cluster
$ kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=&quot;https://172.16.35.12:6443&quot; \
    --kubeconfig=../kube-proxy.conf

# kube-proxy set-credentials
$ kubectl config set-credentials system:kube-proxy \
    --client-key=kube-proxy-key.pem \
    --client-certificate=kube-proxy.pem \
    --embed-certs=true \
    --kubeconfig=../kube-proxy.conf

# kube-proxy set-context
$ kubectl config set-context system:kube-proxy@kubernetes \
    --cluster=kubernetes \
    --user=system:kube-proxy \
    --kubeconfig=../kube-proxy.conf

# kube-proxy set default context
$ kubectl config use-context system:kube-proxy@kubernetes \
    --kubeconfig=../kube-proxy.conf
</code></pre>
<p>完成後刪除不必要檔案：</p>
<pre><code class="sh">$ rm -rf *.json
</code></pre>
<p>確認<code>/etc/kubernetes</code>有以下檔案：</p>
<pre><code class="sh">$ ls /etc/kubernetes/
admin.conf        bootstrap.conf           encryption.yml  kube-proxy.conf  pki             token.csv
audit-policy.yml  controller-manager.conf  kubelet.conf    manifests        scheduler.conf
</code></pre>
<p>在<code>master1</code>將<code>kube-proxy</code>相關檔案複製到 Node 節點上：</p>
<pre><code class="sh">$ for NODE in node1 node2; do
    echo &quot;--- $NODE ---&quot;
    for FILE in pki/kube-proxy.pem pki/kube-proxy-key.pem kube-proxy.conf; do
      scp /etc/kubernetes/${FILE} ${NODE}:/etc/kubernetes/${FILE}
    done
  done
</code></pre>
<p>完成後，在<code>master1</code>透過 kubectl 來建立 kube-proxy daemon：</p>
<pre><code class="sh">$ export ADDON_URL=&quot;https://kairen.github.io/files/manual-v1.8/addon&quot;
$ mkdir -p /etc/kubernetes/addons &amp;&amp; cd /etc/kubernetes/addons
$ wget &quot;${ADDON_URL}/kube-proxy.yml.conf&quot; -O kube-proxy.yml
$ kubectl apply -f kube-proxy.yml
$ kubectl -n kube-system get po -l k8s-app=kube-proxy
NAME               READY     STATUS    RESTARTS   AGE
kube-proxy-bpp7q   1/1       Running   0          47s
kube-proxy-cztvh   1/1       Running   0          47s
kube-proxy-q7mm4   1/1       Running   0          47s
</code></pre>
<h3 id="Kube-dns-addon"><a href="#Kube-dns-addon" class="headerlink" title="Kube-dns addon"></a>Kube-dns addon</h3><p><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns" target="_blank" rel="noopener">Kube DNS</a> 是 Kubernetes 叢集內部 Pod 之間互相溝通的重要 Addon，它允許 Pod 可以透過 Domain Name 方式來連接 Service，其主要由 Kube DNS 與 Sky DNS 組合而成，透過 Kube DNS 監聽 Service 與 Endpoint 變化，來提供給 Sky DNS 資訊，已更新解析位址。</p>
<p>安裝只需要在<code>master1</code>透過 kubectl 來建立 kube-dns deployment 即可：</p>
<pre><code class="sh">$ export ADDON_URL=&quot;https://kairen.github.io/files/manual-v1.8/addon&quot;
$ wget &quot;${ADDON_URL}/kube-dns.yml.conf&quot; -O kube-dns.yml
$ kubectl apply -f kube-dns.yml
$ kubectl -n kube-system get po -l k8s-app=kube-dns
NAME                        READY     STATUS    RESTARTS   AGE
kube-dns-6cb549f55f-h4zr5   0/3       Pending   0          40s
</code></pre>
<h2 id="Calico-Network-安裝與設定"><a href="#Calico-Network-安裝與設定" class="headerlink" title="Calico Network 安裝與設定"></a>Calico Network 安裝與設定</h2><p>Calico 是一款純 Layer 3 的資料中心網路方案(不需要 Overlay 網路)，Calico 好處是他已與各種雲原生平台有良好的整合，而 Calico 在每一個節點利用 Linux Kernel 實現高效的 vRouter 來負責資料的轉發，而當資料中心複雜度增加時，可以用 BGP route reflector 來達成。</p>
<p>首先在<code>master1</code>透過 kubectl 建立 Calico policy controller：</p>
<pre><code class="sh">$ export CALICO_CONF_URL=&quot;https://kairen.github.io/files/manual-v1.8/network&quot;
$ wget &quot;${CALICO_CONF_URL}/calico-controller.yml.conf&quot; -O calico-controller.yml
$ kubectl apply -f calico-controller.yml
$ kubectl -n kube-system get po -l k8s-app=calico-policy
NAME                                        READY     STATUS    RESTARTS   AGE
calico-policy-controller-5ff8b4549d-tctmm   0/1       Pending   0          5s
</code></pre>
<blockquote>
<p>若節點 IP 不同，需要修改<code>calico-controller.yml</code>的<code>ETCD_ENDPOINTS</code>。</p>
</blockquote>
<p>在<code>master1</code>下載 Calico CLI 工具：</p>
<pre><code class="sh">$ wget https://github.com/projectcalico/calicoctl/releases/download/v1.6.1/calicoctl
$ chmod +x calicoctl &amp;&amp; mv calicoctl /usr/local/bin/
</code></pre>
<p>然後在<code>所有</code>節點下載 Calico，並執行以下步驟：</p>
<pre><code class="sh">$ export CALICO_URL=&quot;https://github.com/projectcalico/cni-plugin/releases/download/v1.11.0&quot;
$ wget -N -P /opt/cni/bin ${CALICO_URL}/calico
$ wget -N -P /opt/cni/bin ${CALICO_URL}/calico-ipam
$ chmod +x /opt/cni/bin/calico /opt/cni/bin/calico-ipam
</code></pre>
<p>接著在<code>所有</code>節點下載 CNI plugins設定檔，以及 calico-node.service：</p>
<pre><code class="sh">$ mkdir -p /etc/cni/net.d
$ export CALICO_CONF_URL=&quot;https://kairen.github.io/files/manual-v1.8/network&quot;
$ wget &quot;${CALICO_CONF_URL}/10-calico.conf&quot; -O /etc/cni/net.d/10-calico.conf
$ wget &quot;${CALICO_CONF_URL}/calico-node.service&quot; -O /lib/systemd/system/calico-node.service
</code></pre>
<blockquote>
<blockquote>
<p>若節點 IP 不同，需要修改<code>10-calico.conf</code>的<code>etcd_endpoints</code>。</p>
<ul>
<li>若部署的機器是使用虛擬機，如 Virtualbox 等的話，請修改<code>calico-node.service</code>檔案，並在<code>IP_AUTODETECTION_METHOD</code>(包含 IP6)部分指定綁定的網卡，以避免預設綁定到 NAT 網路上。</li>
</ul>
</blockquote>
</blockquote>
<p>之後在<code>所有</code>節點啟動 Calico-node:</p>
<pre><code class="sh">$ systemctl enable calico-node.service &amp;&amp; systemctl start calico-node.service
</code></pre>
<p>在<code>master1</code>查看 Calico nodes:</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; ~/calico-rc
export ETCD_ENDPOINTS=&quot;https://172.16.35.12:2379&quot;
export ETCD_CA_CERT_FILE=&quot;/etc/etcd/ssl/etcd-ca.pem&quot;
export ETCD_CERT_FILE=&quot;/etc/etcd/ssl/etcd.pem&quot;
export ETCD_KEY_FILE=&quot;/etc/etcd/ssl/etcd-key.pem&quot;
EOF

$ . ~/calico-rc
$ calicoctl get node -o wide
NAME      ASN       IPV4              IPV6
master1   (64512)   172.16.35.12/24
node1     (64512)   172.16.35.10/24
node2     (64512)   172.16.35.11/24
</code></pre>
<p>查看 pending 的 pod 是否已執行：</p>
<pre><code class="sh">$ kubectl -n kube-system get po
NAME                                        READY     STATUS    RESTARTS   AGE
calico-policy-controller-5ff8b4549d-tctmm   1/1       Running   0          4m
kube-apiserver-master1                      1/1       Running   0          20m
kube-controller-manager-master1             1/1       Running   0          20m
kube-dns-6cb549f55f-h4zr5                   3/3       Running   0          5m
kube-proxy-fnrkb                            1/1       Running   0          6m
kube-proxy-l72bq                            1/1       Running   0          6m
kube-proxy-m6rfw                            1/1       Running   0          6m
kube-scheduler-master1                      1/1       Running   0          20m
</code></pre>
<p>最後若想省事，可以直接用 <a href="https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/hosted/hosted" target="_blank" rel="noopener">Standard Hosted</a> 方式安裝。</p>
<h2 id="Kubernetes-Extra-Addons-部署"><a href="#Kubernetes-Extra-Addons-部署" class="headerlink" title="Kubernetes Extra Addons 部署"></a>Kubernetes Extra Addons 部署</h2><p>本節說明如何部署一些官方常用的 Addons，如 Dashboard、Heapster 等。</p>
<h3 id="Dashboard-addon"><a href="#Dashboard-addon" class="headerlink" title="Dashboard addon"></a>Dashboard addon</h3><p><a href="https://github.com/kubernetes/dashboard" target="_blank" rel="noopener">Dashboard</a> 是 Kubernetes 社區官方開發的儀表板，有了儀表板後管理者就能夠透過 Web-based 方式來管理 Kubernetes 叢集，除了提升管理方便，也讓資源視覺化，讓人更直覺看見系統資訊的呈現結果。</p>
<p>在<code>master1</code>透過 kubectl 來建立 kubernetes dashboard 即可：</p>
<pre><code class="sh">$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
$ kubectl -n kube-system get po,svc -l k8s-app=kubernetes-dashboard
NAME                                      READY     STATUS    RESTARTS   AGE
po/kubernetes-dashboard-747c4f7cf-md5m8   1/1       Running   0          56s

NAME                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
svc/kubernetes-dashboard   ClusterIP   10.98.120.209   &lt;none&gt;        443/TCP   56s
</code></pre>
<p>這邊會額外建立一個名稱為<code>open-api</code> Cluster Role Binding，這僅作為方便測試時使用，在一般情況下不要開啟，不然就會直接被存取所有 API:</p>
<pre><code class="sh">$ cat &lt;&lt;EOF | kubectl create -f -
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: open-api
  namespace: &quot;&quot;
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: system:anonymous
EOF
</code></pre>
<blockquote>
<p>P.S. 管理者可以針對特定使用者來開放 API 存取權限，但這邊方便使用直接綁在 cluster-admin cluster role。</p>
</blockquote>
<p>完成後，就可以透過瀏覽器存取 <a href="https://172.16.35.12:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/" target="_blank" rel="noopener">Dashboard</a>。</p>
<p>在 1.7 版本以後的 Dashboard 將不再提供所有權限，因此需要建立一個 service account 來綁定 cluster-admin role：</p>
<pre><code class="sh">$ kubectl -n kube-system create sa dashboard
$ kubectl create clusterrolebinding dashboard --clusterrole cluster-admin --serviceaccount=kube-system:dashboard
$ SECRET=$(kubectl -n kube-system get sa dashboard -o yaml | awk &#39;/dashboard-token/ {print $3}&#39;)
$ kubectl -n kube-system describe secrets ${SECRET} | awk &#39;/token:/{print $2}&#39;
eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtdG9rZW4tdzVocmgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYWJmMTFjYzMtZjRlYi0xMWU3LTgzYWUtMDgwMDI3NjdkOWI5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZCJ9.Xuyq34ci7Mk8bI97o4IldDyKySOOqRXRsxVWIJkPNiVUxKT4wpQZtikNJe2mfUBBD-JvoXTzwqyeSSTsAy2CiKQhekW8QgPLYelkBPBibySjBhJpiCD38J1u7yru4P0Pww2ZQJDjIxY4vqT46ywBklReGVqY3ogtUQg-eXueBmz-o7lJYMjw8L14692OJuhBjzTRSaKW8U2MPluBVnD7M2SOekDff7KpSxgOwXHsLVQoMrVNbspUCvtIiEI1EiXkyCNRGwfnd2my3uzUABIHFhm0_RZSmGwExPbxflr8Fc6bxmuz-_jSdOtUidYkFIzvEWw2vRovPgs3MXTv59RwUw
</code></pre>
<blockquote>
<p>複製<code>token</code>，然後貼到 Kubernetes dashboard。</p>
</blockquote>
<h3 id="Heapster-addon"><a href="#Heapster-addon" class="headerlink" title="Heapster addon"></a>Heapster addon</h3><p><a href="https://github.com/kubernetes/heapster" target="_blank" rel="noopener">Heapster</a> 是 Kubernetes 社區維護的容器叢集監控與效能分析工具。Heapster 會從 Kubernetes apiserver 取得所有 Node 資訊，然後再透過這些 Node 來取得 kubelet 上的資料，最後再將所有收集到資料送到 Heapster 的後台儲存 InfluxDB，最後利用 Grafana 來抓取 InfluxDB 的資料源來進行視覺化。</p>
<p>在<code>master1</code>透過 kubectl 來建立 kubernetes monitor  即可：</p>
<pre><code class="sh">$ export ADDON_URL=&quot;https://kairen.github.io/files/manual-v1.8/addon&quot;
$ wget ${ADDON_URL}/kube-monitor.yml.conf -O kube-monitor.yml
$ kubectl apply -f kube-monitor.yml
$ kubectl -n kube-system get po,svc
NAME                                           READY     STATUS    RESTARTS   AGE
...
po/heapster-74fb5c8cdc-62xzc                   4/4       Running   0          7m
po/influxdb-grafana-55bd7df44-nw4nc            2/2       Running   0          7m

NAME                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
...
svc/heapster               ClusterIP   10.100.242.225   &lt;none&gt;        80/TCP              7m
svc/monitoring-grafana     ClusterIP   10.101.106.180   &lt;none&gt;        80/TCP              7m
svc/monitoring-influxdb    ClusterIP   10.109.245.142   &lt;none&gt;        8083/TCP,8086/TCP   7m
···
</code></pre>
<p>完成後，就可以透過瀏覽器存取 <a href="https://172.16.35.12:6443/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana" target="_blank" rel="noopener">Grafana Dashboard</a>。</p>
<h2 id="簡單部署-Nginx-服務"><a href="#簡單部署-Nginx-服務" class="headerlink" title="簡單部署 Nginx 服務"></a>簡單部署 Nginx 服務</h2><p>Kubernetes 可以選擇使用指令直接建立應用程式與服務，或者撰寫 YAML 與 JSON 檔案來描述部署應用程式的配置，以下將建立一個簡單的 Nginx 服務：</p>
<pre><code class="sh">$ kubectl run nginx --image=nginx --port=80
$ kubectl expose deploy nginx --port=80 --type=LoadBalancer --external-ip=172.16.35.12
$ kubectl get svc,po
NAME             TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)        AGE
svc/kubernetes   ClusterIP      10.96.0.1       &lt;none&gt;         443/TCP        1h
svc/nginx        LoadBalancer   10.97.121.243   172.16.35.12   80:30344/TCP   22s

NAME                        READY     STATUS    RESTARTS   AGE
po/nginx-7cbc4b4d9c-7796l   1/1       Running   0          28s       192.160.57.181   ,172.16.35.12   80:32054/TCP   21s
</code></pre>
<blockquote>
<p>這邊<code>type</code>可以選擇 NodePort 與 LoadBalancer，在本地裸機部署，兩者差異在於<code>NodePort</code>只映射 Host port 到 Container port，而<code>LoadBalancer</code>則繼承<code>NodePort</code>額外多出映射 Host target port 到 Container port。</p>
</blockquote>
<p>確認沒問題後即可在瀏覽器存取 <a href="http://172.16.35.12。" target="_blank" rel="noopener">http://172.16.35.12。</a></p>
<h3 id="擴展服務數量"><a href="#擴展服務數量" class="headerlink" title="擴展服務數量"></a>擴展服務數量</h3><p>若叢集<code>node</code>節點增加了，而想讓 Nginx 服務提供可靠性的話，可以透過以下方式來擴展服務的副本：</p>
<pre><code class="sh">$ kubectl scale deploy nginx --replicas=2

$ kubectl get pods -o wide
NAME                    READY     STATUS    RESTARTS   AGE       IP             NODE
nginx-158599303-0h9lr   1/1       Running   0          25s       10.244.100.5   node2
nginx-158599303-k7cbt   1/1       Running   0          1m        10.244.24.3    node1
</code></pre>
]]></content>
      
        <categories>
            
            <category> Kubernetes </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Docker </tag>
            
            <tag> Kubernetes </tag>
            
            <tag> Calico </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[利用 Kuryr 整合 OpenStack 與 Kubernetes 網路]]></title>
      <url>https://kairen.github.io/2017/08/29/openstack/kuryr-kubernetes/</url>
      <content type="html"><![CDATA[<p><a href="https://github.com/openstack/kuryr-kubernetes" target="_blank" rel="noopener">Kubernetes Kuryr</a> 是 OpenStack Neutron 的子專案，其主要目標是透過該專案來整合 OpenStack 與 Kubernetes 的網路。該專案在 Kubernetes 中實作了原生 Neutron-based 的網路，因此使用 Kuryr-Kubernetes 可以讓你的 OpenStack VM 與 Kubernetes Pods 能夠選擇在同一個子網路上運作，並且能夠使用 Neutron 的 L3 與 Security Group 來對網路進行路由，以及阻擋特定來源 Port。</p>
<p><img src="https://i.imgur.com/2XfP3vb.png" alt=""></p>
<a id="more"></a>
<p>Kuryr-Kubernetes 整合有兩個主要組成部分：</p>
<ol>
<li><strong>Kuryr Controller</strong>:<br>Controller 主要目的是監控 Kubernetes API 的來獲取 Kubernetes 資源的變化，然後依據 Kubernetes 資源的需求來執行子資源的分配和資源管理。</li>
<li><strong>Kuryr CNI</strong>：主要是依據 Kuryr Controller 分配的資源來綁定網路至 Pods 上。</li>
</ol>
<p>本篇我們將說明如何利用<code>DevStack</code>與<code>Kubespray</code>建立一個簡單的測試環境。</p>
<h2 id="環境資源與事前準備"><a href="#環境資源與事前準備" class="headerlink" title="環境資源與事前準備"></a>環境資源與事前準備</h2><p>準備兩台實體機器，這邊測試的作業系統為<code>CentOS 7.x</code>，該環境將在扁平的網路下進行。</p>
<table>
<thead>
<tr>
<th>IP Address 1</th>
<th>Role</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.24.0.34</td>
<td>controller, k8s-master</td>
</tr>
<tr>
<td>172.24.0.80</td>
<td>compute, k8s-node1</td>
</tr>
</tbody>
</table>
<p>更新每台節點的 CentOS 7.x packages:</p>
<pre><code class="shell=">$ sudo yum --enablerepo=cr update -y
</code></pre>
<p>然後關閉 firewalld 以及 SELinux 來避免實現發生問題：</p>
<pre><code class="shell=">$ sudo setenforce 0
$ sudo systemctl disable firewalld &amp;&amp; sudo systemctl stop firewalld
</code></pre>
<h2 id="OpenStack-Controller-安裝"><a href="#OpenStack-Controller-安裝" class="headerlink" title="OpenStack Controller 安裝"></a>OpenStack Controller 安裝</h2><p>首先進入<code>172.24.0.34（controller）</code>，並且執行以下指令。</p>
<p>然後執行以下指令來建立 DevStack 專用使用者：</p>
<pre><code class="shell=">$ sudo useradd -s /bin/bash -d /opt/stack -m stack
$ echo &quot;stack ALL=(ALL) NOPASSWD: ALL&quot; | sudo tee /etc/sudoers.d/stack
</code></pre>
<blockquote>
<p>選用 DevStack 是因為現在都是用 Systemd 來管理服務，不用再用 screen 了，雖然都很方便。</p>
</blockquote>
<p>接著切換至該使用者環境來建立 OpenStack：</p>
<pre><code class="shell=">$ sudo su - stack
</code></pre>
<p>下載 DevStack 安裝套件：</p>
<pre><code class="shell=">$ git clone https://git.openstack.org/openstack-dev/devstack
$ cd devstack
</code></pre>
<p>新增<code>local.conf</code>檔案，來描述部署資訊：</p>
<pre><code>[[local|localrc]]
HOST_IP=172.24.0.34
GIT_BASE=https://github.com

ADMIN_PASSWORD=passwd
DATABASE_PASSWORD=passwd
RABBIT_PASSWORD=passwd
SERVICE_PASSWORD=passwd
SERVICE_TOKEN=passwd
MULTI_HOST=1
</code></pre><blockquote>
<p>[color=#fc9fca]Tips:<br>修改 HOST_IP 為自己的 IP 位置。</p>
</blockquote>
<p>完成後，執行以下指令開始部署：</p>
<pre><code class="shell=">$ ./stack.sh
</code></pre>
<h2 id="Openstack-Compute-安裝"><a href="#Openstack-Compute-安裝" class="headerlink" title="Openstack Compute 安裝"></a>Openstack Compute 安裝</h2><p>進入到<code>172.24.0.80（compute）</code>，並且執行以下指令。</p>
<p>然後執行以下指令來建立 DevStack 專用使用者：</p>
<pre><code class="shell=">$ sudo useradd -s /bin/bash -d /opt/stack -m stack
$ echo &quot;stack ALL=(ALL) NOPASSWD: ALL&quot; | sudo tee /etc/sudoers.d/stack
</code></pre>
<blockquote>
<p>選用 DevStack 是因為現在都是用 Systemd 來管理服務，不用再用 screen 了，雖然都很方便。</p>
</blockquote>
<p>接著切換至該使用者環境來建立 OpenStack：</p>
<pre><code class="shell=">$ sudo su - stack
</code></pre>
<p>下載 DevStack 安裝套件：</p>
<pre><code class="shell=">$ git clone https://git.openstack.org/openstack-dev/devstack
$ cd devstack
</code></pre>
<p>新增<code>local.conf</code>檔案，來描述部署資訊：</p>
<pre><code>[[local|localrc]]
HOST_IP=172.24.0.80
GIT_BASE=https://github.com
MULTI_HOST=1
LOGFILE=/opt/stack/logs/stack.sh.log
ADMIN_PASSWORD=passwd
DATABASE_PASSWORD=passwd
RABBIT_PASSWORD=passwd
SERVICE_PASSWORD=passwd
DATABASE_TYPE=mysql

SERVICE_HOST=172.24.0.34
MYSQL_HOST=$SERVICE_HOST
RABBIT_HOST=$SERVICE_HOST
GLANCE_HOSTPORT=$SERVICE_HOST:9292
ENABLED_SERVICES=n-cpu,q-agt,n-api-meta,c-vol,placement-client
NOVA_VNC_ENABLED=True
NOVNCPROXY_URL=&quot;http://$SERVICE_HOST:6080/vnc_auto.html&quot;
VNCSERVER_LISTEN=$HOST_IP
VNCSERVER_PROXYCLIENT_ADDRESS=$VNCSERVER_LISTEN
</code></pre><blockquote>
<p>Tips:<br>修改 HOST_IP 為自己的主機位置。<br>修改 SERVICE_HOST 為 Master 的IP位置。</p>
</blockquote>
<p>完成後，執行以下指令開始部署：</p>
<pre><code class="shell=">$ ./stack.sh
</code></pre>
<h2 id="建立-Kubernetes-叢集環境"><a href="#建立-Kubernetes-叢集環境" class="headerlink" title="建立 Kubernetes 叢集環境"></a>建立 Kubernetes 叢集環境</h2><p>首先確認所有節點之間不需要 SSH 密碼即可登入，接著進入到<code>172.24.0.34（k8s-master）</code>並且執行以下指令。</p>
<p>接著安裝所需要的套件：</p>
<pre><code class="shell=">$ sudo yum -y install software-properties-common ansible git gcc python-pip python-devel libffi-devel openssl-devel
$ sudo pip install -U kubespray
</code></pre>
<p>完成後，新增 kubespray 設定檔：</p>
<pre><code class="shell=">$ cat &lt;&lt;EOF &gt;  ~/.kubespray.yml
kubespray_git_repo: &quot;https://github.com/kubernetes-incubator/kubespray.git&quot;
# Logging options
loglevel: &quot;info&quot;
EOF
</code></pre>
<p>然後利用 kubespray-cli 快速產生環境的<code>inventory</code>檔，並修改部分內容：</p>
<pre><code class="shell=">$ sudo -i
$ kubespray prepare --masters master --etcds master --nodes node1
</code></pre>
<p>編輯<code>/root/.kubespray/inventory/inventory.cfg</code>，修改以下內容：</p>
<pre><code>[all]
master  ansible_host=172.24.0.34 ansible_user=root ip=172.24.0.34
node1    ansible_host=172.24.0.80 ansible_user=root ip=172.24.0.80

[kube-master]
master

[kube-node]
master
node1

[etcd]
master

[k8s-cluster:children]
kube-node1
kube-master
</code></pre><p>完成後，即可利用 kubespray-cli 指令來進行部署：</p>
<pre><code class="shell=">$ kubespray deploy --verbose -u root -k .ssh/id_rsa -n calico
</code></pre>
<p>經過一段時間後就會部署完成，這時候檢查節點是否正常：</p>
<pre><code class="shell=">$ kubectl get no
NAME      STATUS         AGE       VERSION
master    Ready,master   2m        v1.7.4
node1     Ready          2m        v1.7.4
</code></pre>
<p>接著為了方便讓 Kuryr Controller 簡單取得 K8s API Server，這邊修改<code>/etc/kubernetes/manifests/kube-apiserver.yml</code>檔案，加入以下內容：</p>
<pre><code>- &quot;--insecure-bind-address=0.0.0.0&quot;
- &quot;--insecure-port=8080&quot;
</code></pre><blockquote>
<p>Tips:<br>將 insecure 綁定到 0.0.0.0 之上，以及開啟 8080 Port。</p>
</blockquote>
<h2 id="安裝-Openstack-Kuryr"><a href="#安裝-Openstack-Kuryr" class="headerlink" title="安裝 Openstack Kuryr"></a>安裝 Openstack Kuryr</h2><p>進入到<code>172.24.0.34（controller）</code>，並且執行以下指令。</p>
<p>首先在節點安裝所需要的套件：</p>
<pre><code class="shell=">$ sudo yum -y install  gcc libffi-devel python-devel openssl-devel install python-pip
</code></pre>
<p>然後下載 kuryr-kubernetes 並進行安裝：</p>
<pre><code class="shell=">$ git clone http://git.openstack.org/openstack/kuryr-kubernetes
$ pip install -e kuryr-kubernetes
</code></pre>
<p>新增<code>kuryr.conf</code>至<code>/etc/kuryr</code>目錄：</p>
<pre><code class="shell=">$ cd kuryr-kubernetes
$ ./tools/generate_config_file_samples.sh
$ sudo mkdir -p /etc/kuryr/
$ sudo cp etc/kuryr.conf.sample /etc/kuryr/kuryr.conf
</code></pre>
<p>接著使用 OpenStack Dashboard 建立相關專案，在瀏覽器輸入<a href="http://172.24.0.34" target="_blank" rel="noopener">Dashboard</a>，並執行以下步驟。</p>
<ol>
<li>新增 K8s project。</li>
<li>修改 K8s project member 加入到 service project。</li>
<li>在該 Project 中新增 Security Groups，參考 <a href="https://docs.openstack.org/kuryr-kubernetes/latest/installation/manual.html" target="_blank" rel="noopener">kuryr-kubernetes manually</a>。</li>
<li>在該 Project 中新增 pod_subnet 子網路。</li>
<li>在該 Project 中新增 service_subnet 子網路。</li>
</ol>
<p>完成後，修改<code>/etc/kuryr/kuryr.conf</code>檔案，加入以下內容：</p>
<pre><code>[DEFAULT]
use_stderr = true
bindir = /usr/local/libexec/kuryr

[kubernetes]
api_root = http://172.24.0.34:8080

[neutron]
auth_url = http://172.24.0.34/identity
username = admin
user_domain_name = Default
password = admin
project_name = service
project_domain_name = Default
auth_type = password

[neutron_defaults]
ovs_bridge = br-int
pod_security_groups = {id_of_secuirity_group_for_pods}
pod_subnet = {id_of_subnet_for_pods}
project = {id_of_project}
service_subnet = {id_of_subnet_for_k8s_services}
</code></pre><p>完成後執行 kuryr-k8s-controller：</p>
<pre><code class="shell=">$ kuryr-k8s-controller --config-file /etc/kuryr/kuryr.conf
</code></pre>
<h2 id="安裝-Kuryr-CNI"><a href="#安裝-Kuryr-CNI" class="headerlink" title="安裝 Kuryr-CNI"></a>安裝 Kuryr-CNI</h2><p>進入到<code>172.24.0.80（node1）</code>並且執行以下指令。</p>
<p>首先在節點安裝所需要的套件：</p>
<pre><code class="shell=">$ sudo yum -y install  gcc libffi-devel python-devel openssl-devel python-pip
</code></pre>
<p>然後安裝 Kuryr-CNI 來提供給 kubelet 使用：</p>
<pre><code class="shell=">$ git clone http://git.openstack.org/openstack/kuryr-kubernetes
$ sudo pip install -e kuryr-kubernetes
</code></pre>
<p>新增<code>kuryr.conf</code>至<code>/etc/kuryr</code>目錄：</p>
<pre><code class="shell=">$ cd kuryr-kubernetes
$ ./tools/generate_config_file_samples.sh
$ sudo mkdir -p /etc/kuryr/
$ sudo cp etc/kuryr.conf.sample /etc/kuryr/kuryr.conf
</code></pre>
<p>修改<code>/etc/kuryr/kuryr.conf</code>檔案，加入以下內容：</p>
<pre><code>[DEFAULT]
use_stderr = true
bindir = /usr/local/libexec/kuryr
[kubernetes]
api_root = http://172.24.0.34:8080
</code></pre><p>建立 CNI bin 與 Conf 目錄：</p>
<pre><code class="shell=">$ sudo mkdir -p /opt/cni/bin
$ sudo ln -s $(which kuryr-cni) /opt/cni/bin/
$ sudo mkdir -p /etc/cni/net.d/
</code></pre>
<p>新增<code>/etc/cni/net.d/10-kuryr.conf</code> CNI 設定檔：</p>
<pre><code>{
    &quot;cniVersion&quot;: &quot;0.3.0&quot;,
    &quot;name&quot;: &quot;kuryr&quot;,
    &quot;type&quot;: &quot;kuryr-cni&quot;,
    &quot;kuryr_conf&quot;: &quot;/etc/kuryr/kuryr.conf&quot;,
    &quot;debug&quot;: true
}
</code></pre><p>完成後，更新 oslo 與 vif python 函式庫：</p>
<pre><code class="shell=">
$ sudo pip install &#39;oslo.privsep&gt;=1.20.0&#39; &#39;os-vif&gt;=1.5.0&#39;
</code></pre>
<p>最後重新啟動相關服務：</p>
<pre><code>sudo systemctl daemon-reload &amp;&amp; systemctl restart kubelet.service
</code></pre><h2 id="測試結果"><a href="#測試結果" class="headerlink" title="測試結果"></a>測試結果</h2><p>我們這邊開一個 Pod 與 OpenStack VM 來進行溝通：<br><img src="https://i.imgur.com/UYXdKud.png" alt=""></p>
<p><img src="https://i.imgur.com/dwoEytW.png" alt=""></p>
]]></content>
      
        <categories>
            
            <category> OpenStack </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Docker </tag>
            
            <tag> OpenStack </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[利用 OpenStack Ironic 提供裸機部署服務]]></title>
      <url>https://kairen.github.io/2017/08/16/openstack/ironic/</url>
      <content type="html"><![CDATA[<p><a href="https://docs.openstack.org/ironic/latest/user/index.html" target="_blank" rel="noopener">Ironic</a> 是 OpenStack 專案之一，主要目的是提供裸機機器部署服務(Bare-metal service)。它能夠單獨或整合 OpenStack 其他服務被使用，而可整合服務包含 Keystone、Nova、Neutron、Glance 與 Swift 等核心服務。當使用 Compute 與 Network 服務對 Bare-metal 進行適當的配置時，OpenStack 可以透過 Compute API 同時部署虛擬機(Virtual machines)與裸機(Bare machines)。</p>
<p>本篇為了精簡安裝過程，故這邊不採用手動安裝教學(會在 Gitbook 書上更新)，因此採用 <a href="https://docs.openstack.org/devstack/latest/" target="_blank" rel="noopener">DevStack</a> 來部署服務，再手動設定一些步驟。</p>
<p>本環境安裝資訊：</p>
<ul>
<li>OpenStack Pike</li>
<li>DevStack Pike</li>
<li>Pike Pike Pike ….</li>
</ul>
<a id="more"></a>
<p><img src="/images/openstack/openstack-ironic.png" alt=""></p>
<blockquote>
<p>P.S. 這邊因為我的 Manage net 已經有 MAAS 的服務，所以才用其他張網卡進行部署。</p>
</blockquote>
<h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本次安裝作業系統採用<code>Ubuntu 16.04 Server</code>，測試環境為實體主機：</p>
<table>
<thead>
<tr>
<th>Role</th>
<th>CPU</th>
<th>Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>controller</td>
<td>4</td>
<td>16G</td>
</tr>
<tr>
<td>bare-node1</td>
<td>4</td>
<td>16G</td>
</tr>
</tbody>
</table>
<blockquote>
<p>這邊 controller 為主要控制節點，將安裝大部分 OpenStack 服務。而 bare-node 為被用來做裸機部署的機器。</p>
</blockquote>
<p>網卡若是實體主機，請設定為固定 IP，如以下：</p>
<pre><code>auto eth0
iface eth0 inet static
           address 172.20.3.93/24
           gateway    172.20.3.1
           dns-nameservers 8.8.8.8
</code></pre><blockquote>
<p>若想修改主機的網卡名稱，可以編輯<code>/etc/udev/rules.d/70-persistent-net.rules</code>。</p>
</blockquote>
<p>其中<code>controller</code>的<code>eth2</code>需設定為以下：</p>
<pre><code>auto &lt;ethx&gt;
iface &lt;ethx&gt; inet manual
        up ip link set dev $IFACE up
        down ip link set dev $IFACE down
</code></pre><h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>安裝前需要確認叢集滿足以下幾點：</p>
<ul>
<li>確認所有節點網路可以溝通。</li>
<li>Bare-node IPMI 設定完成。包含 Address、User 與 Password。</li>
<li>修改 Controller 的 <code>/etc/apt/sources.list</code>，使用<code>tw.archive.ubuntu.com</code>。</li>
</ul>
<h2 id="安裝-OpenStack-服務"><a href="#安裝-OpenStack-服務" class="headerlink" title="安裝 OpenStack 服務"></a>安裝 OpenStack 服務</h2><p>這邊採用 DevStack 來部署測試環境，首先透過以下指令取得 DevStack：</p>
<pre><code class="sh">$ sudo useradd -s /bin/bash -d /opt/stack -m stack
$ echo &quot;stack ALL=(ALL) NOPASSWD: ALL&quot; | sudo tee /etc/sudoers.d/stack
$ sudo su - stack
$ git clone https://git.openstack.org/openstack-dev/devstack
$ cd devstack
</code></pre>
<p>接著撰寫 local.conf 來描述部署過程所需的服務：</p>
<pre><code class="sh">$ wget https://kairen.github.io/files/devstack/ironic-local.conf -O local.conf
$ sed -i &#39;s/HOST_IP=.*/HOST_IP=172.22.132.93/g&#39; local.conf
</code></pre>
<blockquote>
<p><code>HOST_IP</code>請更換為自己環境 IP。有其他 Driver 請記得加入。</p>
</blockquote>
<p>完成後執行部署腳本進行建置：</p>
<pre><code class="sh">$ ./stack.sh
</code></pre>
<blockquote>
<p>大約經過 15 min 就可以完成整個環境安裝。</p>
</blockquote>
<p>測試 OpenStack 環境：</p>
<pre><code class="sh">$ source openrc admin
$ openstack user list
+----------------------------------+----------------+
| ID                               | Name           |
+----------------------------------+----------------+
| 3ba4e813270e4e98ad781f4103284e0d | demo           |
| 40c6014bc18f407fbfbc22aadedb1ca0 | placement      |
| 567156ad1c7b4ccdbcd4ea02e7c44ce3 | alt_demo       |
| 7a22ce5036614993a707dd976c505ccd | swift          |
| 8d392f051afe45008289abca4dadf3ca | swiftusertest1 |
| a6e616af3bf04611bc23625e71a22e64 | swiftusertest4 |
| a835f1674648427396a7c6ac7e5eef06 | neutron        |
| b2bf73ef2eaa425c93e4f552e9266056 | swiftusertest2 |
| b7de1af8522b495c8a9fb743eb6e7f59 | nova           |
| cada5913a03e4f2794066902144264d3 | admin          |
| f03e39680b234474b139d00c3fbca989 | swiftusertest3 |
| f0a4033463f64c00858ff05525545b6d | glance-swift   |
| f2a1b186e7e84b10ae7e8f810e5c2412 | glance         |
| ff31787d136f4fba96c19af419b8559c | ironic         |
+----------------------------------+----------------+
</code></pre>
<p>測試 ironic 是否正常運行：</p>
<pre><code class="sh">$ ironic driver-list
+---------------------+----------------+
| Supported driver(s) | Active host(s) |
+---------------------+----------------+
| agent_ipmitool      | ironic-dev     |
| fake                | ironic-dev     |
| ipmi                | ironic-dev     |
| pxe_ipmitool        | ironic-dev     |
+---------------------+----------------+
</code></pre>
<h3 id="建立-Bare-metal-網路"><a href="#建立-Bare-metal-網路" class="headerlink" title="建立 Bare metal 網路"></a>建立 Bare metal 網路</h3><p>首先我們需要設定一個網路來提供 DHCP, PXE 與其他需求使用，這部分會說明如何建立一個 Flat network 來提供裸機配置用。詳細可參考 <a href="https://docs.openstack.org/ironic/latest/install/configure-networking.html" target="_blank" rel="noopener">Configure the Networking service for bare metal provisioning</a>。</p>
<p>首先編輯<code>/etc/neutron/plugins/ml2/ml2_conf.ini</code>修改以下內容：</p>
<pre><code>[ml2_type_flat]
flat_networks = public, physnet1

[ovs]
datapath_type = system
bridge_mappings = public:br-ex, physnet1:br-eth2
tunnel_bridge = br-tun
local_ip = 172.22.132.93
</code></pre><p>接著建立 bridge 來處理實體網路與 OpenStack 之間的溝通：</p>
<pre><code class="sh">$ sudo ovs-vsctl add-br br-eth2
$ sudo ovs-vsctl add-port br-eth2 eth2
</code></pre>
<p>完成後重新啟動 Neutron server 與 agent：</p>
<pre><code class="sh">$ sudo systemctl restart devstack@q-svc.service
$ sudo systemctl restart devstack@q-agt.service
</code></pre>
<p>建立完成後，OVS bridges 會類似如下：</p>
<pre><code class="sh">$ sudo ovs-vsctl show

    Bridge br-int
        fail_mode: secure
        Port &quot;int-br-eth2&quot;
            Interface &quot;int-br-eth2&quot;
                type: patch
                options: {peer=&quot;phy-br-eth2&quot;}
        Port br-int
            Interface br-int
                type: internal
    Bridge &quot;br-eth2&quot;
        Port &quot;phy-br-eth2&quot;
            Interface &quot;phy-br-eth2&quot;
                type: patch
                options: {peer=&quot;int-br-eth2&quot;}
        Port &quot;eth2&quot;
            Interface &quot;eth2&quot;
        Port &quot;br-eth2&quot;
            Interface &quot;br-eth2&quot;
                type: internal
</code></pre>
<p>接著建立 Neutron flat 網路來提供使用：</p>
<pre><code class="sh">$ neutron net-create sharednet1 \
                     --shared \
                     --provider:network_type flat \
                     --provider:physical_network physnet1

$ neutron subnet-create sharednet1 172.22.132.0/24 \
                        --name sharedsubnet1 \
                        --ip-version=4 --gateway=172.22.132.254 \
                        --allocation-pool start=172.22.132.180,end=172.22.132.200 \
                        --enable-dhcp
</code></pre>
<blockquote>
<p>P.S. neutron-client 在未來會被移除，故請轉用 <a href="https://docs.openstack.org/install-guide/launch-instance-networks-provider.html" target="_blank" rel="noopener">Provider network</a>。</p>
</blockquote>
<h3 id="設定-Ironic-cleaning-network"><a href="#設定-Ironic-cleaning-network" class="headerlink" title="設定 Ironic cleaning network"></a>設定 Ironic cleaning network</h3><p>當使用到 <a href="http://docs.openstack.org/ironic/latest/admin/cleaning.html#node-cleaning" target="_blank" rel="noopener">Node cleaning</a> 時，我們必須設定<code>cleaning_network</code>選項來提供使用。首先取得 Network 資訊，透過以下指令：</p>
<pre><code class="sh">$ openstack network list
+--------------------------------------+------------+----------------------------------------------------------------------------+
| ID                                   | Name       | Subnets                                                                    |
+--------------------------------------+------------+----------------------------------------------------------------------------+
| 03de10a0-d4d2-43ce-83db-806a5277dd29 | private    | 2a651bfb-776d-47f4-a958-f8a418f7fcd5, 99bdbd78-7a20-41b7-afa3-7cf7bf25b95b |
| 349a6a5b-1e26-4e36-8444-f6a6bbbdd227 | public     | 032a516e-3d55-4623-995d-06ee033eaee4, daf733a9-492e-4ea6-8a45-6364b88a8f6f |
| ade096bd-6a86-4d90-9cf4-bce9921f7257 | sharednet1 | 3f9f2a47-fdd9-472b-a6a2-ce6570e490ff                                       |
+--------------------------------------+------------+----------------------------------------------------------------------------+
</code></pre>
<p>編輯<code>/etc/ironic/ironic.conf</code>修改一下內容：</p>
<pre><code>[neutron]
cleaning_network = sharednet1
</code></pre><p>完成後，重新啟動 Ironic 服務：</p>
<pre><code class="sh">$ sudo systemctl restart devstack@ir-api.service
$ sudo systemctl restart devstack@ir-cond.service
</code></pre>
<h3 id="建立-Deploy-與-User-映像檔"><a href="#建立-Deploy-與-User-映像檔" class="headerlink" title="建立 Deploy 與 User 映像檔"></a>建立 Deploy 與 User 映像檔</h3><p>裸機服務在配置時需要兩組映像檔，分別為 <code>Deploy</code> 與 <code>User</code> 映像檔，其功能如下：</p>
<ul>
<li><code>Deploy images</code>: 用來準備裸機服務機器以進行實際的作業系統部署，在 Cleaning 等階段會使用到。</li>
<li><code>User images</code>:最後安裝至裸機服務提供給使用者使用的作業系統映像檔。</li>
</ul>
<p>由於 DevStack 預設會建立一組 Deploy 映像檔，這邊只針對 User 映像檔做手動建構說明，若要建構 Deploy 映像檔可以參考 <a href="https://docs.openstack.org/ironic/latest/install/deploy-ramdisk.html#deploy-ramdisk" target="_blank" rel="noopener">Building or downloading a deploy ramdisk image</a>。</p>
<p>首先我們必須先安裝<code>disk-image-builder</code>工具來提供建構映像檔：</p>
<pre><code class="sh">$ virtualenv dib
$ source dib/bin/activate
(dib) $ pip install diskimage-builder
</code></pre>
<p>接著執行以下指令來進行建構映像檔：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; k8s.repo
[kubernetes]
name=Kubernetes
baseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
       https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

$ DIB_YUM_REPO_CONF=k8s.repo \
  DIB_DEV_USER_USERNAME=kyle \
  DIB_DEV_USER_PWDLESS_SUDO=yes \
  DIB_DEV_USER_PASSWORD=r00tme \
  disk-image-create \
        centos7 \
        dhcp-all-interfaces \
        devuser \
        yum \
        epel \
        baremetal \
        -o k8s.qcow2 \
        -p vim,docker,kubelet,kubeadm,kubectl,kubernetes-cni

...
Converting image using qemu-img convert
Image file k8s.qcow2 created...
</code></pre>
<p>完成後會看到以下檔案：</p>
<pre><code class="sh">$ ls
dib  k8s.d  k8s.initrd  k8s.qcow2  k8s.repo  k8s.vmlinuz
</code></pre>
<p>上傳至 Glance 以提供使用：</p>
<pre><code class="sh"># 上傳 Kernel
$ openstack image create k8s.kernel \
                      --public \
                      --disk-format aki \
                      --container-format aki &lt; k8s.vmlinuz
# 上傳 Initrd
$ openstack image create k8s.initrd \
                      --public \
                      --disk-format ari \
                      --container-format ari &lt; k8s.initrd
# 上傳 Qcow2
$ export MY_VMLINUZ_UUID=$(openstack image list | awk &#39;/k8s.kernel/ { print $2 }&#39;)
$ export MY_INITRD_UUID=$(openstack image list | awk &#39;/k8s.initrd/ { print $2 }&#39;)
$ openstack image create k8s \
                      --public \
                      --disk-format qcow2 \
                      --container-format bare \
                      --property kernel_id=$MY_VMLINUZ_UUID \
                      --property ramdisk_id=$MY_INITRD_UUID &lt; k8s.qcow2
</code></pre>
<h2 id="建立-Ironic-節點"><a href="#建立-Ironic-節點" class="headerlink" title="建立 Ironic 節點"></a>建立 Ironic 節點</h2><p>在所有服務配置都完成後，這時候要註冊實體機器資訊，來提供給 Compute 服務部署時使用。首先確認 Ironic 的 Driver 是否有資源機器的 Power driver：</p>
<pre><code class="sh">$ ironic driver-list
+---------------------+----------------+
| Supported driver(s) | Active host(s) |
+---------------------+----------------+
| agent_ipmitool      | ironic-dev     |
| fake                | ironic-dev     |
| ipmi                | ironic-dev     |
| pxe_ipmitool        | ironic-dev     |
+---------------------+----------------+
</code></pre>
<blockquote>
<p>若有缺少的話，請參考 <a href="https://docs.openstack.org/ironic/latest/install/setup-drivers.html" target="_blank" rel="noopener">Set up the drivers for the Bare Metal service</a>。</p>
</blockquote>
<p>確認有支援後，透過以下指令來建立 Node，並進行註冊：</p>
<pre><code class="sh">$ export DEPLOY_VMLINUZ_UUID=$(openstack image list | awk &#39;/ipmitool.kernel/ { print $2 }&#39;)
$ export DEPLOY_INITRD_UUID=$(openstack image list | awk &#39;/ipmitool.initramfs/ { print $2 }&#39;)
$ ironic node-create -d agent_ipmitool \
                     -n bare-node-1 \
                     -i ipmi_address=172.20.3.194 \
                     -i ipmi_username=maas \
                     -i ipmi_password=passwd \
                     -i ipmi_port=623 \
                     -i deploy_kernel=$DEPLOY_VMLINUZ_UUID \
                     -i deploy_ramdisk=$DEPLOY_INITRD_UUID
</code></pre>
<blockquote>
<p>若使用 Console 的話，要加入<code>-i ipmi_terminal_port=9000</code>，可參考 <a href="https://docs.openstack.org/ironic/latest/admin/console.html" target="_blank" rel="noopener">Configuring Web or Serial Console</a>。</p>
</blockquote>
<p>接著更新機器資訊，這邊透過手動方式來更新資訊：</p>
<pre><code class="sh">$ export NODE_UUID=$(ironic node-list | awk &#39;/bare-node-1/ { print $2 }&#39;)
$ ironic node-update $NODE_UUID add \
                     properties/cpus=4 \
                     properties/memory_mb=8192 \
                     properties/local_gb=100 \
                     properties/root_gb=100 \
                     properties/cpu_arch=x86_64
</code></pre>
<p>(option)也可以使用 inspector 來識別裸機機器的硬體資訊，但需要修改<code>/etc/ironic-inspector/dnsmasq.conf</code>修改一下：</p>
<pre><code>no-daemon
port=0
interface=eth1
bind-interfaces
dhcp-range=172.22.132.200,172.22.132.210
dhcp-match=ipxe,175
dhcp-boot=tag:!ipxe,undionly.kpxe
dhcp-boot=tag:ipxe,http://172.22.132.93:3928/ironic-inspector.ipxe
dhcp-sequential-ip
</code></pre><blockquote>
<p>完成後，透過 systemctl 重新啟動背景服務<code>devstack@ironic-inspector-dhcp.service</code>與<code>devstack@ironic-inspector.service</code>。</p>
</blockquote>
<p>透過 port create 來把 Node 的所有網路資訊進行註冊：</p>
<pre><code class="sh">$ ironic port-create -n $NODE_UUID -a NODE_MAC_ADDRESS
</code></pre>
<blockquote>
<p>這邊<code>NODE_MAC_ADDRESS</code>是指<code>bare-node-1</code>節點的 PXE(eth1)網卡 Mac Address，如 54:a0:50:85:d5:fa。</p>
</blockquote>
<p>完成後透過 validate 指令來檢查：</p>
<pre><code class="sh">$ ironic node-validate $NODE_UUID
+------------+--------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Interface  | Result | Reason                                                                                                                                                                                                |
+------------+--------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| boot       | False  | Cannot validate image information for node 8e6fd86a-8eed-4e24-a510-3f5ebb0a336a because one or more parameters are missing from its instance_info. Missing are: [&#39;ramdisk&#39;, &#39;kernel&#39;, &#39;image_source&#39;] |
| console    | False  | Missing &#39;ipmi_terminal_port&#39; parameter in node\&#39;s driver_info.                                                                                                                                         |
| deploy     | False  | Cannot validate image information for node 8e6fd86a-8eed-4e24-a510-3f5ebb0a336a because one or more parameters are missing from its instance_info. Missing are: [&#39;ramdisk&#39;, &#39;kernel&#39;, &#39;image_source&#39;] |
| inspect    | True   |                                                                                                                                                                                                       |
| management | True   |                                                                                                                                                                                                       |
| network    | True   |                                                                                                                                                                                                       |
| power      | True   |                                                                                                                                                                                                       |
| raid       | True   |                                                                                                                                                                                                       |
| storage    | True   |                                                                                                                                                                                                       |
+------------+--------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
</code></pre>
<blockquote>
<p>P.S. 這邊<code>boot</code>與<code>deploy</code>的錯誤若是如上所示的話，可以直接忽略，這是因為使用 Nova 來管理 baremetal 會出現的問題。</p>
</blockquote>
<p>最後利用 provision 指令來測試節點是否能夠提供服務：</p>
<pre><code class="sh">$ ironic --ironic-api-version 1.34 node-set-provision-state $NODE_UUID manage
$ ironic --ironic-api-version 1.34 node-set-provision-state $NODE_UUID provide
$ ironic node-list
+--------------------------------------+--------+---------------+-------------+--------------------+-------------+
| UUID                                 | Name   | Instance UUID | Power State | Provisioning State | Maintenance |
+--------------------------------------+--------+---------------+-------------+--------------------+-------------+
| 0c20cf7d-0a36-46f4-ac38-721ff8bfb646 | bare-0 | None          | power off   | cleaning           | False       |
+--------------------------------------+--------+---------------+-------------+--------------------+-------------+
</code></pre>
<blockquote>
<p>這時候機器會進行 clean 過程，經過一點時間就會完成，若順利完成則該節點就可以進行部署了。若要了解細節狀態，可以參考 <a href="https://docs.openstack.org/ironic/latest/contributor/states.html" target="_blank" rel="noopener">Ironic’s State Machine</a>。</p>
</blockquote>
<p><img src="/images/openstack/ironic-clean.png" alt=""></p>
<h2 id="透過-Nova-部署-baremetal-機器"><a href="#透過-Nova-部署-baremetal-機器" class="headerlink" title="透過 Nova 部署 baremetal 機器"></a>透過 Nova 部署 baremetal 機器</h2><p>最後我們要透過 Nova API 來部署裸機，在開始前要建立一個 flavor 跟上傳 keypair 來提供使用：</p>
<pre><code class="sh">$ ssh-keygen -t rsa
$ openstack keypair create --public-key ~/.ssh/id_rsa.pub default
$ openstack flavor create --vcpus 4 --ram 8192 --disk 100 baremetal.large
</code></pre>
<p>完成後，即可透過以下指令進行部署：</p>
<pre><code class="sh">$ NET_ID=$(openstack network list | awk &#39;/sharednet1/ { print $2 }&#39;)
$ openstack server create --flavor baremetal.large \
                          --nic net-id=$NET_ID \
                          --image k8s \
                          --key-name default k8s-01
</code></pre>
<p>經過一段時間後，就會看到部署完成，這時候可以透過以下指令來確認部署結果：</p>
<pre><code class="sh">$ openstack server list
+--------------------------------------+--------+--------+---------------------------+-------+-----------------+
| ID                                   | Name   | Status | Networks                  | Image | Flavor          |
+--------------------------------------+--------+--------+---------------------------+-------+-----------------+
| a40e5cb1-dfc6-44d5-b638-648e8c0975fb | k8s-01 | ACTIVE | sharednet1=172.22.132.187 | k8s   | baremetal.large |
+--------------------------------------+--------+--------+---------------------------+-------+-----------------+

$ openstack baremetal list
+--------------------------------------+--------+--------------------------------------+-------------+--------------------+-------------+
| UUID                                 | Name   | Instance UUID                        | Power State | Provisioning State | Maintenance |
+--------------------------------------+--------+--------------------------------------+-------------+--------------------+-------------+
| 0c20cf7d-0a36-46f4-ac38-721ff8bfb646 | bare-0 | a40e5cb1-dfc6-44d5-b638-648e8c0975fb | power on    | active             | False       |
+--------------------------------------+--------+--------------------------------------+-------------+--------------------+-------------+
</code></pre>
<p>最後透過 ssh 來進入部署機器來建立應用：</p>
<pre><code class="sh">$ ssh kyle@172.22.132.187
[kyle@host-172-22-132-187 ~]$ sudo systemctl start kubelet.service
[kyle@host-172-22-132-187 ~]$ sudo systemctl start docker.service
[kyle@host-172-22-132-187 ~]$ sudo kubeadm init --service-cidr 10.96.0.0/12 \
                                                --kubernetes-version v1.7.4 \
                                                --pod-network-cidr 10.244.0.0/16 \
                                                --apiserver-advertise-address 172.22.132.187 \
                                                --token b0f7b8.8d1767876297d85c
</code></pre>
<blockquote>
<p>整合<code>Magnum</code>有空再寫，先簡單玩玩吧。</p>
</blockquote>
<p>若是懶人可以用 Dashboard 來部署，另外本教學的 DevStack 有使用 Ironic UI，因此可以在以下頁面看到 node 資訊。<br><img src="/images/openstack/ironic-ui.png" alt=""></p>
]]></content>
      
        <categories>
            
            <category> OpenStack </category>
            
        </categories>
        
        
        <tags>
            
            <tag> OpenStack </tag>
            
            <tag> DevStack </tag>
            
            <tag> Bare-metal </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[利用 LinuxKit 建立 Kubernetes 叢集]]></title>
      <url>https://kairen.github.io/2017/07/22/kubernetes/deploy/linuxkit-k8s/</url>
      <content type="html"><![CDATA[<p>LinuxKit 是以 Container 來建立最小、不可變的 Linux 作業系統映像檔框架，先前有簡單介紹與操作過，可以參考<a href="https://kairen.github.io/2017/04/23/container/linuxkit/">LinuxKit</a>。本篇則將利用 LinuxKit 來建立 Kubernetes 的映像檔，並部署簡單的 Kubernetes 叢集。</p>
<p><img src="/images/kube/moby+kubernetes.png" alt=""></p>
<a id="more"></a>
<p>本次教學會在<code>Mac OS X</code>作業系統上進行，而部署的軟體資訊如下：</p>
<ul>
<li>Kubernetes v1.7.2(2017-08-07, Update)</li>
<li>Etcd v3</li>
<li>Weave</li>
<li>Docker v17.06.0-ce</li>
</ul>
<h2 id="預先準備資訊"><a href="#預先準備資訊" class="headerlink" title="預先準備資訊"></a>預先準備資訊</h2><ul>
<li>主機已安裝與啟動<code>Docker</code>工具。</li>
<li>主機已安裝<code>Git</code>工具。</li>
<li>主機以下載 LinuxKit 專案，並建構了 Moby 與 LinuxKit 工具。</li>
</ul>
<p>建構 Moby 與 LinuxKit 方法如以下操作：</p>
<pre><code class="sh">$ git clone https://github.com/linuxkit/linuxkit.git
$ cd linuxkit
$ make
$ ./bin/moby version
moby version 0.0
commit: c2b081ed8a9f690820cc0c0568238e641848f58f

$ ./bin/linuxkit version
linuxkit version 0.0
commit: 0e3ca695d07d1c9870eca71fb7dd9ede31a38380
</code></pre>
<h2 id="建構-Kubernetes-系統映像檔"><a href="#建構-Kubernetes-系統映像檔" class="headerlink" title="建構 Kubernetes 系統映像檔"></a>建構 Kubernetes 系統映像檔</h2><p>首先我們要建立一個包好 Kubernetes 的 Linux 映像檔，而官方已經有做好範例，只要利用以下方式即可建構：</p>
<pre><code class="sh">$ cd linuxkit/projects/kubernetes/
$ make build-vm-images
...
Create outputs:
  kube-node-kernel kube-node-initrd.img kube-node-cmdline
</code></pre>
<h2 id="建置-Kubernetes-cluster"><a href="#建置-Kubernetes-cluster" class="headerlink" title="建置 Kubernetes cluster"></a>建置 Kubernetes cluster</h2><p>完成建構映像檔後，就可以透過以下指令來啟動 Master OS 映像檔，然後獲取節點 IP：</p>
<pre><code class="sh">$ ./boot.sh

(ns: getty) linuxkit-025000000002:~\# ip addr show dev eth0
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 02:50:00:00:00:02 brd ff:ff:ff:ff:ff:ff
    inet 192.168.65.3/24 brd 192.168.65.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::abf0:9fa4:d0f4:8da2/64 scope link
       valid_lft forever preferred_lft forever
</code></pre>
<p>啟動後，開啟新的 Console 來 SSH 進入 Master，來利用 kubeadm 初始化 Master：</p>
<pre><code class="sh">$ cd linuxkit/projects/kubernetes/
$ ./ssh_into_kubelet.sh 192.168.65.3
linuxkit-025000000002:/\# kubeadm-init.sh
...
kubeadm join --token 4236d3.29f61af661c49dbf 192.168.65.3:6443
</code></pre>
<p>一旦 kubeadm 完成後，就會看到 Token，這時請記住 Token 資訊。接著開啟新 Console，然後執行以下指令來啟動 Node：</p>
<pre><code class="sh">console1&gt;$ ./boot.sh 1 --token 4236d3.29f61af661c49dbf 192.168.65.3:6443
</code></pre>
<blockquote>
<p>P.S. 開啟節點格式為<code>./boot.sh &lt;n&gt; [&lt;join_args&gt; ...]</code>。</p>
</blockquote>
<p>接著分別在開兩個 Console 來加入叢集：</p>
<pre><code class="sh">console2&gt; $ ./boot.sh 2 --token 4236d3.29f61af661c49dbf 192.168.65.3:6443
console3&gt; $ ./boot.sh 3 --token 4236d3.29f61af661c49dbf 192.168.65.3:6443
</code></pre>
<p>完成後回到 Master 節點上，執行以下指令來查看節點狀況：</p>
<pre><code class="sh">$ kubectl get no
NAME                    STATUS    AGE       VERSION
linuxkit-025000000002   Ready     16m       v1.7.2
linuxkit-025000000003   Ready     6m        v1.7.2
linuxkit-025000000004   Ready     1m        v1.7.2
linuxkit-025000000005   Ready     1m        v1.7.2
</code></pre>
<h2 id="簡單部署-Nginx-服務"><a href="#簡單部署-Nginx-服務" class="headerlink" title="簡單部署 Nginx 服務"></a>簡單部署 Nginx 服務</h2><p>Kubernetes 可以選擇使用指令直接建立應用程式與服務，或者撰寫 YAML 與 JSON 檔案來描述部署應用程式的配置，以下將建立一個簡單的 Nginx 服務：</p>
<pre><code class="sh">$ kubectl run nginx --image=nginx --replicas=1 --port=80
$ kubectl get pods -o wide
NAME                     READY     STATUS    RESTARTS   AGE       IP          NODE
nginx-1423793266-v0hpb   1/1       Running   0          38s       10.42.0.1   linuxkit-025000000004
</code></pre>
<p>完成後要接著建立 svc(Service)，來提供外部網路存取應用程式，使用以下指令建立：</p>
<pre><code class="sh">$ kubectl expose deploy nginx --port=80 --type=NodePort
$ kubectl get svc
NAME         CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes   10.96.0.1       &lt;none&gt;        443/TCP        19m
nginx        10.108.41.230   &lt;nodes&gt;       80:31773/TCP   5s
</code></pre>
<p>由於這邊不是使用實體機器部署，因此網路使用 Docker namespace 網路，故這邊透過<code>ubuntu-desktop-lxde-vnc</code>來瀏覽 Nginx 應用：</p>
<pre><code class="sh">$ docker run -it --rm -p 6080:80 dorowu/ubuntu-desktop-lxde-vnc
</code></pre>
<blockquote>
<p>完成後透過瀏覽器連接 <a href="localhost:6080" target="_blank" rel="noopener">HTLM VNC</a></p>
</blockquote>
<p><img src="/images/kube/docker-desktop.png" alt=""></p>
<p>最後關閉節點只需要執行以下即可：</p>
<pre><code class="sh">$ halt
[ 1503.034689] reboot: Power down
</code></pre>
]]></content>
      
        <categories>
            
            <category> Kubernetes </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Docker </tag>
            
            <tag> Kubernetes </tag>
            
            <tag> LinuxKit </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[智能合約(Smart contracts)]]></title>
      <url>https://kairen.github.io/2017/05/28/blockchain/smart-contracts/</url>
      <content type="html"><![CDATA[<p><code>智能合約(Smart Contracts)</code> 是在 Ethereum 區塊鏈中所屬的物件。它們包含程式碼函式以及能夠與其他合約進行互動、做出決策、儲存資料與傳送乙太幣給其他人。合約是由創建者所定義，但是它們的執行與他們所提供的服務，都是由 Ethereum 網路本身提供。它們將存在且可被執行，只要整個網路存在，並且只會因程式中有撰寫自我銷毀的功能才會消失。</p>
<p>我可以用合約做什麼呢？只要想像力夠豐富，要做什麼幾乎都沒問題，但以下指南只會是入門，讓我們去實現一些簡單的事情。</p>
<a id="more"></a>
<h3 id="Smart-Sponsor"><a href="#Smart-Sponsor" class="headerlink" title="Smart Sponsor"></a>Smart Sponsor</h3><p>本節將說明一智能合約範例，透過建構一個合約來允許以下賬戶持有人進行互動。</p>
<ul>
<li>一個慈善機構舉行籌款活動，我們稱之為 <strong>thebenefactor</strong>。</li>
<li>一個受贊助的 runner 想為慈善機構募款，我們稱之為 <strong>therunner</strong>。</li>
<li>其他的人想要贊助 runner，我們稱之為 <strong>thesponsor</strong>。</li>
<li>一個 Ethereum 節點，用來開採區塊鏈以驗證交易，我們稱之為 <strong>theminer</strong>。</li>
</ul>
<p>我們的合約(smartSponsor)：</p>
<ul>
<li>是由一位 <strong>runner</strong> 透過贊助的執行來為慈善機構募款。</li>
<li>當建立合約時，<strong>runner</strong> 會任命為募集錢的捐助者。</li>
<li><strong>runner</strong> 則邀情其他人去進行贊助。用戶透過呼叫一個在智能合約上的函式，將乙太幣從 <strong>贊助商的帳戶</strong> 轉移到 <strong>合約</strong>，並保持乙太幣於合約，直到有進一步的通知。</li>
<li>在合約的時限期間的所有人都能看到誰是 <strong>捐助者</strong>，有多少的乙太幣被從誰捐(雖然贊助者可以匿名，當然:p)。</li>
</ul>
<p><img src="/images/blockchain/smartSponsor-1.png" alt="flow-1"></p>
<p>那麼有兩件事情可能發生：</p>
<ul>
<li>執行都按計劃進行，以及 <strong>runner</strong> 指示合約轉移到所有資金的捐助者。</li>
</ul>
<p><img src="/images/blockchain/smartSponsor-2.png" alt="flow-2"></p>
<ul>
<li>執行由於謀些原因無法承擔，而 runner 指示合約將退還贊助商的抵押。</li>
</ul>
<p><img src="/images/blockchain/smartSponsor-3.png" alt="flow-3"></p>
<p>Ethereum 允許智能合約由撰寫 Solidity 語言來定義。Solidity 的合約是類似於 Java 的類別定義。成員變數的儲存採用區塊鍊交易與合約的方法，來詢問合約或改變的其狀態。作為區塊鏈的副本會分散到網路中的所有節點，任何人都可以詢問合約，以從中搜尋公開的訊息。</p>
<p>合約有以下幾種方法：</p>
<ul>
<li><strong><code>smartSponsor</code></strong>：合約的建構子。它初始化合約的狀態。合約的建立者傳入賬戶的位址，有利於合約的 drawdown。</li>
<li><strong><code>pledge</code></strong>：任何人都可以呼叫捐贈乙太幣贊助基金。贊助商提供支援的選擇性訊息</li>
<li><strong><code>getPot</code></strong>：回傳當前儲存在合約的總乙太幣。</li>
<li><strong><code>refund</code></strong>：把贊助商的錢退回給贊助商。只有合約的擁有者才能呼叫這個函式。</li>
<li><strong><code>drawdown</code></strong>：傳送合約的總價值給捐助者賬戶。只有合約的擁有者才能呼叫這個函式。</li>
</ul>
<p>這個想法是使一個合約擁有約束力。他們不能拿回任何資金，除非整個合約被退還。在這種情況下，所有資料都是被公開存取的，這意味著任何人都有存取 Ethereum 區塊鏈，來查看誰建立了合約，誰是捐助者，以及誰透過存取合約程式碼本身保證了每一筆資金。</p>
<p>要注意很重要的一點，任何改變合約的狀態(建立、承若、退還或 drawing down)都需要在區塊鏈上建立交易，這意味著這些交易不會被儲存，要直到這些交易的區塊被開採。操作只能讀取到一個現有合約狀態(getPot 或讀取公有成員變數)都不需要進行挖礦。這是一個很重要且微妙的點：寫入操作是很慢的(因為我們要等到採礦完成)。由於這情況合約可能永遠不會被建立到區塊鍊中，因此呼叫方需要提供一些獎勵，來促進礦工去工作。這是被稱為 gas 的 Ethereum 術語，所有的寫入操作都是需要 gas 的支出來改變區塊鍊的狀態。</p>
<p>幸運的是我們不需要購買真正的乙太幣，以及參與 Ethereum 網路。我們可以使用相同的軟體，但要配置它運行一個本地測試區塊鏈，以及產生自己的假乙太幣。</p>
<p>以下為一個 Solidity 語言的智能合約範例：</p>
<pre><code class="js">contract smartSponsor {
  address public owner;
  address public benefactor;
  bool public refunded;
  bool public complete;
  uint public numPledges;
  struct Pledge {
    uint amount;
    address eth_address;
    bytes32 message;
  }
  mapping(uint =&gt; Pledge) public pledges;

  // constructor
  function smartSponsor(address _benefactor) {
    owner = msg.sender;
    numPledges = 0;
    refunded = false;
    complete = false;
    benefactor = _benefactor;
  }

  // add a new pledge
  function pledge(bytes32 _message) {
    if (msg.value == 0 || complete || refunded) throw;
    pledges[numPledges] = Pledge(msg.value, msg.sender, _message);
    numPledges++;
  }

  function getPot() constant returns (uint) {
    return this.balance;
  }

  // refund the backers
  function refund() {
    if (msg.sender != owner || complete || refunded) throw;
    for (uint i = 0; i &lt; numPledges; ++i) {
      pledges[i].eth_address.send(pledges[i].amount);
    }
    refunded = true;
    complete = true;
  }

  // send funds to the contract benefactor
  function drawdown() {
    if (msg.sender != owner || complete || refunded) throw;
    benefactor.send(this.balance);
    complete = true;
  }
}
</code></pre>
<blockquote>
<ul>
<li><p>一個<code>Pledge</code>結構模型的捐贈，儲存著贊助商的帳戶 ID、承若押金，以及一些訊息字串。</p>
</li>
<li><p>這個<code>pledges</code>陣列儲存了一個承若方的列表。</p>
</li>
<li><p>合約中的所有成員變數都是公開的，所以<code>getters</code>將自動被建立。</p>
</li>
<li><p><code>throw</code>被稱為某些函式(functions)，用以防止資料被寫入錯誤的資料到該區塊鏈中。</p>
</li>
</ul>
</blockquote>
<h2 id="參考連結"><a href="#參考連結" class="headerlink" title="參考連結"></a>參考連結</h2><ul>
<li><a href="https://medium.com/@kpcb_edge/our-thoughts-on-ethereum-31520b164e00#.2q1i88278" target="_blank" rel="noopener">Our thoughts on Ethereum</a></li>
<li><a href="https://www.ethereum.org/greeter" target="_blank" rel="noopener">Building a smart contract using the command line</a></li>
<li><a href="https://developer.ibm.com/clouddataservices/2016/05/19/block-chain-technology-smart-contracts-and-ethereum/" target="_blank" rel="noopener">Block chain technology, smart contracts and Ethereum</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Blockchain </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Ethereum </tag>
            
            <tag> Blockchain </tag>
            
            <tag> Solidity </tag>
            
            <tag> Smart Contract </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[利用 Browser Solidity 部署智能合約]]></title>
      <url>https://kairen.github.io/2017/05/27/blockchain/browser-solidity/</url>
      <content type="html"><![CDATA[<p>Browser Solidity 是一個 Web-based 的 Solidity 編譯器與 IDE。本節將說明如何安裝於 Linux 與 Docker 中。</p>
<p>這邊可以連結官方的 <a href="https://ethereum.github.io/browser-solidity" target="_blank" rel="noopener">https://ethereum.github.io/browser-solidity</a> 來使用; 該網站會是該專案的最新版本預覽。</p>
<a id="more"></a>
<h3 id="Ubuntu-Server-手動安裝"><a href="#Ubuntu-Server-手動安裝" class="headerlink" title="Ubuntu Server 手動安裝"></a>Ubuntu Server 手動安裝</h3><p>首先安裝 Browser Solidity 要使用到的相關套件：</p>
<pre><code class="sh">$ sudo apt-get install -y apache2 make g++ git
</code></pre>
<p>接著安裝 node.js 平台，來建置 App：</p>
<pre><code class="sh">$ curl -sL https://deb.nodesource.com/setup_6.x | sudo -E bash -
$ sudo apt-get install nodejs
</code></pre>
<p>然後透過 git 將專案抓到 local 端，並進入目錄：</p>
<pre><code class="sh">$ git clone https://github.com/ethereum/browser-solidity.git
$ cd browser-solidity
</code></pre>
<p>安裝相依套件與建置應用程式：</p>
<pre><code class="sh">$ sudo npm install
$ sudo npm run build
</code></pre>
<p>完成後，將所以有目錄的資料夾與檔案搬移到 Apache HTTP Server 的網頁根目錄：</p>
<pre><code class="sh">$ sudo cp ./* /var/www/html/
</code></pre>
<blockquote>
<p>完成後就可以開啟網頁了。</p>
</blockquote>
<h3 id="Docker-快速安裝"><a href="#Docker-快速安裝" class="headerlink" title="Docker 快速安裝"></a>Docker 快速安裝</h3><p>目前 Browser Solidity 有提供 <a href="https://hub.docker.com/r/kairen/solidity/" target="_blank" rel="noopener">Docker Image</a> 下載。這邊只需要透過以下指令就能夠建立 Browser Solidity Dashboard 環境：</p>
<pre><code class="sh">$ docker run -d \
            -p 80:80 \
            --name solidity \
            kairen/solidity
</code></pre>
]]></content>
      
        <categories>
            
            <category> Blockchain </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Ethereum </tag>
            
            <tag> Blockchain </tag>
            
            <tag> Solidity </tag>
            
            <tag> Smart Contract </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[監控 Go Ethereum 的區塊鏈狀況]]></title>
      <url>https://kairen.github.io/2017/05/26/blockchain/geth-monitoring/</url>
      <content type="html"><![CDATA[<p>Ethereum 提供了一個 Web-based 的監控儀表板，可以部署該儀表板，並透過 Clinet 端傳送 Ethereum 節點的資訊，來查看整個區塊鏈狀態。本節將說明如何安裝監控儀表板於 Linux 與 Docker 容器中。</p>
<p>這邊可以連結官方的 <a href="https://ethstats.net/" target="_blank" rel="noopener">https://ethstats.net/</a> 來查看主節點網路的狀態。</p>
<a id="more"></a>
<h3 id="Ubuntu-Server-手動安裝"><a href="#Ubuntu-Server-手動安裝" class="headerlink" title="Ubuntu Server 手動安裝"></a>Ubuntu Server 手動安裝</h3><p>本部分說明如何手動安裝 eth-netstats 服務，其中會包含以下兩個部分：</p>
<ul>
<li><a href="#monitoring-site">Monitoring site</a></li>
<li><a href="#client-side">Client side</a></li>
</ul>
<h4 id="Monitoring-site"><a href="#Monitoring-site" class="headerlink" title="Monitoring site"></a>Monitoring site</h4><p>首先安裝 Browser Solidity 要使用到的相關套件：</p>
<pre><code class="sh">$ sudo apt-get install -y make g++ git
</code></pre>
<p>接著安裝 node.js 平台，來建置 App：</p>
<pre><code class="sh">$ curl -sL https://deb.nodesource.com/setup_6.x | sudo -E bash -
$ sudo apt-get install nodejs
</code></pre>
<p>然後透過 git 將專案抓到 local 端，並進入目錄：</p>
<pre><code class="sh">$ git clone https://github.com/cubedro/eth-netstats
$ cd eth-netstats
</code></pre>
<p>安裝相依套件與建置應用程式，並啟動服務：</p>
<pre><code class="sh">$ sudo npm install
$ sudo npm install -g grunt-cli
$ grunt
$ PORT=&quot;3000&quot; WS_SECRET=&quot;admin&quot; npm start
</code></pre>
<blockquote>
<p>接著就可以開啟 <a href="http://localhost:3000" target="_blank" rel="noopener">eth-netstats</a>。</p>
<p>在沒有任何 Clinet 節點連上情況下，會是一個空的網頁。</p>
</blockquote>
<p>撰寫一個腳本<code>eth-netstats.sh</code>放置到背景服務執行：</p>
<pre><code class="sh">#!/bin/bash
# History:
# 2016/05/22 Kyle Bai Release
#
export PORT=&quot;3000&quot;
export WS_SECRET=&quot;admin&quot;

echo &quot;Starting private eth-netstats ...&quot;
screen -dmS netstats /usr/bin/npm start
</code></pre>
<p>透過以下方式執行：</p>
<pre><code class="sh">$ chmod u+x eth-netstats.sh
$ ./eth-netstats.sh
Starting private eth-netstats ...
</code></pre>
<blockquote>
<p>透過<code>screen -x netstats</code>取得當前畫面。</p>
</blockquote>
<h4 id="Client-side"><a href="#Client-side" class="headerlink" title="Client side"></a>Client side</h4><p>首先安裝 Browser Solidity 要使用到的相關套件：</p>
<pre><code class="sh">$ sudo apt-get install -y make g++ git
</code></pre>
<p>接著安裝 node.js 平台，來建置 App：</p>
<pre><code class="sh">$ curl -sL https://deb.nodesource.com/setup_6.x | sudo -E bash -
$ sudo apt-get install nodejs
</code></pre>
<p>然後透過 git 將專案抓到 local 端，並進入目錄：</p>
<pre><code class="sh">$ git clone https://github.com/cubedro/eth-net-intelligence-api
$ cd eth-net-intelligence-api
</code></pre>
<p>安裝相依套件與建置應用程式：</p>
<pre><code class="sh">$ sudo npm install &amp;&amp; sudo npm install -g pm2
</code></pre>
<p>編輯<code>app.json</code>設定檔，並修改以下內容：</p>
<pre><code class="sh">[
  {
    &quot;name&quot;        : &quot;mynode&quot;,
    &quot;cwd&quot;         : &quot;.&quot;,
    &quot;script&quot;      : &quot;app.js&quot;,
    &quot;log_date_format&quot;   : &quot;YYYY-MM-DD HH:mm Z&quot;,
    &quot;merge_logs&quot;    : false,
    &quot;watch&quot;       : false,
    &quot;exec_interpreter&quot;  : &quot;node&quot;,
    &quot;exec_mode&quot;     : &quot;fork_mode&quot;,
    &quot;env&quot;:
    {
      &quot;NODE_ENV&quot;    : &quot;production&quot;,
      &quot;RPC_HOST&quot;    : &quot;localhost&quot;,
      &quot;RPC_PORT&quot;    : &quot;8545&quot;,
      &quot;INSTANCE_NAME&quot;   : &quot;mynode-1&quot;,
      &quot;WS_SERVER&quot;     : &quot;http://localhost:3000&quot;,
      &quot;WS_SECRET&quot;     : &quot;admin&quot;,
    }
  },
]
</code></pre>
<blockquote>
<ul>
<li><p><code>RPC_HOST</code>為 ethereum 的 rpc ip address。</p>
</li>
<li><p><code>RPC_PORT</code>為 ethereum 的 rpc port。</p>
</li>
<li><p><code>INSTANCE_NAME</code>為 ethereum 的監控實例名稱。</p>
</li>
<li><p><code>WS_SERVER</code>為 eth-netstats 的 URL。</p>
</li>
<li><p><code>WS_SECRET</code>為 eth-netstats 的 secret。</p>
</li>
</ul>
</blockquote>
<p>確認完成後，即可啟動服務：</p>
<pre><code class="sh">$ pm2 start app.json
$ sudo tail -f $HOME/.pm2/logs/mynode-out-0.log
</code></pre>
<h3 id="Docker-快速安裝"><a href="#Docker-快速安裝" class="headerlink" title="Docker 快速安裝"></a>Docker 快速安裝</h3><p>本部分說明如何手動安裝 eth-netstats 服務，其中會包含以下兩個部分：</p>
<ul>
<li><a href="#docker-monitoring-site">Docker Monitoring site</a></li>
<li><a href="#docker-client-side">Docker Client side</a></li>
</ul>
<h4 id="Docker-Monitoring-site"><a href="#Docker-Monitoring-site" class="headerlink" title="Docker Monitoring site"></a>Docker Monitoring site</h4><p>自動建置的映像檔現在可以在 <a href="https://hub.docker.com/r/kairen/ethstats/" target="_blank" rel="noopener">DockerHub</a> 找到，建議直接執行以下指令來啟動 eth-netstats 容器：</p>
<pre><code class="sh">$ docker run -d \
            -p 3000:3000 \
            -e WS_SECRET=&quot;admin&quot; \
            --name ethstats \
            kairen/ethstats
</code></pre>
<blockquote>
<p>接著就可以開啟 <a href="http://localhost:3000" target="_blank" rel="noopener">eth-netstats</a>。</p>
<p>在沒有任何 Clinet 節點連上情況下，會是一個空的網頁。</p>
</blockquote>
<h4 id="Docker-Client-side"><a href="#Docker-Client-side" class="headerlink" title="Docker Client side"></a>Docker Client side</h4><p>自動建置的映像檔現在可以在 <a href="https://hub.docker.com/r/kairen/ethnetintel/" target="_blank" rel="noopener">DockerHub</a> 找到，也推薦透過執行以下指令來啟動 eth-netintel 容器：</p>
<pre><code class="sh">$ docker run -d \
            -p 30303:30303 \
            -p 30303:30303/udp \
            -e NAME_PREFIX=&quot;geth-1&quot; \
            -e WS_SERVER=&quot;http://172.17.1.200:3000&quot; \
            -e WS_SECRET=&quot;admin&quot; \
            -e RPC_HOST=&quot;172.17.1.199&quot; \
            -e RPC_PORT=&quot;8545&quot; \
            --name geth-1 \
            kairen/ethnetintel
</code></pre>
<blockquote>
<p>Monitor 與 Client 需要統一<code>WS_SECRET</code>。</p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Blockchain </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Ethereum </tag>
            
            <tag> Blockchain </tag>
            
            <tag> Go lang </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[建立 Go Ethereum 私有網路鏈]]></title>
      <url>https://kairen.github.io/2017/05/25/blockchain/multi-node-geth/</url>
      <content type="html"><![CDATA[<p>Ethereum 專案是以區塊鏈原理，並進一步增加容納值、儲存資料，並且能封裝程式碼來建立智能合約(Smart Contracts)，形成區塊鏈應用程式，來執行運算任務。類似於比特幣(Bitcoin)，Ethereum 也具有一種貨幣，它叫做<code>乙太幣(Ether)</code>。乙太幣是開採於儲存在共享一致性的區塊鏈前驗證交易節點。乙太幣可以在賬戶(公有金鑰, Pubilc keys)與智能合約(Smart Contracts)之間進行轉移。</p>
<center><img src="/images/blockchain/ethereum-logo.png" alt="ethereum-logo"></center>

<p>本節將說明如何透過 Ubuntu 部署 Go Ethereum。並利用簡單的指令來進行 Demo。</p>
<a id="more"></a>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>本次會使用到兩個節點來建立 Geth Instances，其規格如下：</p>
<table>
<thead>
<tr>
<th>Role</th>
<th>CPUs</th>
<th>RAM</th>
<th>Disk</th>
</tr>
</thead>
<tbody>
<tr>
<td>geth-1</td>
<td>2vCPU</td>
<td>4 GB</td>
<td>40 GB</td>
</tr>
<tr>
<td>geth-2</td>
<td>2vCPU</td>
<td>4 GB</td>
<td>40 GB</td>
</tr>
</tbody>
</table>
<p>首先在每個節點安裝 Ethereum 最新版本，可以依照官方透過以下方式快速安裝：</p>
<pre><code class="sh">$ sudo apt-get install -y software-properties-common
$ sudo add-apt-repository -y ppa:ethereum/ethereum
$ sudo apt-get update &amp;&amp; sudo apt-get install ethereum
</code></pre>
<p>在每個節點建立一個<code>private.json</code>檔案來定義起源區塊(Genesis Block)，內容如下：</p>
<pre><code class="json">{
  &quot;coinbase&quot; : &quot;0x0000000000000000000000000000000000000000&quot;,
  &quot;difficulty&quot; : &quot;0x40000&quot;,
  &quot;extraData&quot; : &quot;Custem Ethereum Genesis Block&quot;,
  &quot;gasLimit&quot; : &quot;0xffffffff&quot;,
  &quot;nonce&quot; : &quot;0x0000000000000042&quot;,
  &quot;mixhash&quot; : &quot;0x0000000000000000000000000000000000000000000000000000000000000000&quot;,
  &quot;parentHash&quot; : &quot;0x0000000000000000000000000000000000000000000000000000000000000000&quot;,
  &quot;timestamp&quot; : &quot;0x00&quot;,
  &quot;config&quot;: {
        &quot;chainId&quot;: 123,
        &quot;homesteadBlock&quot;: 0,
        &quot;eip155Block&quot;: 0,
        &quot;eip158Block&quot;: 0
    },
    &quot;alloc&quot;: { }
}
</code></pre>
<p>初始化創世區塊：</p>
<pre><code class="sh">$ geth init --datadir=data/ private.json
</code></pre>
<p>在每個節點新增一名稱為<code>geth-private.sh</code>的腳本程式，將用於啟動 geth，並放置背景：</p>
<pre><code class="sh">#!/bin/bash
# Program:
#       This program is a private geth runner.
# History:
# 2016/05/22 Kyle Bai Release
#
echo &quot;Starting private geth&quot;
screen -dmS geth /usr/bin/geth \
            --datadir data/ \
            --networkid 123 \
            --nodiscover \
            --maxpeers 5 \
            --port 30301 \
            --rpc \
            --rpcaddr &quot;0.0.0.0&quot; \
            --rpcport &quot;8545&quot; \
            --rpcapi &quot;admin,db,eth,debug,miner,net,shh,txpool,personal,web3&quot; \
            --rpccorsdomain &quot;*&quot; \
            -verbosity 6
</code></pre>
<blockquote>
<p>更多的參數，請參考 <a href="https://github.com/ethereum/go-ethereum/wiki/Command-Line-Options" target="_blank" rel="noopener">Command-Line-Options</a>。</p>
</blockquote>
<p>建立完成後，修改執行權限：</p>
<pre><code class="sh">$ chmod u+x geth-private.sh
</code></pre>
<h2 id="建立-Ethereum-環境"><a href="#建立-Ethereum-環境" class="headerlink" title="建立 Ethereum 環境"></a>建立 Ethereum 環境</h2><p>首先進入到<code>geth-1</code>節點透過以下方式來啟動：</p>
<pre><code class="sh">$ ./geth-private.sh
Starting private geth
</code></pre>
<blockquote>
<p>這時候會透過 screen 執行於背景，我們可以透過<code>screen -x geth</code>來進行前景。若要回到背景則透過<code>[Ctrl-A] + [Ctrl-D]</code>來 detached。要關閉 screen 則透過 <code>[Ctrl-C]</code>。</p>
</blockquote>
<p>接著為了確認是否正確啟動，我們可以透過 geth 的 attach 指令來連接 console：</p>
<pre><code class="sh">$ geth attach ipc:data/geth.ipc
</code></pre>
<blockquote>
<p>也可以透過 HTTP 方式 attach，<code>geth attach http://localhost:8545</code>。</p>
<p>若一開始建立沒有 RPC，但想要加入 RPC 可以 attach 後，輸入以下 function：</p>
<pre><code class="sh">admin.startRPC(&quot;0.0.0.0&quot;, 8545, &quot;*&quot;, &quot;web3,db,net,eth&quot;)
</code></pre>
</blockquote>
<p>進入後透過 admin API 來取得節點的資訊：</p>
<pre><code class="sh">&gt; admin.nodeInfo.enode
&quot;enode://e3dd0392a2971c4b0c4c43a01cd682e19f31aaa573c43a9b227685af7ffed5070217392ae5ada278968d5c4bfddd9c93547bcf4592852196a8facbcdad64d257@[::]:30301?discport=0&quot;
</code></pre>
<blockquote>
<p>這邊要取代<code>[::]</code>為主機 IP，如以下：</p>
<pre><code>&quot;enode://e3dd0392a2971c4b0c4c43a01cd682e19f31aaa573c43a9b227685af7ffed5070217392ae5ada278968d5c4bfddd9c93547bcf4592852196a8facbcdad64d257@172.16.1.99:30301?discport=0&quot;
</code></pre></blockquote>
<p>上面沒問題後，接著進入到<code>geth-2</code>節點，然後透過以下指令開啟 console：</p>
<pre><code class="sh">$ geth init --datadir=data/ private.json
$ geth --datadir data/ \
       --networkid 123 \
       --nodiscover \
       --maxpeers 5 \
       --port 30301 \
       --rpc \
       --rpcaddr &quot;0.0.0.0&quot; \
       --rpcport &quot;8545&quot; \
       --rpcapi &quot;admin,db,eth,debug,miner,net,shh,txpool,personal,web3&quot; \
      --rpccorsdomain &quot;*&quot; \
      -verbosity 6 \
       console
</code></pre>
<blockquote>
<p>也可以透過上一個節點的方式將服務放到背景，在 attach。</p>
</blockquote>
<p>完成上面指令會直接進入 console，接著透過以下方式來連接<code>geth-1</code>：</p>
<pre><code class="sh">&gt; admin.addPeer(&quot;enode://e3dd0392a2971c4b0c4c43a01cd682e19f31aaa573c43a9b227685af7ffed5070217392ae5ada278968d5c4bfddd9c93547bcf4592852196a8facbcdad64d257@172.16.1.99:30301?discport=0&quot;)
true

I0525 12:56:40.623642 eth/downloader/downloader.go:239] Registering peer e3dd0392a2971c4b
I0525 12:57:10.622920 p2p/server.go:467] &lt;-taskdone: wait for dial hist expire (29.99999387s)
</code></pre>
<p>接著透過 net API 進行查看連接狀態：</p>
<pre><code class="sh">&gt; net.peerCount
1

&gt; admin.peers
[{
    caps: [&quot;eth/61&quot;, &quot;eth/62&quot;, &quot;eth/63&quot;],
    id: &quot;e3dd0392a2971c4b0c4c43a01cd682e19f31aaa573c43a9b227685af7ffed5070217392ae5ada278968d5c4bfddd9c93547bcf4592852196a8facbcdad64d257&quot;,
    name: &quot;Geth/v1.4.5-stable/linux/go1.5.1&quot;,
    network: {
      localAddress: &quot;172.16.1.100:51038&quot;,
      remoteAddress: &quot;172.16.1.99:30301&quot;
    },
    protocols: {
      eth: {
        difficulty: 131072,
        head: &quot;882048e0d045ea48903eddb4c50825a4e3c6c1a055df6a32244e9a9239f8c5e8&quot;,
        version: 63
      }
    }
}]
</code></pre>
<h2 id="驗證服務"><a href="#驗證服務" class="headerlink" title="驗證服務"></a>驗證服務</h2><p>這部分將透過幾個指令與流程來驗證服務，首先在<code>geth-1</code>透過 attach 進入，並建立一個賬戶與查看乙太幣：</p>
<pre><code class="sh">$ geth attach http://localhost:8545

&gt; kairen = personal.newAccount();
Passphrase:
Repeat passphrase:
&quot;0xcb41ad8ba28c4b8b52eee159ef3bb6da197ff60b&quot;

&gt; personal.listAccounts
[&quot;0xcb41ad8ba28c4b8b52eee159ef3bb6da197ff60b&quot;]

&gt; web3.fromWei(eth.getBalance(kairen), &quot;ether&quot;);
0
</code></pre>
<blockquote>
<p>P.S. 若要移除帳號，可以刪除<code>data/keystore</code>底下的檔案。</p>
</blockquote>
<p>接著在<code>geth-2</code>透過以下指令建立一個賬戶與查看乙太幣：</p>
<pre><code class="sh">&gt; pingyu = personal.newAccount();
Passphrase:
Repeat passphrase:
&quot;0xf8c70df559cb9225f6e426d0f139fd6e8752c644&quot;

&gt; personal.listAccounts
[&quot;0xf8c70df559cb9225f6e426d0f139fd6e8752c644&quot;]

&gt; web3.fromWei(eth.getBalance(pingyu), &quot;ether&quot;);
0
</code></pre>
<p>接著回到<code>geth-1</code>來賺取一些要交易的乙太幣：</p>
<pre><code class="sh">&gt; miner.setEtherbase(kairen)
true
</code></pre>
<p>當賬戶設定完成後，就可以執行以下指令進行採礦：</p>
<pre><code class="sh">&gt; miner.start(1)
true
</code></pre>
<blockquote>
<p>這邊需要一點時間產生 DAG，可以開一個新的命令列透過<code>screen -x geth</code>查看。</p>
<p>經過一段時間後，當 DAG 完成並開始採擴時就可以<code>miner.stop()</code>。</p>
</blockquote>
<p>接著在<code>geth-1</code>查看賬戶的乙太幣：</p>
<pre><code class="sh">&gt; web3.fromWei(eth.getBalance(kairen), &quot;ether&quot;);
40.78125
</code></pre>
<p>當成開採區塊後，就可以查看<code>geth-1</code>共採集的 ether balance 的數值：</p>
<pre><code>&gt; eth.getBalance(eth.coinbase).toNumber()
40781250000000000000
</code></pre><blockquote>
<p>即為<code>40.78125</code>乙太幣。</p>
</blockquote>
<p>接著我們要在將<code>geth-1</code>的賬戶乙太幣轉移到<code>geth-2</code>上，首先在<code>geth-1</code>上建立一個變數來存<code>geth-2</code>的賬戶位址：</p>
<pre><code class="sh">&gt; consumer = &quot;0xf8c70df559cb9225f6e426d0f139fd6e8752c644&quot;
&quot;0xf8c70df559cb9225f6e426d0f139fd6e8752c644&quot;
</code></pre>
<p>完成上述後，首先要將賬戶解鎖：</p>
<pre><code class="sh">&gt; personal.unlockAccount(kairen)
true
</code></pre>
<blockquote>
<p>輸入當初建立賬戶的密碼。</p>
</blockquote>
<p>並透過 eth API 的交易函式還將 ether balance 數值轉移：</p>
<pre><code class="sh">$ eth.sendTransaction({from: kairen, to: consumer, value: web3.toWei(10, &quot;ether&quot;)})
&quot;0x1aee9082a55751c59077a273e7b08acd028d5099a4986f002518b0c8919d9e36&quot;
</code></pre>
<p>若有在每一台 geth 節點上進入 debug 模式的話，會發現該交易資訊被存到一個區塊，這邊也可以透過 txpool 與 eth API 來查看：</p>
<pre><code class="sh">&gt; txpool.status
{
  pending: 1,
  queued: 0
}

&gt; eth.getBlock(&quot;pending&quot;, true).transactions
[{
    blockHash: &quot;0x0b58d0b17e02f56746b0b5b22f195b6ae71d47343bf778763c4c476386ad7db7&quot;,
    blockNumber: 112,
    from: &quot;0xcb41ad8ba28c4b8b52eee159ef3bb6da197ff60b&quot;,
    gas: 90000,
    gasPrice: 20000000000,
    hash: &quot;0x1aee9082a55751c59077a273e7b08acd028d5099a4986f002518b0c8919d9e36&quot;,
    input: &quot;0x&quot;,
    nonce: 0,
    to: &quot;0xf8c70df559cb9225f6e426d0f139fd6e8752c644&quot;,
    transactionIndex: 0,
    value: 10000000000000000000
}]
</code></pre>
<blockquote>
<p>這邊的<code>pending</code>表示目前還沒有被驗證，因此我們需要一些節點來進行採礦驗證。這邊也可以發現該交易資訊被存在區塊編號<code>112</code>，可以提供往後查詢之用。</p>
</blockquote>
<p>接著回到<code>geth-2</code>節點，查看目前的數值變化：</p>
<pre><code class="sh">&gt; web3.fromWei(eth.getBalance(pingyu), &quot;ether&quot;);
0
</code></pre>
<p>這邊會發現沒有任何錢進來，Why? so sad。其實是因為該區塊還沒有被採集與認證，因此該交易不會被執行。</p>
<p>因此我們需要在任一節點提供運算，這邊在<code>geth-1</code>執行以下指令來進行採礦，這樣就可以看到該交易被驗證與接受：</p>
<pre><code class="sh">&gt; miner.start(1)
true

TX(1aee9082a55751c59077a273e7b08acd028d5099a4986f002518b0c8919d9e36)
Contract: false
From:     cb41ad8ba28c4b8b52eee159ef3bb6da197ff60b
To:       f8c70df559cb9225f6e426d0f139fd6e8752c644
Nonce:    0
GasPrice: 20000000000
GasLimit  90000
Value:    10000000000000000000
Data:     0x
V:        0x1c
R:        0x9de7d843959f55a553577dc68a887893adf1b80eccd872021dfa6b8bcf3db43
S:        0x287f8e01640ccd5924308725d2d274def7edc4a18169b36ae26c95216fdf0fed
Hex:      f86d808504a817c80083015f9094f8c70df559cb9225f6e426d0f139fd6e8752c644888ac7230489e80000801ca009de7d843959f55a553577dc68a887893adf1b80eccd872021dfa6b8bcf3db43a0287f8e01640ccd5924308725d2d274def7edc4a18169b36ae26c95216fdf0fed
</code></pre>
<blockquote>
<p>當該區塊的交易確認沒問題被執行後，就可以透過<code>miner.stop()</code>停止採礦。</p>
</blockquote>
<p>這時再回到<code>geth-2</code>節點，查看目前的數值變化，會發現增加了 10 枚乙太幣：</p>
<pre><code class="sh">&gt; web3.fromWei(eth.getBalance(pingyu), &quot;ether&quot;);
10
</code></pre>
<p>之後可以在任一節點透過 eth web3 的 API 來查找指定區塊的交易資訊：</p>
<pre><code class="sh">&gt; eth.getTransactionFromBlock(40)
{
  blockHash: &quot;0xe839c1392657731417fc04b9aecf7a181dd339086d5f7cdea0bccc2b1483b885&quot;,
  blockNumber: 112,
  from: &quot;0xcb41ad8ba28c4b8b52eee159ef3bb6da197ff60b&quot;,
  gas: 90000,
  gasPrice: 20000000000,
  hash: &quot;0x1aee9082a55751c59077a273e7b08acd028d5099a4986f002518b0c8919d9e36&quot;,
  input: &quot;0x&quot;,
  nonce: 0,
  to: &quot;0xf8c70df559cb9225f6e426d0f139fd6e8752c644&quot;,
  transactionIndex: 0,
  value: 10000000000000000000
}
</code></pre>
<h2 id="簡單的-Contract"><a href="#簡單的-Contract" class="headerlink" title="簡單的 Contract"></a>簡單的 Contract</h2><p>這邊將說明如何建立一個簡單的合約(Contract)來部署於區塊鏈上，首先複製以下內容：</p>
<pre><code>contract SimpleStorage {
    uint storedData;
    function set(uint x) {
        storedData = x;
    }
    function get() constant returns (uint retVal) {
        return storedData;
    }
}
</code></pre><p>接著將內容貼到 <a href="https://ethereum.github.io/browser-solidity" target="_blank" rel="noopener">browser-solidity</a> 進行編譯成 JavaScript。如快照畫面所示。</p>
<center><img src="/images/blockchain/snapshot-contract.png" alt=""></center>

<p>透過這個 IDE 可以將 Solidity 語言轉換成 web3 code(JavaScript)，複製 web3 code 的內容，並儲存成<code>SimpleStorage.js</code>檔案放置到<code>geth-1</code>上。接著 attach 進入 geth 執行以下指令：</p>
<pre><code class="sh">&gt; loadScript(&#39;SimpleStorage.js&#39;);
</code></pre>
<p>若有自行安裝<code>browser-solidity</code>的話，則可以使用如下圖一樣的方式連接。</p>
<center><img src="/images/blockchain/snapshot-dash-web3-provider.png" alt=""></center>
]]></content>
      
        <categories>
            
            <category> Blockchain </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Ethereum </tag>
            
            <tag> Blockchain </tag>
            
            <tag> Go lang </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Enterprise 的 Docker registry 平台 Harbor]]></title>
      <url>https://kairen.github.io/2017/05/10/container/harbor-install/</url>
      <content type="html"><![CDATA[<p>Harbor 是一個企業級 Registry 伺服器用於儲存和分散 Docker Image 的，透過新增一些企業常用的功能，例如：安全性、身分驗證和管理等功能擴展了開源的 <a href="https://github.com/docker/distribution" target="_blank" rel="noopener">Docker Distribution</a>。作為一個企業級的私有 Registry 伺服器，Harbor 提供了更好的效能與安全性。Harbor 支援安裝多個 Registry 並將 Image 在多個 Registry 做 replicated。除此之外，Harbor 亦提供了高級的安全性功能，像是用戶管理(user managment)，存取控制(access control)和活動審核(activity auditing)。</p>
<p><img src="/images/docker/harbor_logo.png" alt=""><br><a id="more"></a></p>
<h2 id="功能特色"><a href="#功能特色" class="headerlink" title="功能特色"></a>功能特色</h2><ul>
<li><strong>基於角色為基礎的存取控制(Role based access control)</strong>：使用者和 Repository 透過 Project 進行組織管理，一個使用者在同一個 Project 下，對於每個 Image 可以有不同權限。</li>
<li><strong>基於 Policy 的 Image 複製</strong>：Image 可以在多得 Registry instance 中同步複製。適合於附載平衡、高可用性、混合雲與多雲的情境。</li>
<li><strong>支援 LDAP/AD</strong>：Harbor 可以整合企業已有的 LDAP/AD，來管理使用者的認證與授權。</li>
<li><strong>使用者的圖形化介面</strong>：使用者可以透過瀏覽器，查詢 Image 和管理 Project</li>
<li><strong>審核管理</strong>：所有對 Repositroy 的操作都被記錄。</li>
<li><strong>RESTful API</strong>：RESTful APIs 提供給管理的操作，可以輕易的整合額外的系統。</li>
<li><strong>快速部署</strong>：提供 Online installer 與 Offline installer。</li>
</ul>
<h2 id="安裝指南"><a href="#安裝指南" class="headerlink" title="安裝指南"></a>安裝指南</h2><p>Harbor 提供兩種方法進行安裝：</p>
<ol>
<li>Online installer<br> 這種安裝方式會從 Docker hub 下載 Harbor 所需的映像檔，因此 installer 檔案較輕量。</li>
<li>Offline installer<br> 當無任何網際網路連接的情況下使用此種安裝方式，預先將所需的映像檔打包，因此 installer 檔案較大。</li>
</ol>
<h3 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h3><p>Harbor 會部署數個 Docker container，所以部署的主機需要能支援 Docker 的 Linux distribution。而部署主機需要安裝以下套件：</p>
<ul>
<li>Python 版本<code>2.7+</code>。</li>
<li>Docker Engine 版本 <code>1.10+</code>。Docker 安裝方式，請參考：<a href="https://docs.docker.com/engine/installation/" target="_blank" rel="noopener">Install Docker</a></li>
<li>Docker Compose 版本 <code>1.6.0+</code>。Docker Compose 安裝方式，請參考：<a href="https://docs.docker.com/compose/install/" target="_blank" rel="noopener">Install Docker Compose</a></li>
</ul>
<blockquote>
<p>官方安裝指南說明是 Linux 且要支援 Docker，但 Windows 支援 Docker 部署 Harbor 還需要驗證是否可行。</p>
</blockquote>
<p>安裝步驟大致可分為以下階段：</p>
<ol>
<li>下載 installer</li>
<li>設定 Harbor</li>
<li>執行安裝腳本</li>
</ol>
<h4 id="下載-installer"><a href="#下載-installer" class="headerlink" title="下載 installer"></a>下載 installer</h4><p>installer 的二進制檔案可以從 <a href="https://github.com/vmware/harbor/releases" target="_blank" rel="noopener">release page</a> 下載，選擇您需要 Online installer 或者 Offline installer，下載完成後，使用<code>tar</code>將 package 解壓縮：</p>
<p>Online installer：</p>
<pre><code class="sh">$ tar xvf harbor-online-installer-&lt;version&gt;.tgz
</code></pre>
<p>Offline installer：</p>
<pre><code class="sh">$ tar xvf harbor-offline-installer-&lt;version&gt;.tgz
</code></pre>
<h4 id="設定-Harbor"><a href="#設定-Harbor" class="headerlink" title="設定 Harbor"></a>設定 Harbor</h4><p>Harbor 的設定與參數都在<code>harbor.cfg</code>中。</p>
<p><code>harbor.cfg</code>中的參數分為<strong>required parameters</strong>與<strong>optional parameters</strong></p>
<ul>
<li><strong>required parameters</strong><br>  這類的參數是必須設定的，且會影響使用者更新<code>harbor.cfg</code>後，重新執行安裝腳本來重新安裝 Harbor。</li>
<li><strong>optional parameters</strong><br>  這類的參數為使用者自行決定是否設定，且只會在第一次安裝時，這些參數的配置才會生效。而 Harbor 啟動後，可以透過 Web UI 進行修改。</li>
</ul>
<h5 id="Configuring-storage-backend-optional"><a href="#Configuring-storage-backend-optional" class="headerlink" title="Configuring storage backend (optional)"></a>Configuring storage backend (optional)</h5><p>預設的情況下，Harbor 會將 Docker image 儲存在本機的檔案系統上，在生產環境中，您可以考慮使用其他 storage backend 而不是本機的檔案系統，像是 S3, OpenStack Swift, Ceph 等。而僅需更改 <code>common/templates/registry/config.yml</code>。以下為一個接 OpenStack Swift 的範例：</p>
<pre><code class="sh">storage:
  swift:
    username: admin
    password: ADMIN_PASS
    authurl: http://keystone_addr:35357/v3/auth
    tenant: admin
    domain: default
    region: regionOne
    container: docker_images
</code></pre>
<blockquote>
<p>更多 storage backend 的資訊，請參考：<a href="https://docs.docker.com/registry/configuration/" target="_blank" rel="noopener">Registry Configuration Reference</a>。<br>另外官方提供的是改 <code>common/templates/registry/config.yml</code>，感覺寫錯，需再測試其正確性。</p>
</blockquote>
<h4 id="執行安裝腳本"><a href="#執行安裝腳本" class="headerlink" title="執行安裝腳本"></a>執行安裝腳本</h4><p>一旦<code>harbor.cfg</code>與 storage backend (optional) 設定完成後，可以透過<code>install.sh</code>腳本開始安裝 Harbor。從 Harbor 1.1.0 版本之後，已經整合<code>Notary</code>，但是預設的情況下安裝是不包含<code>Notary</code>支援：</p>
<pre><code class="sh">$ sudo ./install.sh
</code></pre>
<blockquote>
<p>Online installer 會從 Docker hub 下載 Harbor 所需的映像檔，因此會花較久的時間。</p>
</blockquote>
<p>如果安裝過程正常，您可以打開瀏覽器並輸入在<code>harbor.cfg</code>中設定的<code>hostname</code>，來存取 Harbor 的 Web UI。<br><img src="https://i.imgur.com/jBVsr49.png" alt="Harbor Web UI"></p>
<blockquote>
<p>預設的管理者帳號密碼為 <code>admin</code>/<code>Harbor12345</code>。</p>
</blockquote>
<h4 id="開始使用-Harbor"><a href="#開始使用-Harbor" class="headerlink" title="開始使用 Harbor"></a>開始使用 Harbor</h4><p>登入成功後，可以創建一個新的 Project，並使用 Docker command 進行登入，但在登入之前，需要對 Docker daemon 新增<code>--insecure-registry</code>參數。新增<code>--insecure-registry</code>參數至<code>/etc/default/docker</code>中：</p>
<pre><code class="sh">DOCKER_OPTS=&quot;--insecure-registry &lt;your harbor.cfg hostname&gt;&quot;
</code></pre>
<blockquote>
<p>其他細節，請參考：<a href="https://docs.docker.com/registry/insecure/#deploying-a-plain-http-registry" target="_blank" rel="noopener">Test an insecure registry</a>。</p>
<p>若在<code>Ubuntu 16.04</code>的作業系統版本，需要修改<code>/lib/systemd/system/docker.service</code>檔案，並加入一下內容。另外在 CentOS 7.x 版本則不需要加入<code>-H fd://</code>資訊：</p>
<pre><code class="sh">EnvironmentFile=/etc/default/docker
ExecStart=/usr/bin/dockerd -H fd:// $DOCKER_OPTS
</code></pre>
</blockquote>
<p>修改完成後，重新啟動服務：</p>
<pre><code class="sh">$ sudo systemctl daemon-reload
</code></pre>
<p>服務重啟成功後，透過 Docker command 進行 login：</p>
<pre><code class="sh">$ docker login &lt;your harbor.cfg hostname&gt;
</code></pre>
<p>將映像檔上 tag 之後，上傳至 Harbor：</p>
<pre><code class="sh">$ docker tag ubuntu:&lt;your harbor.cfg hostname&gt;/&lt;your project&gt;/ubuntu:16.04
$ docker push &lt;your harbor.cfg hostname&gt;/&lt;your project&gt;/ubunut:16.04
</code></pre>
<p>從 Harbor 抓取上傳的映像檔：</p>
<pre><code class="sh">$ docker pull &lt;your harbor.cfg hostname&gt;/&lt;your project&gt;/ubunut:16.04
</code></pre>
<blockquote>
<p>更多使用者操作，請參考：<a href="https://github.com/vmware/harbor/blob/master/docs/user_guide.md" target="_blank" rel="noopener">Harbor User Guide</a>。</p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Container </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux Container </tag>
            
            <tag> Docker </tag>
            
            <tag> Docker registry </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[品嚐 Moby LinuxKit 的 Linux 作業系統]]></title>
      <url>https://kairen.github.io/2017/04/23/container/linuxkit/</url>
      <content type="html"><![CDATA[<p><a href="https://github.com/linuxkit/linuxkit" target="_blank" rel="noopener">LinuxKit</a> 是 <a href="http://www.nebulaworks.com/blog/2017/04/22/docker-captains-dockercon-2017-review/" target="_blank" rel="noopener">DockerCon 2017</a> 中推出的工具之一，其主要是以 Container 來建立最小、不可變的 Linux 作業系統映像檔框架，Docker 公司一直透過 LinuxKit 來建立相關產品，如 Docker for Mac 等。由於要最快的了解功能，因此這邊透過建立簡單的映像檔來學習。</p>
<center><img src="/images/docker/linux-kit.png" alt=""></center>

<a id="more"></a>
<p>在開始前需要準備完成一些事情：</p>
<ul>
<li>安裝 Git client。</li>
<li>安裝 Docker engine，這邊建立使用 Docker-ce 17.04.0。</li>
<li>安裝 GUN make 工具。</li>
<li>安裝 GUN tar 工具。</li>
</ul>
<h2 id="建構-Moby-工具"><a href="#建構-Moby-工具" class="headerlink" title="建構 Moby 工具"></a>建構 Moby 工具</h2><p>首先我們要建構名為 Moby 的工具，這個工具主要提供指定的 YAML 檔來執行描述的建構流程與功能，並利用 Docker 來建構出 Linux 作業系統。在本教學中，最後我們會利用 <a href="https://github.com/mist64/xhyve" target="_blank" rel="noopener">xhyve</a> 這個 OS X 的虛擬化來提供執行系統實例，當然也可以透過官方的 <a href="https://github.com/moby/hyperkit" target="_blank" rel="noopener">HyperKit</a> 來進行。</p>
<p>首先透過 Git 來抓取 LinuxKit repos，並進入建構 Moby：</p>
<pre><code class="sh">$ git clone https://github.com/linuxkit/linuxkit.git
$ cd linuxkit
$ make &amp;&amp; sudo make install
$ moby version
moby version 0.0
commit: 34d508562d7821cb812dd7b9caf4d9fbcdbc9fef
</code></pre>
<h3 id="建立-Linux-映像檔"><a href="#建立-Linux-映像檔" class="headerlink" title="建立 Linux 映像檔"></a>建立 Linux 映像檔</h3><p>當完成建構 Moby 工具後，就可以透過撰寫 YAML 檔來描述 Linux 的建構功能與流程了，這邊建立一個 Docker + SSH 的 Linux 映像檔。首先建立檔名為<code>docker-sshd.yml</code>的檔案，然後加入以下內容：</p>
<pre><code class="yaml">kernel:
  image: &quot;linuxkit/kernel:4.9.x&quot;
  cmdline: &quot;console=ttyS0 console=tty0 page_poison=1&quot;
init:
  - linuxkit/init:63eed9ca7a09d2ce4c0c5e7238ac005fa44f564b
  - linuxkit/runc:b0fb122e10dbb7e4e45115177a61a3f8d68c19a9
  - linuxkit/containerd:18eaf72f3f4f9a9f29ca1951f66df701f873060b
  - linuxkit/ca-certificates:e091a05fbf7c5e16f18b23602febd45dd690ba2f
onboot:
  - name: sysctl
    image: &quot;linuxkit/sysctl:1f5ec5d5e6f7a7a1b3d2ff9dd9e36fd6fb14756a&quot;
    net: host
    pid: host
    ipc: host
    capabilities:
     - CAP_SYS_ADMIN
    readonly: true
  - name: sysfs
    image: linuxkit/sysfs:6c1d06f28ddd9681799d3950cddf044b930b221c
  - name: binfmt
    image: &quot;linuxkit/binfmt:c7e69ebd918a237dd086a5c58dd888df772746bd&quot;
    binds:
     - /proc/sys/fs/binfmt_misc:/binfmt_misc
    readonly: true
  - name: format
    image: &quot;linuxkit/format:53748000acf515549d398e6ae68545c26c0f3a2e&quot;
    binds:
     - /dev:/dev
    capabilities:
     - CAP_SYS_ADMIN
     - CAP_MKNOD
  - name: mount
    image: &quot;linuxkit/mount:d2669e7c8ddda99fa0618a414d44261eba6e299a&quot;
    binds:
     - /dev:/dev
     - /var:/var:rshared,rbind
    capabilities:
     - CAP_SYS_ADMIN
    rootfsPropagation: shared
    command: [&quot;/mount.sh&quot;, &quot;/var/lib/docker&quot;]
services:
  - name: rngd
    image: &quot;linuxkit/rngd:c42fd499690b2cb6e4e6cb99e41dfafca1cf5b14&quot;
    capabilities:
     - CAP_SYS_ADMIN
    oomScoreAdj: -800
    readonly: true
  - name: dhcpcd
    image: &quot;linuxkit/dhcpcd:57a8ef29d3a910645b2b24c124f9ce9ef53ce703&quot;
    binds:
     - /var:/var
     - /tmp/etc:/etc
    capabilities:
     - CAP_NET_ADMIN
     - CAP_NET_BIND_SERVICE
     - CAP_NET_RAW
    net: host
    oomScoreAdj: -800
  - name: ntpd
    image: &quot;linuxkit/openntpd:a570316d7fc49ca1daa29bd945499f4963d227af&quot;
    capabilities:
      - CAP_SYS_TIME
      - CAP_SYS_NICE
      - CAP_SYS_CHROOT
      - CAP_SETUID
      - CAP_SETGID
    net: host
  - name: docker
    image: &quot;linuxkit/docker-ce:741bf21513328f674e0cdcaa55492b0b75974e08&quot;
    capabilities:
     - all
    net: host
    mounts:
     - type: cgroup
       options: [&quot;rw&quot;,&quot;nosuid&quot;,&quot;noexec&quot;,&quot;nodev&quot;,&quot;relatime&quot;]
    binds:
     - /var/lib/docker:/var/lib/docker
     - /lib/modules:/lib/modules
  - name: sshd
    image: &quot;linuxkit/sshd:e108d208adf692c8a0954f602743e0eec445364e&quot;
    capabilities:
    - all
    net: host
    pid: host
    binds:
      - /root/.ssh:/root/.ssh
      - /etc/resolv.conf:/etc/resolv.conf
  - name: test-docker-bench
    image: &quot;linuxkit/test-docker-bench:2f941429d874c5dcf05e38005affb4f10192e1a8&quot;
    ipc: host
    pid: host
    net: host
    binds:
    - /run:/var/run
    capabilities:
    - all
files:
  - path: etc/docker/daemon.json
    contents: &#39;{&quot;debug&quot;: true}&#39;
  - path: root/.ssh/authorized_keys
    contents: &#39;SSH_KEY&#39;
trust:
  image:
    - linuxkit/kernel
    - linuxkit/binfmt
    - linuxkit/rngd
outputs:
  - format: kernel+initrd
  - format: iso-bios
</code></pre>
<blockquote>
<p><code>P.S.</code>請修改<code>SSH_KEY</code>內容為你的系統 ssh public key。</p>
</blockquote>
<p>這邊說明幾個 YAML 格式意義：</p>
<ul>
<li><strong>kernel</strong>: 指定 Docker 映像檔的核心版本，會包含一個 Linux 核心與檔案系統的 tar 檔，會將核心建構在<code>/kernel</code>目錄中。</li>
<li><strong>init</strong>: 是一個 Docker Container 的 init 行程基礎，裡面包含<code>init</code>、<code>containerd</code>、<code>runC</code>與其他等工具。</li>
<li><strong>onboot</strong>: 指定要建構的系統層級工具，會依據定義順序來執行，該類型如: dhcpd 與 ntpd 等。</li>
<li><strong>services</strong>: 指定要建構服務，通常會是系統開啟後執行，如 ngnix、apache2。</li>
<li><strong>files</strong>:要複製到該 Linux 系統映像檔中的檔案。</li>
<li><strong>outputs</strong>:輸出的映像檔格式。</li>
</ul>
<blockquote>
<p>更多 YAML 格式說明可以參考官方 <a href="https://github.com/linuxkit/linuxkit/blob/master/docs/yaml.md" target="_blank" rel="noopener">LinuxKit YAML</a>。目前 LinuxKit 的映像檔來源可以參考 <a href="https://hub.docker.com/u/linuxkit/" target="_blank" rel="noopener">Docker Hub</a></p>
</blockquote>
<p>撰寫完後，就可以透過 Moby 工具進行建構 Linux 映像檔了：</p>
<pre><code class="sh">$ moby build docker-sshd.yml
Extract kernel image: linuxkit/kernel:4.9.x
Pull image: linuxkit/kernel:4.9.x
...
Create outputs:
  docker-sshd-kernel docker-sshd-initrd.img docker-sshd-cmdline
  docker-sshd.iso
</code></pre>
<p>完成後會看到以下幾個檔案：</p>
<ul>
<li>docker-sshd-kernel: 為 RAW Kernel 映像檔.</li>
<li>docker-sshd-initrd.img: 為初始化 RAW Disk 檔案.</li>
<li>docker-sshd-cmdline: Command line options 檔案.</li>
<li>docker-sshd.iso: Docker SSHD ISO 格式映像檔.</li>
</ul>
<h3 id="測試映像檔"><a href="#測試映像檔" class="headerlink" title="測試映像檔"></a>測試映像檔</h3><p>當完成建構映像檔後，就可以透過一些工具來進行測試，這邊採用 <a href="https://github.com/mist64/xhyve" target="_blank" rel="noopener">xhyve</a> 來執行實例，首先透過 Git 取得 xhyve repos，並建構與安裝：</p>
<pre><code class="sh">$ git clone https://github.com/mist64/xhyve
$ cd xhyve
$ make &amp;&amp; cp build/xhyve /usr/local/bin/
$ xhyve
Usage: xhyve [-behuwxMACHPWY] [-c vcpus] [-g &lt;gdb port&gt;] [-l &lt;lpc&gt;]
             [-m mem] [-p vcpu:hostcpu] [-s &lt;pci&gt;] [-U uuid] -f &lt;fw&gt;
</code></pre>
<blockquote>
<p>xhyve 是 FreeBSD 虛擬化技術 bhyve 的 OS X 版本，是以  <a href="https://developer.apple.com/library/mac/documentation/DriversKernelHardware/Reference/Hypervisor/index.html" target="_blank" rel="noopener">Hypervisor.framework</a> 為基底的上層工具，這是除了 VirtualBox 與 VMwar 的另外選擇，並且該工具非常的輕巧，只有幾 KB 的容量。</p>
</blockquote>
<p>接著撰寫 xhyve 腳本來啟動映像檔：</p>
<pre><code class="sh">#!/bin/sh

KERNEL=&quot;docker-sshd-kernel&quot;
INITRD=&quot;docker-sshd-initrd.img&quot;
CMDLINE=&quot;console=ttyS0 console=tty0 page_poison=1&quot;

MEM=&quot;-m 1G&quot;
PCI_DEV=&quot;-s 0:0,hostbridge -s 31,lpc&quot;
LPC_DEV=&quot;-l com1,stdio&quot;
ACPI=&quot;-A&quot;
#SMP=&quot;-c 2&quot;

# sudo if you want networking enabled
NET=&quot;-s 2:0,virtio-net&quot;

xhyve $ACPI $MEM $SMP $PCI_DEV $LPC_DEV $NET -f kexec,$KERNEL,$INITRD,&quot;$CMDLINE&quot;
</code></pre>
<blockquote>
<p>修改<code>KERNEL</code>與<code>INITRD</code>為 docker-sshd 的映像檔。</p>
</blockquote>
<p>完成後就可以進行啟動測試：</p>
<pre><code>$ chmod u+x run.sh
$ sudo ./run.sh
Welcome to LinuxKit

                        ##         .
                  ## ## ##        ==
               ## ## ## ## ##    ===
           /&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\___/ ===
      ~~~ {~~ ~~~~ ~~~ ~~~~ ~~~ ~ /  ===- ~~~
           \______ o           __/
             \    \         __/
              \____\_______/
...
/ # ls
bin         etc         lib         root        srv         usr
containers  home        media       run         sys         var
dev         init        proc        sbin        tmp
/ # ip
...
4: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
    inet 192.168.64.4/24 brd 192.168.64.255 scope global eth0
       valid_lft forever preferred_lft forever
14: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN
    inet 172.17.0.1/16 scope global docker0
       valid_lft forever preferred_lft forever
</code></pre><h3 id="驗證映像檔服務"><a href="#驗證映像檔服務" class="headerlink" title="驗證映像檔服務"></a>驗證映像檔服務</h3><p>當看到上述結果後，表示作業系統開啟無誤，這時候我們要測試系統服務是否正常，首先透過 SSH 來進行測試，在剛剛新增的 ssh public key 主機上執行以下：</p>
<pre><code>$ ssh root@192.168.64.4
moby-aa16c789d03b:~# uname -r
4.9.25-linuxkit

moby-aa16c789d03b:~# exit
</code></pre><p>查看 Docker 是否啟動：</p>
<pre><code>moby-aa16c789d03b:~# netstat -xp
Active UNIX domain sockets (w/o servers)
Proto RefCnt Flags       Type       State         I-Node PID/Program name    Path
unix  2      [ ]         DGRAM                     33822 606/dhcpcd
unix  3      [ ]         STREAM     CONNECTED      33965 748/ntpd: dns engin
unix  3      [ ]         STREAM     CONNECTED      33960 747/ntpd: ntp engin
unix  3      [ ]         STREAM     CONNECTED      33964 747/ntpd: ntp engin
unix  3      [ ]         STREAM     CONNECTED      33959 642/ntpd
unix  3      [ ]         STREAM     CONNECTED      34141 739/dockerd
unix  3      [ ]         STREAM     CONNECTED      34142 751/docker-containe /var/run/docker/libcontainerd/docker-containerd.sock
</code></pre><p>最後關閉虛擬機可以透過以下指令完成：</p>
<pre><code>moby-aa16c789d03b:~# halt
Terminated
</code></pre>]]></content>
      
        <categories>
            
            <category> Container </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Docker </tag>
            
            <tag> Linux </tag>
            
            <tag> Moby </tag>
            
            <tag> Microkernel </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow 基本使用與分散式概念]]></title>
      <url>https://kairen.github.io/2017/04/10/tensorflow/intro/</url>
      <content type="html"><![CDATA[<p>TensorFlow™ 是利用資料流圖(Data Flow Graphs)來表達數值運算的開放式原始碼函式庫。資料流圖中的節點(Nodes)被用來表示數學運算，而邊(Edges)則用來表示在節點之間互相聯繫的多維資料陣列，即張量(Tensors)。它靈活的架構讓你能夠在不同平台上執行運算，例如 PC 中的一個或多的 CPU(或GPU)、智慧手持裝置與伺服器等。TensorFlow 最初是 Google 機器智能研究所的研究員和工程師開發而成，主要用於機器學習與深度神經網路方面研究。</p>
<a id="more"></a>
<p>TensorFlow 其實在意思上是要用兩個部分來解釋，Tensor 與 Flow：</p>
<ul>
<li><strong>Tensor</strong>：是中文翻譯是<code>張量</code>，其實就是一個<code>n</code>維度的陣列或列表。如一維 Tensor 就是向量，二維 Tensor 就是矩陣等等.</li>
<li><strong>Flow</strong>：是指 Graph 運算過程中的資料流.</li>
</ul>
<center><img src="/images/tf/tf-logo.png" alt=""></center>

<h2 id="Data-Flow-Graphs"><a href="#Data-Flow-Graphs" class="headerlink" title="Data Flow Graphs"></a>Data Flow Graphs</h2><p>資料流圖(Data Flow Graphs)是一種有向圖的節點(Node)與邊(Edge)來描述計算過程。圖中的節點表示數學操作，亦表示資料 I/O 端點; 而邊則表示節點之間的關析，用來傳遞操作之間互相使用的多維陣列(Tensors)，而 Tensor 是在圖中流動的資料表示。一旦節點相連的邊傳來資料流，這時節點就會被分配到運算裝置上異步(節點之間)或同步(節點之內)的執行。</p>
<center><img src="https://www.tensorflow.org/images/tensors_flowing.gif" alt=""></center>

<h2 id="TensorFlow-基本使用"><a href="#TensorFlow-基本使用" class="headerlink" title="TensorFlow 基本使用"></a>TensorFlow 基本使用</h2><p>在開始進行 TensorFlow 之前，需要了解幾個觀念：</p>
<ul>
<li>使用 <a href="https://www.tensorflow.org/api_docs/python/tf/Graph" target="_blank" rel="noopener">tf.Graph</a> 來表示計算任務.</li>
<li>採用<code>tensorflow::Session</code>的上下文(Context)來執行圖.</li>
<li>以 Tensor 來表示所有資料，可看成擁有靜態資料類型，但有動態大小的多維陣列與列表，如 Boolean 或 String 轉成數值類型.</li>
<li>透過<code>tf.Variable</code>來維護狀態.</li>
<li>透過 feed 與 fetch 來任意操作(Arbitrary operation)給予值或從中取得資料.</li>
</ul>
<p>TensorFlow 的圖中的節點被稱為 <a href="https://www.tensorflow.org/api_docs/python/tf/Operation" target="_blank" rel="noopener">op(operation)</a>。一個<code>op</code>會有 0 至多個 Tensor，而每個 Tensor 是一種類別化的多維陣列，例如把一個圖集合表示成四維浮點陣列，分別為<code>[batch, height, width, channels]</code>。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/2630831-5da81623d4661886.jpg?imageMogr2/auto-orient/strip" alt=""></p>
<p>利用三種不同稱呼來描述 Tensor 的維度，Shape、Rank 與 Dimension。可參考 <a href="https://www.tensorflow.org/programmers_guide/dims_types" target="_blank" rel="noopener">Rank, Shape, 和 Type</a>。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/2630831-3625a021343b5da3.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>一般只有 shape 能夠直接被 print，而 Tensor 則需要 Session 來提供，一般需要三個操作步驟：</p>
<ol>
<li>建立 Tensor.</li>
<li>新增 op.</li>
<li>建立 Session(包含一個 Graph)來執行運算.</li>
</ol>
<p>以下是一個簡單範例，說明如何建立運算：</p>
<pre><code class="py"># coding=utf-8
import tensorflow as tf

a = tf.constant(1)
b = tf.constant(2)
c = tf.constant(3)
d = tf.constant(4)
add1 = tf.add(a, b)
mul1 = tf.multiply(b, c)
add2 = tf.add(c, d)
output = tf.add(add1, mul1)

with tf.Session() as sess:
    print sess.run(output)
</code></pre>
<p>執行流程如下圖：<br><img src="https://github.com/lienhua34/notes/raw/master/tensorflow/asserts/graph_compute_flow.jpg?_=5998853" alt=""></p>
<p>以下是一個簡單範例，說明如何建立多個 Graph：</p>
<pre><code class="python="># coding=utf-8
import tensorflow as tf

logs_path = &#39;./basic_tmp&#39;

# 建立一個 graph，並建立兩個常數 op ，這些 op 稱為節點
g1 = tf.Graph()
with g1.as_default():
    a = tf.constant([1.5, 6.0])
    b = tf.constant([1.5, 3.2])
    c = a * b

with tf.Graph().as_default() as g2:
    # 建立一個 1x2 矩陣與 2x1 矩陣 op
    m1 = tf.constant([[1., 0., 2.], [-1., 3., 1.]])
    m2 = tf.constant([[3., 1.], [2., 1.], [1., 0.]])
    m3 = tf.matmul(m1, m2) # 矩陣相乘

# 在 session 執行 graph，並進行資料數據操作 `c`。
# 然後指派給 cpu 做運算
with tf.Session(graph=g1) as sess_cpu:
  with tf.device(&quot;/cpu:0&quot;):
      writer = tf.summary.FileWriter(logs_path, graph=g1)
      print(sess_cpu.run(c))

with tf.Session(graph=g2) as sess_gpu:
  with tf.device(&quot;/gpu:0&quot;):
      result = sess_gpu.run(m3)
      print(result)

# 使用 tf.InteractiveSession 方式來印出內容(不會實際執行)
it_sess = tf.InteractiveSession()
x = tf.Variable([1.0, 2.0])
a = tf.constant([3.0, 3.0])

# 使用初始器 initializer op 的 run() 方法初始化 &#39;x&#39;
x.initializer.run()
sub = tf.subtract(x, a)

print sub.eval()
it_sess.close()
</code></pre>
<blockquote>
<ul>
<li>範例來至 <a href="https://www.tensorflow.org/versions/r0.10/get_started/basic_usage" target="_blank" rel="noopener">Basic Usage</a>。</li>
<li>指定 Device 可以看這邊 <a href="https://www.tensorflow.org/versions/r0.10/how_tos/using_gpu/" target="_blank" rel="noopener">Using GPU</a>.</li>
</ul>
</blockquote>
<p>上面範例可以看到建立了一個 Graph 的計算過程<code>c</code>，而當直接執行到<code>c</code>時，並不會真的執行運算，而是在<code>sess</code>會話建立後，並透過<code>sess</code>執行分配給 CPU 或 GPU 之類設備進行運算後，才會回傳一個節點的 Tensor，在 Python 中 Tensor 是一個 Numpy 的 ndarry 物件。</p>
<p>TensorFlow 也可以透過變數來維護 Graph 的執行過程狀態，這邊提供一個簡單的累加器：</p>
<pre><code class="python="># coding=utf-8
import tensorflow as tf

# 建立一個變數 counter，並初始化為 0
state = tf.Variable(0, name=&quot;counter&quot;)

# 建立一個常數 op 為 1，並用來累加 state
one = tf.constant(1)
new_value = tf.add(state, one)
update = tf.assign(state, new_value)

# 啟動 Graph 前，變數必須先被初始化(init) op
init_op = tf.global_variables_initializer()

# 啟動 Graph 來執行 op
with tf.Session() as sess:
  sess.run(init_op)
  print sess.run(state)
  # 執行 op 並更新 state
  for _ in range(3):
    sess.run(update)
    print sess.run(state)
</code></pre>
<blockquote>
<p>更多細節可以查看 <a href="https://www.tensorflow.org/programmers_guide/variables" target="_blank" rel="noopener">Variables</a>。</p>
</blockquote>
<p>另外可以利用 Fetch 方式來一次取得多個節點的 Tensor，範例如下：</p>
<pre><code class="python="># coding=utf-8
import tensorflow as tf

input1 = tf.constant(3.0)
input2 = tf.constant(2.0)
input3 = tf.constant(5.0)
intermed = tf.add(input2, input3)
mul = tf.multiply(input1, intermed)

with tf.Session() as sess:
  # 一次取得多個 Tensor
  result = sess.run([mul, intermed])
  print result
</code></pre>
<p>而當我們想要在執行 Session 時，臨時替換 Tensor 內容的話，就可以利用 TensorFlow 內建的 Feed 方法來解決：</p>
<pre><code class="python="># coding=utf-8
import tensorflow as tf

input1 = tf.placeholder(tf.float32)
input2 = tf.placeholder(tf.float32)
output = tf.multiply(input1, input2)

with tf.Session() as sess:
  # 透過 feed 來更改 op 內容，這只會在執行時有效
  print sess.run([output], feed_dict={input1:[7.], input2:[2.]})
  print sess.run([output])
</code></pre>
<h2 id="TensorFlow-分散式運算"><a href="#TensorFlow-分散式運算" class="headerlink" title="TensorFlow 分散式運算"></a>TensorFlow 分散式運算</h2><p>本節將以 TensorFlow 分散式深度學習為例。</p>
<h3 id="gRPC"><a href="#gRPC" class="headerlink" title="gRPC"></a>gRPC</h3><p>gRPC(google Remote Procedure Call) 是 Google 開發的基於 HTTP/2 和 Protocol Buffer 3 的 RPC 框架，該框架有各種常見語言的實作，如 C、Java 與 Go 等語言，提供輕鬆跨語言的呼叫。</p>
<h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>說明客戶端(Client)、叢集(Cluster)、工作(Job)、任務(Task)、TensorFlow 伺服器、Master 與 Worker 服務。</p>
<p><img src="http://www.pittnuts.com/wp-content/uploads/2016/08/TFramework.png" alt=""></p>
<p>如圖所示，幾個流程說明如下：</p>
<ul>
<li>整個系统映射到 TensorFlow 叢集.</li>
<li>參數伺服器映射到一個 Job.</li>
<li>每個模型(Model)副本映射到一個 Job.</li>
<li>每台實體運算節點映射到其 Job 中的 Task.</li>
<li>每個 Task 都有一個 TF Server，並利用 Master 服務來進行溝通與協調工作，而 Worker 服務則透過本地裝置(CPU 或 GPU)進行 TF graph 運算.</li>
</ul>
<p>TensorFlow 叢集裡包含了一個或多個工作(Job)，每個工作又可以拆分成一個或多個任務(Task)，簡單說 Cluster 是 Job 的集合，而 Job 是 Task 的集合。叢集概念主要用在一個特定層次對象，如訓練神經網路、平行操作多台機器等，一個叢集物件可以透過<code>tf.train.ClusterSpec</code>來定義。</p>
<p>如上所述，TensorFlow 的叢集就是一組工作任務，每個任務是一個服務，而服務又分成<code>Master</code>與<code>Worker</code>這兩種，並提供給<code>Client</code>進行操作。</p>
<ul>
<li><strong>Client</strong>：是用於建立 TensorFlow 計算 Graph，並建立與叢集進行互動的<code>tensorflow::Session</code>行程，一般由 Python 或 C++ 實作，單一客戶端可以同時連接多個 TF 伺服器連接，同時也能被多個 TF 伺服器連接.</li>
<li><strong>Master Service</strong>：是一個 RPC 服務行程，用來遠端連線一系列分散式裝置，主要提供<code>tensorflow::Session</code>介面，並負責透過 Worker Service 與工作的任務進行溝通.</li>
<li><strong>Worker Service</strong>：是一個可以使用本地裝置(CPU 或 GPU)對部分 Graph 進行運算的 RPC 邏輯，透過<code>worker_service.proto</code>介面來實作，所有 TensorFlow 伺服器均包含了 Worker Service 邏輯.</li>
</ul>
<blockquote>
<p><strong>TensorFlow 伺服器</strong>是運行<code>tf.train.Server</code>實例的行程，其為叢集一員，並有 Master 與 Worker 之分。</p>
</blockquote>
<p>而 TensorFlow 的工作(Job)可拆成多個相同功能的任務(Task)，這些工作又分成<code>Parameter server</code>與<code>Worker</code>，兩者功能說明如下：</p>
<p><img src="https://img.tipelse.com/uploads/B/6A/B6A07C1923.jpeg" alt=""></p>
<ul>
<li><strong>Parameter server(ps)</strong>:是分散式系統縮放至工業大小機器學習的問題，它提供工作節點與伺服器節點之間的非同步與零拷貝 key-value 的溝通，並支援資料的一致性模型的分散式儲存。在 TensorFlow 中主要根據梯度更新變數，並儲存於<code>tf.Variable</code>，可理解成只儲存 TF Model 的變數，並存放 Variable 副本.</li>
</ul>
<p><img src="http://arimo.com/wp-content/uploads/2016/03/TF_Image_0.png" alt=""></p>
<ul>
<li><strong>Worker</strong>:通常稱為計算節點，一般管理無狀態(Stateless)，且執行密集型的 Graph 運算資源，並根據變數運算梯度。存放 Graph 副本.</li>
</ul>
<p><img src="http://arimo.com/wp-content/uploads/2016/03/TF_Image_1.png" alt=""></p>
<blockquote>
<ul>
<li><a href="http://blog.csdn.net/cyh_24/article/details/50545780" target="_blank" rel="noopener">Parameter Server 詳解</a></li>
</ul>
</blockquote>
<p>一般對於<code>小型規模訓練</code>，這種資料與參數量不多時，可以用一個 CPU 來同時執行兩種任務。而<code>中型規模訓練</code>，資料量較大，但參數量不多時，計算梯度的工作負載較高，而參數更新負載較低，所以計算梯度交給若干個 CPU 或 GPU 去執行，而更新參數則交給一個 CPU 即可。對於<code>大型規模訓練</code>，資料與參數量多時，不僅計算梯度需要部署多個 CPU 或 GPU，連更新參數也要不說到多個 CPU 中。</p>
<p>然而單一節點能夠裝載的 CPU 與 GPU 是有限的，所以在大量訓練時就需要多台機器來提供運算能力的擴展。</p>
<h3 id="分散式變數伺服器-Parameter-Server"><a href="#分散式變數伺服器-Parameter-Server" class="headerlink" title="分散式變數伺服器(Parameter Server)"></a>分散式變數伺服器(Parameter Server)</h3><p>當在較大規模的訓練時，隨著模型的變數越來越多，很可能造成單一節點因為效能問題，而無法負荷模型變數儲存與更新時，這時候就需要將變數分開到不同機器來做儲存與更新。而 TensorFlow 提供了變數伺服器的邏輯實現，並可以用多台機器來組成叢集，類似分散式儲存結構，主要用來解決變數的儲存與更新效能問題。</p>
<h3 id="撰寫分散式程式注意概念"><a href="#撰寫分散式程式注意概念" class="headerlink" title="撰寫分散式程式注意概念"></a>撰寫分散式程式注意概念</h3><p>當我們在寫分散式程式時，需要知道使用的副本與訓練模式。</p>
<p><img src="https://camo.githubusercontent.com/0b7a1232bd3f8861dfbccab568a30591588384dc/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f74656e736f72666c6f775f666967757265372e706e67" alt=""></p>
<h4 id="In-graph-與-Between-graph-副本模式"><a href="#In-graph-與-Between-graph-副本模式" class="headerlink" title="In-graph 與 Between-graph 副本模式"></a>In-graph 與 Between-graph 副本模式</h4><p>下圖顯示兩者差異，而這邊也在進行描述。</p>
<ul>
<li><strong>In-graph</strong>：只有一個 Clinet(主要呼叫<code>tf::Session</code>行程)，並將裡面變數與 op 指定給對應的 Job 完成，因此資料分發只由一個 Client 完成。這種方式設定簡單，其他節點只需要 join 操作，並提供一個 gRPC 位址來等待任務。但是訓練資料只在單一節點，因此要把資料分發到不同機器時，會影響平行訓練效能。可理解成所有 op 都在同一個 Graph 中，伺服器只需要做<code>join()</code>功能.</li>
<li><strong>Between-graph</strong>：多個獨立 Client 建立相同 Graph(包含變數)，並透過<code>tf.train.replica_device_setter</code>將這些參數映射到 ps 上，即訓練的變數儲存在 Parameter Server，而資料不用分發，資料分片(Shards)會存在個計算節點，因此個節點自己算自己的，算完後，把要更新變數告知 Parameter Server 進行更新。適合在 TB 級別的資料量使用，節省大量資料傳輸時間，也是深度學習推薦模式。</li>
</ul>
<h4 id="同步-Synchronous-訓練與非同步-Asynchronous-訓鍊"><a href="#同步-Synchronous-訓練與非同步-Asynchronous-訓鍊" class="headerlink" title="同步(Synchronous)訓練與非同步(Asynchronous)訓鍊"></a>同步(Synchronous)訓練與非同步(Asynchronous)訓鍊</h4><p>TensorFlow 的副本擁有 in-graph 和 between-graph 模式，這兩者都支援了同步與非同步更新。本節將說明同步與非同步兩者的差異為何。</p>
<ul>
<li><strong>Synchronous</strong>：每個 Graph 的副本讀取相同 Parameter 的值，然後平行計算梯度(gradients)，將所有計算完的梯度放在一起處理，當每次更新梯度時，需要等所以分發的資料計算完成，並回傳結果來把梯度累加計算平均，在進行更新變數。好處在於使用 loss 的下降時比較穩定，壞處就是要等最慢的分片計算時間。</li>
</ul>
<blockquote>
<p>可以利用<code>tf.train.SyncReplicasOptimizer</code>來解決這個問題(在 Between-graph 情況下)，而在 In-graph 則將所有梯度平均即可。</p>
</blockquote>
<ul>
<li><strong>Asynchronous</strong>：自己計算完梯度後，就去更新 paramenter，不同副本之前不會進行協調進度，因此計算資源被充分的利用。缺點是 loss 的下降不穩定。</li>
</ul>
<p><img src="http://img.blog.csdn.net/20161114005141032" alt=""></p>
<p>一般在資料量小，且各節點計算能力平均下，適合使用同步模式; 反之在資料量大與各節點效能差異不同時，適合用非同步。</p>
<h3 id="簡單分散式訓練程式"><a href="#簡單分散式訓練程式" class="headerlink" title="簡單分散式訓練程式"></a>簡單分散式訓練程式</h3><p>TensorFlow 提供建立 Server 函式來進行測試使用，以下是建立一個分散式訓練 Server 程式<code>server.py</code>：</p>
<pre><code class="python="># coding=utf-8
import tensorflow as tf

# 定義 Cluster
cluster = tf.train.ClusterSpec({&quot;worker&quot;: [&quot;localhost:2222&quot;]})

# 建立 Worker server
server = tf.train.Server(cluster,job_name=&quot;worker&quot;,task_index=0)
server.join()
</code></pre>
<blockquote>
<p>也可以透過<code>tf.train.Server.create_local_server()</code> 來建立 Local Server</p>
</blockquote>
<p>當確認程式沒有任何問題後，就可以透過以下方式啟動：</p>
<pre><code class="shell=">$ python server.py
2017-04-10 18:19:41.953448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 650, pci bus id: 0000:01:00.0)
2017-04-10 18:19:41.983913: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job local -&gt; {0 -&gt; localhost:2222}
2017-04-10 18:19:41.984946: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:240] Started server with target: grpc://localhost:2222
</code></pre>
<p>接著我們要撰寫 Client 端來進行定義 Graph 運算的程式<code>client.py</code>：</p>
<pre><code class="python="># coding=utf-8
import tensorflow as tf

# 執行目標 Session
server_target = &quot;grpc://localhost:2222&quot;
logs_path = &#39;./basic_tmp&#39;

# 指定 worker task 0 使用 CPU 運算
with tf.device(&quot;/job:worker/task:0&quot;):
    with tf.device(&quot;/cpu:0&quot;):
        a = tf.constant([1.5, 6.0], name=&#39;a&#39;)
        b = tf.Variable([1.5, 3.2], name=&#39;b&#39;)
        c = (a * b) + (a / b)
        d = c * a
        y = tf.assign(b, d)

# 啟動 Session
with tf.Session(server_target) as sess:
    sess.run(tf.global_variables_initializer())
    writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())
    print(sess.run(y))
</code></pre>
<p>完成後即可透過以下指令測試：</p>
<pre><code class="python=">$ python client.py
[   4.875       126.45000458]
</code></pre>
<h3 id="線性迴歸訓練程式"><a href="#線性迴歸訓練程式" class="headerlink" title="線性迴歸訓練程式"></a>線性迴歸訓練程式</h3><p>上面範例提供了很簡單的 Client 與 Server 運算操作。而這邊建立一個 Between-graph 執行程式<code>bg_dist.py</code>：</p>
<pre><code class="python="># coding=utf-8
import tensorflow as tf
import numpy as np

parameter_servers = [&quot;localhost:2222&quot;]
workers = [&quot;localhost:2223&quot;, &quot;localhost:2224&quot;]

tf.app.flags.DEFINE_string(&quot;job_name&quot;, &quot;&quot;, &quot;輸入 &#39;ps&#39; 或是 &#39;worker&#39;&quot;)
tf.app.flags.DEFINE_integer(&quot;task_index&quot;, 0, &quot;Job 的任務 index&quot;)
FLAGS = tf.app.flags.FLAGS


def main(_):

    cluster = tf.train.ClusterSpec({&quot;ps&quot;: parameter_servers, &quot;worker&quot;: workers})
    server = tf.train.Server(cluster,job_name=FLAGS.job_name,task_index=FLAGS.task_index)

    if FLAGS.job_name == &quot;ps&quot;:
        server.join()
    elif FLAGS.job_name == &quot;worker&quot;:

        train_X = np.linspace(-1.0, 1.0, 100)
        train_Y = 2.0 * train_X + np.random.randn(*train_X.shape) * 0.33 + 10.0

        X = tf.placeholder(&quot;float&quot;)
        Y = tf.placeholder(&quot;float&quot;)

        # Assigns ops to the local worker by default.
        with tf.device(tf.train.replica_device_setter(
                worker_device=&quot;/job:worker/task:%d&quot; % FLAGS.task_index,
                cluster=cluster)):

            w = tf.Variable(0.0, name=&quot;weight&quot;)
            b = tf.Variable(0.0, name=&quot;bias&quot;)
            # 損失函式，用於描述模型預測值與真實值的差距大小，常見為`均方差(Mean Squared Error)`
            loss = tf.square(Y - tf.multiply(X, w) - b)

            global_step = tf.Variable(0)

            train_op = tf.train.AdagradOptimizer(0.01).minimize(
                loss, global_step=global_step)

            saver = tf.train.Saver()
            summary_op = tf.summary.merge_all()
            init_op = tf.global_variables_initializer()

        # 建立 &quot;Supervisor&quot; 來負責監督訓練過程
        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),
                                 logdir=&quot;/tmp/train_logs&quot;,
                                 init_op=init_op,
                                 summary_op=summary_op,
                                 saver=saver,
                                 global_step=global_step,
                                 save_model_secs=600)

        with sv.managed_session(server.target) as sess:
            loss_value = 100
            while not sv.should_stop() and loss_value &gt; 70.0:
                # 執行一個非同步 training 步驟.
                # 若要執行同步可利用`tf.train.SyncReplicasOptimizer` 來進行
                for (x, y) in zip(train_X, train_Y):
                    _, step = sess.run([train_op, global_step],
                                       feed_dict={X: x, Y: y})

                loss_value = sess.run(loss, feed_dict={X: x, Y: y})
                print(&quot;步驟: {}, loss: {}&quot;.format(step, loss_value))

        sv.stop()


if __name__ == &quot;__main__&quot;:
    tf.app.run()
</code></pre>
<blockquote>
<p><code>tf.train.replica_device_setter(ps_tasks=0, ps_device=&#39;/job:ps&#39;, worker_device=&#39;/job:worker&#39;, merge_devices=True, cluster=None, ps_ops=None)</code> 指定方式。</p>
</blockquote>
<p>撰寫完成後，透過以下指令來進行測試：</p>
<pre><code class="shell=">$ python liner_dist.py --job_name=ps --task_index=0
$ python liner_dist.py --job_name=worker --task_index=0
$ python liner_dist.py --job_name=worker --task_index=1
</code></pre>
<h2 id="Tensorboard-視覺化工具"><a href="#Tensorboard-視覺化工具" class="headerlink" title="Tensorboard 視覺化工具"></a>Tensorboard 視覺化工具</h2><p>Tensorboard 是 TensorFlow 內建的視覺化工具，我們可以透過讀取事件紀錄結構化的資料，來顯示以下幾個項目來提供視覺化：</p>
<ul>
<li><strong>Event</strong>：訓練過程中統計資料(平均值等)變化狀態.</li>
<li><strong>Image</strong>：訓練過程中紀錄的 Graph.</li>
<li><strong>Audio</strong>：訓練過程中紀錄的 Audio.</li>
<li><strong>Histogram</strong>：順練過程中紀錄的資料分散圖</li>
</ul>
<p>一個範例程式如下所示：</p>
<pre><code class="python"># coding=utf-8
import tensorflow as tf

logs_path = &#39;./tmp/1&#39;

# 建立一個 graph，並建立兩個常數 op ，這些 op 稱為節點
g1 = tf.Graph()
with g1.as_default():
    a = tf.constant([1.5, 6.0], name=&#39;a&#39;)
    b = tf.Variable([1.5, 3.2], name=&#39;b&#39;)
    c = (a * b) + (a / b)
    d = c * a
    y = tf.assign(b, d)

# 在 session 執行 graph，並進行資料數據操作 `c`。
# 然後指派給 cpu 做運算
with tf.Session(graph=g1) as sess_cpu:
  with tf.device(&quot;/cpu:0&quot;):
      sess_cpu.run(tf.global_variables_initializer())
      writer = tf.summary.FileWriter(logs_path, graph=g1)
      print(sess_cpu.run(y))
</code></pre>
<p>執行後會看到當前目錄產生<code>tmp_mnist</code> logs 檔案，這時候就可以透過 thensorboard 來視覺化訓練結果：</p>
<pre><code class="shell=">$ tensorboard --logdir=run1:./tmp/1 --port=6006
</code></pre>
<blockquote>
<p>run1 是當有多次 log 被載入時做為區別用。</p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> TensorFlow </category>
            
        </categories>
        
        
        <tags>
            
            <tag> TensorFlow </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> Ubuntu </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Kuberentes Helm 介紹]]></title>
      <url>https://kairen.github.io/2017/03/25/kubernetes/helm-quickstart/</url>
      <content type="html"><![CDATA[<p><a href="https://github.com/kubernetes/helm" target="_blank" rel="noopener">Helm</a> 是 Kubernetes Chart 的管理工具，Kubernetes Chart 是一套預先組態的 Kubernetes 資源套件。使用 Helm 有以下幾個好處：</p>
<ul>
<li>查詢與使用熱門的 <a href="https://github.com/kubernetes/charts" target="_blank" rel="noopener">Kubernetes Chart</a> 軟體套件。</li>
<li>以 Kuberntes Chart 來分享自己的應用程式。</li>
<li>可利用 Chart 來重複建立應用程式。</li>
<li>智能地管理 Kubernetes manifest 檔案。</li>
<li>管理釋出的 Helm 版本。</li>
</ul>
<a id="more"></a>
<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>Helm 有三個觀念需要我們去了解，分別為 Chart、Release 與 Repository，其細節如下：</p>
<ul>
<li><strong>Chart</strong>：主要定義要被執行的應用程式中，所需要的工具、資源、服務等資訊，有點類似 Homebrew 的 Formula 或是 APT 的 dpkg 檔案。</li>
<li><strong>Release</strong>：一個被執行於 Kubernetes 的 Chart 實例。Chart 能夠在一個叢集中擁有多個 Release，例如 MySQL Chart，可以在叢集建立基於該 Chart 的兩個資料庫實例，其中每個 Release 都會有獨立的名稱。</li>
<li><strong>Repository</strong>：主要用來存放 Chart 的倉庫，如 <a href="https://kubeapps.com/" target="_blank" rel="noopener">KubeApps</a>。</li>
</ul>
<p>可以理解 Helm 主要目標就是從 Chart Repository 中，查找部署者需要的應用程式 Chart，然後以 Release 形式來部署到 Kubernetes 中進行管理。</p>
<h2 id="Helm-系統元件"><a href="#Helm-系統元件" class="headerlink" title="Helm 系統元件"></a>Helm 系統元件</h2><p>Helm 主要分為兩種元件，Helm Client 與 Tiller Server，兩者功能如下：</p>
<ul>
<li><strong>Helm Client</strong>：一個安裝 Helm CLI 的機器，該機器透過 gRPC 連接 Tiller Server 來對 Repository、Chart 與 Release 等進行管理與操作，如建立、刪除與升級等操作，細節可以查看 <a href="https://github.com/kubernetes/helm/blob/master/docs/index.md" target="_blank" rel="noopener">Helm Documentation</a>。</li>
<li><strong>Tiller Server</strong>：主要負責接收來至 Client 的指令，並透過 kube-apiserver 與 Kubernetes 叢集做溝通，根據 Chart 定義的內容，來產生與管理各種對應 API 物件的 Kubernetes 部署檔案(又稱為 <code>Release</code>)。</li>
</ul>
<p>兩者溝通架構圖如下所示：</p>
<center><img src="/images/kube/helm-peer.png" alt=""></center>

<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>安裝前需要確認環境滿足以下幾膽：</p>
<ul>
<li>已部署 Kubernetes 叢集。</li>
<li>操作端安裝 kubectl 工具。</li>
<li>操作端可以透過 kubectl 工具管理到 Kubernetes（可用的 kubectl config）。</li>
</ul>
<h2 id="安裝-Helm"><a href="#安裝-Helm" class="headerlink" title="安裝 Helm"></a>安裝 Helm</h2><p>Helm 有許多種安裝方式，這邊個人比較喜歡用 binary 檔案來進行安裝：</p>
<pre><code class="sh">$ wget -qO- https://kubernetes-helm.storage.googleapis.com/helm-v2.8.1-linux-amd64.tar.gz | tar -zx
$ sudo mv linux-amd64/helm /usr/local/bin/
$ helm version
</code></pre>
<blockquote>
<p>OS X 為下載 <code>helm-v2.4.1-darwin-amd64.tar.gz</code>。</p>
</blockquote>
<h2 id="初始化-Helm"><a href="#初始化-Helm" class="headerlink" title="初始化 Helm"></a>初始化 Helm</h2><p>在開始使用 Helm 之前，我們需要建置 Tiller Server 來對 Kubernetes 的管理，而 Helm CLI 內建也提供了快速初始化指令，如下：</p>
<pre><code class="sh">$ kubectl -n kube-system create sa tiller
$ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
$ helm init --service-account tiller
$HELM_HOME has been configured at /root/.helm.

Tiller (the helm server side component) has been installed into your Kubernetes Cluster.
Happy Helming!
</code></pre>
<blockquote>
<p>若之前只用舊版想要更新可以透過以下指令<code>helm init --upgrade</code>來達到效果。</p>
</blockquote>
<p>完成後，就可以透過 kubectl 來查看 Tiller Server 是否被建立：</p>
<pre><code class="sh">$ kubectl get po,svc -n kube-system -l app=helm
NAME                                READY     STATUS    RESTARTS   AGE
po/tiller-deploy-1651596238-5lsdw   1/1       Running   0          3m

NAME                CLUSTER-IP        EXTERNAL-IP   PORT(S)     AGE
svc/tiller-deploy   192.162.204.144   &lt;none&gt;        44134/TCP   3m
</code></pre>
<p>接著透過 helm ctl 來查看資訊：</p>
<pre><code class="sh">$ export KUBECONFIG=/etc/kubernetes/admin.conf
$ export HELM_HOST=$(kubectl describe svc/tiller-deploy -n kube-system | awk &#39;/Endpoints/{print $2}&#39;)

# wait for a few minutes
$ helm version
Client: &amp;version.Version{SemVer:&quot;v2.8.1&quot;, GitCommit:&quot;6af75a8fd72e2aa18a2b278cfe5c7a1c5feca7f2&quot;, GitTreeState:&quot;clean&quot;}
Server: &amp;version.Version{SemVer:&quot;v2.8.1&quot;, GitCommit:&quot;6af75a8fd72e2aa18a2b278cfe5c7a1c5feca7f2&quot;, GitTreeState:&quot;clean&quot;}
</code></pre>
<h2 id="部署-Chart-Release-實例"><a href="#部署-Chart-Release-實例" class="headerlink" title="部署 Chart Release 實例"></a>部署 Chart Release 實例</h2><p>當完成初始化後，就可以透過 helm ctl 來管理與部署 Chart Release，我們可以到 <a href="https://kubeapps.com/" target="_blank" rel="noopener">KubeApps</a> 查找想要部署的 Chart，如以下快速部屬 Jenkins　範例，首先先透過搜尋來查看目前應用程式版本：</p>
<pre><code class="sh">$ helm search jenkins
NAME              VERSION    DESCRIPTION
stable/jenkins    0.6.3      Open source continuous integration server. It s...
</code></pre>
<p>接著透過<code>inspect</code>指令查看該 Chart 的參數資訊：</p>
<pre><code class="sh">$ helm inspect stable/jenkins
...
Persistence:
  Enabled: true
</code></pre>
<blockquote>
<p>從中我們會發現需要建立一個 PVC 來提供持久性儲存。</p>
</blockquote>
<p>因此需要建立一個 PVC 提供給 Jenkins Chart 來儲存使用，這邊我們自己手動建立<code>jenkins-pv-pvc.yml</code>檔案：</p>
<pre><code class="yaml">apiVersion: v1
kind: PersistentVolume
metadata:
  name: jenkins-pv
  labels:
    app: jenkins
spec:
  capacity:
    storage: 10Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: /var/nfs/jenkins
    server: 172.20.3.91

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: jenkins-pvc
  labels:
    app: jenkins
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
</code></pre>
<p>接著透過 kubectl 來建立：</p>
<pre><code class="sh">$ kubectl create -f jenkins-pv-pvc.yml
persistentvolumeclaim &quot;jenkins-pvc&quot; created
persistentvolume &quot;jenkins-pv&quot; created

$ kubectl get pv,pvc
NAME            CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                 STORAGECLASS   REASON    AGE
pv/jenkins-pv   10Gi       RWO           Recycle         Bound     default/jenkins-pvc                            20s

NAME              STATUS    VOLUME       CAPACITY   ACCESSMODES   STORAGECLASS   AGE
pvc/jenkins-pvc   Bound     jenkins-pv   10Gi       RWO                          20s
</code></pre>
<p>當 PVC 建立完成後，就可以開始透過 Helm 來建立 Jenkins Release：</p>
<pre><code class="sh">$ export PVC_NAME=$(kubectl get pvc -l app=jenkins --output=template --template=&quot;{{with index .items 0}}{{.metadata.name}}{{end}}&quot;)
$ helm install --name demo --set Persistence.ExistingClaim=${PVC_NAME} stable/jenkins
NAME:   demo
LAST DEPLOYED: Thu May 25 17:53:50 2017
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
==&gt; v1beta1/Deployment
NAME          DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
demo-jenkins  1        1        1           0          1s

==&gt; v1/Secret
NAME          TYPE    DATA  AGE
demo-jenkins  Opaque  2     1s

==&gt; v1/ConfigMap
NAME                DATA  AGE
demo-jenkins-tests  1     1s
demo-jenkins        3     1s

==&gt; v1/Service
NAME          CLUSTER-IP       EXTERNAL-IP  PORT(S)                         AGE
demo-jenkins  192.169.143.140  &lt;pending&gt;    8080:30152/TCP,50000:31806/TCP  1s
...
</code></pre>
<blockquote>
<p>P.S. <code>install</code> 指令可以安裝來至<code>Chart repository</code>、<code>壓縮檔 Chart</code>、<code>一個 Chart 目錄</code>與<code>Chart URL</code>。</p>
<p>這邊 install 可以額外透過以下兩種方式來覆寫參數，在這之前可以先透過<code>helm inspect values &lt;chart&gt;</code>來取得使用的變數。</p>
<ul>
<li><strong>–values</strong>：指定一個 YAML 檔案來覆寫設定。</li>
</ul>
<pre><code class="sh">$ echo -e &#39;Master:\n  AdminPassword: r00tme&#39; &gt; config.yaml
$ helm install -f config.yaml stable/jenkins
</code></pre>
<ul>
<li><strong>–sets</strong>：指定一對 Key/value 指令來覆寫。</li>
</ul>
<pre><code class="sh">$ helm install --set Master.AdminPassword=r00tme stable/jenkins
</code></pre>
</blockquote>
<p>完成後就可以透過 helm 與 kubectl 來查看建立狀態：</p>
<pre><code class="sh">$ helm ls
NAME    REVISION    UPDATED                     STATUS      CHART            NAMESPACE
demo    1           Thu May 25 17:53:50 2017    DEPLOYED    jenkins-0.6.3    default

$ kubectl get po,svc
NAME                               READY     STATUS    RESTARTS   AGE
po/demo-jenkins-3139496662-c0lzk   1/1       Running   0          1m

NAME               CLUSTER-IP        EXTERNAL-IP   PORT(S)                          AGE
svc/demo-jenkins   192.169.143.140   &lt;pending&gt;     8080:30152/TCP,50000:31806/TCP   1m
</code></pre>
<p>由於預設只使用 LoadBalancerSourceRanges 來定義存取策略，但沒有指定任何外部 IP，因此要手動加入以下內容：</p>
<pre><code class="sh">$ kubectl edit svc demo-jenkins

spec:
  externalIPs:
  - 172.20.3.90
</code></pre>
<p>完成後再次查看 Service 資訊：</p>
<pre><code class="sh">$ kubectl get svc
NAME           CLUSTER-IP        EXTERNAL-IP    PORT(S)                          AGE
demo-jenkins   192.169.143.140   ,172.20.3.90   8080:30152/TCP,50000:31806/TCP   10m
</code></pre>
<blockquote>
<p>這時候就可以透過 <a href="http://172.20.3.90:8080" target="_blank" rel="noopener">http://172.20.3.90:8080</a> 連進去 Jenkins 了，其預設帳號為 <code>admin</code>。</p>
</blockquote>
<p>透過以下指令來取得 Jenkins admin 密碼：</p>
<pre><code class="sh">$ printf $(kubectl get secret --namespace default demo-jenkins -o jsonpath=&quot;{.data.jenkins-admin-password}&quot; | base64 --decode);echo
buQ1ik2Q7x
</code></pre>
<blockquote>
<p>該 Chart 會產生亂數密碼存放到 secret 中。</p>
</blockquote>
<p><img src="/images/kube/helm-jenkins.png" alt=""></p>
<p>最後我們也可以透過<code>upgrade</code>指令來更新已經 Release 的 Chart：</p>
<pre><code class="sh">$ helm upgrade --set Master.AdminPassword=r00tme --set Persistence.ExistingClaim=jenkins-pvc demo stable/jenkins
Release &quot;demo&quot; has been upgraded. Happy Helming!

$ helm get values demo
Master:
  AdminPassword: r00tme
Persistence:
  ExistingClaim: jenkins-pvc

$ helm ls
NAME    REVISION        UPDATED                         STATUS          CHART           NAMESPACE
demo    2               Tue May 30 21:18:43 2017        DEPLOYED        jenkins-0.6.3   default
</code></pre>
<blockquote>
<p>這邊會看到<code>REVISION</code>會 +1，這可以用來做 rollback 的版本號使用。</p>
</blockquote>
<h2 id="刪除-Release"><a href="#刪除-Release" class="headerlink" title="刪除 Release"></a>刪除 Release</h2><p>Helm 除了基本的建立功能外，其還包含了整個 Release 的生命週期管理功能，如我們不需要該 Release 時，就可以透過以下方式刪除：</p>
<pre><code class="sh">$ helm del demo
$ helm status demo | grep STATUS
STATUS: DELETED
</code></pre>
<p>當刪除後，該 Release 並沒有真的被刪除，我們可以透過 helm ls 來查看被刪除的 Release：</p>
<pre><code class="sh">$ helm ls --all
NAME    REVISION        UPDATED                         STATUS  CHART           NAMESPACE
demo    2               Tue May 30 21:18:43 2017        DELETED jenkins-0.6.3   default
</code></pre>
<blockquote>
<p>當執行 <code>helm ls</code> 指令為加入 <code>--all</code> 時，表示只列出<code>DEPLOYED</code>狀態的 Release。</p>
</blockquote>
<p>而當 Release 處於 <code>DELETED</code> 狀態時，我們可以進行一些操作，如 Roll back 或完全刪除 Release：</p>
<pre><code class="sh">$ helm rollback demo 1
Rollback was a success! Happy Helming!

$ printf $(kubectl get secret --namespace default demo-jenkins -o jsonpath=&quot;{.data.jenkins-admin-password}&quot; | base64 --decode);echo
BIsLlQTN9l

$ helm del demo --purge
release &quot;demo&quot; deleted

# 這時執行以下指令就不會再看到已刪除的 Release.
$ helm ls --all
</code></pre>
<h2 id="建立簡單-Chart-結構"><a href="#建立簡單-Chart-結構" class="headerlink" title="建立簡單 Chart 結構"></a>建立簡單 Chart 結構</h2><p>Helm 提供了 create 指令來建立一個 Chart 基本結構：</p>
<pre><code class="sh">$ helm create example
$ tree example/
example/
├── charts
├── Chart.yaml
├── templates
│   ├── deployment.yaml
│   ├── _helpers.tpl
│   ├── ingress.yaml
│   ├── NOTES.txt
│   └── service.yaml
└── values.yaml
</code></pre>
<p>當我們設定完 Chart 後，就可以透過 helm 指令來打包：</p>
<pre><code class="sh">$ helm package example/
example-0.1.0.tgz
</code></pre>
<p>最後可以用 helm 來安裝：</p>
<pre><code class="sh">$ helm install ./example-0.1.0.tgz
</code></pre>
<h2 id="自己建立-Repository"><a href="#自己建立-Repository" class="headerlink" title="自己建立 Repository"></a>自己建立 Repository</h2><p>Helm 指令除了可以建立 Chart 基本結構外，很幸運的也提供了建立 Helm Repository 的功能，建立方式如下：</p>
<pre><code class="sh">$ helm serve --repo-path example-0.1.0.tgz
$ helm repo add example http://repo-url
</code></pre>
<blockquote>
<p>另外 helm repo 也可以加入來至於 Github 與 HTTP 伺服器的網址來提供服務。</p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Kubernetes </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Docker </tag>
            
            <tag> Kubernetes </tag>
            
            <tag> Helm </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Kubespray 部署實體機 Kubernetes v1.6 叢集]]></title>
      <url>https://kairen.github.io/2017/03/17/kubernetes/deploy/kubespray-baremetal/</url>
      <content type="html"><![CDATA[<p><a href="https://github.com/kubernetes-incubator/kubespray" target="_blank" rel="noopener">Kubespray</a> 是 Kubernetes incubator 中的專案，目標是提供 Production Ready Kubernetes 部署方案，該專案基礎是透過 Ansible Playbook 來定義系統與 Kubernetes 叢集部署的任務，目前 Kubespray 有以下幾個特點：</p>
<ul>
<li>可以部署在 AWS, GCE, Azure, OpenStack 或者 Baremetal.</li>
<li>部署 High Available Kubernetes 叢集.</li>
<li>可組合性(Composable)，可自行選擇 Network Plugin (flannel, calico, canal, weave) 來部署.</li>
<li>支援多種 Linux distributions(CoreOS, Debian Jessie, Ubuntu 16.04, CentOS/RHEL7).</li>
</ul>
<p>本篇將說明如何透過 Kubespray 部署 Kubernetes 至實體機器節點，安裝版本如下所示：</p>
<ul>
<li>Kubernetes v1.6.4</li>
<li>Etcd v3.1.6</li>
<li>Flannel v0.7.1</li>
<li>Docker v17.04.0-ce</li>
</ul>
<a id="more"></a>
<h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本次安裝測試環境的作業系統採用<code>Ubuntu 16.04 Server</code>，其他細節內容如下：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Role</th>
<th>CPU</th>
<th>Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>192.168.121.179</td>
<td>master1 + deploy</td>
<td>2</td>
<td>4G</td>
</tr>
<tr>
<td>192.168.121.106</td>
<td>node1</td>
<td>2</td>
<td>4G</td>
</tr>
<tr>
<td>192.168.121.197</td>
<td>node2</td>
<td>2</td>
<td>4G</td>
</tr>
<tr>
<td>192.168.121.123</td>
<td>node3</td>
<td>2</td>
<td>4G</td>
</tr>
</tbody>
</table>
<blockquote>
<p>這邊 master 為主要控制節點，node 為應用程式工作節點。</p>
</blockquote>
<h2 id="預先準備資訊"><a href="#預先準備資訊" class="headerlink" title="預先準備資訊"></a>預先準備資訊</h2><ul>
<li>所有節點的網路之間可以互相溝通。</li>
<li><code>部署節點(這邊為 master1)</code>對其他節點不需要 SSH 密碼即可登入。</li>
<li>所有節點都擁有 Sudoer 權限，並且不需要輸入密碼。</li>
<li>所有節點需要安裝 <code>Python</code>。</li>
<li><p>所有節點需要設定<code>/etc/host</code>解析到所有主機。</p>
</li>
<li><p>修改所有節點<code>/etc/resolv.conf</code>：</p>
</li>
</ul>
<pre><code class="sh">$ echo &quot;nameserver 8.8.8.8&quot; | sudo tee /etc/resolv.conf
</code></pre>
<ul>
<li><code>部署節點(這邊為 master1)</code>需要安裝 Ansible &gt; 2.3.0。</li>
</ul>
<p>Ubuntu 16.04 安裝最新版 Ansible:</p>
<pre><code class="sh">$ sudo sed -i &#39;s/us.archive.ubuntu.com/tw.archive.ubuntu.com/g&#39; /etc/apt/sources.list
$ sudo apt-get install -y software-properties-common
$ sudo apt-add-repository -y ppa:ansible/ansible
$ sudo apt-get update &amp;&amp; sudo apt-get install -y ansible git cowsay python-pip python-netaddr libssl-dev
</code></pre>
<h2 id="安裝-Kubespray-與準備部署資訊"><a href="#安裝-Kubespray-與準備部署資訊" class="headerlink" title="安裝 Kubespray 與準備部署資訊"></a>安裝 Kubespray 與準備部署資訊</h2><p>首先透過 pypi 安裝 kubespray-cli，雖然官方說已經改成 Go 語言版本的工具，但是根本沒在更新，所以目前暫時用 pypi 版本：</p>
<pre><code class="sh">$ sudo pip install -U kubespray
</code></pre>
<p>安裝完成後，新增設定檔<code>~/.kubespray.yml</code>，並加入以下內容：</p>
<pre><code class="sh">$ mkdir /etc/kubespray
$ cat &lt;&lt;EOF &gt; ~/.kubespray.yml
kubespray_git_repo: &quot;https://github.com/kubernetes-incubator/kubespray.git&quot;
# Logging options
loglevel: &quot;info&quot;
EOF
</code></pre>
<p>接著用 kubespray cli 來產生 inventory 檔案：</p>
<pre><code class="sh">$ kubespray prepare --masters master1 --etcds master1 --nodes node1 node2 node3
$ cat ~/.kubespray/inventory/inventory.cfg
</code></pre>
<blockquote>
<p>也可以自己建立<code>inventory</code>來描述部署節點。</p>
</blockquote>
<p>完成後就可以透過以下指令進行部署 Kubernetes 叢集：</p>
<pre><code class="sh">$ time kubespray deploy --verbose -u root -k .ssh/id_rsa -n flannel
Run kubernetes cluster deployment with the above command ? [Y/n]y
...
master1                    : ok=368  changed=89   unreachable=0    failed=0
node1                      : ok=305  changed=73   unreachable=0    failed=0
node2                      : ok=276  changed=62   unreachable=0    failed=0
node3                      : ok=276  changed=62   unreachable=0    failed=0

Kubernetes deployed successfuly
</code></pre>
<blockquote>
<p>其中<code>-n</code>為部署的網路插件類型，目前支援 calico、flannel、weave 與 canal。</p>
</blockquote>
<h2 id="驗證叢集"><a href="#驗證叢集" class="headerlink" title="驗證叢集"></a>驗證叢集</h2><p>當 Ansible 執行完成後，若沒發生錯誤就可以開始進行操作 Kubernetes，如取得版本資訊：</p>
<pre><code class="sh">$ kubectl version
Client Version: version.Info{Major:&quot;1&quot;, Minor:&quot;6&quot;, GitVersion:&quot;v1.6.4+coreos.0&quot;, GitCommit:&quot;9212f77ed8c169a0afa02e58dce87913c6387b3e&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2017-04-04T00:32:53Z&quot;, GoVersion:&quot;go1.7.5&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}
Server Version: version.Info{Major:&quot;1&quot;, Minor:&quot;6&quot;, GitVersion:&quot;v1.6.4+coreos.0&quot;, GitCommit:&quot;9212f77ed8c169a0afa02e58dce87913c6387b3e&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2017-04-04T00:32:53Z&quot;, GoVersion:&quot;go1.7.5&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}
</code></pre>
<p>取得目前節點狀態：</p>
<pre><code class="sh">$ kubectl get node
NAME      STATUS                     AGE       VERSION
master1   Ready,SchedulingDisabled   11m       v1.6.4+coreos.0
node1     Ready                      11m       v1.6.4+coreos.0
node2     Ready                      11m       v1.6.4+coreos.0
node3     Ready                      11m       v1.6.4+coreos.0
</code></pre>
<p>查看目前系統 Pod 狀態：</p>
<pre><code class="sh">$ kubectl get po -n kube-system
NAME                                  READY     STATUS    RESTARTS   AGE
dnsmasq-975202658-6jj3n               1/1       Running   0          14m
dnsmasq-975202658-h4rn9               1/1       Running   0          14m
dnsmasq-autoscaler-2349860636-kfpx0   1/1       Running   0          14m
flannel-master1                       1/1       Running   1          14m
flannel-node1                         1/1       Running   1          14m
flannel-node2                         1/1       Running   1          14m
flannel-node3                         1/1       Running   1          14m
kube-apiserver-master1                1/1       Running   0          15m
kube-controller-manager-master1       1/1       Running   0          15m
kube-proxy-master1                    1/1       Running   1          14m
kube-proxy-node1                      1/1       Running   1          14m
kube-proxy-node2                      1/1       Running   1          14m
kube-proxy-node3                      1/1       Running   1          14m
kube-scheduler-master1                1/1       Running   0          15m
kubedns-1519522227-thmrh              3/3       Running   0          14m
kubedns-autoscaler-2999057513-tx14j   1/1       Running   0          14m
nginx-proxy-node1                     1/1       Running   1          14m
nginx-proxy-node2                     1/1       Running   1          14m
nginx-proxy-node3                     1/1       Running   1          14m
</code></pre>
]]></content>
      
        <categories>
            
            <category> Kubernetes </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Docker </tag>
            
            <tag> Kubernetes </tag>
            
            <tag> Ansible </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Ubuntu 16.04 安裝 TensorFlow GPU GTX 1060]]></title>
      <url>https://kairen.github.io/2017/03/12/tensorflow/install-source/</url>
      <content type="html"><![CDATA[<p>本篇主要因為自己買了一片<code>Nvidia GTX 1060 6G</code>顯卡，但是購買至今只用來玩過一個遊戲，因此才拿來試跑 TensorFlow。</p>
<a id="more"></a>
<p>本次安裝硬體與規格如下：</p>
<ul>
<li>作業系統: Ubuntu 16.04 Desktop</li>
<li>GPU: GeForce® GTX 1060 6G</li>
<li>NVIDIA Driver: nvidia-367</li>
<li>Python: 2.7+</li>
<li>TensorFlow: r1.0.1</li>
<li>CUDA: v8.0</li>
<li>cuDNN: v5.1</li>
</ul>
<h1 id="環境部署"><a href="#環境部署" class="headerlink" title="環境部署"></a>環境部署</h1><p>如果要安裝 TensorFlow with GPU support 的話，需要滿足以下幾點：</p>
<ul>
<li>Nvidia Driver.</li>
<li>已安裝 CUDA® Toolkit 8.0.</li>
<li>已安裝 cuDNN v5.1.</li>
<li>GPU card with CUDA Compute Capability 6.1(GTX 10-series).</li>
<li>libcupti-dev 函式庫.</li>
</ul>
<h2 id="Nvidia-Driver-安裝"><a href="#Nvidia-Driver-安裝" class="headerlink" title="Nvidia Driver 安裝"></a>Nvidia Driver 安裝</h2><p>由於預設 Ubuntu 的 Nvidia 版本比較舊，或者並沒有安裝相關驅動，因此這邊需要安裝顯卡對應的版本才能夠正常使用，可以透過以下方式進行：</p>
<pre><code class="sh">$ sudo add-apt-repository -y ppa:graphics-drivers/ppa
$ sudo apt-get update
$ sudo apt-get install -y nvidia-367
</code></pre>
<p>完成後，需重新啟動機器。</p>
<h2 id="CUDA-Toolkit-8-0-安裝"><a href="#CUDA-Toolkit-8-0-安裝" class="headerlink" title="CUDA Toolkit 8.0 安裝"></a>CUDA Toolkit 8.0 安裝</h2><p>由於 TensorFlow 支援 GPU 運算時，會需要使用到 CUDA Toolkit 相關功能，可以到 <a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">CUDA Toolkit</a> 頁面下載，這邊會下載 Ubuntu Run file 檔案，來進行安裝：</p>
<pre><code class="sh">$ wget &quot;https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda_8.0.61_375.26_linux-run&quot;
$ sudo chmod u+x cuda_8.0.61_375.26_linux-run
$ ./cuda_8.0.61_375.26_linux-run

Do you accept the previously read EULA?
accept/decline/quit: accept
Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 361.77?
(y)es/(n)o/(q)uit: n
Install the CUDA 8.0 Toolkit?
(y)es/(n)o/(q)uit: y
Enter Toolkit Location
[ default is /usr/local/cuda-8.0]: enter
Do you want to install a symbolic link at /usr/local/cuda?
(y)es/(n)o/(q)uit:y
Install the CUDA 8.0 Samples?
(y)es/(n)o/(q)uit:y
Enter CUDA Samples Location
[ defualt is /home/kylebai ]: enter
</code></pre>
<blockquote>
<p>這邊<code>enter</code>為鍵盤直接按壓，而不是輸入 enter。</p>
</blockquote>
<p>安裝完成後，編輯 Home 目錄底下的<code>.bashrc</code>檔案加入以下內容：</p>
<pre><code class="sh">export PATH=${PATH}:/usr/local/cuda-8.0/bin
export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64
</code></pre>
<p>最後 Source Bash 檔案與測試 CUDA Toolkit：</p>
<pre><code class="sh">$ source .bashrc
$ sudo nvidia-smi
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 106...  Off  | 0000:01:00.0      On |                  N/A |
| 28%   29C    P8     6W / 120W |    130MiB /  6069MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1165    G   /usr/lib/xorg/Xorg                              98MiB |
|    0      1764    G   compiz                                          29MiB |
+-----------------------------------------------------------------------------+
</code></pre>
<h2 id="cuDNN-5-1-安裝"><a href="#cuDNN-5-1-安裝" class="headerlink" title="cuDNN 5.1 安裝"></a>cuDNN 5.1 安裝</h2><p><a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="noopener">NVIDIA cuDNN</a> 是一個深度神經網路運算的 GPU 加速原函式庫，這邊需要點選前面的連結，下載<code>cuDNN v5.1 Library for Linux</code>檔案：</p>
<pre><code class="sh">$ tar xvf cudnn-8.0-linux-x64-v5.1.tgz
$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include/
$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/
</code></pre>
<h2 id="TensorFlow-GPU-套件建構"><a href="#TensorFlow-GPU-套件建構" class="headerlink" title="TensorFlow GPU 套件建構"></a>TensorFlow GPU 套件建構</h2><p>本次教學將透過 <a href="https://github.com/tensorflow/tensorflow" target="_blank" rel="noopener">Source code</a> 建構安裝檔，再進行安裝 TensorFlow，首先安裝相依套件：</p>
<pre><code class="sh">$ sudo add-apt-repository -y ppa:webupd8team/java
$ sudo apt-get update
$ sudo apt-get install -y libcupti-dev python-numpy python-dev python-setuptools python-pip python-wheel git oracle-java8-installer
$ echo &quot;deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8&quot; | sudo tee /etc/apt/sources.list.d/bazel.list
$ curl -s &quot;https://storage.googleapis.com/bazel-apt/doc/apt-key.pub.gpg&quot; | sudo apt-key add -
$ sudo apt-get update &amp;&amp; sudo apt-get -y install bazel
$ sudo apt-get upgrade -y bazel
</code></pre>
<p>接著取得 TensorFlow 專案原始碼，然後進入到 TensorFlow 專案目錄進行 bazel 設定：</p>
<pre><code class="sh">$ git clone &quot;https://github.com/tensorflow/tensorflow&quot;
$ cd tensorflow
$ ./configure

...
Do you wish to build TensorFlow with CUDA support? [y/N] y
Please specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0
Please specify the cuDNN version you want to use. [Leave empty to use system default]: 5
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: &quot;3.5,5.2&quot;]: 6.1
...
Configuration finished
</code></pre>
<blockquote>
<p><code>6.1</code>為 GTX 10-series 系列顯卡，其他可以查看 <a href="https://developer.nvidia.com/cuda-gpus" target="_blank" rel="noopener">CUDA GPUS</a>。這邊除了上述特定要輸入外，其餘都是直接鍵盤<code>enter</code>。</p>
</blockquote>
<p>當完成組態後，即可透過 bazel 進行建構 pip 套件腳本：</p>
<pre><code class="sh">$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
</code></pre>
<p>當腳本建構完成後，即可透過以下指令來建構 .whl 檔案：</p>
<pre><code class="sh">$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tf_pkg
</code></pre>
<p>完成後，可以在<code>/tmp/tf_pkg</code>目錄底下找到安裝檔<code>tensorflow-1.0.1-py2-none-any.whl</code>，最後就可以透過 pip 來進行安裝了：</p>
<pre><code class="sh">$ sudo pip install /tmp/tf_pkg/tensorflow-1.0.1-cp27-cp27mu-linux_x86_64.whl
</code></pre>
<h2 id="測試安裝結果"><a href="#測試安裝結果" class="headerlink" title="測試安裝結果"></a>測試安裝結果</h2><p>最後透過簡單程式來驗證安裝是否成功：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; simple.py
import tensorflow as tf

hello = tf.constant(&#39;Hello, TensorFlow!&#39;)
sess = tf.Session()
print(sess.run(hello))
EOF

$ python simple.py
...
roperties:
name: GeForce GTX 1060 6GB
major: 6 minor: 1 memoryClockRate (GHz) 1.7845
pciBusID 0000:01:00.0
Total memory: 5.93GiB
Free memory: 5.74GiB
2017-03-12 21:43:56.477084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0
2017-03-12 21:43:56.477092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y
2017-03-12 21:43:56.503464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0)
Hello, TensorFlow!
</code></pre>
<blockquote>
<p><code>...</code>部分會顯示一些 GPU 使用狀態。</p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> TensorFlow </category>
            
        </categories>
        
        
        <tags>
            
            <tag> TensorFlow </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> Ubuntu </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[kube-ansible 快速部署實體機 HA 叢集]]></title>
      <url>https://kairen.github.io/2017/02/19/kubernetes/deploy/kube-ansible-metal/</url>
      <content type="html"><![CDATA[<p>本篇說明如何透過 <a href="https://github.com/kairen/kube-ansible" target="_blank" rel="noopener">kube-ansible</a> 部署多節點實體機 Kubernetes 叢集。</p>
<p>本安裝各軟體版本如下：</p>
<ul>
<li>Kubernetes v1.8.3</li>
<li>Etcd v3.2.9</li>
<li>Docker v1.13.0+</li>
</ul>
<a id="more"></a>
<p>而在 OS 部分以支援<code>Ubuntu 16.x</code>及<code>CentOS 7.x</code>的虛擬機與實體機部署。</p>
<h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本次安裝作業系統採用<code>Ubuntu 16.04 Server</code>，測試環境為實體主機：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Role</th>
<th>CPU</th>
<th>Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.20.3.90</td>
<td>VIP</td>
<td></td>
<td></td>
</tr>
<tr>
<td>172.20.3.91</td>
<td>master1</td>
<td>2</td>
<td>4G</td>
</tr>
<tr>
<td>172.20.3.92</td>
<td>master2</td>
<td>2</td>
<td>4G</td>
</tr>
<tr>
<td>172.20.3.93</td>
<td>master3</td>
<td>2</td>
<td>4G</td>
</tr>
<tr>
<td>172.20.3.94</td>
<td>node1</td>
<td>4</td>
<td>8G</td>
</tr>
<tr>
<td>172.20.3.95</td>
<td>node2</td>
<td>4</td>
<td>8G</td>
</tr>
<tr>
<td>172.20.3.96</td>
<td>node3</td>
<td>4</td>
<td>8G</td>
</tr>
<tr>
<td>172.20.3.97</td>
<td>node4</td>
<td>4</td>
<td>8G</td>
</tr>
<tr>
<td>172.20.3.98</td>
<td>node5</td>
<td>4</td>
<td>8G</td>
</tr>
</tbody>
</table>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>安裝前需要確認以下幾個項目：</p>
<ul>
<li>所有節點的網路之間可以互相溝通。</li>
<li><code>部署節點(這邊為 master1)</code>對其他節點不需要 SSH 密碼即可登入。</li>
<li>所有節點都擁有 Sudoer 權限，並且不需要輸入密碼。</li>
<li>所有節點需要安裝 <code>Python</code>。</li>
<li>所有節點需要設定<code>/etc/host</code>解析到所有主機。</li>
<li><code>部署節點(這邊為 master1)</code>需要安裝 Ansible。</li>
</ul>
<p>Ubuntu 16.04 安裝 Ansible:</p>
<pre><code class="sh">$ sudo apt-get install -y software-properties-common git cowsay
$ sudo apt-add-repository -y ppa:ansible/ansible
$ sudo apt-get update &amp;&amp; sudo apt-get install -y ansible
</code></pre>
<p>CentOS 7 安裝 Ansible：</p>
<pre><code class="sh">$ sudo yum install -y epel-release
$ sudo yum -y install ansible cowsay
</code></pre>
<h2 id="部署-Kubernetes-叢集"><a href="#部署-Kubernetes-叢集" class="headerlink" title="部署 Kubernetes 叢集"></a>部署 Kubernetes 叢集</h2><p>首先透過 Git 取得 HA Kubernetes Ansible 的專案：</p>
<pre><code class="sh">$ git clone &quot;https://github.com/kairen/kube-ansible.git&quot;
$ cd kube-ansible
</code></pre>
<p>然後編輯<code>inventory</code>檔案，來加入要部署的節點角色：</p>
<pre><code>[etcds]
172.20.3.[91:93]

[masters]
172.20.3.[91:93]

[nodes]
172.20.3.[94:98]

[kube-cluster:children]
masters
nodes

[kube-addon:children]
masters
</code></pre><p>完成後接著編輯<code>group_vars/all.yml</code>，來根據需求設定參數，範例如下：</p>
<pre><code class="yml"># Kubenrtes version, only support 1.8.0+.
kube_version: 1.8.3

# CRI plugin,
# Supported runtime: docker, containerd.
cri_plugin: docker

# CNI plugin,
# Supported network: flannel, calico, canal, weave or router.
network: calico
pod_network_cidr: 10.244.0.0/16

# Kubernetes cluster network
cluster_subnet: 10.96.0
kubernetes_service_ip: &quot;{{ cluster_subnet }}.1&quot;
service_ip_range: &quot;{{ cluster_subnet }}.0/12&quot;
service_node_port_range: 30000-32767

# apiserver lb 與 vip
lb_vip_address: 172.20.3.90
lb_secure_port: 6443
lb_api_url: &quot;https://{{ lb_vip_address }}:{{ lb_secure_port }}&quot;

# 若有內部 registry 則需要設定
insecure_registrys:
# - &quot;gcr.io&quot;

# Core addons (Strongly recommend)
kube_dns: true
dns_name: cluster.local # cluster dns name
dns_ip: &quot;{{ cluster_subnet }}.10&quot;

kube_proxy: true
kube_proxy_mode: iptables # &quot;ipvs(1.8+)&quot;, &quot;iptables&quot; or &quot;userspace&quot;.

# Extra addons
kube_dashboard: true # Kubenetes dasobhard console.
kube_logging: false # EFK stack for Kubernetes
kube_monitoring: true # Grafana + Infuxdb + Heapster monitoring

# Ingress controller
ingress: true
ingress_type: nginx # &#39;nginx&#39;, &#39;haproxy&#39;, &#39;traefik&#39;
</code></pre>
<p>確認<code>group_vars/all.yml</code>完成後，透過 ansible ping 來檢查叢集狀態：</p>
<pre><code class="sh">$ ansible all -m ping
172.20.3.91 | SUCCESS =&gt; {
    &quot;changed&quot;: false,
    &quot;ping&quot;: &quot;pong&quot;
}
...
</code></pre>
<p>接著就可以透過以下指令進行部署叢集：</p>
<pre><code class="sh">$ ansible-playbook cluster.yml
</code></pre>
<p>執行後需要等一點時間，當完成後就可以進入任何一台 Master 進行操作：</p>
<pre><code class="sh">$ kubectl get node
NAME      STATUS            AGE
master1   Ready,master      3m
master2   Ready,master      3m
master3   Ready,master      3m
node1     Ready             1m
node2     Ready             1m
node3     Ready             1m
node4     Ready             1m
node5     Ready             1m
</code></pre>
<p>接著就可以部署 Addons 了，透過以下方式進行：</p>
<pre><code class="sh">$ ansible-playbook addons.yml
</code></pre>
<h2 id="驗證叢集"><a href="#驗證叢集" class="headerlink" title="驗證叢集"></a>驗證叢集</h2><p>當完成上述步驟後，就可以在任一台<code>master</code>節點進行操作 Kubernetes：</p>
<pre><code class="sh">$ kubectl get po -n kube-system
NAME                                    READY     STATUS    RESTARTS   AGE
...
kubernetes-dashboard-1765530275-rxbkw   1/1       Running   0          1m
</code></pre>
<blockquote>
<p>確認都是<code>Running</code>後，就可以進入 <a href="https://172.20.3.90:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/" target="_blank" rel="noopener">Dashboard</a>。</p>
</blockquote>
<p>接著透過 Etcd 來查看目前 Leader 狀態：</p>
<pre><code class="sh">$ export CA=&quot;/etc/etcd/ssl&quot;
$ ETCDCTL_API=3 etcdctl \
    --cacert=${CA}/etcd-ca.pem \
    --cert=${CA}/etcd.pem \
    --key=${CA}/etcd-key.pem \
    --endpoints=&quot;https://172.20.3.91:2379&quot; \
    etcdctl member list

2de3b0eee054a36f: name=master1 peerURLs=http://172.20.3.91:2380 clientURLs=http://172.20.3.91:2379 isLeader=false
75809e2ee8d8d4b4: name=master2 peerURLs=http://172.20.3.92:2380 clientURLs=http://172.20.3.92:2379 isLeader=false
af31edd02fc70872: name=master3 peerURLs=http://172.20.3.93:2380 clientURLs=http://172.20.3.93:2379 isLeader=true
</code></pre>
<h2 id="重置叢集"><a href="#重置叢集" class="headerlink" title="重置叢集"></a>重置叢集</h2><p>若想要將整個叢集進行重置的話，可以使用以下方式：</p>
<pre><code class="sh">$ ansible-playbook reset.yml
</code></pre>
]]></content>
      
        <categories>
            
            <category> Kubernetes </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Docker </tag>
            
            <tag> Kubernetes </tag>
            
            <tag> Ansible </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[kube-ansible 快速部署 HA 測試環境]]></title>
      <url>https://kairen.github.io/2017/02/17/kubernetes/deploy/kube-ansible-test/</url>
      <content type="html"><![CDATA[<p><a href="https://github.com/kairen/kube-ansible" target="_blank" rel="noopener">kube-ansible</a> 提供自動化部署 Kubernetes High Availability 叢集於虛擬機與實體機上，並且支援部署 Ceph 叢集於 Kubernetes 中提供共享式儲存系統給 Pod 應用程式使用。該專案最主要是想要快速建立測試環境來進行 Kubernetes 練習與驗證。</p>
<p>kube-ansible 提供了以下幾項功能：</p>
<ul>
<li>Kubernetes 1.8.0+.</li>
<li>Ceph on Kubernetes cluster.</li>
<li>Common addons.</li>
</ul>
<a id="more"></a>
<p>而在 OS 部分以支援<code>Ubuntu 16.x</code>及<code>CentOS 7.x</code>的虛擬機與實體機部署。未來會以 Python 工具形式來提供使用。</p>
<h2 id="快速開始"><a href="#快速開始" class="headerlink" title="快速開始"></a>快速開始</h2><p>kube-ansible 支援了 Vagrant 腳本來快速提供 VirtualBox 環境，若想單一主機模擬 Kubernetes 叢集的話，主機需要安裝以下軟體工具：</p>
<ul>
<li><a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="noopener">Vagrant</a> &gt;= 1.7.0</li>
<li><a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="noopener">VirtualBox</a> &gt;= 5.0.0</li>
</ul>
<p>當主機確認安裝完成後，即可透過 Git 下載最新版本程式，並使用<code>setup-vagrant</code>腳本：</p>
<pre><code class="sh">$ git clone &quot;https://github.com/kairen/kube-ansible.git&quot;
$ cd kube-ansible
$ ./tools/setup -h
Usage : setup-vagrant [options]

 -b|--boss         Number of master.
 -w|--worker       Number of worker.
 -c|--cpu          Number of cores per vm.
 -m|--memory       Memory size per vm.
 -p|--provider     Virtual machine provider(virtualbox, libvirt).
 -o|--os-image     Virtual machine operation system(ubuntu16, centos7).
 -i|--interface    Network bind interface.
 -n|--network      Container Network plugin.
 -f|--force        Force deployment.
 --combine-master  Combine number of worker into masters.
 --combine-etcd    Combine number of worker into etcds.
</code></pre>
<p>這邊執行以下指令來建立三台 Master 與三台 Node 的環境：</p>
<pre><code class="sh">$ ./tools/setup -m 2048 -n calico -i eth1
Cluster Size: 1 master, 2 worker.
     VM Size: 1 vCPU, 2048 MB
     VM Info: ubuntu16, virtualbox
         CNI: calico, Binding iface: eth1
Start deploying?(y): y
</code></pre>
<p>執行後需要等一點時間，當完成後就可以進入任何一台 Master 進行操作：</p>
<pre><code class="sh">$ kubectl -n kube-system get po
NAME                                        READY     STATUS    RESTARTS   AGE
calico-node-657hv                           2/2       Running   0          57s
calico-node-gmd8b                           2/2       Running   0          57s
calico-node-w7nj8                           2/2       Running   0          57s
calico-policy-controller-55dfcd9c69-t8s8z   1/1       Running   0          57s
haproxy-master1                             1/1       Running   0          22s
haproxy-node2                               1/1       Running   0          1m
keepalived-master1                          1/1       Running   0          30s
keepalived-node2                            1/1       Running   0          1m
kube-apiserver-master1                      1/1       Running   0          23s
kube-apiserver-node2                        1/1       Running   0          1m
kube-controller-manager-master1             1/1       Running   0          17s
kube-controller-manager-node2               1/1       Running   0          1m
kube-dns-6cb549f55f-8mgsd                   3/3       Running   0          46s
kube-proxy-l54d7                            1/1       Running   0          1m
kube-proxy-rm4nn                            1/1       Running   0          1m
kube-proxy-tvfs7                            1/1       Running   0          1m
kube-scheduler-master1                      1/1       Running   0          39s
kube-scheduler-node2                        1/1       Running   0          1m
</code></pre>
<p>這樣一個 HA 叢集就部署完成了，可以試著將一台 Master 關閉來驗證可靠性，若 Master 是三台的話，即表示可容忍最多一台故障。</p>
<h2 id="簡單部署-Nginx-服務"><a href="#簡單部署-Nginx-服務" class="headerlink" title="簡單部署 Nginx 服務"></a>簡單部署 Nginx 服務</h2><p>當完成部署後，可以透過簡單的應用程式部署來驗證系統是否正常運作：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; deploy.yml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
EOF

$ kubectl create -f deploy.yml
$ kubectl get po
NAME                     READY     STATUS    RESTARTS   AGE
nginx-4087004473-g6635   1/1       Running   0          15s
</code></pre>
<p>然後透過建置 Service 來提供外部存取 Nginx HTTP 伺服器服務：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; service.yml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 80
    protocol: TCP
    nodePort: 30000
  selector:
    app: nginx
EOF

$ kubectl create -f service.yml
$ kubectl get svc
NAME            CLUSTER-IP        EXTERNAL-IP   PORT(S)        AGE
kubernetes      192.160.0.1       &lt;none&gt;        443/TCP        15m
nginx-service   192.173.165.220   &lt;nodes&gt;       80:30000/TCP   11s
</code></pre>
<p>由於範例使用 NodePort 的類型，所以任何一台節點都可以透過 TCP 30000 Port 來存取服務，包含 VIP 172.16.35.9 也可以存取。</p>
<p>最後，我們可以關閉 master1 來測試是否有 HA 效果。</p>
]]></content>
      
        <categories>
            
            <category> Kubernetes </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Docker </tag>
            
            <tag> Kubernetes </tag>
            
            <tag> Ansible </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Kubernetes v1.6.x 全手動苦工安裝教學]]></title>
      <url>https://kairen.github.io/2016/12/16/kubernetes/deploy/manual-v1.6/</url>
      <content type="html"><![CDATA[<p>Kubernetes 提供了許多雲端平台與作業系統的安裝方式，本章將以<code>全手動安裝方式</code>來部署，主要是學習與了解 Kubernetes 建置流程。若想要瞭解更多平台的部署可以參考 <a href="https://kubernetes.io/docs/getting-started-guides/" target="_blank" rel="noopener">Picking the Right Solution</a>來選擇自己最喜歡的方式。</p>
<p>本次安裝版本為：</p>
<ul>
<li>Kubernetes v1.6.4</li>
<li>Etcd v3.1.6</li>
<li>Flannel v0.7.1</li>
<li>Docker v17.05.0-ce</li>
</ul>
<a id="more"></a>
<h2 id="預先準備資訊"><a href="#預先準備資訊" class="headerlink" title="預先準備資訊"></a>預先準備資訊</h2><p>本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統可採用<code>Ubuntu 16.x</code>與<code>CentOS 7.x</code>：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Role</th>
<th>CPU</th>
<th>Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.16.35.12</td>
<td>master</td>
<td>1</td>
<td>2G</td>
</tr>
<tr>
<td>172.16.35.10</td>
<td>node1</td>
<td>1</td>
<td>2G</td>
</tr>
<tr>
<td>172.16.35.11</td>
<td>node2</td>
<td>1</td>
<td>2G</td>
</tr>
</tbody>
</table>
<blockquote>
<p>這邊 master 為主要控制節點，node 為應用程式工作節點。</p>
</blockquote>
<p>首先安裝前要確認以下幾項都已將準備完成：</p>
<ul>
<li>所有節點彼此網路互通，並且不需要 SSH 密碼即可登入。</li>
<li>所有防火牆與 SELinux 已關閉。如 CentOS：</li>
</ul>
<pre><code class="sh">$ systemctl stop firewalld &amp;&amp; systemctl disable firewalld
$ setenforce 0
</code></pre>
<ul>
<li>所有節點需要設定<code>/etc/host</code>解析到所有主機。</li>
<li>所有節點需要安裝<code>Docker</code>或<code>rtk</code>引擎。這邊採用<code>Docker</code>來當作容器引擎，安裝方式如下：</li>
</ul>
<pre><code class="sh">$ curl -fsSL &quot;https://get.docker.com/&quot; | sh
</code></pre>
<blockquote>
<p>不管是在 <code>Ubuntu</code> 或 <code>CentOS</code> 都只需要執行該指令就會自動安裝最新版 Docker。<br>CentOS 安裝完成後，需要再執行以下指令：</p>
<pre><code class="sh">$ systemctl enable docker &amp;&amp; systemctl start docker
</code></pre>
</blockquote>
<h2 id="Etcd-安裝與設定"><a href="#Etcd-安裝與設定" class="headerlink" title="Etcd 安裝與設定"></a>Etcd 安裝與設定</h2><p>在開始安裝 Kubernetes 之前，需要先將一些必要系統建置完成，其中 Etcd 就是 Kubernetes 最為需要的一環，Kubernetes 會將部分資訊儲存於 Etcd 上，來提供給其他節點索取，以確保整個叢集的狀態。</p>
<p>首先在<code>master</code>節點下載 Etcd，並解壓縮放到 /opt 底下與安裝：</p>
<pre><code class="sh">$ cd /opt
$ wget -qO- &quot;https://github.com/coreos/etcd/releases/download/v3.1.6/etcd-v3.1.6-linux-amd64.tar.gz&quot; | tar -zx
$ mv etcd-v3.1.6-linux-amd64 etcd
$ cd etcd/ &amp;&amp; ln etcd /usr/bin/ &amp;&amp; ln etcdctl /usr/bin/
</code></pre>
<p>完成後新建 Etcd Group 與 User，並建立 Etcd 設定檔目錄：</p>
<pre><code class="sh">$ groupadd etcd
$ useradd -c &quot;Etcd user&quot; -g etcd -s /sbin/nologin -r etcd
$ mkdir /etc/etcd
</code></pre>
<p>新增<code>/etc/etcd/etcd.conf</code>檔案，加入以下內容：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/etcd/etcd.conf
ETCD_NAME=master
ETCD_DATA_DIR=/var/lib/etcd
ETCD_INITIAL_ADVERTISE_PEER_URLS=http://172.16.35.12:2380
ETCD_INITIAL_CLUSTER=master=http://172.16.35.12:2380
ETCD_INITIAL_CLUSTER_STATE=new
ETCD_INITIAL_CLUSTER_TOKEN=etcd-k8s-cluster
ETCD_LISTEN_PEER_URLS=http://0.0.0.0:2380
ETCD_ADVERTISE_CLIENT_URLS=http://172.16.35.12:2379
ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379
ETCD_PROXY=off
EOF
</code></pre>
<blockquote>
<p>P.S. 若與該教學 IP 不同的話，請用自己 IP 取代<code>172.16.35.12</code>。</p>
</blockquote>
<p>新增<code>/lib/systemd/system/etcd.service</code>來管理 Etcd，並加入以下內容：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; /lib/systemd/system/etcd.service
[Unit]
Description=Etcd Service
After=network.target

[Service]
Environment=ETCD_DATA_DIR=/var/lib/etcd/default
EnvironmentFile=-/etc/etcd/etcd.conf
Type=notify
User=etcd
PermissionsStartOnly=true
ExecStart=/usr/bin/etcd
Restart=always
RestartSec=10
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<p>建立 var 存放資訊，然後啟動 Etcd 服務:</p>
<pre><code class="sh">$ mkdir -p /var/lib/etcd &amp;&amp; chown etcd:etcd -R /var/lib/etcd
$ systemctl enable etcd.service &amp;&amp; systemctl start etcd.service
</code></pre>
<p>透過簡單指令驗證：</p>
<pre><code class="sh">$ etcdctl cluster-health
member 95b428c288413b46 is healthy: got healthy result from http://172.16.35.12:2379
cluster is healthy
</code></pre>
<p>接著回到<code>master</code>節點，新增一個<code>/tmp/flannel-config.json</code>檔，並加入以下內容：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; /tmp/flannel-config.json
{ &quot;Network&quot;: &quot;10.244.0.0/16&quot;, &quot;SubnetLen&quot;: 24, &quot;Backend&quot;: { &quot;Type&quot;: &quot;vxlan&quot; } }
EOF
</code></pre>
<p>然後將 Flannel 網路設定儲存到 etcd 中：</p>
<pre><code class="sh">$ etcdctl --no-sync set /atomic.io/network/config &lt; /tmp/flannel-config.json
$ etcdctl ls /atomic.io/network/
/atomic.io/network/config
</code></pre>
<h2 id="Flannel-安裝與設定"><a href="#Flannel-安裝與設定" class="headerlink" title="Flannel 安裝與設定"></a>Flannel 安裝與設定</h2><p>Flannel 是 CoreOS 團隊針對 Kubernetes 設計的一個<code>覆蓋網絡(Overlay Network)</code>工具，其目的在於幫助每一個使用 Kuberentes 的主機擁有一個完整的子網路。</p>
<p>首先在<code>所有</code>節點下載 Flannel，並執行以下步驟。首先解壓縮放到 /opt 底下與安裝：</p>
<pre><code class="sh">$ cd /opt &amp;&amp; mkdir flannel
$ wget -qO- &quot;https://github.com/coreos/flannel/releases/download/v0.7.1/flannel-v0.7.1-linux-amd64.tar.gz&quot; | tar -zxC flannel/
$ cd flannel/ &amp;&amp; ln flanneld /usr/bin/ &amp;&amp; ln mk-docker-opts.sh /usr/bin/
</code></pre>
<p>建立 Docker Drop-in 目錄，並新增<code>flannel.conf</code>檔案：</p>
<pre><code class="sh">$ mkdir -p /etc/systemd/system/docker.service.d
$ cat &lt;&lt;EOF &gt; /etc/systemd/system/docker.service.d/flannel.conf
[Service]
EnvironmentFile=-/run/flannel/docker
EOF
</code></pre>
<p>新增<code>/etc/default/flanneld</code>檔案，加入以下內容：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/default/flanneld
FLANNEL_ETCD_ENDPOINTS=&quot;http://172.16.35.12:2379&quot;
FLANNEL_ETCD_PREFIX=&quot;/atomic.io/network&quot;
FLANNEL_OPTIONS=&quot;--iface=enp0s8&quot;
EOF
</code></pre>
<blockquote>
<p><code>FLANNEL_ETCD_ENDPOINTS</code> 請修改成自己的 <code>master</code> IP。<br><code>FLANNEL_OPTIONS</code>可以依據需求加入，這邊主要指定 flannel 使用的網卡。</p>
</blockquote>
<p>新增<code>/lib/systemd/system/flanneld.service</code>來管理 Flannel：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; /lib/systemd/system/flanneld.service
[Unit]
Description=Flanneld Service
After=network-online.target
Wants=network-online.target
After=etcd.service
Before=docker.service

[Service]
Type=notify
EnvironmentFile=/etc/default/flanneld
ExecStart=/usr/bin/flanneld -etcd-endpoints=\${FLANNEL_ETCD_ENDPOINTS} -etcd-prefix=\${FLANNEL_ETCD_PREFIX} \${FLANNEL_OPTIONS}
ExecStartPost=/usr/bin/mk-docker-opts.sh -d /run/flannel/docker
Restart=always

[Install]
WantedBy=multi-user.target
RequiredBy=docker.service
EOF
</code></pre>
<p>之後到<code>每台</code>節點啟動 Flannel:</p>
<pre><code class="sh">$ systemctl enable flanneld.service &amp;&amp; systemctl start flanneld.service
</code></pre>
<p>完成後透過以下指令簡單驗證：</p>
<pre><code class="sh">$ ip -4 addr show flannel.1

5: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default
    inet 10.244.11.0/32 scope global flannel.1
       valid_lft forever preferred_lft forever
</code></pre>
<p>確認有網路後，修改<code>/lib/systemd/system/docker.service</code>檔案以下內容：</p>
<pre><code>ExecStartPost=/sbin/iptables -I FORWARD -s 0.0.0.0/0 -j ACCEPT
ExecStart=/usr/bin/dockerd -H fd:// $DOCKER_OPTS
</code></pre><blockquote>
<p>若是 CentOS 7 則不需要加入 <code>-H fd://</code>。</p>
</blockquote>
<p>重新啟動 Docker 來使用 Flannel：</p>
<pre><code class="sh">$ systemctl daemon-reload &amp;&amp; systemctl restart docker
$ ip -4 a show docker0

4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    inet 10.244.11.1/24 scope global docker0
       valid_lft forever preferred_lft forever
</code></pre>
<p>最後在任一台節點去 Ping 其他節點的 docker0 網路，若 Ping 的到表示部署沒問題。</p>
<h2 id="Kubernetes-Master-安裝與設定"><a href="#Kubernetes-Master-安裝與設定" class="headerlink" title="Kubernetes Master 安裝與設定"></a>Kubernetes Master 安裝與設定</h2><p>Master 是 Kubernetes 的大總管，主要建置<code>API Server</code>、<code>Controller Manager Server</code>與<code>Scheduler</code>來元件管理所有 Node。首先加入取得 Packages 來源並安裝：</p>
<pre><code class="sh">$ curl -s &quot;https://packages.cloud.google.com/apt/doc/apt-key.gpg&quot; | apt-key add -
$ echo &quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&quot; &gt; /etc/apt/sources.list.d/kubernetes.list
$ apt-get update &amp;&amp; apt-get install -y kubectl kubelet kubernetes-cni
</code></pre>
<blockquote>
<p>CentOS 7 則使用以下指令安裝：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
       https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
$ yum install -y kubelet kubectl kubernetes-cni
</code></pre>
</blockquote>
<p>然後準備 OpenSSL 的設定檔資訊：</p>
<pre><code class="sh">$ mkdir -p /etc/kubernetes/pki
$ DIR=/etc/kubernetes/pki
$ cat &lt;&lt;EOF &gt; ${DIR}/openssl.conf
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names
[alt_names]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster.local
IP.1 = 192.160.0.1
IP.2 = 172.16.35.12
EOF
</code></pre>
<blockquote>
<p><code>IP.2</code> 請修改成自己的<code>master</code> IP。<br>細節請參考<a href="https://coreos.com/kubernetes/docs/latest/openssl.html" target="_blank" rel="noopener">Cluster TLS using OpenSSL</a>。</p>
</blockquote>
<p>建立 OpenSSL Keypairs 與 Certificate：</p>
<pre><code class="sh">DIR=/etc/kubernetes/pki

openssl genrsa -out ${DIR}/ca-key.pem 2048
openssl req -x509 -new -nodes -key ${DIR}/ca-key.pem -days 1000 -out ${DIR}/ca.pem -subj &#39;/CN=kube-ca&#39;
openssl genrsa -out ${DIR}/admin-key.pem 2048
openssl req -new -key ${DIR}/admin-key.pem -out ${DIR}/admin.csr -subj &#39;/CN=kube-admin&#39;
openssl x509 -req -in ${DIR}/admin.csr -CA ${DIR}/ca.pem -CAkey ${DIR}/ca-key.pem -CAcreateserial -out ${DIR}/admin.pem -days 1000
openssl genrsa -out ${DIR}/apiserver-key.pem 2048
openssl req -new -key ${DIR}/apiserver-key.pem -out ${DIR}/apiserver.csr -subj &#39;/CN=kube-apiserver&#39; -config ${DIR}/openssl.conf
openssl x509 -req -in ${DIR}/apiserver.csr -CA ${DIR}/ca.pem -CAkey ${DIR}/ca-key.pem -CAcreateserial -out ${DIR}/apiserver.pem -days 1000 -extensions v3_req -extfile ${DIR}/openssl.conf
</code></pre>
<blockquote>
<p>細節請參考 <a href="https://coreos.com/kubernetes/docs/latest/openssl.html" target="_blank" rel="noopener">Cluster TLS using OpenSSL</a>。</p>
</blockquote>
<p>接著下載 Kubernetes 相關檔案至<code>/etc/kubernetes</code>：</p>
<pre><code class="sh">cd /etc/kubernetes/
URL=&quot;https://kairen.github.io/files/manual/master&quot;
wget ${URL}/kube-apiserver.conf -O manifests/kube-apiserver.yml
wget ${URL}/kube-controller-manager.conf -O manifests/kube-controller-manager.yml
wget ${URL}/kube-scheduler.conf -O manifests/kube-scheduler.yml
wget ${URL}/admin.conf -O admin.conf
wget ${URL}/kubelet.conf -O kubelet
cat &lt;&lt;EOF &gt; /etc/kubernetes/user.csv
p@ssw0rd,admin,admin
EOF
</code></pre>
<blockquote>
<p>若<code>IP</code>與教學設定不同的話，請記得修改<code>kube-apiserver.yml</code>、<code>kube-controller-manager.yml</code>、<code>kube-scheduler.yml</code>與<code>admin.yml</code>。</p>
</blockquote>
<p>新增<code>/lib/systemd/system/kubelet.service</code>來管理 kubelet：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; /lib/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet Server
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
EnvironmentFile=-/etc/kubernetes/kubelet
ExecStart=/usr/bin/kubelet \$KUBELET_ADDRESS \$KUBELET_POD_INFRA_CONTAINER \
\$KUBELET_ARGS \$KUBE_NODE_LABEL \$KUBE_LOGTOSTDERR \
\$KUBE_ALLOW_PRIV \$KUBELET_NETWORK_ARGS \
\$KUBELET_DNS_ARGS
Restart=always

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<blockquote>
<p><code>/etc/systemd/system/kubelet.service</code>為<code>CentOS 7</code>使用的路徑。</p>
</blockquote>
<p>最後建立 var 存放資訊，然後啟動 kubelet 服務:</p>
<pre><code class="sh">$ mkdir -p /var/lib/kubelet
$ systemctl daemon-reload &amp;&amp; systemctl restart kubelet.service
</code></pre>
<p>完成後會需要一段時間來下載與啟動元件，可以利用該指令來監看：</p>
<pre><code class="sh">$ watch -n 1 netstat -ntlp
tcp   0  0 127.0.0.1:10248  0.0.0.0:*  LISTEN  20613/kubelet
tcp   0  0 127.0.0.1:10251  0.0.0.0:*  LISTEN  19968/kube-schedule
tcp   0  0 127.0.0.1:10252  0.0.0.0:*  LISTEN  20815/kube-controll
tcp6  0  0 :::8080          :::*       LISTEN  20333/kube-apiserve
</code></pre>
<blockquote>
<p>若看到以上已經被 binding 後，就可以透過瀏覽器存取 <a href="https://172.16.35.12:6443/" target="_blank" rel="noopener">API Service</a>，並輸入帳號<code>admin</code>與密碼<code>p@ssw0rd</code>。</p>
</blockquote>
<p>透過簡單指令驗證：</p>
<pre><code class="sh">$ kubectl get node
NAME      STATUS         AGE
master   Ready,master   1m

$ kubectl get po --all-namespaces
NAMESPACE     NAME                              READY     STATUS    RESTARTS   AGE
kube-system   kube-apiserver-master1            1/1       Running   0          3m
kube-system   kube-controller-manager-master1   1/1       Running   0          2m
kube-system   kube-scheduler-master1            1/1       Running   0          2m
</code></pre>
<h2 id="Kubernetes-Node-安裝與設定"><a href="#Kubernetes-Node-安裝與設定" class="headerlink" title="Kubernetes Node 安裝與設定"></a>Kubernetes Node 安裝與設定</h2><p>Node 是主要的工作節點，上面將運行許多容器應用。到所有<code>node</code>節點加入取得 Packages 來源，並安裝：</p>
<pre><code class="sh">$ curl -s &quot;https://packages.cloud.google.com/apt/doc/apt-key.gpg&quot; | apt-key add -
$ echo &quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&quot; &gt; /etc/apt/sources.list.d/kubernetes.list
$ apt-get update &amp;&amp; apt-get install -y kubelet kubernetes-cni
</code></pre>
<blockquote>
<p>CentOS 7 則使用以下指令安裝：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
       https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
$ yum install -y kubelet kubernetes-cni
</code></pre>
</blockquote>
<p>然後準備 OpenSSL 的設定檔資訊：</p>
<pre><code class="sh">$ mkdir -p /etc/kubernetes/pki
$ DIR=/etc/kubernetes/pki
$ cat &lt;&lt;EOF &gt; ${DIR}/openssl.conf
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names
[alt_names]
IP.1=172.16.35.10
DNS.1=node1
EOF
</code></pre>
<blockquote>
<p>P.S. 這邊<code>IP.1</code>與<code>DNS.1</code>需要隨機器不同設定。細節請參考 <a href="https://coreos.com/kubernetes/docs/latest/openssl.html" target="_blank" rel="noopener">Cluster TLS using OpenSSL</a>。</p>
</blockquote>
<p>將<code>master1</code>上的 OpenSSL key 複製到<code>/etc/kubernetes/pki</code>：</p>
<pre><code class="sh">for file in ca-key.pem ca.pem admin.pem admin-key.pem; do
  scp /etc/kubernetes/pki/${file} &lt;NODE&gt;:/etc/kubernetes/pki/
done
</code></pre>
<blockquote>
<p>P.S. 該操作在<code>master1</code>執行。並記得修改<code>&lt;NODE&gt;</code>為所有工作節點。</p>
</blockquote>
<p>建立 OpenSSL Keypairs 與 Certificate：</p>
<pre><code class="sh">DIR=/etc/kubernetes/pki

openssl genrsa -out ${DIR}/node-key.pem 2048
openssl req -new -key ${DIR}/node-key.pem -out ${DIR}/node.csr -subj &#39;/CN=kube-node&#39; -config ${DIR}/openssl.conf
openssl x509 -req -in ${DIR}/node.csr -CA ${DIR}/ca.pem -CAkey ${DIR}/ca-key.pem -CAcreateserial -out ${DIR}/node.pem -days 1000 -extensions v3_req -extfile ${DIR}/openssl.conf
</code></pre>
<blockquote>
<p>細節請參考 <a href="https://coreos.com/kubernetes/docs/latest/openssl.html" target="_blank" rel="noopener">Cluster TLS using OpenSSL</a>。</p>
</blockquote>
<p>接著下載 Kubernetes 相關檔案至<code>/etc/kubernetes/</code>：</p>
<pre><code class="sh">cd /etc/kubernetes/
URL=&quot;https://kairen.github.io/files/manual/node&quot;
wget ${URL}/kubelet-user.conf -O kubelet-user.conf
wget ${URL}/admin.conf -O admin.conf
wget ${URL}/kubelet.conf -O kubelet
</code></pre>
<blockquote>
<p>若<code>IP</code>與教學設定不同的話，請記得修改<code>kubelet-user.conf</code>與<code>admin.conf</code>。</p>
</blockquote>
<p>新增<code>/lib/systemd/system/kubelet.service</code>來管理 kubelet：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; /lib/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet Server
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
EnvironmentFile=-/etc/kubernetes/kubelet
ExecStart=/usr/bin/kubelet \$KUBELET_ADDRESS \$KUBELET_POD_INFRA_CONTAINER \
\$KUBELET_ARGS \$KUBE_LOGTOSTDERR \
\$KUBE_ALLOW_PRIV \$KUBELET_NETWORK_ARGS \
\$KUBELET_DNS_ARGS
Restart=always

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<blockquote>
<p><code>/etc/systemd/system/kubelet.service</code>為<code>CentOS 7</code>使用的路徑。</p>
</blockquote>
<p>最後建立 var 存放資訊，然後啟動 kubelet 服務:</p>
<pre><code class="sh">$ mkdir -p /var/lib/kubelet
$ systemctl daemon-reload &amp;&amp; systemctl restart kubelet.service
</code></pre>
<p>當所有節點都完成後，回到<code>master</code>透過簡單指令驗證：</p>
<pre><code class="sh">$ kubectl get node
NAME      STATUS         AGE       VERSION
master1   Ready,master   17m       v1.6.4
node1     Ready          20s       v1.6.4
node2     Ready          18s       v1.6.4
</code></pre>
<h2 id="Kubernetes-Addons-部署"><a href="#Kubernetes-Addons-部署" class="headerlink" title="Kubernetes Addons 部署"></a>Kubernetes Addons 部署</h2><p>當環境都建置完成後，就可以進行部署附加元件，首先到<code>master1</code>，並進入<code>/etc/kubernetes/</code>目錄下載 Addon 檔案：</p>
<pre><code class="sh">cd /etc/kubernetes/ &amp;&amp; mkdir addon
URL=&quot;https://kairen.github.io/files/manual/addon&quot;
wget ${URL}/kube-proxy.conf -O addon/kube-proxy.yml
wget ${URL}/kube-dns.conf -O addon/kube-dns.yml
wget ${URL}/kube-dash.conf -O addon/kube-dash.yml
wget ${URL}/kube-monitor.conf -O addon/kube-monitor.yml
</code></pre>
<blockquote>
<p>若<code>IP</code>與教學設定不同的話，請記得修改<code>&lt;YOUR_MASTER_IP&gt;</code>。</p>
<pre><code class="sh">$ sed -i &#39;s/172.16.35.12/&lt;YOUR_MASTER_IP&gt;/g&#39; addon/kube-monitor.yml
$ sed -i &#39;s/172.16.35.12/&lt;YOUR_MASTER_IP&gt;/g&#39; addon/kube-proxy.yml
</code></pre>
</blockquote>
<p>接著透過 kubectl 來指定檔案建立附加元件：</p>
<pre><code class="sh">$ kubectl apply -f addon/
</code></pre>
<blockquote>
<p>若想要刪除則將<code>apply</code>改成<code>delete</code>即可。</p>
</blockquote>
<p>透過以下指令來驗證部署是否有效：</p>
<pre><code class="sh">$ kubectl get po -n kube-system
NAME                                   READY     STATUS    RESTARTS   AGE
heapster-v1.2.0-1753406648-wsb3z       1/1       Running   0          2m
influxdb-grafana-42195489-vtmnl        2/2       Running   0          2m
kube-apiserver-master1                 1/1       Running   0          33m
kube-controller-manager-master1        1/1       Running   0          33m
kube-dns-3701766129-0p28b              3/3       Running   0          2m
kube-proxy-amd64-44rft                 1/1       Running   0          2m
kube-proxy-amd64-fz77b                 1/1       Running   0          2m
kube-proxy-amd64-gqq2p                 1/1       Running   0          2m
kube-scheduler-master1                 1/1       Running   0          33m
kubernetes-dashboard-210558060-zw814   1/1       Running   2          2m
</code></pre>
<p>確定都啟動後，可以開啟 <a href="https://172.16.35.12:6443/ui" target="_blank" rel="noopener">https://172.16.35.12:6443/ui</a> 來查看。<br><img src="/images/kube/dash-preview.png" alt=""></p>
<h2 id="簡單部署-Nginx-服務"><a href="#簡單部署-Nginx-服務" class="headerlink" title="簡單部署 Nginx 服務"></a>簡單部署 Nginx 服務</h2><p>Kubernetes 可以選擇使用指令直接建立應用程式與服務，或者撰寫 YAML 與 JSON 檔案來描述部署應用程式的配置，以下將建立一個簡單的 Nginx 服務：</p>
<pre><code class="sh">$ kubectl run nginx --image=nginx --replicas=1 --port=80
$ kubectl get pods -o wide
NAME                    READY     STATUS    RESTARTS   AGE       IP            NODE
nginx-158599303-k7cbt   1/1       Running   0          14s       10.244.24.3   node1
</code></pre>
<p>完成後要接著建立 svc(Service)，來提供外部網路存取應用程式，使用以下指令建立：</p>
<pre><code class="sh">$ kubectl expose deploy nginx --port=80 --type=LoadBalancer --external-ip=172.16.35.12
$ kubectl get svc

NAME             CLUSTER-IP       EXTERNAL-IP     PORT(S)        AGE
svc/kubernetes   192.160.0.1      &lt;none&gt;          443/TCP        2h
svc/nginx        192.160.57.181   ,172.16.35.12   80:32054/TCP   21s
</code></pre>
<blockquote>
<p>這邊<code>type</code>可以選擇 NodePort 與 LoadBalancer。另外需隨機器 IP 不同而修改 <code>external-ip</code>。</p>
</blockquote>
<p>確認沒問題後即可在瀏覽器存取 <a href="http://172.16.35.12/。" target="_blank" rel="noopener">http://172.16.35.12/。</a></p>
<h3 id="擴展服務數量"><a href="#擴展服務數量" class="headerlink" title="擴展服務數量"></a>擴展服務數量</h3><p>若叢集<code>node</code>節點增加了，而想讓 Nginx 服務提供可靠性的話，可以透過以下方式來擴展服務的副本：</p>
<pre><code class="sh">$ kubectl scale deploy nginx --replicas=2

$ kubectl get pods -o wide
NAME                    READY     STATUS    RESTARTS   AGE       IP             NODE
nginx-158599303-0h9lr   1/1       Running   0          25s       10.244.100.5   node2
nginx-158599303-k7cbt   1/1       Running   0          1m        10.244.24.3    node1
</code></pre>
]]></content>
      
        <categories>
            
            <category> Kubernetes </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Docker </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Ceph 使用 SPDK 加速 NVMe SSD]]></title>
      <url>https://kairen.github.io/2016/12/03/ceph/ceph-spdk/</url>
      <content type="html"><![CDATA[<p><a href="https://github.com/spdk/spdk" target="_blank" rel="noopener">SPDK(Storage Performance Development Kit)</a> 是 Intel 釋出的儲存效能開發工具，主要提供一套撰寫高效能、可擴展與 User-mode 的儲存應用程式工具與函式庫，而中國公司 XSKY 藉由該開發套件來加速 Ceph 在 NVMe SSD 的效能。</p>
<a id="more"></a>
<p>首先進入 root，並 clone 專案到 local：</p>
<pre><code class="shell=">$ sudo su -
$ git clone http://github.com/ceph/ceph
$ cd ceph
</code></pre>
<p>編輯<code>CMakeLists.txt</code>檔案，修改以下內容：</p>
<pre><code class="shell=">option(WITH_SPDK &quot;Enable SPDK&quot; ON)
</code></pre>
<p>接著安裝一些相依套件與函式庫：</p>
<pre><code class="shell=">$ ./install-deps.sh
$ sudo apt-get install -y libpciaccess-dev
</code></pre>
<p>接著需要在環境安裝 DPDK 開發套件，首先進入 src 底下的 dpdk 目錄，編輯<code>config/common_linuxapp</code>檔案修改以下內容：</p>
<pre><code class="shell=">CONFIG_RTE_BUILD_SHARED_LIB=
</code></pre>
<p>完成後建置與安裝 DPDK：</p>
<pre><code class="shell=">$ make config T=x86_64-native-linuxapp-gcc
$ make &amp;&amp; make install
</code></pre>
<p>接著回到 ceph root 目錄進行建構 Ceph 準備，透過以下指令進行：</p>
<pre><code class="shell=">$ ./do_cmake.sh
....
-- Configuring done
-- Generating done
-- Build files have been written to: /root/ceph/build
+ cat
+ echo 40000
+ echo done.
done.
</code></pre>
<p>確認上面無誤後就可以進行 compile 包含 SPDK 的 Ceph：</p>
<pre><code class="shell=">$ cd build
$ make -j2
</code></pre>
<p>完成後就可以執行 test cluster，首先建構 vstart 程式：</p>
<pre><code class="shell=">$ make vstart
$ ../src/vstart.sh -d -n -x -l
$ ./bin/ceph -s
</code></pre>
<blockquote>
<p>若要關閉則使用以下方式：</p>
<pre><code class="shell=">$ ../src/stop.sh
</code></pre>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Ceph </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> Storage </tag>
            
            <tag> Distribution System </tag>
            
            <tag> SPDK </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Using bluestore in Kraken]]></title>
      <url>https://kairen.github.io/2016/11/28/ceph/deploy/ceph-deploy-bluestore/</url>
      <content type="html"><![CDATA[<p>本篇說明如何安裝 Kraken 版本的 Ceph，並將 objectstore backend 修改成 Bluestore，過程包含建立 RBD 等操作。</p>
<a id="more"></a>
<h2 id="硬體規格說明"><a href="#硬體規格說明" class="headerlink" title="硬體規格說明"></a>硬體規格說明</h2><p>本安裝由於實體機器數量受到限制，故只進行一台 MON 與兩台 OSD，而 OSD 數量則總共兩顆，硬體規格如下所示：</p>
<table>
<thead>
<tr>
<th>Role</th>
<th>RAM</th>
<th>CPUs</th>
<th>Disk</th>
<th>IP Address</th>
</tr>
</thead>
<tbody>
<tr>
<td>mon1(deploy)</td>
<td>4 GB</td>
<td>4 core</td>
<td>500 GB</td>
<td>172.16.1.200</td>
</tr>
<tr>
<td>osd1</td>
<td>16 GB</td>
<td>8 core</td>
<td>2 TB</td>
<td>172.16.1.201</td>
</tr>
<tr>
<td>osd2</td>
<td>16 GB</td>
<td>8 core</td>
<td>2 TB</td>
<td>172.16.1.202</td>
</tr>
<tr>
<td>osd3</td>
<td>16 GB</td>
<td>8 core</td>
<td>2 TB</td>
<td>172.16.1.203</td>
</tr>
</tbody>
</table>
<p>作業系統採用<code>Ubuntu 16.04 LTS Server</code>，Kernel 版本為<code>Linux 4.4.0-31-generic</code>。</p>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>在開始部署 Ceph 叢集之前，我們需要在每個節點做一些基本的準備，來確保叢集安裝的過程是流暢的，本次安裝會擁有四台節點。</p>
<p>首先在每一台節點新增以下內容到<code>/etc/hosts</code>：</p>
<pre><code>127.0.0.1    localhost

172.16.1.200 mon1
172.16.1.201 osd1
172.16.1.202 osd2
172.16.1.203 osd3
</code></pre><p>然後設定各節點 sudo 指令的權限，使之不用輸入密碼(若使用 root 則忽略)：</p>
<pre><code class="sh">$ echo &quot;ubuntu ALL = (root) NOPASSWD:ALL&quot; | \
sudo tee /etc/sudoers.d/ubuntu &amp;&amp; sudo chmod 440 /etc/sudoers.d/ubuntu
</code></pre>
<p>接著在設定<code>deploy</code>節點能夠以無密碼方式進行 SSH 登入其他節點，請依照以下執行：</p>
<pre><code class="sh">$ ssh-keygen -t rsa
$ ssh-copy-id mon1
$ ssh-copy-id osd1
...
</code></pre>
<blockquote>
<p>若不同節點之間使用不同 User 進行 SSH 部署的話，可以設定 ~/.ssh/config</p>
</blockquote>
<p>之後在<code>deploy</code>節點安裝部署工具，首先使用 apt-get 來進行安裝基本相依套件，再透過 pypi 進行安裝 ceph-deploy 工具：</p>
<pre><code class="sh">$ sudo apt-get install -y python-pip
$ sudo pip install -U ceph-deploy
</code></pre>
<h2 id="節點部署"><a href="#節點部署" class="headerlink" title="節點部署"></a>節點部署</h2><p>首先建立一個名稱為 local 的目錄，並進到目錄底下：</p>
<pre><code class="sh">$ sudo mkdir local &amp;&amp; cd local
</code></pre>
<p>接著透過 ceph-deploy 在各節點安裝 ceph：</p>
<pre><code class="sh">$ ceph-deploy install --release kraken mon1 osd1 osd2 osd3
</code></pre>
<p>完成後建立 Monitor 節點資訊到 ceph.conf 中：</p>
<pre><code class="sh">$ ceph-deploy new mon1 &lt;other_mons&gt;
</code></pre>
<p>接著編輯目錄底下的 ceph.conf，並加入以下內容：</p>
<pre><code class="sh">[global]
...
rbd_default_features = 3

osd pool default size = 3
osd pool default min size = 1

public network = 172.16.1.0/24
cluster network = 172.16.1.0/24

filestore_xattr_use_omap = true
enable experimental unrecoverable data corrupting features = bluestore rocksdb
bluestore fsck on mount = true
bluestore block db size = 134217728
bluestore block wal size = 268435456
bluestore block size = 322122547200
osd objectstore = bluestore

[osd]
bluestore = true
</code></pre>
<p>若確認沒問題，即可透過以下指令初始化 mon：</p>
<pre><code class="sh">$ ceph-deploy mon create-initial
</code></pre>
<p>上述沒有問題後，就可以開始部署實際作為儲存的 OSD 節點，我們可以透過以下指令進行：</p>
<pre><code class="sh">$ ceph-deploy osd prepare --bluestore osd1:&lt;device&gt;
</code></pre>
<h2 id="系統驗證"><a href="#系統驗證" class="headerlink" title="系統驗證"></a>系統驗證</h2><h3 id="叢集檢查"><a href="#叢集檢查" class="headerlink" title="叢集檢查"></a>叢集檢查</h3><p>首先要驗證環境是否有部署成功，可以透過 ceph 提供的基本指令做檢查：</p>
<pre><code class="sh">$ ceph -v
ceph version v11.0.2 (697fe64f9f106252c49a2c4fe4d79aea29363be7)

$ ceph -s

    cluster 6da24ae5-755f-4077-bfa0-78681dfc6bde
     health HEALTH_OK
     monmap e1: 1 mons at {r-mon00=172.16.1.200:6789/0}
            election epoch 7, quorum 0 mon1
        mgr no daemons active
     osdmap e256: 3 osds: 3 up, 3 in
            flags sortbitwise,require_jewel_osds
      pgmap v920162: 128 pgs, 1 pools, 6091 MB data, 1580 objects
            12194 MB used, 588 GB / 600 GB avail
                 128 active+clean
</code></pre>
<p>另外也可以用 osd 指令來查看部屬的 osd 資訊：</p>
<pre><code class="sh">$ ceph osd tree

ID WEIGHT  TYPE NAME        UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1 0.58618 root default
-2 0.29309     host osd1
 0 0.29309         osd.0         up  1.00000          1.00000
-3 0.29309     host osd2
 1 0.29309         osd.1         up  1.00000          1.00000
-4 0.29309     host osd3
 1 0.29309         osd.2         up  1.00000          1.00000
</code></pre>
<h3 id="RBD-建立"><a href="#RBD-建立" class="headerlink" title="RBD 建立"></a>RBD 建立</h3><p>本節說明在 Kraken 版本建立 RBD 來進行使用，在預設部署起來的叢集下會存在一個儲存池 rbd，因此可以省略建立新的儲存池。</p>
<p>首先透過以下指令建立一個區塊裝置映像檔：</p>
<pre><code class="sh">$ rbd create rbd/bd -s 50G
</code></pre>
<p>接著透過 info 指令查看區塊裝置映像檔資訊：</p>
<pre><code class="sh">$ rbd info rbd/bd

rbd image &#39;bd&#39;:
    size 51200 MB in 12800 objects
    order 22 (4096 kB objects)
    block_name_prefix: rbd_data.102d474b0dc51
    format: 2
    features: layering, striping
    flags:
    stripe unit: 4096 kB
    stripe count: 1
</code></pre>
<blockquote>
<p>P.S. 這邊由於 Kernel 版本問題有些特性無法支援，因此在 conf 檔只設定使用 layering, striping。</p>
<p>P.S. 若預設未修改 feature 設定的話，可以透過以下指令修改:</p>
<pre><code class="sh">$ rbd feature disable rbd/bd &lt;feature_name&gt;
</code></pre>
<p>以下為目前支援的特性：</p>
<table>
<thead>
<tr>
<th>屬性名稱</th>
<th>說明</th>
<th>Bit Code</th>
</tr>
</thead>
<tbody>
<tr>
<td>layering</td>
<td>支援分層</td>
<td>1</td>
</tr>
<tr>
<td>striping</td>
<td>支援串連(v2)</td>
<td>2</td>
</tr>
<tr>
<td>exclusive-lock</td>
<td>支援互斥鎖定</td>
<td>4</td>
</tr>
<tr>
<td>object-map</td>
<td>支援物件映射(相依於 exclusive-lock )</td>
<td>8</td>
</tr>
<tr>
<td>fast-diff</td>
<td>支援快速計算差異(相依於 object-map )</td>
<td>16</td>
</tr>
<tr>
<td>deep-flatten</td>
<td>支援快照扁平化操作</td>
<td>32</td>
</tr>
<tr>
<td>journaling</td>
<td>支援紀錄 I/O 操作(相依於 exclusive-lock )</td>
<td>64</td>
</tr>
</tbody>
</table>
</blockquote>
<p>接著就可以透過 Linux mkfs 指令來格式化 rbd：</p>
<pre><code class="sh">$ sudo mkfs.ext4 /dev/rbd0
$ sudo mount /dev/rbd0 /mnt
</code></pre>
<p>最後透過 dd 指令測試 rbd 寫入效能：</p>
<pre><code class="sh">$ dd if=/dev/zero of=/mnt/test bs=4096 count=4000000

4000000+0 records in
4000000+0 records out
16384000000 bytes (16 GB) copied, 119.947 s, 137 MB/s
</code></pre>
<p>另外有些需求為了測試 feature，卻又礙於 Kernel 不支援等問題，而造成無法 Map 時，可以透過 rbd-nbd 來進行 Map，安裝跟使用方式如下：</p>
<pre><code class="sh">$ sudo apt-get install -y rbd-nbd
$ sudo rbd-nbd map rbd/bd
/dev/nbd0
</code></pre>
<blockquote>
<p>P.S. 在新版的 ceph 已經有內建 rbd nbd，參考 <a href="http://docs.ceph.com/docs/jewel/man/8/rbd/#commands" target="_blank" rel="noopener">rbd - manage command</a>。</p>
</blockquote>
<p>最後透過 dd 指令測試 nbd 寫入效能：</p>
<pre><code class="sh">$ dd if=/dev/zero of=./mnt-nbd/test bs=4096 count=4000000

4000000+0 records in
4000000+0 records out
16384000000 bytes (16 GB) copied, 168.201 s, 97.4 MB/s
</code></pre>
]]></content>
      
        <categories>
            
            <category> Ceph </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> Storage </tag>
            
            <tag> BlueStore </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[簡單部署 Docker Swarm 測試叢集]]></title>
      <url>https://kairen.github.io/2016/11/16/container/docker-swarm/</url>
      <content type="html"><![CDATA[<p>Docker Swarm 是 Docker 公司的 Docker 編配引擎，最早是在 2014 年 12 月發佈。Docker Swarm 目的即管理多台節點的 Docker 上應用程式與節點資源的排程等，並提供標準的 Docker API 介面當作前端存取入口，因此可以跟現有 Docker 工具與函式庫進行整合，本篇將介紹簡單的建立 Swarm cluster。</p>
<p>Docker Swarm 具備了以下幾個特性：</p>
<ul>
<li>Docker engine 原生支援。(Docker 1.12+)。</li>
<li>去中心化設計。</li>
<li>宣告式服務模型(Declarative Service Model)。</li>
<li>服務可擴展與容錯。</li>
<li>可協調預期狀態與實際狀態的一致性。</li>
<li>多種網路支援。</li>
<li>提供服務發現、負載平衡與安全策略。</li>
<li>支援滾動升級(Rolling Update)。</li>
</ul>
<a id="more"></a>
<h2 id="基本架構"><a href="#基本架構" class="headerlink" title="基本架構"></a>基本架構</h2><p>Docker Swarm 具備基本叢集功能，能讓多個 Docker 組合成一個群組，來提供容器服務。Docker 採用標準 Docker API 來管理容器的生命週期，而 Swarm 最主要核心是處理容器如何選擇一台主機來啟動容器這件事。以下為 Docker Swarm 架構：</p>
<p><img src="/images/docker/docker-swarm-architecture.png" alt="Docker"></p>
<p>Docker Swarm 一般分為兩個角色<code>Manager</code>與<code>Worker</code>，兩者主要工作如下：</p>
<ul>
<li><strong>Manager</strong>: 主要負責排程 Task，Task 可以表示為 Swarm 節點中的 Node 上啟動的容器。同時還負責編配容器與叢集管理功能，簡單說就是 Manager 具備管理 Node 的工作，除了以上外，Manager 還會維護叢集狀態。另外 Manager 也具備 Worker 的功能，當然也可以設定只做管理 Node 的職務。</li>
<li><strong>Worker</strong>: Worker 主要接收來自 Manager 的 Task 指派，並依據指派內容啟動 Docker 容器服務，並在完成後向 Manager 匯報 Task 執行狀態。</li>
</ul>
<h2 id="預先準備資訊"><a href="#預先準備資訊" class="headerlink" title="預先準備資訊"></a>預先準備資訊</h2><p>本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統可採用<code>Ubuntu 16.x</code>與<code>CentOS 7.x</code>：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Role</th>
<th>CPU</th>
<th>Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.16.35.12</td>
<td>manager</td>
<td>1</td>
<td>2G</td>
</tr>
<tr>
<td>172.16.35.10</td>
<td>node1</td>
<td>1</td>
<td>2G</td>
</tr>
<tr>
<td>172.16.35.11</td>
<td>node2</td>
<td>1</td>
<td>2G</td>
</tr>
</tbody>
</table>
<blockquote>
<p>這邊 Manager 為主要控制節點，node 為應用程式工作節點。</p>
</blockquote>
<p>首先安裝前要確認以下幾項都已將準備完成：</p>
<ul>
<li>所有節點彼此網路互通，並且不需要 SSH 密碼即可登入。</li>
<li>所有防火牆與 SELinux 已關閉。如 CentOS：</li>
</ul>
<pre><code class="sh">$ systemctl stop firewalld &amp;&amp; systemctl disable firewalld
$ setenforce 0
</code></pre>
<ul>
<li>所有節點需要設定<code>/etc/host</code>解析到所有主機。</li>
<li>所有節點需要安裝<code>Docker</code>引擎，安裝方式如下：</li>
</ul>
<pre><code class="sh">$ curl -fsSL &quot;https://get.docker.com/&quot; | sh
</code></pre>
<blockquote>
<p>不管是在 <code>Ubuntu</code> 或 <code>CentOS</code> 都只需要執行該指令就會自動安裝最新版 Docker。<br>CentOS 安裝完成後，需要再執行以下指令：</p>
<pre><code class="sh">$ systemctl enable docker &amp;&amp; systemctl start docker
</code></pre>
</blockquote>
<h2 id="Manager-節點建置"><a href="#Manager-節點建置" class="headerlink" title="Manager 節點建置"></a>Manager 節點建置</h2><p>當我們完成安裝 Docker Engine 後，就可以透過 Docker 指令來初始化 Manager 節點：</p>
<pre><code class="sh">$ docker swarm init --advertise-addr 172.16.35.12

Swarm initialized: current node (olluuvvz340ze64zhjpw03uke) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-0q0ohnexs40lb9z4kmvqb6zcrmp22hul9tmh6zpfztxzv5cv61-73yubitun1ufm0yhwx7h38p85 172.16.35.12:2377

To add a manager to this swarm, run &#39;docker swarm join-token manager&#39; and follow the instructions.
</code></pre>
<p>當看到上述內容，表示 Manager 初始化完成，這時候可以透過以下指令檢查：</p>
<pre><code class="sh">$ docker info
$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS
olluuvvz340ze64zhjpw03uke *   manager             Ready               Active              Leader
</code></pre>
<p>接著建立 Docker swarm network 來提供容器跨節點的溝通：</p>
<pre><code class="sh"># Deploy network
$ docker network create --driver=overlay --attachable cnblogs

# Docker flow proxy network
$ docker network create --driver overlay proxy
</code></pre>
<p>檢查 Docker 網路狀態：</p>
<pre><code class="sh">$ docker network ls | grep swarm
NETWORK ID          NAME                DRIVER              SCOPE
57nq0rux7akh        cnblogs             overlay             swarm
ihyg6uixeiov        ingress             overlay             swarm
b8vqturisod8        proxy               overlay             swarm
</code></pre>
<h2 id="Worker-節點建置"><a href="#Worker-節點建置" class="headerlink" title="Worker 節點建置"></a>Worker 節點建置</h2><p>完成 Manager 初始化後，就可以透過以下指令來將節點加入叢集：</p>
<pre><code class="sh">$ docker swarm join --token SWMTKN-1-0q0ohnexs40lb9z4kmvqb6zcrmp22hul9tmh6zpfztxzv5cv61-73yubitun1ufm0yhwx7h38p85 172.16.35.12:2377

This node joined a swarm as a worker.
</code></pre>
<blockquote>
<p>P.S. 其他節點一樣請用上述指令加入。</p>
</blockquote>
<p>在<code>Manager</code>節點，查看節點狀態：</p>
<pre><code class="sh">$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS
cwkta4o37daxed3otrqab9zdq     node2               Ready               Active
olluuvvz340ze64zhjpw03uke *   manager             Ready               Active              Leader
sfs49249kv8mad2qzr4ev4fy0     node1               Ready               Active
</code></pre>
<p>(option)將節點改為 Manager：</p>
<pre><code class="sh">$ docker node promote &lt;HOSTNAME&gt;
</code></pre>
<blockquote>
<p>另外降級為<code>docker node demote &lt;HOSTNAME&gt;</code>。</p>
</blockquote>
<h2 id="透過指令建立簡單服務"><a href="#透過指令建立簡單服務" class="headerlink" title="透過指令建立簡單服務"></a>透過指令建立簡單服務</h2><p>要建立 Docker 服務，可以使用<code>docker service</code>指令來達成，如下指令：</p>
<pre><code class="sh">$ docker service create --replicas 1 --name ping alpine ping 8.8.8.8
$ docker service logs ping
ping.1.auqefe3iq9yk@node2    | PING 8.8.8.8 (8.8.8.8): 56 data bytes
ping.1.auqefe3iq9yk@node2    | 64 bytes from 8.8.8.8: seq=0 ttl=61 time=7.042 ms
ping.1.auqefe3iq9yk@node2    | 64 bytes from 8.8.8.8: seq=1 ttl=61 time=7.029 ms
ping.1.auqefe3iq9yk@node2    | 64 bytes from 8.8.8.8: seq=2 ttl=61 time=7.668 m
...
</code></pre>
<p>建立兩份副本數的應用，如以下指令：</p>
<pre><code class="sh">$ docker service create --replicas 2 --name redis redis
$ docker service ps redis
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
ngtegx9vk4gu        redis.1             redis:latest        node1               Running             Running 43 seconds ago
n95vu3dzewu7        redis.2             redis:latest        manager             Running             Running 44 seconds ago
</code></pre>
<p>完成後，想要刪除可以使用以下指令：</p>
<pre><code class="sh">$ docker service rm ping
$ docker service rm redis
</code></pre>
<h2 id="部署簡單的-Stack"><a href="#部署簡單的-Stack" class="headerlink" title="部署簡單的 Stack"></a>部署簡單的 Stack</h2><p>這邊利用簡單範例來部署應用程式於 Swarm 叢集中，首先新增<code>stack.yml</code>檔案，並加入以下內容：</p>
<pre><code class="yaml">version: &#39;3.2&#39;
services:
  api:
    image: open-api:latest
    deploy:
      replicas: 2
      update_config:
        delay: 5s
      labels:
        - com.df.notify=true
        - com.df.distribute=true
        - com.df.serviceDomain=api.cnblogs.com
        - com.df.port=80
    networks:
      - cnblogs
      - proxy
networks:
  cnblogs:
    external: true
  proxy:
    external: true
</code></pre>
<p>完成後，透過以下指令來進行部署：</p>
<pre><code class="sh">$ docker stack deploy -c stack.yml openapi
</code></pre>
]]></content>
      
        <categories>
            
            <category> Container </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Docker </tag>
            
            <tag> Docker Swarm </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Minikube 部署 Local 測試環境]]></title>
      <url>https://kairen.github.io/2016/10/23/kubernetes/deploy/minikube/</url>
      <content type="html"><![CDATA[<p><a href="https://github.com/kubernetes/minikube" target="_blank" rel="noopener">Minikube</a> 是提供簡單與快速部署本地 Kubernetes 環境的工具，透過執行虛擬機來執行單節點 Kubernetes 叢集，以便開發者使用 Kubernetes 與開發用。</p>
<p>本環境安裝資訊：</p>
<ul>
<li>Minikube v0.22.3</li>
<li>Kubernetes v1.7.5</li>
</ul>
<a id="more"></a>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>安裝前需要確認叢集滿足以下幾點：</p>
<ul>
<li>安裝 <code>xhyve driver</code>, <code>VirtualBox</code> 或 <code>VMware Fusion</code>。</li>
<li>安裝 kubectl 工具。</li>
</ul>
<p>以下為<code>Mac OS X</code>下載方式：</p>
<pre><code class="sh">$ curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.7.5/bin/darwin/amd64/kubectl
$ chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/
</code></pre>
<p>如果是<code>Linux</code>則使用以下方式：</p>
<pre><code class="sh">$ curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.7.5/bin/linux/amd64/kubectl
$ chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/
</code></pre>
<h2 id="快速開始"><a href="#快速開始" class="headerlink" title="快速開始"></a>快速開始</h2><p>Minikube 支援了許多作業系統，若是 <code>OS X</code> 的開發者，可以透過該指令安裝：</p>
<pre><code class="sh">$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.22.3/minikube-darwin-amd64
$ chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/
</code></pre>
<p>若是 <code>Linux</code> 開發者則利用以下指令安裝：</p>
<pre><code class="sh">$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.22.3/minikube-linux-amd64
$ chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/
</code></pre>
<p>下載完成後，就可以透過以下指令建立環境：</p>
<pre><code class="sh">$ minikube get-k8s-versions
The following Kubernetes versions are available:
    - v1.8.0
    - v1.7.5
...

$ minikube start
Starting local Kubernetes v1.7.5 cluster...
Starting VM...
Getting VM IP address...
Moving files into cluster...
Setting up certs...
Connecting to cluster...
Setting up kubeconfig...
Starting cluster components...
Kubectl is now configured to use the cluster.
</code></pre>
<p>看到上述資訊表示已完成啟動 Kubernetes 虛擬機，這時候可以透過 kubectl 來查看資訊：</p>
<pre><code class="sh">$ kubectl get node
NAME       STATUS    AGE       VERSION
minikube   Ready     2m        v1.7.5

$ kubectl get po,svc -n kube-system
NAME                                READY     STATUS    RESTARTS   AGE
po/default-http-backend-2jk83       1/1       Running   0          2m
po/kube-addon-manager-minikube      1/1       Running   0          2m
po/kube-dns-1326421443-8br9x        3/3       Running   0          2m
po/kubernetes-dashboard-mdc9f       1/1       Running   0          2m
po/nginx-ingress-controller-dspc0   1/1       Running   0          2m

NAME                       CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE
svc/default-http-backend   10.0.0.237   &lt;nodes&gt;       80:30001/TCP    2m
svc/kube-dns               10.0.0.10    &lt;none&gt;        53/UDP,53/TCP   2m
svc/kubernetes-dashboard   10.0.0.222   &lt;nodes&gt;       80:30000/TCP    2m
</code></pre>
<blockquote>
<p>新版本 Minikube 預設會自動啟動上述 Addons。這時可以透過瀏覽器進入 <a href="http://192.168.99.100:30000/" target="_blank" rel="noopener">Dashboard</a>。</p>
</blockquote>
<p>想啟動 Extra addons 的話，可以透過以下指令來達成：</p>
<pre><code class="sh">$ minikube addons list
$ minikube addons enable heapster
</code></pre>
<blockquote>
<p>若要移除則使用<code>minikube addons disable heapster</code>指令。</p>
</blockquote>
<p>取得虛擬機裡面的 Docker env：</p>
<pre><code class="sh">$ eval $(minikube docker-env)
$ docker version
Client:
 Version:      17.10.0-ce
 API version:  1.23
 Go version:   go1.8.3
 Git commit:   f4ffd25
 Built:        Tue Oct 17 19:00:43 2017
 OS/Arch:      darwin/amd64

Server:
 Version:      1.12.6
 API version:  1.24 (minimum version )
 Go version:   go1.6.4
 Git commit:   78d1802
 Built:        Wed Jan 11 00:23:16 2017
 OS/Arch:      linux/amd64
 Experimental: false
</code></pre>
<p>(option)若想要移除與刪除虛擬機的話，可以透過以下指令進行：</p>
<pre><code class="sh">$ minikube stop
$ minikube delete
</code></pre>
<h2 id="執行簡單測試應用程式"><a href="#執行簡單測試應用程式" class="headerlink" title="執行簡單測試應用程式"></a>執行簡單測試應用程式</h2><p>這邊利用 echoserver 來測試 Minikube 功能，首先透過以下指令啟動一個 Deployment：</p>
<pre><code class="sh">$ kubectl run hello-minikube --image=gcr.io/google_containers/echoserver:1.4 --port=8080
$ kubectl get deploy,po
NAME                    DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deploy/hello-minikube   1         1         1            1           11m

NAME                                READY     STATUS    RESTARTS   AGE
po/hello-minikube-938614450-31rtv   1/1       Running   0          11m
</code></pre>
<p>接著 expose 服務來進行存取：</p>
<pre><code class="sh">$ kubectl expose deployment hello-minikube --type=NodePort
$ kubectl get svc
NAME             CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE
hello-minikube   10.0.0.164   &lt;nodes&gt;       8080:30371/TCP   4s
kubernetes       10.0.0.1     &lt;none&gt;        443/TCP          29m
</code></pre>
<p>最後透過 cURL 來存取服務：</p>
<pre><code class="sh">$ curl $(minikube service hello-minikube --url)
CLIENT VALUES:
client_address=172.17.0.1
command=GET
real path=/
...
</code></pre>
]]></content>
      
        <categories>
            
            <category> Kubernetes </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Docker </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow on Docker]]></title>
      <url>https://kairen.github.io/2016/10/01/tensorflow/tensorflow-docker/</url>
      <content type="html"><![CDATA[<p>本篇主要整理使用 Docker 來執行 TensorFlow 的一些問題，這邊 Google 官方已經提供了相關的映像檔提供使用，因此會簡單說明安裝過程與需求。</p>
<center><img src="/images/tf/docker-tf.png" alt=""></center>

<a id="more"></a>
<p><br></p>
<h2 id="環境準備"><a href="#環境準備" class="headerlink" title="環境準備"></a>環境準備</h2><p>環境採用 Ubuntu 16.04 Desktop 作業系統，然後顯卡是撿朋友不要的來使用，環境硬體資源如下：</p>
<table>
<thead>
<tr>
<th>名稱</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>CPU</td>
<td>i7-4790 CPU @ 3.60GHz</td>
</tr>
<tr>
<td>Memory</td>
<td>32GB</td>
</tr>
<tr>
<td>GPU</td>
<td>GeForce GTX 650</td>
</tr>
</tbody>
</table>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>開始進行 TensorFlow on Docker 之前，需要確認環境已經安裝以下驅動與軟體等。</p>
<ul>
<li>系統安裝了 Docker Engine：</li>
</ul>
<pre><code class="sh">$ curl -fsSL &quot;https://get.docker.com/&quot; | sh
$ sudo iptables -P FORWARD ACCEPT
</code></pre>
<ul>
<li>安裝最新版本 NVIDIA Driver 軟體：</li>
</ul>
<pre><code class="sh">$ sudo add-apt-repository -y ppa:graphics-drivers/ppa
$ sudo apt-get update
$ sudo apt-get install -y nvidia-367
$ sudo dpkg -l | grep nvidia-367
... 375.39-0ubuntu0.16.04.1 ..
</code></pre>
<ul>
<li>編譯與安裝 nvidia-modprobe：</li>
</ul>
<pre><code class="sh">$ sudo apt-get install -y m4
$ git clone &quot;https://github.com/NVIDIA/nvidia-modprobe.git&quot;
$ cd nvidia-modprobe
$ make &amp;&amp; sudo make install
$ sudo nvidia-modprobe -u -c=0
</code></pre>
<ul>
<li>安裝 Nvidia Docker Plugin:</li>
</ul>
<pre><code class="sh">$ wget -P /tmp &quot;https://github.com/NVIDIA/nvidia-docker/releases/download/v1.0.1/nvidia-docker_1.0.1-1_amd64.deb&quot;
$ sudo dpkg -i /tmp/nvidia-docker*.deb &amp;&amp; rm /tmp/nvidia-docker*.deb
$ sudo systemctl start nvidia-docker.service
$ sudo nvidia-docker run --rm nvidia/cuda nvidia-smi
...
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 650     Off  | 0000:01:00.0     N/A |                  N/A |
| 10%   34C    P8    N/A /  N/A |    267MiB /   975MiB |     N/A      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
</code></pre>
<h2 id="利用-Docker-執行-TensorFlow"><a href="#利用-Docker-執行-TensorFlow" class="headerlink" title="利用 Docker 執行 TensorFlow"></a>利用 Docker 執行 TensorFlow</h2><p>TensorFlow on Docker 官方已經提供了相關映像檔，這邊透過單一指令就可以取得該映像檔，並啟動提供使用，以下為只有 CPU 的版本：</p>
<pre><code class="sh">$ docker run -d -p 8888:8888 --name tf-cpu tensorflow/tensorflow
$ docker logs tf-cpu
...
to login with a token:
        http://localhost:8888/?token=7ddd6ef31fed5f22696c1003a905782b9219a6ec9a19b97c
</code></pre>
<blockquote>
<p>這時候就可以登入 <a href="http://localhost:8888" target="_blank" rel="noopener">Jupyter notebook</a>，這邊登入需要<code>token</code>後面的值。</p>
</blockquote>
<p>若要支援 GPU(CUDA) 的容器的話，可以透過以下指令來提供：</p>
<pre><code class="sh">$ nvidia-docker run -d -p 8888:8888 --name tf-gpu tensorflow/tensorflow:latest-gpu
$ docker logs tf-cpu
</code></pre>
<blockquote>
<p>其他版本可以參考 <a href="https://hub.docker.com/r/tensorflow/tensorflow/tags/" target="_blank" rel="noopener">tags</a>。</p>
</blockquote>
<h2 id="利用-Docker-提供-Serving"><a href="#利用-Docker-提供-Serving" class="headerlink" title="利用 Docker 提供 Serving"></a>利用 Docker 提供 Serving</h2><p>TensorFlow Serving 是靈活、高效能的機器學習模型服務系統，是專門為生產環境而設計的，它可以很簡單部署新的演算法與實驗來提供同樣的架構與 API 進行服務。</p>
<p>首先我們下載官方寫好的 <a href="https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/tools/docker/Dockerfile.devel" target="_blank" rel="noopener">Dockerfile </a> 來進行建置：</p>
<pre><code class="sh">$ mkdir serving &amp;&amp; cd serving
$ wget &quot;https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/tools/docker/Dockerfile.devel&quot;
$ sed -i &#39;s/BAZEL_VERSION.*0.4.2/BAZEL_VERSION 0.4.5/g&#39; Dockerfile.devel
$ docker build --pull -t kyle/serving:0.1.0 -f Dockerfile.devel .
</code></pre>
<p>建置完成映像檔後，透過以下指令執行，並在容器內建置 Serving：</p>
<pre><code class="sh">$ docker run -itd --name=tf-serving kyle/serving:0.1.0
$ docker exec -ti tf-serving bash
root@459a89a3cf5a$ git clone --recurse-submodules &quot;https://github.com/tensorflow/serving&quot;
root@459a89a3cf5a$ cd serving/tensorflow
root@459a89a3cf5a$ ./configure
root@459a89a3cf5a$ cd .. &amp;&amp; bazel build -c opt tensorflow_serving/...
</code></pre>
<p>當建置完 Serving 後，就可以透過以下指令來確認是否正確：</p>
<pre><code class="sh">root@459a89a3cf5a$ bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server
usage: bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server
Flags:
    --port=8500                          int32    port to listen on
    --enable_batching=false
...
</code></pre>
<p>接著使用 Inception v3 模型來提供服務，透過以下步驟來完成：</p>
<pre><code class="sh">root@459a89a3cf5a$ curl -O &quot;http://download.tensorflow.org/models/image/imagenet/inception-v3-2016-03-01.tar.gz&quot;
root@459a89a3cf5a$ tar xzf inception-v3-2016-03-01.tar.gz
root@459a89a3cf5a$ ls inception-v3
README.txt  checkpoint  model.ckpt-157585

root@459a89a3cf5a$ bazel-bin/tensorflow_serving/example/inception_saved_model --checkpoint_dir=inception-v3 --output_dir=inception-export
Successfully exported model to inception-export

root@459a89a3cf5a$ ls inception-export
1
</code></pre>
<p>當完成匯入後離開容器，並 commit 成新版本映像檔：</p>
<pre><code class="sh">$ docker commit tf-serving kyle/serving-inception:0.1.0
$ docker images
REPOSITORY               TAG                 IMAGE ID            CREATED             SIZE
kyle/serving-inception   0.1.0               1d866ff60d38        3 minutes ago       5.55 GB
</code></pre>
<p>接著執行剛 commit 的映像檔，並啟動 Serving 服務：</p>
<pre><code class="sh">$ docker run -it kyle/serving-inception:0.1.0
root@5b9a89eeef5a$ cd serving
root@5b9a89eeef5a$ bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9000 --model_name=inception --model_base_path=inception-export &amp;&gt; inception_log &amp;
[1] 15
</code></pre>
<p>最後透過 <code>inception_client.py</code> 來測試功能：</p>
<pre><code class="sh">root@5b9a89eeef5a$ curl &quot;https://s-media-cache-ak0.pinimg.com/736x/32/00/3b/32003bd128bebe99cb8c655a9c0f00f5.jpg&quot; --output rabbit.jpg
root@5b9a89eeef5a$ bazel-bin/tensorflow_serving/example/inception_client --server=localhost:9000 --image=rabbit.jpg

outputs {
  key: &quot;classes&quot;
  value {
    dtype: DT_STRING
    tensor_shape {
      dim {
        size: 1
      }
      dim {
        size: 5
      }
    }
    string_val: &quot;hare&quot;
    string_val: &quot;wood rabbit, cottontail, cottontail rabbit&quot;
    string_val: &quot;Angora, Angora rabbit&quot;
    string_val: &quot;mouse, computer mouse&quot;
    string_val: &quot;gazelle&quot;
  }
}
outputs {
  key: &quot;scores&quot;
  value {
    dtype: DT_FLOAT
    tensor_shape {
      dim {
        size: 1
      }
      dim {
        size: 5
      }
    }
    float_val: 10.3059120178
    float_val: 8.19226741791
    float_val: 4.00839996338
    float_val: 2.34308481216
    float_val: 2.00992465019
  }
}
</code></pre>
]]></content>
      
        <categories>
            
            <category> TensorFlow </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Docker </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[只要用 kubeadm 小朋友都能部署 Kubernetes]]></title>
      <url>https://kairen.github.io/2016/09/29/kubernetes/deploy/kubeadm/</url>
      <content type="html"><![CDATA[<p><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/" target="_blank" rel="noopener">kubeadm</a>是 Kubernetes 官方推出的部署工具，該工具實作類似 Docker swarm 一樣的部署方式，透過初始化 Master 節點來提供給 Node 快速加入，kubeadm 目前屬於測試環境用階段，但隨著時間推移會越來越多功能被支援，這邊可以看 <a href="https://github.com/kubernetes/kubeadm" target="_blank" rel="noopener">kubeadm Roadmap</a> 來更進一步知道功能發展狀態。</p>
<blockquote>
<p>若想利用 Ansible 安裝的話，可以參考這邊 <a href="https://github.com/kairen/kubeadm-ansible" target="_blank" rel="noopener">kubeadm-ansible</a>。</p>
</blockquote>
<p>本環境安裝資訊：</p>
<ul>
<li>Kubernetes v1.9.6</li>
<li>Etcd v3</li>
<li>Flannel v0.9.1</li>
<li>Docker v18.02.0-ce</li>
</ul>
<a id="more"></a>
<h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本次安裝作業系統採用<code>Ubuntu 16.04 Server</code>，測試環境為 Vagrant with Libvirt：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Role</th>
<th>CPU</th>
<th>Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.16.35.12</td>
<td>master1</td>
<td>1</td>
<td>2G</td>
</tr>
<tr>
<td>172.16.35.10</td>
<td>node1</td>
<td>1</td>
<td>2G</td>
</tr>
<tr>
<td>172.16.35.11</td>
<td>node2</td>
<td>1</td>
<td>2G</td>
</tr>
</tbody>
</table>
<blockquote>
<p>目前 kubeadm 只支援在<code>Ubuntu 16.04+</code>、<code>CentOS 7</code>與<code>HypriotOS v1.0.1+</code>作業系統上使用。</p>
</blockquote>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>安裝前需要確認叢集滿足以下幾點：</p>
<ul>
<li>所有節點網路可以溝通。</li>
<li>所有節點需要設定 APT Docker Repository：</li>
</ul>
<pre><code class="sh">$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
$ sudo add-apt-repository \
   &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable&quot;
</code></pre>
<blockquote>
<p>CentOS 7 EPEL 有支援 Docker Package:</p>
<pre><code class="sh">$ sudo yum install -y epel-release
</code></pre>
</blockquote>
<ul>
<li>所有節點需要設定 APT 與 YUM Kubernetes Repository：</li>
</ul>
<pre><code class="sh">$ curl -s &quot;https://packages.cloud.google.com/apt/doc/apt-key.gpg&quot; | sudo apt-key add -
$ echo &quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&quot; | sudo tee /etc/apt/sources.list.d/kubernetes.list
</code></pre>
<blockquote>
<p>若是 CentOS 7 則執行以下方式：</p>
<pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
       https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
</code></pre>
</blockquote>
<ul>
<li>CentOS 7 要額外確認 SELinux 或 Firewall 關閉。</li>
<li>Kubernetes v1.8+ 要求關閉系統 Swap，若不關閉則需要修改 kubelet 設定參數，這邊可以利用以下指令關閉：</li>
</ul>
<pre><code class="sh">$ swapoff -a &amp;&amp; sysctl -w vm.swappiness=0

# 不同機器有差異
$ sed &#39;/swap.img/d&#39; -i  /etc/fstab
</code></pre>
<blockquote>
<p>記得<code>/etc/fstab</code>也要註解掉<code>SWAP</code>掛載。</p>
</blockquote>
<h2 id="Kubernetes-Master-建立"><a href="#Kubernetes-Master-建立" class="headerlink" title="Kubernetes Master 建立"></a>Kubernetes Master 建立</h2><p>首先更新 APT 來源，並且安裝 Kubernetes 元件與工具：</p>
<pre><code class="sh">$ export KUBE_VERSION=&quot;1.9.6&quot;
$ sudo apt-get update &amp;&amp; sudo apt-get install -y kubelet=${KUBE_VERSION}-00 kubeadm=${KUBE_VERSION}-00 kubectl=${KUBE_VERSION}-00 docker-ce
</code></pre>
<p>進行初始化 Master，這邊需要進入<code>root</code>使用者執行以下指令：</p>
<pre><code class="sh">$ sudo su -
$ kubeadm token generate
b0f7b8.8d1767876297d85c

$ kubeadm init --service-cidr 10.96.0.0/12 \
               --kubernetes-version v${KUBE_VERSION} \
               --pod-network-cidr 10.244.0.0/16 \
               --token b0f7b8.8d1767876297d85c \
               --apiserver-advertise-address 172.16.35.12
# output               
...
kubeadm join --token b0f7b8.8d1767876297d85c 172.16.35.12:6443 --discovery-token-ca-cert-hash sha256:739d936954a752d44d2f2282dd645083259826f2c24a651608a6ac2081106cd7
</code></pre>
<p>當出現如上面資訊後，表示 Master 初始化成功，不過這邊還是一樣透過 kubectl 測試一下：</p>
<pre><code class="sh">$ mkdir ~/.kube &amp;&amp; cp /etc/kubernetes/admin.conf ~/.kube/config
$ kubectl get node
NAME       STATUS     ROLES     AGE       VERSION
master1    NotReady   master    4m        v1.9.6
</code></pre>
<p>當執行正確後要接著部署網路，但要注意<code>一個叢集只能用一種網路</code>，這邊採用 Flannel：</p>
<pre><code class="sh">$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml
clusterrole &quot;flannel&quot; created
clusterrolebinding &quot;flannel&quot; created
serviceaccount &quot;flannel&quot; configured
configmap &quot;kube-flannel-cfg&quot; configured
daemonset &quot;kube-flannel-ds&quot; configured
</code></pre>
<blockquote>
<ul>
<li>若參數 <code>--pod-network-cidr=10.244.0.0/16</code> 改變時，在<code>kube-flannel.yml</code>檔案也需修改<code>net-conf.json</code>欄位的 CIDR。</li>
<li>若使用 Virtualbox 的話，請修改<code>kube-flannel.yml</code>中的 command 綁定 iface，如<code>command: [ &quot;/opt/bin/flanneld&quot;, &quot;--ip-masq&quot;, &quot;--kube-subnet-mgr&quot;, &quot;--iface=eth1&quot; ]</code>。</li>
<li>其他 Pod Network 可以參考 <a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network" target="_blank" rel="noopener">Installing a pod network</a>。</li>
</ul>
</blockquote>
<p>確認 Flannel 部署正確：</p>
<pre><code class="sh">$ kubectl get po -n kube-system
NAME                                         READY     STATUS    RESTARTS   AGE
kube-flannel-ds-3b66l                        1/1       Running   0          9s
kube-flannel-ds-m6874                        1/1       Running   0          9s
kube-flannel-ds-vmb38                        1/1       Running   0          9s
...

$ ip -4 a show flannel.1
5: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default
    inet 10.244.0.0/32 scope global flannel.1
       valid_lft forever preferred_lft forever

$ route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
10.244.0.0      0.0.0.0         255.255.0.0     U     0      0        0 flannel.1
</code></pre>
<h2 id="Kubernetes-Node-建立"><a href="#Kubernetes-Node-建立" class="headerlink" title="Kubernetes Node 建立"></a>Kubernetes Node 建立</h2><p>首先更新 APT 來源，並且安裝 Kubernetes 元件與工具：</p>
<pre><code class="sh">$ export KUBE_VERSION=&quot;1.9.6&quot;
$ sudo apt-get update &amp;&amp; sudo apt-get install -y kubelet=${KUBE_VERSION}-00 kubeadm=${KUBE_VERSION}-00 docker-ce
</code></pre>
<p>完成後就可以開始加入 Node，這邊需要進入<code>root</code>使用者執行以下指令：</p>
<pre><code class="sh">$ kubeadm join --token b0f7b8.8d1767876297d85c 172.16.35.12:6443 --discovery-token-ca-cert-hash sha256:739d936954a752d44d2f2282dd645083259826f2c24a651608a6ac2081106cd7
# output
...
Run &#39;kubectl get nodes&#39; on the master to see this machine join.
</code></pre>
<p>回到<code>master1</code>查看節點狀態：</p>
<pre><code class="sh">$ kubectl get node
NAME      STATUS    ROLES     AGE       VERSION
master1   Ready     master    10m       v1.9.6
node1     Ready     &lt;none&gt;    9m        v1.9.6
node2     Ready     &lt;none&gt;    9m        v1.9.6
</code></pre>
<p>為了多加利用資源這邊透過 taint 來讓 masters 也會被排程執行容器：</p>
<pre><code class="sh">$ kubectl taint nodes --all node-role.kubernetes.io/master-
</code></pre>
<h2 id="Add-ons-建立"><a href="#Add-ons-建立" class="headerlink" title="Add-ons 建立"></a>Add-ons 建立</h2><p>當完成後就可以建立一些 Addons，如 Dashboard。這邊執行以下指令進行建立：</p>
<pre><code class="sh">$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
</code></pre>
<blockquote>
<p>Dashboard 1.7.x 版本有做一些改變，會用到 SSL Cert，可參考這邊 <a href="https://github.com/kubernetes/dashboard/wiki/Installation" target="_blank" rel="noopener">Installation</a>。</p>
</blockquote>
<p>確認沒問題後，透過 kubectl 查看：</p>
<pre><code class="sh">kubectl get svc --namespace=kube-system
NAME                   CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE
kube-dns               10.96.0.10       &lt;none&gt;        53/UDP,53/TCP   36m
kubernetes-dashboard   10.111.162.184   &lt;nodes&gt;       80:32546/TCP    33s
</code></pre>
<p>最後就可以存取 <a href="https://172.16.35.12:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/" target="_blank" rel="noopener">Kube Dashboard</a>。</p>
<p>在 1.7 版本以後的 Dashboard 將不再提供所有權限，因此需要建立一個 service account 來綁定 cluster-admin role：</p>
<pre><code class="sh">$ kubectl -n kube-system create sa dashboard
$ kubectl create clusterrolebinding dashboard --clusterrole cluster-admin --serviceaccount=kube-system:dashboard
$ kubectl -n kube-system get sa dashboard -o yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: 2017-11-27T17:06:41Z
  name: dashboard
  namespace: kube-system
  resourceVersion: &quot;69076&quot;
  selfLink: /api/v1/namespaces/kube-system/serviceaccounts/dashboard
  uid: 56b880bf-d395-11e7-9528-448a5ba4bd34
secrets:
- name: dashboard-token-vg52j

$ kubectl -n kube-system describe secrets dashboard-token-vg52j
...
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtdG9rZW4tdmc1MmoiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNTZiODgwYmYtZDM5NS0xMWU3LTk1MjgtNDQ4YTViYTRiZDM0Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZCJ9.bVRECfNS4NDmWAFWxGbAi1n9SfQ-TMNafPtF70pbp9Kun9RbC3BNR5NjTEuKjwt8nqZ6k3r09UKJ4dpo2lHtr2RTNAfEsoEGtoMlW8X9lg70ccPB0M1KJiz3c7-gpDUaQRIMNwz42db7Q1dN7HLieD6I4lFsHgk9NPUIVKqJ0p6PNTp99pBwvpvnKX72NIiIvgRwC2cnFr3R6WdUEsuVfuWGdF-jXyc6lS7_kOiXp2yh6Ym_YYIr3SsjYK7XUIPHrBqWjF-KXO_AL3J8J_UebtWSGomYvuXXbbAUefbOK4qopqQ6FzRXQs00KrKa8sfqrKMm_x71Kyqq6RbFECsHPA
</code></pre>
<blockquote>
<p>複製<code>token</code>，然後貼到 Kubernetes dashboard。</p>
</blockquote>
<h2 id="簡單部署一個服務"><a href="#簡單部署一個服務" class="headerlink" title="簡單部署一個服務"></a>簡單部署一個服務</h2><p>這邊利用 Weave 公司提供的服務來驗證系統，透過以下方式建立：</p>
<pre><code class="sh">$ kubectl create namespace sock-shop
$ kubectl apply -n sock-shop -f &quot;https://github.com/microservices-demo/microservices-demo/blob/master/deploy/kubernetes/complete-demo.yaml?raw=true&quot;
</code></pre>
<p>接著透過 kubectl 查看資訊：</p>
<pre><code class="sh">$ kubectl describe svc front-end -n sock-shop
$ kubectl get pods -n sock-shop
</code></pre>
<p>最後存取 <a href="http://172.16.35.12:30001" target="_blank" rel="noopener">http://172.16.35.12:30001</a> 即可看到服務的 Frontend。</p>
<h2 id="移除節點"><a href="#移除節點" class="headerlink" title="移除節點"></a>移除節點</h2><p>最後，若要將現有節點移除的話，kubeadm 已經有內建的指令來完成這件事，只要執行以下即可：</p>
<pre><code class="sh">$ kubeadm reset
</code></pre>
]]></content>
      
        <categories>
            
            <category> Kubernetes </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> Ubuntu </tag>
            
            <tag> CentOS </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Go 語言環境安裝]]></title>
      <url>https://kairen.github.io/2016/08/19/golang/go-install/</url>
      <content type="html"><![CDATA[<p>Go 語言是 Google 開發的該世代 C 語言，延續 C 語言的一些優點，是一種靜態強刑別、編譯型，且具有並行機制與垃圾回收功能的語言。由於其<code>並行機制</code>讓 Go 在撰寫多執行緒與網路程式都非常容易。值得一提的是 Go 語言的設計者也包含過去設計 C 語言的 <a href="https://en.wikipedia.org/wiki/Ken_Thompson" target="_blank" rel="noopener">Ken Thompson</a>。目前 Go 語言基於 1.x 每半年發布一個版本。</p>
<a id="more"></a>
<h2 id="Go-語言安裝"><a href="#Go-語言安裝" class="headerlink" title="Go 語言安裝"></a>Go 語言安裝</h2><p>Go 語言安裝非常容易，目前已支援多個平台的作業系統，以下針對幾個常見的作業系統進行教學。</p>
<blockquote>
<p>P.S. 以下教學皆使用 64 bit 進行安裝。</p>
</blockquote>
<h3 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h3><p>首先透過網路下載 Go 語言的壓縮檔：</p>
<pre><code class="sh">$ wget https://storage.googleapis.com/golang/go1.8.linux-amd64.tar.gz
</code></pre>
<p>然後將壓縮檔內的資料全部解壓縮到<code>/usr/local</code>底下：</p>
<pre><code class="sh">$ sudo tar -C /usr/local -xzf go1.8.linux-amd64.tar.gz
</code></pre>
<p>之後編輯<code>.bashrc</code>檔案，在最下面加入以下內容：</p>
<pre><code>export GOROOT=/usr/local/go
export GOPATH=${HOME}/go
export PATH=$PATH:$GOPATH/bin:$GOROOT/bin
</code></pre><h3 id="Mac-OS-X"><a href="#Mac-OS-X" class="headerlink" title="Mac OS X"></a>Mac OS X</h3><p>Mac OS X 安裝可以透過官方封裝好的檔案來進行安裝，下載 <a href="https://storage.googleapis.com/golang/go1.8.darwin-amd64.pkg" target="_blank" rel="noopener">go1.8.darwin-amd64.pkg</a>，然後雙擊進行安裝。</p>
<p>完成後，編輯<code>.bashrc</code>檔案來加入套件的安裝路徑：</p>
<pre><code>export PATH=$PATH:/usr/local/go/bin
export GOPATH=~/Go
export PATH=$PATH:$GOPATH/bin
</code></pre><h3 id="簡單入門"><a href="#簡單入門" class="headerlink" title="簡單入門"></a>簡單入門</h3><p>建立目錄<code>hello-go</code>，並新增檔案<code>hello.go</code>：</p>
<pre><code class="sh">$ mkdir hello-go
$ cd hello-go &amp;&amp; touch hello.go
</code></pre>
<p>接著編輯<code>hello.go</code>加入以下內容：</p>
<pre><code class="go">package main
import &quot;fmt&quot;

func main() {
   fmt.Println(&quot;Hello world, GO !&quot;)
}
</code></pre>
<p>完成後執行以下指令：</p>
<pre><code class="sh">$ go build
$ ./hello
Hello world, GO !
</code></pre>
<h2 id="其他-Framework-與網站"><a href="#其他-Framework-與網站" class="headerlink" title="其他 Framework 與網站"></a>其他 Framework 與網站</h2><p>以下整理相關 Go 語言的套件與不錯網站。</p>
<table>
<thead>
<tr>
<th>種類</th>
<th>名稱</th>
</tr>
</thead>
<tbody>
<tr>
<td>Web 框架</td>
<td>Beego、Martini、Gorilla、GoCraft、Net/HTTP、Revel、girl、XWeb、go-start、goku、web.go</td>
</tr>
<tr>
<td>系統處理框架</td>
<td>apifs、goIRC</td>
</tr>
<tr>
<td>影音處理</td>
<td>Gopher-Talkie、Videq</td>
</tr>
<tr>
<td>Social 框架</td>
<td>ChannelMail.io</td>
</tr>
</tbody>
</table>
<h2 id="參考資訊"><a href="#參考資訊" class="headerlink" title="參考資訊"></a>參考資訊</h2><ul>
<li><a href="http://gophergala.com/blog/gopher/gala/2015/01/31/finalists/" target="_blank" rel="noopener">Gopher Gala 2015 Finalists</a></li>
<li><a href="https://github.com/showcases/web-application-frameworks" target="_blank" rel="noopener">Web application frameworks</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Golang </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Golang </tag>
            
            <tag> OS X </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[簡單部署 DC/OS 於 CentOS 上]]></title>
      <url>https://kairen.github.io/2016/08/17/data-engineer/dcos-install/</url>
      <content type="html"><![CDATA[<p>DC/OS(Data Center Operating System，資料中心作業系統)是 Mesosphere 公司開源的系統，該平台提供了諸多巨量資料處理框架與系統的建置，並以分散式系統 Mesos 作為核心，提供系統資源的隔離與調度，使用者可以根據需求與策略來應用系統資源。</p>
<p>而本篇將說明如何透過 UI 與 CLI 進行安裝 DC/OS。</p>
<a id="more"></a>
<h2 id="節點配置"><a href="#節點配置" class="headerlink" title="節點配置"></a>節點配置</h2><p>DC/OS 最低需求要四台主機，以下為本次安裝的硬體設備：</p>
<table>
<thead>
<tr>
<th>Role</th>
<th>RAM</th>
<th>Disk</th>
<th>CPUs</th>
<th>IP Address</th>
</tr>
</thead>
<tbody>
<tr>
<td>bootstrap</td>
<td>16 GB 記憶體</td>
<td>250 GB 儲存空間</td>
<td>四核處理器</td>
<td>10.0.0.101</td>
</tr>
<tr>
<td>master</td>
<td>8 GB 記憶體</td>
<td>250 GB 儲存空間</td>
<td>四核處理器</td>
<td>10.0.0.102</td>
</tr>
<tr>
<td>agent-1</td>
<td>8 GB 記憶體</td>
<td>250 GB 儲存空間</td>
<td>四核處理器</td>
<td>10.0.0.103</td>
</tr>
<tr>
<td>agent-2</td>
<td>8 GB 記憶體</td>
<td>250 GB 儲存空間</td>
<td>四核處理器</td>
<td>10.0.0.104</td>
</tr>
</tbody>
</table>
<p>請以上節點都分別安裝 RHEL 或者 CentOS 作業系統。並且設定 IP 為靜態固定，編輯<code>/etc/sysconfig/network-scripts/ifcfg-&lt;name&gt;</code>檔案，加入以下內容：</p>
<pre><code class="sh">ONBOOT=&quot;yes&quot;
IPADDR=&quot;10.0.0.104&quot;
PREFIX=&quot;24&quot;
GATEWAY=&quot;10.0.0.1&quot;
DNS1=&quot;8.8.8.8&quot;
DNS2=&quot;8.8.8.4&quot;
</code></pre>
<h2 id="安裝前準備"><a href="#安裝前準備" class="headerlink" title="安裝前準備"></a>安裝前準備</h2><p>在開始安裝以前，首先需要在每一台節點將基本環境的軟體更新：</p>
<pre><code class="sh">$ sudo yum upgrade -y
</code></pre>
<blockquote>
<p>完成後檢查是否是最新版本，可以透過以下方式查看 Kernel：</p>
<pre><code class="sh">$ uname -r
3.10.0-327.13.1.el7.x86_64
</code></pre>
<p>如果不是以上版本，請執行以下指令：</p>
<pre><code class="sh">$ sudo yum upgrade --assumeyes --tolerant
$ sudo yum update --assumeyes
</code></pre>
</blockquote>
<p>由於在 CentOS 與 RHEL 預設會開啟防火牆，故要關閉防火牆與開機時自動啟動：</p>
<pre><code class="sh">$ sudo systemctl stop firewalld &amp;&amp; sudo systemctl disable firewalld
</code></pre>
<p>接著安裝一些基本工具軟體：</p>
<pre><code class="sh">$ sudo yum install -y tar xz unzip curl ipset vim
</code></pre>
<p>設定啟動 OverlayFS :</p>
<pre><code class="sh">$ sudo tee /etc/modules-load.d/overlay.conf &lt;&lt;-&#39;EOF&#39;
overlay
EOF
</code></pre>
<p>設定關閉 SELinux 與設定一些資訊，並重新啟動：</p>
<pre><code class="sh">$ sudo sed -i s/SELINUX=enforcing/SELINUX=permissive/g /etc/selinux/config &amp;&amp;
sudo groupadd nogroup &amp;&amp;
sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1 &amp;&amp;
sudo sysctl -w net.ipv6.conf.default.disable_ipv6=1 &amp;&amp;
sudo reboot
</code></pre>
<p>完成重新啟動後，在每一台節點安裝 Docker，首先要取得 Repos，設定以下來讓 yum 可以抓取：</p>
<pre><code class="sh">$ sudo tee /etc/yum.repos.d/docker.repo &lt;&lt;-&#39;EOF&#39;
[dockerrepo]
name=Docker Repository
baseurl=https://yum.dockerproject.org/repo/main/centos/$releasever/
enabled=1
gpgcheck=1
gpgkey=https://yum.dockerproject.org/gpg
EOF
</code></pre>
<p>設定 systemd 執行 Docker daemon 於 OverlayFS：</p>
<pre><code class="sh">$ sudo mkdir -p /etc/systemd/system/docker.service.d &amp;&amp; sudo tee /etc/systemd/system/docker.service.d/override.conf &lt;&lt;- EOF
[Service]
ExecStart=
ExecStart=/usr/bin/docker daemon --storage-driver=overlay -H fd://
EOF
</code></pre>
<p>安裝 Docker engine，並啟動 docker 與設定開機啟動：</p>
<pre><code class="sh">$ sudo yum install --assumeyes --tolerant docker-engine
$ sudo systemctl start docker
$ sudo systemctl enable docker
</code></pre>
<blockquote>
<p>這邊可以設定使用者加入 docker 群組：</p>
<pre><code class="sh">$ sudo gpasswd -a $(whoami) docker
</code></pre>
</blockquote>
<h2 id="安裝-Bootstrap-Node"><a href="#安裝-Bootstrap-Node" class="headerlink" title="安裝 Bootstrap Node"></a>安裝 Bootstrap Node</h2><p>Bootstrap 節點主要提供佈署的功能，可以採用 UI 或 CLI 來進行部署，以下將說明如何透過建置 Bootstrap 來完成 DC/OS 佈署。</p>
<h3 id="GUI-安裝"><a href="#GUI-安裝" class="headerlink" title="GUI 安裝"></a>GUI 安裝</h3><p>這邊採用 GUI 方式來佈署 DC/OS，首先下載 <a href="https://downloads.dcos.io/dcos/EarlyAccess/dcos_generate_config.sh?_ga=1.252969481.1283195233.1461920094" target="_blank" rel="noopener">DC/OS installer</a>：</p>
<pre><code class="sh">$ wget https://downloads.dcos.io/dcos/EarlyAccess/dcos_generate_config.sh
</code></pre>
<p>接著執行啟動 DC/OS UI：</p>
<pre><code class="sh">$ sudo bash dcos_generate_config.sh --web -v
</code></pre>
<p>完成後，即可開啟瀏覽器輸入 <a href="http://&lt;bootstrap-node-public-ip&gt;:9000" target="_blank" rel="noopener">bootstrap web</a></p>
<p>這邊需要輸入 Bootstrap 節點的 SSH 私有金鑰，透過以下方式產生與印出：</p>
<pre><code class="sh">$ ssh-keygen -t rsa
$ cat .ssh/id_rsa
</code></pre>
<p>並建立一個腳本檔案<code>ip-detect</code>，並加入以下內容：</p>
<pre><code class="sh">#!/bin/bash
set -o nounset -o errexit

IP=&lt;MASTER_IP&gt;

echo $(ip route get ${IP} | awk &#39;{print $NF; exit}&#39;)
</code></pre>
<p>上傳以上資訊，並填入 Master 與 Agent 的 IP Address。</p>
<h3 id="CLI-安裝"><a href="#CLI-安裝" class="headerlink" title="CLI 安裝"></a>CLI 安裝</h3><p>DC/OS 除了可以透過 GUI 進行安裝，也可以透過 CLI 的方式來佈署，首先建立目錄<code>genconf</code>：</p>
<pre><code class="sh">$ mkdir -p dcos_cluster
$ cd dcos_cluster
$ mkdir -p genconf
</code></pre>
<p>然後下載 DC/OS installer 來進行安裝，並且查看所有指令：</p>
<pre><code class="sh">$ wget https://downloads.dcos.io/dcos/EarlyAccess/dcos_generate_config.sh
$ sudo bash dcos_generate_config.sh --help
</code></pre>
<p>建立一個腳本檔案<code>genconf/ip-detect</code>，並加入以下內容：</p>
<pre><code class="sh">#!/bin/bash
set -o nounset -o errexit

IP=&lt;MASTER_IP&gt;

echo $(ip route get ${IP} | awk &#39;{print $NF; exit}&#39;)
</code></pre>
<blockquote>
<p><code>MASTER_IP</code>為修改成主節點 IP。</p>
</blockquote>
<p>接著建立設定檔<code>genconf/config.yaml</code>，並加入以下內容：</p>
<pre><code class="sh">---
agent_list:
- &lt;agent-private-ip-1&gt;
- &lt;agent-private-ip-2&gt;
bootstrap_url: file:///opt/dcos_install_tmp
cluster_name: &quot;DC/OS Cluster&quot;
master_discovery: static
master_list:
- &lt;master-private-ip-1&gt;
resolvers:
- 8.8.4.4
- 8.8.8.8
ssh_port: 22
ssh_user: &lt;username&gt;
</code></pre>
<blockquote>
<p><code>&lt;agent-private-ip-1&gt;</code> 修改成 <code>10.0.0.103</code>。</p>
<p><code>&lt;agent-private-ip-2&gt;</code> 修改成 <code>10.0.0.104</code>。</p>
<p><code>&lt;master-private-ip-1&gt;</code> 修改成 <code>10.0.0.102</code>。</p>
<p><code>&lt;username&gt;</code> 修改成 <code>cloud-user</code>。P.S 這邊是用 cloud image。</p>
</blockquote>
<p>完成後複製 Bootstrap 節點的 SSH 私有金鑰到目錄底下：</p>
<pre><code class="sh">$ cp &lt;path-to-key&gt; genconf/ssh_key &amp;&amp; chmod 0600 genconf/ssh_key
</code></pre>
<p>透過 DC/OS installer 產生設定資訊：</p>
<pre><code class="sh">$ sudo bash dcos_generate_config.sh --genconf
Extracting image from this script and loading into docker daemon, this step can take a few minutes
dcos-genconf.e060aa49ac4ab62d5e-1e14856f55e5d5d07b.tar
Running mesosphere/dcos-genconf docker with BUILD_DIR set to /home/centos/genconf
====&gt; EXECUTING CONFIGURATION GENERATION
...
</code></pre>
<p>成功的話目錄結構會類似以下：</p>
<pre><code>├── dcos-genconf.14509fe1e7899f4395-3a2b7e03c45cd615da.tar
├── dcos_generate_config.sh
└── genconf
    ├── cluster_packages.json
    ├── config.yaml
    ├── ip-detect
    ├── serve
    │   ├── bootstrap
    │   │   ├── 3a2b7e03c45cd615da8dfb1b103943894652cd71.active.json
    │   │   └── 3a2b7e03c45cd615da8dfb1b103943894652cd71.bootstrap.tar.xz
    │   ├── bootstrap.latest
    │   ├── cluster-package-info.json
    │   ├── dcos_install.sh
    │   ├── fetch_packages.sh
    │   └── packages
    │       ├── dcos-config
    │       │   └── dcos-config--setup_6dac2d99011d6219b32d9f66cafa9845b7cf6d74.tar.xz
    │       └── dcos-metadata
    │           └── dcos-metadata--setup_6dac2d99011d6219b32d9f66cafa9845b7cf6d74.tar.xz
    ├── ssh_key
    └── state
</code></pre><p>若沒問題就可以 prerequisites 佈署階段：</p>
<pre><code class="sh">$ sudo bash dcos_generate_config.sh --install-prereqs
Running mesosphere/dcos-genconf docker with BUILD_DIR set to /home/centos/genconf
====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; EXECUTING INSTALL PREREQUISITES
====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; START install_prereqs
</code></pre>
<p>若沒問題就可以執行 preflight 來驗證叢集是否可以安裝：</p>
<pre><code class="sh">$ sudo bash dcos_generate_config.sh --preflight
Running mesosphere/dcos-genconf docker with BUILD_DIR set to /home/centos/genconf
====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; EXECUTING PREFLIGHT
====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; START run_preflight
====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; STAGE preflight
</code></pre>
<p>一樣若沒問題就可以執行 deploy 來安裝 DC/OS：</p>
<pre><code class="sh">$ sudo bash dcos_generate_config.sh --deploy
Running mesosphere/dcos-genconf docker with BUILD_DIR set to /home/centos/genconf
====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; EXECUTING DC/OS INSTALLATION
====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; START deploy_master
</code></pre>
<p>最後沒問題就可以執行 postflight 來確認服務是否有啟動：</p>
<pre><code class="sh">$ sudo bash dcos_generate_config.sh --postflight
Running mesosphere/dcos-genconf docker with BUILD_DIR set to /home/centos/genconf
====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; EXECUTING POSTFLIGHT
====&gt; dcos_installer.action_lib.prettyprint:: ====&gt; START run_postflight
</code></pre>
<p>都完成後就可以查看 <a href="http://&lt;master-public-ip&gt;:8181/exhibitor/v1/ui/index.html" target="_blank" rel="noopener">Zookpeer</a> 與 <a href="http://&lt;public-master-ip&gt;/" target="_blank" rel="noopener">DC/OS</a>。</p>
]]></content>
      
        <categories>
            
            <category> Spark </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Spark </tag>
            
            <tag> DC/OS </tag>
            
            <tag> Mesos </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[OpenStack Kolla 初體驗]]></title>
      <url>https://kairen.github.io/2016/07/21/openstack/kolla-ansible/</url>
      <content type="html"><![CDATA[<p>Kolla 提供 OpenStack 生產環境就緒的容器與部署工具，其具備快速、擴展、可靠等特性，並提供社群版本的最佳本版升級實現。</p>
<a id="more"></a>
<h2 id="節點配置"><a href="#節點配置" class="headerlink" title="節點配置"></a>節點配置</h2><p>本安裝將使用三台實體主機與一台虛擬機器來進行簡單叢集部署，主機規格如以下所示：</p>
<table>
<thead>
<tr>
<th>Role</th>
<th>RAM</th>
<th>CPUs</th>
<th>Disk</th>
<th>IP Address</th>
</tr>
</thead>
<tbody>
<tr>
<td>controller1</td>
<td>8 GB</td>
<td>4vCPU</td>
<td>250 GB</td>
<td>10.0.0.11</td>
</tr>
<tr>
<td>network1</td>
<td>2 GB</td>
<td>1vCPU</td>
<td>250 GB</td>
<td>10.0.0.21</td>
</tr>
<tr>
<td>compute1</td>
<td>8 GB</td>
<td>8vCPU</td>
<td>500 GB</td>
<td>10.0.0.31</td>
</tr>
<tr>
<td>docker-registry</td>
<td>2 GB</td>
<td>1vCPU</td>
<td>50 GB</td>
<td>10.0.0.90</td>
</tr>
</tbody>
</table>
<blockquote>
<p>作業系統皆為<code>Ubuntu Server 14.04</code>。</p>
</blockquote>
<p>另外每台實體主機的網卡網路分別為以下：</p>
<table>
<thead>
<tr>
<th>Role</th>
<th>Management</th>
<th>Tunnel</th>
<th>Public</th>
</tr>
</thead>
<tbody>
<tr>
<td>controller1</td>
<td>eth0</td>
<td>eth1</td>
<td>eth2</td>
</tr>
<tr>
<td>network1</td>
<td>eth0</td>
<td>eth1</td>
<td>eth2</td>
</tr>
<tr>
<td>compute1</td>
<td>eth0</td>
<td>eth1</td>
<td>eth2</td>
</tr>
</tbody>
</table>
<p>網卡若是實體主機，請設定為固定 IP，如以下：</p>
<pre><code>auto eth0
iface eth0 inet static
           address 10.0.0.11
           netmask    255.255.255.0
           gateway    10.0.0.1
           dns-nameservers 8.8.8.8
</code></pre><blockquote>
<p>若想修改主機的網卡名稱，可以編輯<code>/etc/udev/rules.d/70-persistent-net.rules</code>。</p>
</blockquote>
<p>其中<code>network</code>節點的 Public 需設定網卡為以下：</p>
<pre><code>auto &lt;ethx&gt;
iface &lt;ethx&gt; inet manual
        up ip link set dev $IFACE up
        down ip link set dev $IFACE down
</code></pre><h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>在開始部署 OpenStack 之前，我們需要先將一些相依軟體與函式庫安裝完成，並且需要部署一用於存取映像檔的倉庫(Docker registry)。</p>
<h3 id="部署-Docker-Registry"><a href="#部署-Docker-Registry" class="headerlink" title="部署 Docker Registry"></a>部署 Docker Registry</h3><p>首先進入到 <code>docker-registry</code> 節點，本教學採用一台虛擬機作為部署使用，並安裝 Docker engine：</p>
<pre><code class="sh">$ curl -fsSL &quot;https://get.docker.com/&quot; | sh
</code></pre>
<p>安裝完成 Docker engine 後，透過以下指令建置 Docker registry：</p>
<pre><code class="sh">$ docker run -d -p 5000:5000 --restart=always --name registry \
-v $(pwd)/data:/var/lib/registry \
registry:2
</code></pre>
<p>接著為了方便檢視 Docker image，這邊另外部署 Docker registry UI：</p>
<pre><code class="sh">$ docker run -d -p 8080:80 \
-e ENV_DOCKER_REGISTRY_HOST=10.26.1.49 \
-e ENV_DOCKER_REGISTRY_PORT=5000 \
konradkleine/docker-registry-frontend:v2
</code></pre>
<p>完成以上即可透過瀏覽器查看該主機 <code>8080</code> Port。也可以透過以下指令檢查是否部署成功：</p>
<pre><code class="sh">$ docker pull ubuntu:14.04
$ docker tag ubuntu:14.04 localhost:5000/ubuntu:14.04
$ docker push localhost:5000/ubuntu:14.04

The push refers to a repository [localhost:5000/ubuntu]
447f88c8358f: Pushed
df9a135a6949: Pushed
...
</code></pre>
<blockquote>
<p>其他 Docker registry 列表：</p>
<ul>
<li><a href="https://github.com/SUSE/Portus" target="_blank" rel="noopener">Portus</a></li>
<li><a href="http://www.projectatomic.io/registry/" target="_blank" rel="noopener">Atomic Registry</a></li>
<li><a href="https://docs.rancher.com/os/configuration/private-registries/" target="_blank" rel="noopener">Private Registries in RancherOS</a></li>
</ul>
</blockquote>
<h3 id="部署節點準備"><a href="#部署節點準備" class="headerlink" title="部署節點準備"></a>部署節點準備</h3><p>當完成上述後，即可進行部署節點的相依軟體安裝，首先在<code>每台節點</code>透過以下指令更新與安裝一些套件：</p>
<pre><code class="sh">$ sudo apt-get update -y &amp;&amp; sudo apt-get upgrade -y
$ sudo apt-get install -y python-pip python-dev
$ curl -fsSL &quot;https://get.docker.com/&quot; | sh
$ sudo timedatectl set-timezone Asia/Taipei
</code></pre>
<blockquote>
<p>由於使用<code>Ubuntu server 14.04</code>，故需要更新 Kernel:</p>
<pre><code class="sh">$ sudo apt-get install -y linux-image-generic-lts-wily
</code></pre>
</blockquote>
<p>接著更新 pip，並安裝 docker-python 函式庫：</p>
<pre><code class="sh">$ sudo pip install -U pip docker-py
</code></pre>
<p>編輯每台節點的<code>/etc/default/docker</code>檔案，加入以下內容：</p>
<pre><code>DOCKER_OPTS=&quot;--insecure-registry &lt;registry-ip&gt;:5000&quot;
</code></pre><blockquote>
<p>這邊<code>{registry-ip}</code>為 Docker registry 的 IP 位址。若使用 Ubuntu Server 16.04 版本的話，需要編輯<code>/lib/systemd/system/docker.service</code>檔案，修改以下：</p>
<pre><code>EnvironmentFile=-/etc/default/%p
ExecStart=/usr/bin/dockerd -H fd:// \
                           $DOCKER_OPTS
</code></pre><p>完成上述後，需要 reload service：</p>
<pre><code class="sh">$ sudo systemctl daemon-reload
</code></pre>
</blockquote>
<p>接著編輯每台節點的<code>/etc/rc.local</code>檔案，加入以下內容：</p>
<pre><code>mount --make-shared /run

# 在 compute 節點額外加入
mount --make-shared /var/lib/nova/mnt
</code></pre><p>接著在<code>compute1</code>節點執行以下指令：</p>
<pre><code class="sh">sudo mkdir -p /var/lib/nova/mnt /var/lib/nova/mnt1
sudo mount --bind /var/lib/nova/mnt1 /var/lib/nova/mnt
sudo mount --make-shared /var/lib/nova/mnt
</code></pre>
<p>完成後<code>重新啟動每台節點</code>。</p>
<p>當上述步驟都完成後，進入到<code>controller1</code>的<code>root</code>使用者執行以下指令安裝與設定額外套件：</p>
<pre><code class="sh">$ sudo apt-get install -y software-properties-common
$ sudo apt-add-repository -y ppa:ansible/ansible &amp;&amp; sudo apt-get update
$ sudo apt-get install -y ansible libffi-dev libssl-dev gcc git
</code></pre>
<blockquote>
<p>NTP 部分可自行設定，這邊不再說明。</p>
</blockquote>
<p>接著繼續在<code>controller1</code>節點的<code>root</code>使用者執行以下指令：</p>
<pre><code class="sh">$ ssh-keygen -t rsa
$ cat .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys
</code></pre>
<p>複製<code>controller1</code>節點的<code>root</code>底下的<code>.ssh/id_rsa.pub</code>公有金鑰到其他節點的<code>root</code>使用者底下的<code>.ssh/authorized_keys</code>。</p>
<h2 id="建立-OpenStack-Kolla-Docker-映像檔"><a href="#建立-OpenStack-Kolla-Docker-映像檔" class="headerlink" title="建立 OpenStack Kolla Docker 映像檔"></a>建立 OpenStack Kolla Docker 映像檔</h2><p>在使用 Kolla 部署 OpenStack 叢集之前，我們需要預先建立用於部署的映像檔，才能進行節點的部署。首先進入到<code>controller1</code>節點，並下載 Kolla 專案：</p>
<pre><code class="sh">$ git clone &quot;https://github.com/openstack/kolla.git&quot; -b stable/newton
$ cd kolla &amp;&amp; pip install .
$ pip install tox python-openstackclient &amp;&amp; tox -e genconfig
$ cp -r etc/kolla /etc/
</code></pre>
<p>接著執行指令進行建立 Docker 映像檔，若不指定名稱預設下將建立全部映像檔，如以下：</p>
<pre><code class="sh">$ kolla-build --base ubuntu --type source --registry {registry-ip}:5000 --push
</code></pre>
<blockquote>
<p>這邊<code>{registry-ip}</code>為 Docker registry 的 IP 位址。</p>
<p>若要改變 OS 與版本，可以編輯<code>/etc/kolla/kolla-build.conf</code>檔案修改以下內容：</p>
<pre><code>base = centos
base_tag = 3.0.3
push = true
install_type = rdo
registry = {registry-ip}:5000
</code></pre><p>可參考官方文件 <a href="http://docs.openstack.org/developer/kolla/image-building.html" target="_blank" rel="noopener">Building Container Images</a>。</p>
</blockquote>
<p>當映像檔建置完成後，即可查看<code>docker-regisrtry</code>節點的 UI 介面，來確認是否上傳成功。</p>
<h2 id="部署-OpenStack-節點"><a href="#部署-OpenStack-節點" class="headerlink" title="部署 OpenStack 節點"></a>部署 OpenStack 節點</h2><p>當完成映像檔建立後，即可開始進行部署 OpenStack 節點。</p>
<p>首先到<code>controller1</code>進入剛下載的<code>kolla-ansible</code>專案目錄，並編輯<code>ansible/inventory/multinode</code>檔案，修改以下內容：</p>
<pre><code class="sh">[control]
controller1

[network]
network1

[compute]
compute1

[storage]
controller1
</code></pre>
<blockquote>
<p>這邊由於機器不夠，所以將 storage 也放到 controller。</p>
</blockquote>
<p>接著編輯<code>/etc/kolla/globals.yml</code>檔案，修改以下內容：</p>
<pre><code class="yml">config_strategy: &quot;COPY_ALWAYS&quot;
kolla_base_distro: &quot;ubuntu&quot;
kolla_install_type: &quot;source&quot;
openstack_release: &quot;3.0.3&quot;
kolla_internal_vip_address: &quot;10.0.0.10&quot;
docker_registry: &quot;10.0.0.90:5000&quot;
network_interface: &quot;eth0&quot;
tunnel_interface: &quot;eth1&quot;
neutron_external_interface: &quot;eth2&quot;
neutron_plugin_agent: &quot;openvswitch&quot;

# OpenStack service
enable_cinder: &quot;yes&quot;
enable_neutron_lbaas: &quot;yes&quot;
cinder_volume_group: &quot;cinder-volumes&quot;
</code></pre>
<blockquote>
<p>若要將 API 開放給 Public network 存取的話，需要設定以下內容：</p>
<pre><code>kolla_external_vip_address: &quot;10.26.1.251&quot;
kolla_external_vip_interface: &quot;eth3&quot;
</code></pre></blockquote>
<p>(Option)建立 Cinder LVM volume group:</p>
<pre><code class="sh">$ sudo apt-get install lvm2 -y
$ sudo pvcreate /dev/sdb
$ sudo vgcreate cinder-volumes /dev/sdb
$ sudo  pvdisplay

--- Physical volume ---
PV Name               /dev/sdb
VG Name               cinder-volumes
PV Size               465.76 GiB / not usable 4.02 MiB
Allocatable           yes
PE Size               4.00 MiB
Total PE              119234
Free PE               119234
Allocated PE          0
PV UUID               5g01hz-Ebds-EVSF-yGm4-evNm-5Kfp-2h8kUR
</code></pre>
<p>然後透過以下指令產生亂數密碼：</p>
<pre><code class="sh">$ kolla-genpwd
</code></pre>
<p>接著要執行預先檢查確認節點是否可以進行部署，透過以下指令：</p>
<pre><code class="sh">$ kolla-ansible prechecks -i ansible/inventory/multinode

PLAY RECAP *********************************************************************
compute1                   : ok=8    changed=0    unreachable=0    failed=0
controller1                : ok=35   changed=0    unreachable=0    failed=0
network1                   : ok=29   changed=0    unreachable=0    failed=0
</code></pre>
<blockquote>
<p>若中途發生錯誤，請檢查錯誤訊息。</p>
</blockquote>
<p>確認沒問題後即可部署 OpenStack，透過以下指令進行：</p>
<pre><code class="sh">$ kolla-ansible deploy -i ansible/inventory/multinode
PLAY RECAP *********************************************************************
compute1                   : ok=44   changed=31   unreachable=0    failed=0
controller1                : ok=154  changed=96   unreachable=0    failed=0
network1                   : ok=37   changed=28   unreachable=0    failed=0
</code></pre>
<blockquote>
<p>P.S. 若發生映像檔無法下載，請自行透過以下指令傳到該節點：</p>
<pre><code class="sh">$ docker save {image} &gt; image.tar
$ scp image.tar network1:~/
$ ssh network1 &quot;docker load &lt; image.tar&quot;
</code></pre>
</blockquote>
<h2 id="驗證服務"><a href="#驗證服務" class="headerlink" title="驗證服務"></a>驗證服務</h2><p>完成後，就可以產生 Credential 檔案來進行系統驗證：</p>
<pre><code class="sh">$ kolla-ansible post-deploy
$ source /etc/kolla/admin-openrc.sh
</code></pre>
<p>透過 OpenStack Client 來檢查 Keystone，並查看所有使用者：</p>
<pre><code class="sh">$ openstack user list

+----------------------------------+-------------------+
| ID                               | Name              |
+----------------------------------+-------------------+
| 19af62000d334c4d9de3aa29b9fd06df | heat_domain_admin |
| 83fa8ba1deff46e58e716470dbdaeb03 | heat              |
| 9661e2a4b1f648a6a3ed5c52f22c52bb | nova              |
| 9fa2a71716d046a893dcbaef3d7375ce | admin             |
| acdfdc4fae2248b0af4c8788a9858cf3 | glance            |
| b683b3c611dd4bdba2c25321a0f03cf0 | cinder            |
| dc57ebf4e6c548e990767f00ffda19d8 | neutron           |
+----------------------------------+-------------------+
</code></pre>
<p>透過 OpenStack Client 檢查 Nova 的服務列表：</p>
<pre><code class="sh">$ openstack compute service list

+----+------------------+-------------+----------+---------+-------+----------------------------+-----------------+
| Id | Binary           | Host        | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+----+------------------+-------------+----------+---------+-------+----------------------------+-----------------+
| 3  | nova-consoleauth | controller1 | internal | enabled | up    | 2016-08-16T05:40:16.000000 | -               |
| 5  | nova-scheduler   | controller1 | internal | enabled | up    | 2016-08-16T05:40:20.000000 | -               |
| 6  | nova-conductor   | controller1 | internal | enabled | up    | 2016-08-16T05:40:12.000000 | -               |
| 10 | nova-compute     | compute1    | nova     | enabled | up    | 2016-08-16T05:40:16.000000 | -               |
+----+------------------+-------------+----------+---------+-------+----------------------------+-----------------+
</code></pre>
<p>透過 OpenStack Client 檢查 Neutron 的服務列表：</p>
<pre><code class="sh">$ openstack network agent list
+------------------+------------------+----------+-------------------+-------+----------------+------------------+
| id               | agent_type       | host     | availability_zone | alive | admin_state_up | binary           |
+------------------+------------------+----------+-------------------+-------+----------------+------------------+
| 2254dcb0-9d5b-   | Open vSwitch     | network1 |                   | :-)   | True           | neutron-         |
| 46da-b1f2-b03719 | agent            |          |                   |       |                | openvswitch-     |
| 5913b1           |                  |          |                   |       |                | agent            |
| acb81021-2efd-4b | DHCP agent       | network1 | nova              | :-)   | True           | neutron-dhcp-    |
| b6-9af7-a3df95ce |                  |          |                   |       |                | agent            |
| 479b             |                  |          |                   |       |                |                  |
| b564c005-779d-   | Metadata agent   | network1 |                   | :-)   | True           | neutron-         |
| 45f4-b10f-       |                  |          |                   |       |                | metadata-agent   |
| 788bb788491b     |                  |          |                   |       |                |                  |
| d95d75e5-6429-44 | Open vSwitch     | compute1 |                   | :-)   | True           | neutron-         |
| 65-aef1-a6a9f4e7 | agent            |          |                   |       |                | openvswitch-     |
| 995f             |                  |          |                   |       |                | agent            |
| e0dbbff4-d7a7-4f | L3 agent         | network1 | nova              | :-)   | True           | neutron-l3-agent |
| a2-8fa2-b3d3de42 |                  |          |                   |       |                |                  |
| d899             |                  |          |                   |       |                |                  |
+------------------+------------------+----------+-------------------+-------+----------------+------------------+
</code></pre>
<p>從網路上下載一個測試用映像檔<code>cirros-0.3.4-x86_64</code>，並上傳至 Glance 服務：</p>
<pre><code class="sh">$ wget &quot;http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img&quot;
$ openstack image create &quot;cirros-0.3.4-x86_64&quot; \
--file cirros-0.3.4-x86_64-disk.img \
--disk-format qcow2 --container-format bare \
--public
</code></pre>
<p>這邊可以透過 Neutron client 來查看建立外部網路：</p>
<pre><code class="sh">$ openstack network create --external \
--provider-physical-network physnet1 \
--provider-network-type flat public1

$ openstack subnet create --no-dhcp --network public1 \
--allocation-pool start=10.26.1.230,end=10.26.1.240 \
--subnet-range 10.26.1.0/24 --gateway 10.26.1.254 public1-subnet

$ openstack network create --provider-network-type vxlan admin-net

$ openstack subnet create --subnet-range 192.168.10.0/24 \
--network admin-net --gateway 192.168.10.1 \
--dns-nameserver 8.8.8.8 admin-subnet

$ openstack router create admin-router
$ openstack router add subnet admin-router admin-subnet
$ openstack router set --external-gateway public1 admin-router
</code></pre>
<p>建立 flavor 來提供虛擬機樣板：</p>
<pre><code class="sh">openstack flavor create --id 1 --ram 512 --disk 1 --vcpus 1 m1.tiny
openstack flavor create --id 2 --ram 2048 --disk 20 --vcpus 1 m1.small
openstack flavor create --id 3 --ram 4096 --disk 40 --vcpus 2 m1.medium
openstack flavor create --id 4 --ram 8192 --disk 80 --vcpus 4 m1.large
openstack flavor create --id 5 --ram 16384 --disk 160 --vcpus 8 m1.xlarge
</code></pre>
<p>上傳 Keypair 來提供虛擬機 SSH 登入認證用：</p>
<pre><code class="sh">$ openstack keypair create --public-key ~/.ssh/id_rsa.pub mykey
</code></pre>
<p>開啟一台虛擬機進行測試：</p>
<pre><code class="sh">$ openstack server create --image cirros-0.3.4-x86_64 \
--flavor m1.tiny \
--nic net-id=admin-net \
--key-name mykey \
admin-instance
</code></pre>
<p>查看 instance 列表：</p>
<pre><code class="sh">$ openstack server list

+--------------------------------------+----------------+--------+------------------------+---------------------+
| ID                                   | Name           | Status | Networks               | Image Name          |
+--------------------------------------+----------------+--------+------------------------+---------------------+
| bb3b31b6-56a4-4c68-bd53-b7efb451f0fc | admin-instance | ACTIVE | admin-net=192.168.10.6 | cirros-0.3.4-x86_64 |
+--------------------------------------+----------------+--------+------------------------+---------------------+
</code></pre>
]]></content>
      
        <categories>
            
            <category> OpenStack </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Docker </tag>
            
            <tag> OpenStack </tag>
            
            <tag> Ansible </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[用 Bcache 來加速硬碟效能]]></title>
      <url>https://kairen.github.io/2016/07/05/linux/ubuntu/bcache/</url>
      <content type="html"><![CDATA[<p>Bcache 是按照固態硬碟特性來設計的技術，只按擦除 Bucket 的大小進行分配，並使用 btree 和 journal 混合方法來追蹤快取資料，快取資料可以是 Bucket 上的任意一個 Sector。Bcache 最大程度上減少了隨機寫入的代價，它按循序的方式填充一個 Bucket，重新使用時只需將 Bucket 設置為無效即可。Bcache 也支援了類似 Flashcache 的快取策略，如write-back、write-through 與 write-around。</p>
<a id="more"></a>
<h2 id="安裝與設定"><a href="#安裝與設定" class="headerlink" title="安裝與設定"></a>安裝與設定</h2><p>首先要先安裝 bcache-tools，這邊採用 ubuntu 的<code>apt-get</code>來進行安裝：</p>
<pre><code class="sh">$ sudo add-apt-repository ppa:g2p/storage
$ sudo apt-get update
$ sudo apt-get install -y bcache-tools
</code></pre>
<p>完成安裝後，要準備一顆 SSD 與 HDD，並安裝於同一台主機上，如以下硬碟結構：</p>
<pre><code class="sh">+-------+----------+       +--------+---------+       
| [ 固態硬碟(SSD)]  |       |  [ 傳統硬碟(HDD)]  |       
|  System   disk   +-------+  System   disk   +
|    (/dev/sdb)    |       |    (/dev/sdc)    |
+------------------+       +------------------+
</code></pre>
<p>當確認以上都沒問題後，即可用 bcache 指令來建立快取，首先建立後端儲存裝置：</p>
<pre><code class="sh">$ sudo make-bcache -B /dev/sdc
UUID:            3b62c662-c739-4621-aca3-80efbf5e1da2
Set UUID:        67828232-2427-46d3-a473-e92e1f213f87
version:        1
block_size:        1
data_offset:        16
</code></pre>
<blockquote>
<ul>
<li><code>-C</code>為快取層。</li>
<li><code>-B</code>為 bcache 後端儲存層。</li>
<li><code>--block</code> 為 Block Size，預設為 1k。</li>
<li><code>--discard</code>為 SSD 上使用 TRIM。</li>
<li><code>--writeback</code>為使用 writeback 模式，預設為 writethrough。</li>
</ul>
<p>P.S 如果有任何錯誤，請使用以下指令：</p>
<pre><code class="sh">$ sudo wipefs -a /dev/sdb
</code></pre>
</blockquote>
<p>之後在透過指令建立快取儲存裝置，如以下：</p>
<pre><code class="sh">$ sudo make-bcache --block 4k --bucket 2M -C /dev/sdb -B /dev/sdc --wipe-bcache
UUID:            192dfaf6-fd2a-4246-b4be-f159c3346850
Set UUID:        ed865522-96a7-43e5-8dab-e8c024fe85db
version:        0
nbuckets:        228946
block_size:        1
bucket_size:        1024
nr_in_set:        1
nr_this_dev:        0
first_bucket:        1
</code></pre>
<p>完成後，可以用<code>bcache-super-show</code>指令確認是否有建立，並取得 UUID：</p>
<pre><code class="sh">$ sudo bcache-super-show /dev/sdb | grep cset.uuid
cset.uuid        b6295aac-34c3-4630-8872-9aa18618daea
</code></pre>
<p>也可以用其他指令查看儲存建立狀況：</p>
<pre><code class="sh">$ lsblk
NAME      MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sda         8:0    0 232.9G  0 disk
└─sda1      8:1    0 232.9G  0 part /
sdb         8:16   0 111.8G  0 disk
└─bcache0 251:0    0 465.8G  0 disk
sdc         8:32   0 465.8G  0 disk
└─bcache0 251:0    0 465.8G  0 disk
</code></pre>
<p>接著將快取儲存裝置附加到後端儲存裝置：</p>
<pre><code class="sh">$ echo &quot;&lt;cset.uuid&gt;&quot; &gt; /sys/block/bcache0/bcache/attach
</code></pre>
<blockquote>
<p><code>bcache0</code>會隨建立的不同而改變。</p>
</blockquote>
<p>之後可以依需求設定 cache mode，透過以下方式：</p>
<pre><code class="sh">$ echo writeback &gt; /sys/block/bcache0/bcache/cache_mode
</code></pre>
<p>一切完成後，可以透過以下方式來檢查 Cache 狀態：</p>
<pre><code class="sh">$ cat /sys/block/bcache0/bcache/state
clean
</code></pre>
<blockquote>
<ul>
<li><code>no cache</code>：表示沒有任何快取裝置連接到後台儲存裝置。</li>
<li><code>clean</code>：表示快取已連接，且快取是乾淨的。</li>
<li><code>dirty</code>：表示一切設定完成，但必須啟用 writeback，且快取不是乾淨的。</li>
<li><code>inconsistent</code>：這表示後端是不被同步的高速快取儲存裝置。記得換爛一點。</li>
</ul>
</blockquote>
<h2 id="測試寫入速度"><a href="#測試寫入速度" class="headerlink" title="測試寫入速度"></a>測試寫入速度</h2><p>這邊採用 Linux 的 dd 工具來看寫入速度：</p>
<pre><code class="sh">$ dd if=/dev/zero of=/dev/bcache0 bs=1G count=1 oflag=direct
</code></pre>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Storage </tag>
            
            <tag> Linux </tag>
            
            <tag> SSD </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Building Spark Source Code]]></title>
      <url>https://kairen.github.io/2016/06/24/data-engineer/build-spark/</url>
      <content type="html"><![CDATA[<p>本節將說明如何透過 mvn 與 sbt 來建置 Spark 最新版的相關檔案，透過提供最新版本來觀看 API 的變動。</p>
<a id="more"></a>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>首先準備一台裝有 Ubuntu 14.04 LTS Server 的主機或 Docker 容器，然後在裡面安裝相依套件：</p>
<pre><code>sudo apt-get purge openjdk*
sudo apt-get -y autoremove
sudo apt-get install -y software-properties-common
sudo add-apt-repository -y ppa:webupd8team/java
sudo apt-get update
echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections
echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections
sudo apt-get -y install oracle-java8-installer git
</code></pre><p>接著安裝 maven 3.3.1 + 工具：</p>
<pre><code class="sh">wget http://ftp.tc.edu.tw/pub/Apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz
tar -zxf apache-maven-3.3.9-bin.tar.gz
sudo cp -R apache-maven-3.3.9 /usr/local/
sudo ln -s /usr/local/apache-maven-3.3.9/bin/mvn /usr/bin/mvn
mvn --version
</code></pre>
<p>安裝 Scala 語言：</p>
<pre><code class="sh">wget www.scala-lang.org/files/archive/scala-2.11.7.deb
sudo dpkg -i scala-2.11.7.deb
</code></pre>
<p>安裝 sbt 工具：</p>
<pre><code class="sh">echo &quot;deb http://dl.bintray.com/sbt/debian /&quot; | sudo tee /etc/apt/sources.list.d/sbt.list
sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 642AC823
sudo apt-get update
sudo apt-get install sbt
</code></pre>
<p>安裝 Python 2.7 語言：</p>
<pre><code class="sh">$ sudo apt-get install -y python
</code></pre>
<p>透過 Git 指令取得 Spark 最新原始碼：</p>
<pre><code class="sh">$ git clone https://github.com/apache/spark.git
</code></pre>
<h2 id="使用-sbt-來建置-spark"><a href="#使用-sbt-來建置-spark" class="headerlink" title="使用 sbt 來建置 spark"></a>使用 sbt 來建置 spark</h2><p>sbt 的 spark 建置指令如下所示，若使用 sbt 需要大約 10 分鐘時間：：</p>
<pre><code class="sh">$ ./build/sbt -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver -DskipTests clean assembly
</code></pre>
<p>當建置完成後，可以透過 spark-shell 查看版本：</p>
<pre><code class="sh">$ ./bin/spark-shell --version
</code></pre>
<h2 id="使用-Apache-Maven-來建置-spark"><a href="#使用-Apache-Maven-來建置-spark" class="headerlink" title="使用 Apache Maven 來建置 spark"></a>使用 Apache Maven 來建置 spark</h2><p>Apache Maven 的 spark 建置指令如下所示:</p>
<pre><code class="sh">$ ./build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver -DskipTests clean install
</code></pre>
<p>當建置完成後，可以透過 spark-shell 查看版本：</p>
<pre><code class="sh">$ ./bin/spark-shell --version
</code></pre>
<h2 id="Making-Distribution"><a href="#Making-Distribution" class="headerlink" title="Making Distribution"></a>Making Distribution</h2><p>make-distribution.sh 是一個 shell 腳本用於建立分散式應用。它使用跟 sbt 與 mvn 一樣的配置檔案。首先新增 Java 環境參數：</p>
<pre><code class="sh">$ export JAVA_HOME=&quot;/usr/lib/jvm/java-8-oracle&quot;
</code></pre>
<p>使用<code>--tgz</code>選項建立一個 tar gz 的 Spark 分散檔案：</p>
<pre><code class="sh">$ ./dev/make-distribution.sh --tgz -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver -DskipTests
</code></pre>
<blockquote>
<p>一旦完成後，你會在當前目錄看到檔案，名稱會是<code>spark-2.0.0-SNAPSHOT-bin-2.6.0.tgz</code>。</p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Spark </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Maven </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Docker 串接 OpenStack Neutron Kuryr 網路]]></title>
      <url>https://kairen.github.io/2016/06/02/openstack/kuryr-install/</url>
      <content type="html"><![CDATA[<p>Kuryr 是 Docker network plugin 之一，主要是使用 Neutron 來提供網路服務給不同主機的 Docker 容器使用，目前也提供了容器化的 Neutron plugin 容器映像檔。</p>
<p>本篇說明如何透過 CentOS 來部署簡單的 Kuryr 與 Docker 串接。</p>
<a id="more"></a>
<h2 id="節點配置"><a href="#節點配置" class="headerlink" title="節點配置"></a>節點配置</h2><p>OpenStack Kuryr 我們使用到三台節點，以下為本次安裝的規格：</p>
<table>
<thead>
<tr>
<th>Role</th>
<th>RAM</th>
<th>Disk</th>
<th>CPUs</th>
<th>IP Address</th>
</tr>
</thead>
<tbody>
<tr>
<td>controller</td>
<td>4 GB</td>
<td>50GB</td>
<td>2vCPU</td>
<td>172.16.1.115</td>
</tr>
<tr>
<td>network-1</td>
<td>4 GB</td>
<td>50GB</td>
<td>2vCPU</td>
<td>172.16.1.118</td>
</tr>
<tr>
<td>network-2</td>
<td>4 GB</td>
<td>50 GB</td>
<td>2vCPU</td>
<td>172.16.1.119</td>
</tr>
</tbody>
</table>
<p>請以上節點都分別安裝 RHEL 或者 CentOS 作業系統。並且設定 IP 為靜態固定，編輯<code>/etc/sysconfig/network-scripts/ifcfg-&lt;name&gt;</code>檔案，加入以下內容：</p>
<pre><code class="sh">ONBOOT=&quot;yes&quot;
IPADDR=&quot;10.0.0.104&quot;
PREFIX=&quot;24&quot;
GATEWAY=&quot;10.0.0.1&quot;
DNS1=&quot;8.8.8.8&quot;
DNS2=&quot;8.8.8.4&quot;
</code></pre>
<blockquote>
<p><font color="red">P.S.</font> 若是虛擬機則不需要設定。</p>
</blockquote>
<h2 id="安裝前準備"><a href="#安裝前準備" class="headerlink" title="安裝前準備"></a>安裝前準備</h2><p>在開始安裝以前，首先需要在每一台節點將基本環境的軟體更新：</p>
<pre><code class="sh">$ sudo yum update -y
</code></pre>
<blockquote>
<p>完成後檢查是否是最新版本，可以透過以下方式查看 Kernel：</p>
<pre><code class="sh">$ uname -r
3.10.0-327.13.1.el7.x86_64
</code></pre>
<p>如果不是以上版本，請執行以下指令：</p>
<pre><code class="sh">$ sudo yum upgrade --assumeyes --tolerant
$ sudo yum update --assumeyes
</code></pre>
</blockquote>
<p>由於在 CentOS 與 RHEL 預設會開啟防火牆，故要關閉防火牆與開機時自動啟動：</p>
<pre><code class="sh">$ sudo systemctl stop firewalld &amp;&amp; sudo systemctl disable firewalld
</code></pre>
<p>設定關閉 SELinux 與設定一些資訊，並重新啟動：</p>
<pre><code class="sh">$ sudo sed -i s/SELINUX=enforcing/SELINUX=permissive/g /etc/selinux/config &amp;&amp;
sudo groupadd nogroup &amp;&amp;
sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1 &amp;&amp;
sudo sysctl -w net.ipv6.conf.default.disable_ipv6=1 &amp;&amp;
sudo reboot
</code></pre>
<p>完成重新啟動後，在所有<code>Network</code>節點安裝 Docker，首先要取得 repos，設定以下來讓 yum 可以抓取：</p>
<pre><code class="sh">$ sudo tee /etc/yum.repos.d/docker.repo &lt;&lt;-&#39;EOF&#39;
[dockerrepo]
name=Docker Repository
baseurl=https://yum.dockerproject.org/repo/main/centos/$releasever/
enabled=1
gpgcheck=1
gpgkey=https://yum.dockerproject.org/gpg
EOF
</code></pre>
<p>在<code>Network</code>節點安裝 Docker engine，並啟動 docker 與設定開機啟動：</p>
<pre><code class="sh">$ sudo yum install --assumeyes --tolerant docker-engine
$ sudo systemctl start docker
$ sudo systemctl enable docker
</code></pre>
<p>在<code>所有</code>節點安裝一些基本工具軟體：</p>
<pre><code class="sh">$ sudo yum install -y tar xz unzip curl ipset vim wget git python-pip
$ sudo pip install --upgrade pip
</code></pre>
<p>在<code>Controller</code>節點，進入<code>root</code>使用者，並建置 ssh keys：</p>
<pre><code class="sh">$ ssh-keygen -t rsa
$ cat .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys
</code></pre>
<p>複製<code>Controller</code>節點的<code>.ssh/id_rsa.pub</code>內容，並貼到<code>Network</code>節點的<code>root</code>使用者的<code>.ssh/authorized_keys</code>。並驗證 ssh 是否可以無密碼登入：</p>
<pre><code class="sh">$ ssh 172.16.1.118
</code></pre>
<h3 id="安裝-OpenStack"><a href="#安裝-OpenStack" class="headerlink" title="安裝 OpenStack"></a>安裝 OpenStack</h3><p>這邊使用 RDO 進行安裝。由於只需要 Neutron 與 Keystone 服務，所以可以修改部署的<code>answer-file</code>設定檔。由於這邊使用的是虛擬機，因此 Neutron 網路採用 VXLAN 方式進行安裝。進入到<code>Controller</code>節點，並且進入到<code>root</code>使用者安裝 PackStack：</p>
<pre><code class="sh">$ yum install -y centos-release-openstack-mitaka.noarch
$ yum install -y openstack-packstack
$ wget https://gist.githubusercontent.com/kairen/637c707b960e188d32aba9044e652c0b/raw/6724a5177ca82ced555554635c6b0893e8c398ab/answer-file
</code></pre>
<blockquote>
<p>這邊可以更改安裝版本，如更改成 Liberty 的穩定版<code>centos-release-openstack-liberty.noarch</code></p>
</blockquote>
<p>編輯<code>answer-file</code>設定檔，修改以下內容：</p>
<pre><code>CONFIG_CONTROLLER_HOST=172.16.1.115
CONFIG_COMPUTE_HOSTS=172.16.1.118,172.16.1.119
CONFIG_NETWORK_HOSTS=172.16.1.115,172.16.1.118,172.16.1.119
CONFIG_STORAGE_HOST=172.16.1.115
CONFIG_AMQP_HOST=172.16.1.115
CONFIG_MARIADB_HOST=172.16.1.115
CONFIG_KEYSTONE_LDAP_URL=ldap://172.16.1.115
</code></pre><p>設定檔都確認無誤後，透過以下指令進行安裝：</p>
<pre><code class="sh">$ packstack --answer-file=answer-file
...
**** Installation completed successfully ******
* To access the OpenStack Dashboard browse to http://172.16.1.115/dashboard .
</code></pre>
<blockquote>
<p>中途若發生安裝套件失敗問題，請直接重新執行一次。</p>
</blockquote>
<p>當成功安裝完成後，透過簡單的 OpenStack 指令來確認：</p>
<pre><code class="sh">$ . keystonerc_admin
$ openstack user list
+----------------------------------+---------+
| ID                               | Name    |
+----------------------------------+---------+
| 70b80593320543bbb32e15d7f06036f0 | admin   |
| 9600aaa2447940e789e548b2f5515690 | neutron |
+----------------------------------+---------+
</code></pre>
<h3 id="安裝-Kuryr"><a href="#安裝-Kuryr" class="headerlink" title="安裝 Kuryr"></a>安裝 Kuryr</h3><p>進入<code>Network</code>節點，並且進入到<code>root</code>使用者，下載 Kuryr 最新的專案：</p>
<pre><code class="sh">$ git clone https://github.com/openstack/kuryr.git
$ pip install -r requirements.txt
$ python setup.py install
</code></pre>
<p>建立 Kuryr 設定檔與 Log 目錄：</p>
<pre><code class="sh">$ mkdir -p /var/log/kuryr /etc/kuryr
$ wget https://gist.githubusercontent.com/kairen/637c707b960e188d32aba9044e652c0b/raw/6724a5177ca82ced555554635c6b0893e8c398ab/kuryr.conf -O /etc/kuryr/kuryr.conf
</code></pre>
<p>編輯<code>/etc/kuryr/kuryr.conf</code>設定檔，修改一下內容：</p>
<pre><code>[keystone_client]
auth_uri = http://172.16.1.115:35357/v2.0

[neutron_client]
neutron_uri = http://172.16.1.115:9696
</code></pre><p>接著啟動 Kuryr 服務：</p>
<pre><code class="sh">$ ./scripts/run_kuryr.sh &amp;
2016-06-02 03:51:33.578 4758 INFO werkzeug [-]  * Running on http://0.0.0.0:2377/ (Press CTRL+C to quit)
</code></pre>
<h3 id="服務驗證"><a href="#服務驗證" class="headerlink" title="服務驗證"></a>服務驗證</h3><p>當所有<code>Network</code>節點都完成 Kuryr 安裝後，就可以透過以下方式來驗證，首先在<code>network-1</code>建立網路：</p>
<pre><code class="sh">$ docker network create --driver=kuryr \
--ipam-driver=kuryr \
--subnet 10.0.0.0/16 \
--gateway 10.0.0.1 \
--ip-range 10.0.0.0/24 kuryr

...
821d6cd53af6c656969c1c96063a60c695a0313c9a119e44d4325ce2a9f2f935
</code></pre>
<p>透過 docker 指令來查看網路：</p>
<pre><code class="sh">$ docker network inspect kuryr
[
    {
        &quot;Name&quot;: &quot;kuryr&quot;,
        &quot;Id&quot;: &quot;821d6cd53af6c656969c1c96063a60c695a0313c9a119e44d4325ce2a9f2f935&quot;,
        &quot;Scope&quot;: &quot;local&quot;,
        &quot;Driver&quot;: &quot;kuryr&quot;,
        &quot;EnableIPv6&quot;: false,
        &quot;IPAM&quot;: {
            &quot;Driver&quot;: &quot;kuryr&quot;,
            &quot;Options&quot;: {},
            &quot;Config&quot;: [
                {
                    &quot;Subnet&quot;: &quot;10.0.0.0/16&quot;,
                    &quot;IPRange&quot;: &quot;10.0.0.0/24&quot;,
                    &quot;Gateway&quot;: &quot;10.0.0.1&quot;
                }
            ]
        },
        &quot;Internal&quot;: false,
        &quot;Containers&quot;: {
            &quot;367763a677ad180c57818631ee3e9151683d0218a15bb002d0995ab5c6e30446&quot;: {
                &quot;Name&quot;: &quot;awesome_dijkstra&quot;,
                &quot;EndpointID&quot;: &quot;326aba6247266cfd0ca3771b0283e2142a3e934bb994d1b77fc93bb467c0df48&quot;,
                &quot;MacAddress&quot;: &quot;fa:16:3e:7c:d8:b6&quot;,
                &quot;IPv4Address&quot;: &quot;10.0.0.5/24&quot;,
                &quot;IPv6Address&quot;: &quot;&quot;
            },
            &quot;b740abd7976d274de233df0bad052c418516ba036132b92ad022a4b52a2d7d25&quot;: {
                &quot;Name&quot;: &quot;awesome_engelbart&quot;,
                &quot;EndpointID&quot;: &quot;fcc642dae6323ce7d86ddda61c9583a19801eae70d8126d35b4b5846cd8598a4&quot;,
                &quot;MacAddress&quot;: &quot;fa:16:3e:5b:6e:7f&quot;,
                &quot;IPv4Address&quot;: &quot;10.0.0.3/24&quot;,
                &quot;IPv6Address&quot;: &quot;&quot;
            }
        },
        &quot;Options&quot;: {},
        &quot;Labels&quot;: {}
    }
]
</code></pre>
<p>接著建立一個 container 來取得 IP：</p>
<pre><code class="sh">$ docker run -it -d --net kuryr --privileged=true ubuntu:14.04
$ docker exec -ti $(docker ps -lq) bash
root@367763a677ad$ ip -4 a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
14: eth0@if15: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    inet 10.0.0.3/24 scope global eth0
       valid_lft forever preferred_lft forever
</code></pre>
<p>接著進入到<code>network-2</code>節點，透過 docker 指令建立網路，這邊採用跟上一個同樣的網路：</p>
<pre><code class="sh">$ docker network create --driver=kuryr \
--ipam-driver=kuryr \
--subnet 10.0.0.0/16 \
--gateway 10.0.0.1 \
--ip-range 10.0.0.0/24 \
-o neutron.net.uuid=8c069d2c-772c-47ae-90bb-d22148f37dc8 kuryr
</code></pre>
<blockquote>
<p>這邊<code>neutron.net.uuid</code>可以透過以下在<code>Controller</code>方式取得：</p>
<pre><code class="sh">$ neutron net-list | awk &#39;/kuryr/ {print $2}&#39;
8c069d2c-772c-47ae-90bb-d22148f37dc8
</code></pre>
</blockquote>
<p>接著一樣建立一個 container 來取得 IP：</p>
<pre><code class="sh">$ docker run -it -d --net kuryr --privileged=true ubuntu:14.04
$ docker exec -ti $(docker ps -lq) bash
root@f2b48a802e92$ ip -4 a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
15: eth0@if16: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    inet 10.0.0.4/24 scope global eth0
       valid_lft forever preferred_lft forever
</code></pre>
<p>透過 ping 來驗證網路是否有連接：</p>
<pre><code class="sh">root@f2b48a802e92$ ping -c 3 10.0.0.3
PING 10.0.0.3 (10.0.0.3) 56(84) bytes of data.
64 bytes from 10.0.0.3: icmp_seq=1 ttl=64 time=1.71 ms
64 bytes from 10.0.0.3: icmp_seq=2 ttl=64 time=1.37 ms
64 bytes from 10.0.0.3: icmp_seq=3 ttl=64 time=1.23 ms
</code></pre>
]]></content>
      
        <categories>
            
            <category> OpenStack </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Docker </tag>
            
            <tag> OpenStack </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[用 Flashcache 建立高容量與高效能儲存]]></title>
      <url>https://kairen.github.io/2016/05/27/linux/ubuntu/flashcache/</url>
      <content type="html"><![CDATA[<p>Flashcache 是 Facebook 的一個開源專案，主要被用於資料庫加速。基本結構為在硬碟（HDD）前面加了一層快取，即採用固態硬碟（SSD）裝置，把熱資料保存於快取中，寫入的過程也是先寫到 SSD，然後由 SSD 同步到傳統硬碟，最後的資料將保存於硬碟中，這樣可以不用擔心 SSD 損壞造成資料遺失問題，同時又可以有大容量、高效能的儲存。</p>
<a id="more"></a>
<h3 id="安裝"><a href="#安裝" class="headerlink" title="安裝"></a>安裝</h3><p>本教學採用 Ubuntu 14.04 LTS 進行安裝，並建立快取。首先安裝相依套件：</p>
<pre><code class="sh">$ sudo apt-get install -y git build-essential dkms linux-headers-`uname -r`
</code></pre>
<p>完成後，透過 git 指令將專案下載至主機上：</p>
<pre><code class="sh">$ git clone https://github.com/facebook/flashcache.git
$ cd flashcache
</code></pre>
<p>進入目錄編譯 flashcache 套件，並透過 make 進行安裝套件：</p>
<pre><code class="sh">$ make
$ sudo make install
</code></pre>
<p>安裝完成後，就可以載入 flashcache 模組，透過以下指令：</p>
<pre><code class="sh">$ sudo modprobe flashcache
</code></pre>
<blockquote>
<p>若要檢查是否載入成功的話，可以使用以下指令：</p>
<pre><code class="sh">$ dmesg | tail
[24181.921706] flashcache: module verification failed: signature and/or  required key missing - tainting kernel
[24181.922785] flashcache: flashcache-3.1.1 initialized
</code></pre>
</blockquote>
<p>設定開機時自動載入模組：</p>
<pre><code class="sh">$ echo &quot;flashcache&quot; | sudo tee -a /etc/modules
</code></pre>
<h3 id="設定快取"><a href="#設定快取" class="headerlink" title="設定快取"></a>設定快取</h3><p>首先準備一顆 SSD 與 HDD，並安裝於同一台主機上，如以下硬碟結構：</p>
<pre><code class="sh">+-------+----------+       +--------+---------+       
| [ 固態硬碟(SSD)]  |       |  [ 傳統硬碟(HDD)]  |       
|  System   disk   +-------+  System   disk   +
|    (/dev/sdb)    |       |    (/dev/sdc)    |
+------------------+       +------------------+
</code></pre>
<p>在開始前，必須先將傳統硬碟進行格式化：</p>
<pre><code class="sh">$ sudo mkfs.ext4 /dev/sdc
</code></pre>
<p>接著要初始化 Flashcache，然後透過 Flashcache 指令來設定快取：</p>
<pre><code class="sh">$ sudo flashcache_create -p back -b 4k cachedev /dev/sdb /dev/sdc
cachedev cachedev, ssd_devname /dev/sdb, disk_devname /dev/sdc cache mode WRITE_BACK
block_size 8, md_block_size 8, cache_size 0
Flashcache metadata will use 614MB of your 7950MB main memory
</code></pre>
<p>完成後，就可以透過 mount 來使用快取：</p>
<pre><code class="sh">$ sudo mount /dev/mapper/cachedev /mnt
</code></pre>
<p>若要在開機時自動 mount 為 Flashcache 的快取固態硬碟，可以在<code>rc.local</code>加入以下內容：</p>
<pre><code class="sh">flashcache_load /dev/sdb
mount /dev/mapper/cachedev /mnt
</code></pre>
<p>若想監控 Flashcache 資訊的話，可以使用以下工具：</p>
<pre><code class="sh">$ flashstat
</code></pre>
<p>最後，若想要刪除 Flashcache 的話，可以使用以下指令：</p>
<pre><code class="sh">$ sudo umount /mnt
$ sudo flashcache_destroy /dev/sdb
$ sudo dmsetup remove cachedev
</code></pre>
<h3 id="fio-測試"><a href="#fio-測試" class="headerlink" title="fio 測試"></a>fio 測試</h3><p>這邊採用 fio 來進行測試，首先透過<code>apt-get</code>安裝套件：</p>
<pre><code class="sh">$ sudo apt-get install fio
</code></pre>
<p>完成後，即可透過 fio 指令進行效能測試：</p>
<pre><code class="sh">$ fio --filename=/dev/sdb --direct=1 \
--rw=randrw --ioengine=libaio --bs=4k \
--rwmixread=100 --iodepth=16 \
--numjobs=16 --runtime=60 \
--group_reporting --name=4ktest
</code></pre>
<blockquote>
<p>fio 測試工具 options 參數：</p>
<ul>
<li><code>--filename=/dev/sdb</code>：指定要測試的磁碟。</li>
<li><code>--direct=1</code>：預設值為 0 ,必須設定為 1 才會測試到真實的 non-buffered I/O。</li>
<li><code>--rw=randrw</code>：可以設定的參數如下 randrw 代表 random(隨機) 的 read(讀) write(寫),其他的請參考下面說明。<ul>
<li><strong>read</strong> : Sequential reads. (循序讀)</li>
<li><strong>write</strong> : Sequential writes. (循序寫)</li>
<li><strong>randread</strong> : Random reads. (隨機讀)</li>
<li><strong>randwrite</strong> : Random writes. (隨機寫)</li>
<li><strong>rw</strong> : Mixed sequential reads and writes. (循序讀寫)</li>
<li><strong>randrw</strong> : Mixed random reads and writes. (隨機讀寫)</li>
</ul>
</li>
<li><code>--ioengine=libaio</code>：定義如何跑 I/O 的方式, libaio 是 Linux 本身非同步(asynchronous) I/O 的方式. 其他還有 sync , psync , vsync , posixaio , mmap , splice , syslet-rw , sg , null , net , netsplice , cpuio , guasi , external。</li>
<li><code>--bs=4k</code>：bs 或是 blocksize ,也就是檔案寫入大小,預設值為 4K。</li>
<li><code>--rwmixread=100</code>： 當設定為 Mixed ,同一時間 read 的比例為多少,預設為 50%。</li>
<li><code>--refill_buffers</code>：refill_buffers 為預設值,應該是跟 I/O Buffer 有關 (refill the IO buffers on every submit),把 Buffer 填滿就不會跑到 Buffer 的值。</li>
<li><code>--iodepth=16</code>：同一時間有多少 I/O 在做存取,越多不代表存儲裝置表現會更好,通常是 RAID 時須要設大一點。</li>
<li><code>--numjobs=16</code>：跟前面的 iodepth 類似,但不一樣,在 Linux 下每一個 job 可以生出不同的 processes/threads ,numjobs 就是在同一個 workload 同時提出多個 I/O 請求,通常負載這個會比較大.預設值為 1。</li>
<li><code>--runtime=60</code>：這一測試所需的時間,單位為 秒。</li>
<li><code>--group_reporting</code>：如果 numjobs 有指定,設定 group_reporting 報告會以 per-group 的顯示方式。</li>
<li><code>--name=4ktest</code>：代表這是一個新的測試 Job。</li>
</ul>
</blockquote>
<h3 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h3><ul>
<li><a href="http://benjr.tw/34632" target="_blank" rel="noopener">Fio – Flexible I/O Tester</a></li>
<li><a href="https://github.com/facebook/flashcache/wiki/QuickStart-Recipe-for-Ubuntu-11.10" target="_blank" rel="noopener">Flashcache Wiki</a></li>
<li><a href="http://navyaijm.blog.51cto.com/4647068/1567698" target="_blank" rel="noopener">Flashcache初次体验</a></li>
<li><a href="http://www.pupuliao.info/2013/12/ubuntu-bonnie%E7%A1%AC%E7%A2%9F%E6%B8%AC%E9%80%9F-linux-%E9%81%A9%E7%94%A8/" target="_blank" rel="noopener">Ubuntu bonnie++硬碟測速 (Linux 適用)</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Storage </tag>
            
            <tag> Linux </tag>
            
            <tag> SSD </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Pacemaker + Corosync 做服務 HA]]></title>
      <url>https://kairen.github.io/2016/05/26/linux/ubuntu/corosync-pacemaker/</url>
      <content type="html"><![CDATA[<p>Pacemaker 與 Corosync 是 Linux 中現今較常用的高可靠性叢集系統組合。Pacemaker 自身提供了很多常用的應用管理功能，不過若要使用 Pacemaker 來管理自己實作的服務，或是一些特別的東西時，就必須要自己實作管理資源。</p>
<a id="more"></a>
<h2 id="節點配置"><a href="#節點配置" class="headerlink" title="節點配置"></a>節點配置</h2><p>本安裝將使用三台實體主機與一台虛擬機器，主機規格如以下所示：</p>
<table>
<thead>
<tr>
<th>Role</th>
<th>IP Address</th>
</tr>
</thead>
<tbody>
<tr>
<td>pacemaker1</td>
<td>172.16.35.10</td>
</tr>
<tr>
<td>pacemaker2</td>
<td>172.16.35.11</td>
</tr>
</tbody>
</table>
<blockquote>
<p>作業系統皆為 <code>Ubuntu 14.04 Server</code>。</p>
</blockquote>
<h2 id="進行安裝與設定"><a href="#進行安裝與設定" class="headerlink" title="進行安裝與設定"></a>進行安裝與設定</h2><p>首先要在所有節點之間設定無密碼 ssh 登入，透過以下方式：</p>
<pre><code class="sh">$ ssh-keygen -t rsa
$ ssh-copy-id pacemaker1
</code></pre>
<p>安裝相關套件軟體：</p>
<pre><code class="sh">$ sudo apt-get install -y corosync pacemaker heartbeat resource-agents fence-agents apache2
</code></pre>
<p>完成後，在<code>pacemaker1</code>進行以下步驟，首先編輯<code>/etc/corosync/corosync.conf</code>設定檔，修改一下內容：</p>
<pre><code># Please read the openais.conf.5 manual page

totem {
    version: 2

    # How long before declaring a token lost (ms)
    token: 3000

    # How many token retransmits before forming a new configuration
    token_retransmits_before_loss_const: 10

    # How long to wait for join messages in the membership protocol (ms)
    join: 60

    # How long to wait for consensus to be achieved before starting a new round of membership configuration (ms)
    consensus: 3600

    # Turn off the virtual synchrony filter
    vsftype: none

    # Number of messages that may be sent by one processor on receipt of the token
    max_messages: 20

    # Limit generated nodeids to 31-bits (positive signed integers)
    clear_node_high_bit: yes

    # Disable encryption
     secauth: off  #啟動認證功能

    # How many threads to use for encryption/decryption
     threads: 0

    # Optionally assign a fixed node id (integer)
    # nodeid: 1234

    # This specifies the mode of redundant ring, which may be none, active, or passive.
     rrp_mode: none

     interface {
        # The following values need to be set based on your environment
        ringnumber: 0
        bindnetaddr: 10.11.8.0  # 主機所在網路位址
        mcastaddr: 226.93.2.1  # 廣播地址，不要被佔用即可 P.S. 範圍:224.0.2.0～238.255.255.255
        mcastport: 5405  # 廣播埠口
    }
}

amf {
    mode: disabled
}

quorum {
    # Quorum for the Pacemaker Cluster Resource Manager
    provider: corosync_votequorum
    expected_votes: 1
}

aisexec {
        user:   root
        group:  root
}

logging {
        fileline: off
        to_stderr: yes  # 輸出到標準输出
        to_logfile: yes  # 輸出到日誌檔案
        logfile: /var/log/corosync.log  # 日誌檔案位置
        to_syslog: no  # 輸出到系统日誌
        syslog_facility: daemon
        debug: off
        timestamp: on
        logger_subsys {
                subsys: AMF
                debug: off
                tags: enter|leave|trace1|trace2|trace3|trace4|trace6
        }
}

# 新增 pacemaker 服務配置
service {
    ver: 1
    name: pacemaker
}
</code></pre><p>接著產生節點之間的溝通時的認證金鑰文件：</p>
<pre><code class="sh">$ corosync-keygen -l
</code></pre>
<p>然後將設定檔與金鑰複製到<code>pacemaker2</code>上：</p>
<pre><code class="sh">$ cd /etc/corosync/
$ scp -p corosync.conf authkey pacemaker2:/etc/corosync/
</code></pre>
<p>接著分別在<code>兩個</code>節點上編輯<code>/etc/default/corosync</code>檔案，修改以下：</p>
<pre><code class="sh"># start corosync at boot [yes|no]
START=yes
</code></pre>
<p>接著將 Corosync 與 Pacemaker 服務啟動：</p>
<pre><code class="sh">$ sudo service corosync start
$ sudo service pacemaker start
</code></pre>
<p>完成後透過 crm 指令來查看狀態：</p>
<pre><code class="sh">$ crm status

Last updated: Tue Dec 27 03:12:07 2016
Last change: Tue Dec 27 02:35:18 2016 via cibadmin on pacemaker1
Stack: corosync
Current DC: pacemaker1 (739255050) - partition with quorum
Version: 1.1.10-42f2063
2 Nodes configured
0 Resources configured


Online: [ pacemaker1 pacemaker2 ]
</code></pre>
<p>關閉 corosync 預設啟動的 stonith 與 quorum 在兩台節點之問題：</p>
<pre><code class="sh">$ crm configure property stonith-enabled=false
$ crm configure property no-quorum-policy=ignore
</code></pre>
<p>完成後，透過指令檢查：</p>
<pre><code class="sh">$ crm configure show

node $id=&quot;739255050&quot; pacemaker1
node $id=&quot;739255051&quot; pacemaker2
property $id=&quot;cib-bootstrap-options&quot; \
    dc-version=&quot;1.1.10-42f2063&quot; \
    cluster-infrastructure=&quot;corosync&quot; \
    stonith-enabled=&quot;false&quot; \
    no-quorum-policy=&quot;ignore&quot;
</code></pre>
<h2 id="設定資源"><a href="#設定資源" class="headerlink" title="設定資源"></a>設定資源</h2><p>Corosync 支援了多種資源代理，如 heartbeat、LSB(Linux Standard Base)與 OCF(Open Cluster Framework) 等。而 Corosync 也可以透過指令來查詢：</p>
<pre><code class="sh">$ crm ra classes

lsb
ocf / heartbeat pacemaker redhat
service
stonith
upstart
</code></pre>
<blockquote>
<p>而更細部的資訊可以透過以下查詢：</p>
<pre><code class="sh">$ crm ra list lsb
$ crm ra list ocf heartbeat
$ crm ra info ocf:heartbeat:IPaddr
</code></pre>
</blockquote>
<p>首先新增一個 heartbeat 資源：</p>
<pre><code class="shell">$ crm configure
# 設定 VIP
crm(live)configure# primitive vip ocf:heartbeat:IPaddr params ip=172.16.35.20 nic=eth2 cidr_netmask=24 op monitor interval=10s timeout=20s on-fail=restart

# 設定 httpd
crm(live)configure# primitive httpd lsb:apache2
crm(live)configure# exit
There are changes pending. Do you want to commit them? yes
</code></pre>
<p>設定 Group 來將 httpd 與 vip 資源放一起：</p>
<pre><code class="sh">crm(live)configure# group webservice vip httpd
</code></pre>
<p>完成後，透過 crm 指令查詢狀態：</p>
<pre><code class="sh">$ crm status

Last updated: Tue Dec 27 03:52:21 2016
Last change: Tue Dec 27 03:52:20 2016 via cibadmin on pacemaker1
Stack: corosync
Current DC: pacemaker1 (739255050) - partition with quorum
Version: 1.1.10-42f2063
2 Nodes configured
2 Resources configured


Online: [ pacemaker1 pacemaker2 ]

 Resource Group: webservice
     vip    (ocf::heartbeat:IPaddr):    Started pacemaker1
     httpd    (lsb:apache2):    Started pacemaker2
</code></pre>
<p>最後就可以在<code>pacemaker1</code>或<code>pacemaker2</code>關閉服務來確認是否正常執行。</p>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Load Balancer </tag>
            
            <tag> High Availability </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[利用 rados-java 存取 Ceph]]></title>
      <url>https://kairen.github.io/2016/05/15/ceph/rados-java/</url>
      <content type="html"><![CDATA[<p><a href="https://github.com/ceph/rados-java" target="_blank" rel="noopener">rados-java</a> 透過 JNA 來綁定 librados (C) 的 API 來提供給 Java 使用，並且實作了 RADOS 與 RBD 的 API，由於透過 JNA 的關析，故不用建構任何的 Header 檔案(.h)。因此我們可以在擁有 JNA 與 librados 的系統上使用本函式庫。</p>
<a id="more"></a>
<h2 id="環境準備"><a href="#環境準備" class="headerlink" title="環境準備"></a>環境準備</h2><p>在開始進行之前，需要滿足以下幾項要求：</p>
<ul>
<li>需要部署一個 Ceph 叢集，可以參考 <a href="https://kairen.github.io/2016/02/11/ceph/deploy/ceph-docker/">Ceph Docker 部署</a>。</li>
<li>執行 rados-java 程式的環境，要能夠與 Ceph 叢集溝通(ceph.conf、admin key)。</li>
<li>需要安裝 Ceph 相關 library。可以透過以下方式安裝：</li>
</ul>
<pre><code class="sh">$ wget -q -O- &#39;https://download.ceph.com/keys/release.asc&#39; | sudo apt-key add -
$ echo &quot;deb https://download.ceph.com/debian-kraken/ $(lsb_release -sc) main&quot; | sudo tee /etc/apt/sources.list.d/ceph.list
$ sudo apt-get update &amp;&amp; sudo apt-get install -y ceph
</code></pre>
<h2 id="建構-rados-java-jar-檔"><a href="#建構-rados-java-jar-檔" class="headerlink" title="建構 rados-java jar 檔"></a>建構 rados-java jar 檔</h2><p>首先需要安裝一些相關軟體來提供建構 rados-java 使用：</p>
<pre><code class="sh">$ sudo apt-get install -y software-properties-common
$ sudo add-apt-repository -y ppa:webupd8team/java
$ sudo apt-get update
$ sudo apt-get -y install oracle-java8-installer git libjna-java
$ sudo ln -s /usr/share/java/jna-4.2.2.jar /usr/lib/jvm/java-8-oracle/jre/lib/ext/
</code></pre>
<p>接著安裝 maven 3.3.1 + 工具：</p>
<pre><code class="sh">wget &quot;http://ftp.tc.edu.tw/pub/Apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz&quot;
tar -zxf apache-maven-3.3.9-bin.tar.gz
sudo cp -R apache-maven-3.3.9 /usr/local/
sudo ln -s /usr/local/apache-maven-3.3.9/bin/mvn /usr/bin/mvn
mvn --version
</code></pre>
<p>然後透過 Git 取得 rados-java 原始碼：</p>
<pre><code class="sh">$ git clone &quot;https://github.com/ceph/rados-java.git&quot;
$ cd rados-java &amp;&amp; git checkout v0.3.0
$ mvn clean install -Dmaven.test.skip=true
</code></pre>
<p>完成後將 rados-java Jar 檔複製到<code>/usr/share/java/</code>底下，並設定 JAR 連結 JVM Class path：</p>
<pre><code class="sh">$ sudo cp target/rados-0.3.0.jar /usr/share/java
$ sudo ln -s /usr/share/java/rados-0.3.0.jar /usr/lib/jvm/java-8-oracle/jre/lib/ext/
</code></pre>
<blockquote>
<p>這邊也可以直接透過下載 Jar 檔來完成：</p>
<pre><code class="sh">$ wget &quot;http://central.maven.org/maven2/com/ceph/rados/0.3.0/rados-0.3.0.jar&quot;
$ sudo cp rados-0.3.0.jar /usr/share/java/
</code></pre>
</blockquote>
<p>最後就可以透過簡單範例程式存取 Ceph 了。</p>
<h2 id="簡單測試程式"><a href="#簡單測試程式" class="headerlink" title="簡單測試程式"></a>簡單測試程式</h2><p>這邊透過 Java 程式連結到 Ceph 叢集，並且存取<code>data</code>儲存池來寫入物件，建立與編輯<code>Example.java</code>檔，加入以下程式內容：</p>
<pre><code class="java">import com.ceph.rados.Rados;
import com.ceph.rados.exceptions.RadosException;

import java.io.File;
import com.ceph.rados.IoCTX;

public class Example {
    public static void main (String args[]){
      try {
          Rados cluster = new Rados(&quot;admin&quot;);
          File f = new File(&quot;/etc/ceph/ceph.conf&quot;);
          cluster.confReadFile(f);

          cluster.connect();
          System.out.println(&quot;Connected to the cluster.&quot;);

          IoCTX io = cluster.ioCtxCreate(&quot;data&quot;); /* Pool Name */
          String oidone = &quot;kyle-say&quot;;
          String contentone = &quot;Hello World!&quot;;
          io.write(oidone, contentone);

          String oidtwo = &quot;my-object&quot;;
          String contenttwo = &quot;This is my object.&quot;;
          io.write(oidtwo, contenttwo);

          String[] objects = io.listObjects();
          for (String object: objects)
              System.out.println(&quot;Put &quot; + object);

          /* io.remove(oidone);
             io.remove(oidtwo); */

          cluster.ioCtxDestroy(io);

        } catch (RadosException e) {
          System.out.println(e.getMessage() + &quot;: &quot; + e.getReturnValue());
        }
    }
}
</code></pre>
<p>撰寫完程式後，執行以下指令來看結果：</p>
<pre><code class="sh">$ javac Example.java
$ sudo java Example
Connected to the cluster.
Put kyle-say
Put my-object
</code></pre>
<h2 id="透過-rados-指令檢查"><a href="#透過-rados-指令檢查" class="headerlink" title="透過 rados 指令檢查"></a>透過 rados 指令檢查</h2><p>當程式正確執行後，就可以透過 rados 指令來確認物件是否正確被寫入：</p>
<pre><code class="sh">$ sudo rados -p data ls
kyle-say
my-object
</code></pre>
<p>透過 Get 指令來取得物件的內容：</p>
<pre><code class="sh">$ sudo rados -p data get kyle-say -
Hello World!

$ sudo rados -p data get my-object -
This is my object.
</code></pre>
]]></content>
      
        <categories>
            
            <category> Ceph </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> Storage </tag>
            
            <tag> Java </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Alluxio 分散式虛擬儲存系統]]></title>
      <url>https://kairen.github.io/2016/05/06/data-engineer/alluxio/</url>
      <content type="html"><![CDATA[<p><code>Alluxio</code> 是分散式虛擬儲存系統，早期名稱為 Tachyon ，而現在已正式改名 Alluxio，並發佈 1.0 版本</p>
<p>Aluxion 是一個記憶體虛擬分散式儲存系統，具有高效能、高容錯以及高可靠性的特色，它能夠統一資料的存取去串接機算框架和儲存系統的橋梁，像是同時可相容於 Hadoop MapReduce 和 Apache Spark 以及 Apache Flink 的計算框架和 Alibaba OSS、Amazon S3、OpenStack Swift,、GlusterFS 及 Ceph 的儲存系統</p>
<p><img src="/images/spark/alluxio_architecture.jpg" alt=""></p>
<a id="more"></a>
<h2 id="Install-Java7"><a href="#Install-Java7" class="headerlink" title="Install Java7"></a>Install Java7</h2><p>首先安裝相關套件：</p>
<pre><code class="sh">$ sudo apt-get purge openjdk*
$ sudo apt-get -y autoremove
$ sudo add-apt-repository -y ppa:webupd8team/java
$ sudo apt-get update
$ echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections
$ echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections
$ sudo apt-get -y install oracle-java7-installer
</code></pre>
<h2 id="Download-Alluxio-1-0-0"><a href="#Download-Alluxio-1-0-0" class="headerlink" title="Download Alluxio 1.0.0"></a>Download Alluxio 1.0.0</h2><p>接著下載 Alluxio：</p>
<pre><code class="sh">$ wget http://alluxio.org/downloads/files/1.0.0/alluxio-1.0.0-bin.tar.gz
$ tar xvfz alluxio-1.0.0-bin.tar.gz
$ cd alluxio-1.0.0
</code></pre>
<p>複製一個<code>conf/alluxio-env.sh</code>檔案</p>
<pre><code class="sh">$ cp conf/alluxio-env.sh.template conf/alluxio-env.sh
</code></pre>
<p><code>conf/alluxio-env.sh</code>中加入<code>ALLUXIO_UNDERFS_ADDRESS</code>參數</p>
<pre><code class="sh">export ALLUXIO_UNDERFS_ADDRESS=/tmp
</code></pre>
<p>確認 <code>ssh localhost</code> 可成功</p>
<pre><code class="sh">$ssh-copy-id localhsot
</code></pre>
<p>格式化 Alluxio FileSystem 並開啟它</p>
<pre><code class="sh">$ ./bin/alluxio format
$ ./bin/alluxio-start.sh local
</code></pre>
<p>驗證 Alluxio 可於瀏覽器輸入<code>http://localhost:19999</code>，也可執行簡單的程式，如下:</p>
<pre><code class="sh">$ ./bin/alluxio runTest Basic CACHE THROUGH
</code></pre>
<p>執行後，如下:</p>
<pre><code>2015-11-20 08:32:22,271 INFO   (ClientBase.java:connect) - Alluxio client (version 1.0.0) is trying to connect with FileSystemMaster master @ localhost/127.0.0.1:19998
2015-11-20 08:32:22,294 INFO   (ClientBase.java:connect) - Client registered with FileSystemMaster master @ localhost/127.0.0.1:19998
2015-11-20 08:32:22,387 INFO   (BasicOperations.java:createFile) - createFile with fileId 33554431 took 127 ms.
2015-11-20 08:32:22,552 INFO   (ClientBase.java:connect) - Alluxio client (version 1.0.0) is trying to connect with BlockMaster master @ localhost/127.0.0.1:19998
2015-11-20 08:32:22,553 INFO   (ClientBase.java:connect) - Client registered with BlockMaster master @ localhost/127.0.0.1:19998
2015-11-20 08:32:22,604 INFO   (WorkerClient.java:connect) - Connecting local worker @ /192.168.2.15:29998
2015-11-20 08:32:22,698 INFO   (BasicOperations.java:writeFile) - writeFile to file /default_tests_files/BasicFile_CACHE_THROUGH took 311 ms.
2015-11-20 08:32:22,759 INFO   (FileUtils.java:createStorageDirPath) - Folder /Volumes/ramdisk/alluxioworker/7226211928567857329 was created!
2015-11-20 08:32:22,809 INFO   (LocalBlockOutStream.java:&lt;init&gt;) - LocalBlockOutStream created new file block, block path: /Volumes/ramdisk/alluxioworker/7226211928567857329/16777216
2015-11-20 08:32:22,886 INFO   (BasicOperations.java:readFile) - readFile file /default_tests_files/BasicFile_CACHE_THROUGH took 187 ms.
Passed the test!
</code></pre><p>執行更複雜的測試:</p>
<pre><code class="sh">$ ./bin/alluxio runTests
</code></pre>
<p>停止服務：</p>
<pre><code class="sh">$ ./bin/alluxio-stop.sh all
</code></pre>
]]></content>
      
        <categories>
            
            <category> Spark </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Storage </tag>
            
            <tag> Java </tag>
            
            <tag> Spark </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[DM-cache 建立混和區塊裝置]]></title>
      <url>https://kairen.github.io/2016/04/21/linux/ubuntu/dm-cache/</url>
      <content type="html"><![CDATA[<p>DM-cache 是一種利用高速的儲存裝置給低速儲存裝置當作快取的技術，透過此一技術使儲存系統兼容容量與效能之間的平衡。DM-cache 目前是 Linunx 核心的一部份，透過裝置映射(Device Mapper)機制允許管理者建立混合的磁區(Volume)。</p>
<a id="more"></a>
<h2 id="快取建立流程"><a href="#快取建立流程" class="headerlink" title="快取建立流程"></a>快取建立流程</h2><p>DM-cache 在比較新版本的 Linux Kernel 已經整合，以下為建置流程：</p>
<pre><code class="sh">$ sudo blockdev --getsize64 /dev/sdb
250059350016

# ssd-metadata : 4194304 + (250059350016 * 16 / 262144) / 512 = 38001
# ssd-blocks :  250059350016 / 512 - 38001 = 488359166
$ sudo dmsetup create ssd-metadata --table &#39;0 38001 linear /dev/sdb 0&#39;
$ sudo dd if=/dev/zero of=/dev/mapper/ssd-metadata
$ sudo dmsetup create ssd-blocks --table &#39;0 189008622 linear /dev/sdb 38001&#39;

$ sudo blockdev --getsz /dev/sdc
1953525168

$ sudo dmsetup create home-cached --table &#39;0 1953525168 cache /dev/mapper/ssd-metadata /dev/mapper/ssd-blocks /dev/sdc 512 1 writeback default 0&#39;
$ ls -l /dev/mapper/home-cached

$ sudo mkdir /mnt/cache
$ sudo mount /dev/mapper/home-cached /mnt/cache
</code></pre>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Storage </tag>
            
            <tag> Linux </tag>
            
            <tag> SSD </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Akka 基本介紹]]></title>
      <url>https://kairen.github.io/2016/04/06/data-engineer/akka-intro/</url>
      <content type="html"><![CDATA[<h1 id="Akka"><a href="#Akka" class="headerlink" title="Akka"></a>Akka</h1><p>Akka 是基於 Actor 模型以 Scala 程式語言開發而成的開源工具，被用在建置可擴展、彈性、高度並行、分散式與快速響應的 JVM 應用程式平台。現在許多高度並行（Concurrent） JVM 應用程式被廣泛應用，尤其以巨量資料處理框架為甚，諸如：Spark、Storm等，甚至可以基於 Akka 建置高平行的 Web 框架，更能建置分散式系統。Akka 最主要的目是要解決同步造成的效能問題，以及可能發生的死鎖問題。</p>
<p>Akka 目前擁有以下幾個特點：</p>
<ul>
<li>高度並行與分散式</li>
<li>可擴展</li>
<li>擁有容錯機制</li>
<li>去中心化，且彈性</li>
<li>基於 Actors 模型</li>
<li>事務性 Actors</li>
<li>支援 JAVA 與 Scala API。</li>
<li>支援叢集</li>
</ul>
<a id="more"></a>
<p>Akka 是基於 <a href="https://zh.wikipedia.org/wiki/%E5%8F%83%E8%88%87%E8%80%85%E6%A8%A1%E5%BC%8F" target="_blank" rel="noopener">Actor</a> 模型來開發，透過 Actor 能夠簡化死鎖與執行緒管理，可以非常容易開發正確並行化行程與系統，在 Akka 中 Actor 是最基本、最重要的元素，被用來完成工作。Actor 具有以下特性：</p>
<ul>
<li>提供高級別的抽象，能簡化在並行（Concurrency）/平行（Parallelism）應用下的程式開發。</li>
<li>提供異步（Async）非阻塞、高效能的事件驅動程式模型。</li>
<li>非常輕量的事件處理（每 GB Heap 記憶體有幾百萬的 Actor）</li>
</ul>
<center><img src="/images/spark/single_play.png" alt=""></center>

<p>Actor 是一個運算實體，在開發程式中就是對實體之間所回應接受到的訊息做互動，同時並行的<code>傳送有限數量的訊息給其他 Actor</code>、<code>建立有限數量的 Actor</code> 以及<code>設計指定接收到下一個訊息時的行為</code>。</p>
<p>Actor 之間是獨立的，多個 Actor 進行互動只能透過自定的訊息（Message）來完成發送與接收處理。如果一個 Actor 在某一個時刻收到多個 Actor 發送的訊息，就會發生並行問題，這時就需要一個訊息佇列來進行訊息的儲存與分散。可參考 <a href="http://www.infoq.com/cn/news/2014/11/intro-actor-model" target="_blank" rel="noopener">Akka 為範例，介紹 Actor 模型</a>。</p>
<p>Akka 應用場景有以下幾個項目，當然這不是全部：</p>
<ul>
<li>交易處理（Transaction Processing）</li>
<li>後端服務（Backend Service）</li>
<li>平行運算（Concurrency/Parallelism）</li>
<li>通訊 Hub（Communications Hub）</li>
<li>複雜事件串流處理（Complex Event Stream Processing）</li>
</ul>
<h2 id="安裝"><a href="#安裝" class="headerlink" title="安裝"></a>安裝</h2><p>Akka 一般在 Java 有兩種安裝方式，如以下：</p>
<ul>
<li>當作 Library 使用，就是直接 Import JAR 使用。可參考 <a href="http://doc.akka.io/docs/akka/2.4.2/java.html?_ga=1.230769695.1061481694.1456285775" target="_blank" rel="noopener">Java Documentation</a>。</li>
<li>將應用放到獨立的微核心（Microkernel）裡使用。可參考 <a href="http://doc.akka.io/docs/akka/2.3.10/scala/microkernel.html" target="_blank" rel="noopener">Microkernel</a>。</li>
</ul>
]]></content>
      
        <categories>
            
            <category> Spark </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Java </tag>
            
            <tag> Spark </tag>
            
            <tag> Concurrent </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Apache Flume 快速上手]]></title>
      <url>https://kairen.github.io/2016/04/04/data-engineer/apache-flume/</url>
      <content type="html"><![CDATA[<p>Apache Flume 是一個分散式日誌收集系統，是由 Cloudera 公司開發的一款高效能、高可靠性和高恢復性的系統。它能從不同來源的大量日誌資料進行高效收集、聚合、移動，最後儲存到一個資料中心儲存系統當中。架構經過重構後，從原來的 Flume OG 到現在的 Flume NG。Flume NG 更像一個輕量化的小套件，簡單使用且容易適應不同方式收集日誌，且支援 Failover 和 Load Balancing</p>
<a id="more"></a>
<h2 id="架構角色說明"><a href="#架構角色說明" class="headerlink" title="架構角色說明"></a>架構角色說明</h2><p>Flume 架構中主要有以下幾個核心:</p>
<ul>
<li><strong>Event</strong>：一個資料單元，會附帶一個可選的訊息來源。ex:日誌紀錄、avro。</li>
<li><strong>Client</strong>：操作位在原點的 Event 且將它傳送到 Flume Agent，主要是產生資料，運行在一個獨立程式。</li>
<li><strong>Agent</strong>：一個獨立的 Flume 程式，包含 Source、Channel、Sink。</li>
<li><strong>Source</strong>：用來消費從 Client 端收集資料到此的 Event，然後傳送到 Channel。</li>
<li><strong>Channel</strong>：轉換 Event 的一個臨時儲存空間，保有從 Source 傳送過來的 Event。</li>
<li><strong>Sink</strong>:從 Channel 中讀取並且移除 Event，將 Event 傳遞到 Flow Pipeline 的下一個 Agent（如果存在的話）。</li>
</ul>
<center><img src="/images/spark/flume_architecture.png" alt=""></center>

<h2 id="安裝-Apache-Flume"><a href="#安裝-Apache-Flume" class="headerlink" title="安裝 Apache Flume"></a>安裝 Apache Flume</h2><p>本節將說明如何部署 Apache Flume，其中包含單機與多機部署方式。</p>
<h3 id="單機"><a href="#單機" class="headerlink" title="單機"></a>單機</h3><p>首先節點需先安裝 Java，這邊採用 Oracle 的 Java8 來進行安裝：</p>
<pre><code class="sh">$ sudo add-apt-repository -y ppa:webupd8team/java
$ sudo apt-get update
$ sudo apt-get install -y oracle-java8-installer
</code></pre>
<p>完成後，在主機上安裝下載 Flume 套件，使用<code>wget</code>下載：</p>
<pre><code class="sh">$ wget &quot;ftp://ftp.twaren.net/Unix/Web/apache/flume/1.6.0/apache-flume-1.6.0-src.tar.gz&quot;
$ wget &quot;ftp://ftp.twaren.net/Unix/Web/apache/flume/1.6.0/apache-flume-1.6.0-bin.tar.gz&quot;
$ tar zxvf apache-flume-1.6.0-src.tar.gz
$ tar zxvf apache-flume-1.6.0-bin.tar.gz
</code></pre>
<p>下載完後，將 src 覆蓋到 bin 底下，並解壓縮到<code>/opt</code>底下:</p>
<pre><code class="sh">$ sudo cp -ri apache-flume-1.6.0-src/* apache-flume-1.6.0-bin
$ sudo mv /opt/apache-flume-1.5.0-bin /opt/flume
</code></pre>
<p>之後到<code>/opt/flume/conf</code>底下建立 example 配置檔:</p>
<pre><code class="sh">$ sudo vim example.conf
</code></pre>
<p>設定以下內容:</p>
<pre><code># example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
</code></pre><p>之後啟動 Flume:</p>
<pre><code class="sh">$ bin/flume-ng agent -c conf -f example.conf -n a1 -Dflume.root.logger=INFO,console
</code></pre>
<blockquote>
<p><code>-c/--conf</code>設定檔目錄，<code>-f/--conf-file</code>設定檔案路徑，<code>-n/--name</code>指定 agent 的名稱</p>
</blockquote>
<p>驗證 Flume 開啟是否已開啟</p>
<pre><code class="sh">$ jps
6760 Jps
6623 Application
</code></pre>
<p>最後開 shell 終端窗口，telnet 到配置監聽 port:</p>
<pre><code>$ telnet localhost 44444
# 輸入
HI!
OK

# 輸出
2016-02-24 11:40:30,389 INFO sink.LoggerSink: Event: { headers:{} body: 48 65 6C 6C 6F 20 77 6F 72 6C 64 21 0D          HI!. }
</code></pre><h3 id="多節點部署"><a href="#多節點部署" class="headerlink" title="多節點部署"></a>多節點部署</h3><p>本節說明多機部署方式，流程為 Agent1 和 Agent2 主要是兩個來源蒐集端，本身會監聽且接收 Flume 本地端的訊息，然後將資料整合到 Collector 做資料日誌整理</p>
<center><img src="/images/spark/flume_cluster.png" alt=""></center>

<p>部署節點角色規則如下:</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Role</th>
</tr>
</thead>
<tbody>
<tr>
<td>192.168.100.94</td>
<td>Agent1</td>
</tr>
<tr>
<td>192.168.100.96</td>
<td>Agent2</td>
</tr>
<tr>
<td>192.168.100.97</td>
<td>Collector</td>
</tr>
</tbody>
</table>
<p>一開始安裝配置與單機相同，從第一步驟到下載完後，將 src 覆蓋到 bin 底下，並解壓縮到<code>/opt</code>底下</p>
<p>然後到各自的<code>/opt/flume/conf</code>底下建立配置檔</p>
<p><code>Agent1</code>和<code>Agent2</code>配置內容如下:</p>
<pre><code># flume-client.properties: Agent1 Flume configuration

#agent1 name
agent1.sources = r1
agent1.sinks = k1
agent1.channels = c1

#set gruop
agent1.sinkgroups = g1

#set channel
agent1.channels.c1.type = memory
agent1.channels.c1.capacity = 1000
agent1.channels.c1.transactionCapacity = 100

#set source
agent1.sources.r1.channels = c1
agent1.sources.r1.type = netcat
agent1.sources.r1.bind = localhost
agent1.sources.r1.port = 52020

agent1.sources.r1.interceptors = i1
agent1.sources.r1.interceptors.i1.type = static
agent1.sources.r1.interceptors.i1.key = Type
agent1.sources.r1.interceptors.i1.value = LOGIN

# set sink
agent1.sinks.k1.channel = c1
agent1.sinks.k1.type = avro
agent1.sinks.k1.hostname = 192.168.100.97
agent1.sinks.k1.port = 44444

#set sink group
agent1.sinkgroups.g1.sinks = k1

#set failover
agent1.sinkgroups.g1.processor.type = failover
agent1.sinkgroups.g1.processor.priority.k1 = 10
agent1.sinkgroups.g1.processor.maxpenalty = 10000
</code></pre><p><code>Collector</code> 配置內容如下:</p>
<pre><code class="sh"># flume-server.properties: Agent1 Flume configuration

#set Agent name
a1.sources = r1
a1.sinks = k1
a1.channels = c1

#set channel
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

#set source
a1.sources.r1.type = avro
a1.sources.r1.bind = 192.168.100.97
a1.sources.r1.port = 44444
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = static
a1.sources.r1.interceptors.i1.key = Collector
a1.sources.r1.interceptors.i1.value = NNA
a1.sources.r1.channels = c1

# set sink
a1.sinks.k1.type=logger
a1.sinks.k1.channel=c1
</code></pre>
<p>最後分別啓動<code>Agent</code>和<code>Collector</code>的 Flume</p>
<blockquote>
<p>Agent:</p>
<pre><code class="sh">$ bin/flume-ng agent -n agent1 -c conf -f flume-client.properties -Dflume.root.logger=DEBUG,console
</code></pre>
<p>Collector:</p>
<pre><code class="sh">$ bin/flume-ng agent -n a1 -c conf -f flume-server.properties -Dflume.root.logger=DEBUG,console
</code></pre>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Spark </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Logging </tag>
            
            <tag> Spark </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[OpenStack 貢獻基本流程]]></title>
      <url>https://kairen.github.io/2016/04/01/openstack/how-to-contribute/</url>
      <content type="html"><![CDATA[<p>本部分將說明如何貢獻程式碼到 OpenStack 社群上。</p>
<a id="more"></a>
<h3 id="申請-OpenStack-帳號流程"><a href="#申請-OpenStack-帳號流程" class="headerlink" title="申請 OpenStack 帳號流程"></a>申請 OpenStack 帳號流程</h3><p>首先註冊 Launchpad.net 帳號：</p>
<ol>
<li>到 <a href="https://login.launchpad.net/" target="_blank" rel="noopener">Launchpad</a> 註冊一個帳號，使用 inwinStack 信箱</li>
<li>完成申請後，進入 <a href="https://launchpad.net/~inwinstack" target="_blank" rel="noopener">inwinSTACK Org</a> 點選<code>加入請求</code></li>
<li>確認自己出現在 inwinSTACK Menber List</li>
</ol>
<p>之後註冊 OpenStack Foundation 的 Foundation Member，到 <a href="https://www.openstack.org/join/register" target="_blank" rel="noopener">Register</a> 申請 <code>Foundation Member</code>。並完成以下步驟：</p>
<ol>
<li>填寫使用者資訊。</li>
<li>在 Affiliations 部分，點選 <code>Add New Affiliations</code>的組織輸入<code>inwinSTACK</code>，選擇開始時間，勾選 <code>Is Current?</code></li>
<li>最後填寫住址與密碼資訊，住址翻譯網站 : <a href="http://goo.gl/qez9tt" target="_blank" rel="noopener">http://goo.gl/qez9tt</a></li>
</ol>
<p>然後使用 Launchpad.net  登入 OpenStack的 Gerrit 平台，到 <a href="https://review.openstack.org/" target="_blank" rel="noopener">Gerrit</a> 點選 <code>sign in</code>，登入 Launchpad 帳號。當第一次登入成功後，會需要你設定唯一的 username（注意設定後就不能更改）。並完成以下步驟：</p>
<ul>
<li>簽署 ICLA，<ul>
<li>到 <a href="https://review.openstack.org/#/settings/new-agreement" target="_blank" rel="noopener">New Agreement</a></li>
<li>選擇ICLA（OpenStack Individual Contributor License Agreement）</li>
</ul>
</li>
<li>上傳 SSH 公有金鑰，<ul>
<li>到 <a href="https://review.openstack.org/#/settings/ssh-keys" target="_blank" rel="noopener">Git Review SSH Keys</a> 上傳 Key。</li>
<li>用<code>ssh-keygen -t rsa -b 4096 -C &quot;your_email@example.com&quot;</code> 指令產生金鑰，複製 <code>~/.ssh/id_rsa.pub</code>到 review.openstack.org 上。產生 key 參考 <a href="https://help.github.com/articles/generating-ssh-keys/" target="_blank" rel="noopener">generating-ssh-keys</a></li>
</ul>
</li>
</ul>
<p>完成後，設定 Git 資訊：</p>
<pre><code class="sh">git config --global user.name &quot;Firstname Lastname&quot;
git config --global user.email &quot;your_email@youremail.com&quot;

git config --global gitreview.username &quot;yourgerritusername&quot;
</code></pre>
<p>之後安裝 git-review，參考 <a href="http://www.mediawiki.org/wiki/Gerrit/git-review" target="_blank" rel="noopener">Git Review install</a> 進行安裝。</p>
<h3 id="貢獻程式碼（已-openstack-manuals-為例）"><a href="#貢獻程式碼（已-openstack-manuals-為例）" class="headerlink" title="貢獻程式碼（已 openstack-manuals 為例）"></a>貢獻程式碼（已 openstack-manuals 為例）</h3><p>一個基本的貢獻流程如下圖所示：</p>
<center><img src="/images/openstack/contribute_flow.jpg" alt=""></center>

<p>首先透過 git clone 來下載程式專案，並設定 review：</p>
<pre><code class="sh">git clone https://github.com/openstack/openstack-manuals
cd openstack-manuals
git review -s
</code></pre>
<blockquote>
<p>成功的話，會在目錄底下產生檔案<code>.gitreview</code>。若 auth 有問題請檢查 ssh key 是否正確。</p>
</blockquote>
<p>並透過 git 來切換到最新版本：</p>
<pre><code class="sh">git checkout master
git pull
</code></pre>
<p>新建一個 branch，在單獨的一行中撰寫 summary（小於50個字），然後第二段進行詳細的描述。如果是實現 bp 或修改 bug，需要註明：</p>
<ul>
<li>blueprint BP-NAME</li>
<li>bug BUG-NUMBER</li>
</ul>
<p>一個簡單範例：</p>
<pre><code>Adds some summary less than  50  characters   

...Long multiline description of the change...   

Implements: blueprint authentication   
Fixes: bug # 123456
</code></pre><blockquote>
<p>詳細的程式碼提交資訊，參考 <a href="https://wiki.openstack.org/wiki/GitCommitMessages" target="_blank" rel="noopener">GitCommitMessages</a>。</p>
</blockquote>
<p>修改完程式碼後，記得跑過UT的測試。然後提交程式碼，並申請 review：</p>
<pre><code class="sh">git commit -a
git review
</code></pre>
<p>提交 review 之後，會出現在 <a href="https://review.openstack.org" target="_blank" rel="noopener">Git review</a>，可以查看狀態和資訊，並自動執行 CI，然後程式碼會由 review 人員進行程式碼的 review。</p>
<p>如果 jenkins 回報了 failure，可以查看 Logs 除錯。如果確認不是自己的 patch 導致，可以在 comment 上留言 <code>recheck no bug</code>，重新再跑 Test。</p>
<p>如果 review 過程中，發現程式碼需要修改，再次提交時直接使用已存在的 Change-Id：</p>
<pre><code class="sh">git commit -a --amend
git review
</code></pre>
]]></content>
      
        <categories>
            
            <category> OpenStack </category>
            
        </categories>
        
        
        <tags>
            
            <tag> OpenStack </tag>
            
            <tag> Git </tag>
            
            <tag> Python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[DRBD 進行跨節點的區塊儲存備份]]></title>
      <url>https://kairen.github.io/2016/04/01/linux/ubuntu/drbd/</url>
      <content type="html"><![CDATA[<p>DRBD（Distributed Replicated Block<br>Device）是一個分散式區塊裝置備份系統，DRBD 是由 Kernel 模組與相關腳本組成，被用來建置高可靠的叢集服務。實現方式是透過網路來 mirror 整個區塊裝置，一般可作為是網路 RAID 的一類。DRBD 允許使用者在遠端機器上建立一個 Local 區塊裝置的即時 mirror。</p>
<a id="more"></a>
<h3 id="安裝-DRBD"><a href="#安裝-DRBD" class="headerlink" title="安裝 DRBD"></a>安裝 DRBD</h3><p>本教學將使用以下主機數量與角色：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Role</th>
<th>Disk</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.16.1.184</td>
<td>master</td>
<td>/dev/vdb</td>
</tr>
<tr>
<td>172.16.1.182</td>
<td>backup</td>
<td>/dev/vdb</td>
</tr>
</tbody>
</table>
<p>在 Ubuntu 14.04 LTS Server 可以直接透過<code>apt-get</code>來安裝 DRBD，指令如下：</p>
<pre><code class="sh">$ sudo apt-get install linux-image-extra-virtual
$ sudo apt-get install -y drbd8-utils
</code></pre>
<blockquote>
<p>完成後可以透過 lsmod 檢查：</p>
<pre><code class="sh">$ lsmod | grep drbd

# 若沒有則使用以下指令
$ sudo modprobe drbd
</code></pre>
<p>P.S 若出現錯誤請重新啟動主機。</p>
</blockquote>
<h3 id="DRBD-設定"><a href="#DRBD-設定" class="headerlink" title="DRBD 設定"></a>DRBD 設定</h3><p>首先在各兩個節點透過<code>fdisk</code>來建立分區：</p>
<pre><code class="sh">$ fdisk /dev/vdb

Command (m for help): n
Partition type:
   p   primary (0 primary, 0 extended, 4 free)
   e   extended
Select (default p): p
Partition number (1-4, default 1): 1
First sector (2048-20971519, default 2048): 2048
Last sector, +sectors or +size{K,M,G} (2048-20971519, default 20971519):
Using default value 20971519

Command (m for help): w
</code></pre>
<p>之後建立<code>/etc/drbd.d/ha.res</code>設定檔，並加入以下內容：</p>
<pre><code class="sh">resource ha {
  on drbd-master {
    device /dev/drbd0;
    disk /dev/vdb1;
    address 172.16.1.184:1166;
    meta-disk internal;
 }
 on drbd-backup {
    device /dev/drbd0;
    disk /dev/vdb1;
    address 172.16.1.182:1166;
    meta-disk internal;
  }
}
</code></pre>
<p>上面都設定完成後，到<code>master</code>接著透過<code>drbdadm</code>指令建立：</p>
<pre><code class="sh">$ drbdadm create-md ha

Writing meta data...
md_offset 10736365568
al_offset 10736332800
bm_offset 10736005120

Found some data

 ==&gt; This might destroy existing data! &lt;==

Do you want to proceed?
[need to type &#39;yes&#39; to confirm] yes

initializing activity log
NOT initializing bitmap
New drbd meta data block successfully created.
</code></pre>
<p>透過指令啟用：</p>
<pre><code class="sh">$ drbdadm up ha
$ drbd-overview
0:ha/0  WFConnection Secondary/Unknown Inconsistent/DUnknown C r----s
</code></pre>
<p>設定某一節點為主節點：</p>
<pre><code class="sh">$ drbdadm -- --force primary ha
$ drbd-overview
0:ha/0  WFConnection Primary/Unknown UpToDate/DUnknown C r----s
</code></pre>
<p>檢查是否有正確啟動：</p>
<pre><code>$ cd /dev/drbd
$ ls
by-disk  by-res

$ ls -al by-disk/
total 0
drwxr-xr-x 2 root root 60 Mar 24 16:46 .
drwxr-xr-x 4 root root 80 Mar 24 16:46 ..
lrwxrwxrwx 1 root root 11 Mar 24 16:49 vdb1 -&gt; ../../drbd0

$ ls -al by-res/ha/
lrwxrwxrwx 1 root root 11 Mar 24 16:49 by-res/ha -&gt; ../../drbd0
</code></pre><p>若沒問題後，即可 mount 使用：</p>
<pre><code class="sh">$ mount /dev/drbd0 /mnt/
</code></pre>
<blockquote>
<p>若出現<code>mount: you must specify the filesystem type</code>的話，記得格式化：</p>
<pre><code class="sh">$ mkfs.ext4 /dev/drbd0
</code></pre>
</blockquote>
<p>這時候再透過指令查詢，可以看到已成功同步：</p>
<pre><code class="sh">$ drbd-overview
0:ha/0  WFConnection Primary/Unknown UpToDate/DUnknown C r----s /mnt ext4 9.8G 23M 9.2G 1%
</code></pre>
<p>接著到<code>backup</code>節點，執行類似上面做法：</p>
<pre><code class="sh">$ drbdadm create-md ha
$ drbdadm up ha
$ drbd-overview
  0:ha/0  SyncTarget Secondary/Primary Inconsistent/UpToDate C r-----
    [========&gt;...........] sync&#39;ed: 47.1% (5420/10236)Mfinish: 0:02:10 speed: 42,600 (45,252) want: 0 K/se
</code></pre>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Storage </tag>
            
            <tag> Linux </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[利用 Keepalived 提供 VIP]]></title>
      <url>https://kairen.github.io/2016/03/29/linux/ubuntu/keepalived/</url>
      <content type="html"><![CDATA[<p>Keepalived 是一種基於 VRRP 協定實現的高可靠 Web 服務方案，用於防止單點故障問題。因此一個 Web 服務運作至少會擁有兩台伺服器執行 Keepalived，一台作為 master，一台作為 backup，並提供一個虛擬 IP（VIP），master 會定期發送特定訊息給 backup 伺服器，當 backup 沒收到 master 訊息時，表示 master 已故障，這時候 backup 會接管 VIP，繼續提供服務，來確保服務的高可靠性。</p>
<a id="more"></a>
<h3 id="VRRP"><a href="#VRRP" class="headerlink" title="VRRP"></a>VRRP</h3><p>VRRP（Virtual Router Redundancy Protocol，虛擬路由器備援協定），是一個提供備援路由器來解決單點故障問題的協定，該協定有兩個重要概念：</p>
<ul>
<li><p><strong>VRRP 路由器與虛擬路由器</strong>：VRRP 路由器是表示運作 VRRP 的路由器，是一個實體裝置，而虛擬路由器是指由 VRRP 建立的邏輯路由器。一組 VRRP 路由器協同運作，並一起構成一台虛擬路由器，該虛擬路由對外提供一個唯一固定的 IP 與 MAC 位址的邏輯路由器。</p>
</li>
<li><p><strong>主控制路由器（master）與備援路由器（backup）</strong>：主要是在一組 VRRP 中的兩種互斥角色。一個 VRRP 群組中只能擁有一台是 master，但可以有多個 backup 路由器。</p>
</li>
</ul>
<p>VRRP 協定使用選擇策略從路由器群組挑選一台作為 master 來負責 ARP 與轉送 IP 封包，群組中其他路由器則作為 backup 的角色處理等待狀態。當由於某種原因造成 master 故障時，backup 會在幾秒內成為 master 繼續提供服務，該階段不用改變任何 IP 與 MAC 位址。</p>
<h3 id="Keepalived-節點配置"><a href="#Keepalived-節點配置" class="headerlink" title="Keepalived 節點配置"></a>Keepalived 節點配置</h3><p>本教學將使用以下主機數量與角色：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Role</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.16.1.101</td>
<td>vip</td>
</tr>
<tr>
<td>172.16.1.102</td>
<td>master</td>
</tr>
<tr>
<td>172.16.1.103</td>
<td>backup</td>
</tr>
</tbody>
</table>
<h3 id="安裝與設定"><a href="#安裝與設定" class="headerlink" title="安裝與設定"></a>安裝與設定</h3><p>這 ubuntu 14.04 LTS Server 中已經內建了 Keepalived 可以透過 apt-get 來安裝：</p>
<pre><code class="sh">$ sudo apt-get install -y keepalived
</code></pre>
<blockquote>
<p>也可以透過 source code 進行安裝，流程如下：</p>
<pre><code class="sh">$ sudo apt-get install build-essential libssl-dev
$ wget http://www.keepalived.org/software/keepalived-1.2.2.tar.gz
$ tar -zxvf keepalived-1.2.2.tar.gz
$ cd keepalived-1.2.2
$ ./configure --prefix=/usr/local/keepalived
$ make &amp;&amp; make install
</code></pre>
</blockquote>
<p>完成後，要將需要的設定檔進行複製到<code>/etc/</code>:</p>
<pre><code>$ cp /usr/local/keepalived/etc/rc.d/init.d/keepalived /etc/init.d/keepalived
$ cp /usr/local/keepalived/sbin/keepalived /usr/sbin/
$ cp /usr/local/keepalived/etc/sysconfig/keepalived /etc/sysconfig/
$ mkdir -p /etc/keepalived/
$ cp /usr/local/etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf
</code></pre><p>安裝完成後編輯<code>/etc/keepalived/keepalived.conf</code>檔案進行設定，在<code>master</code>節點加入以下內容：</p>
<pre><code class="sh">global_defs {
   notification_email {
      user@example.com
   }

   notification_email_from mail@example.org
   smtp_server 172.16.1.100
   smtp_connect_timeout 30
   router_id LVS_DEVEL
}

vrrp_instance VI_1 {
    state MASTER # Tag 為 MASTER
    interface eth0
    virtual_router_id 51
    priority 101   # MASTER 權重高於 BACKUP
    advert_int 1
    mcast_src_ip 172.16.1.102 # VRRP 實體主機的 IP

    authentication {
        auth_type PASS # Master 驗證方式
        auth_pass 1111
    }

    #VIP
    virtual_ipaddress {
        172.16.1.101 # 虛擬 IP
    }
}
</code></pre>
<p>Master 完成後，接著編輯<code>backup</code>節點的<code>/etc/keepalived/keepalived.conf</code>，加入以下內容：</p>
<pre><code class="sh">global_defs {
   notification_email {
       user@example.com
   }

   notification_email_from mail@example.org
   smtp_server 172.16.1.100
   smtp_connect_timeout 30
   router_id LVS_DEVEL
}

vrrp_instance VI_1 {

    state BACKUP # Tag 為 BACKUP
    interface eth0
    virtual_router_id 51
    priority 100  # 權重要低於 MASTER
    advert_int 1
    mcast_src_ip 172.16.1.103 # vrrp 實體主機 IP

    authentication {
        auth_type PASS
        auth_pass 1111
    }

    # VIP
    virtual_ipaddress {
        172.16.1.101 # 提供的 VIP
    }
}
</code></pre>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux </tag>
            
            <tag> High Availability </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[使用 HAProxy 進行負載平衡]]></title>
      <url>https://kairen.github.io/2016/03/28/linux/ubuntu/haproxy/</url>
      <content type="html"><![CDATA[<p>HAProxy 提供了高可靠性、負載平衡（Load Balancing）、基於 TCP 以及 HTTP 的應用程式代理，更支援了虛擬機的使用。HAProxy 是一個開放式原始碼，免費、快速以及非常可靠，根據官方測試結果，該軟體最高能夠支援到 10G 的並行傳輸，因此特別適合使用在負載很大的 Web 伺服器，且這些伺服器通常需要保持 Session 或者 Layer 7 網路的處理，但這些都可以使用 HAProxy 來完成。</p>
<p>HAProxy 具有以下幾個優點：</p>
<ul>
<li>開放式原始碼，因此免費，且穩定性高</li>
<li>能夠負荷 10G 網路的並行傳輸</li>
<li>支援連線拒絕功能</li>
<li>支援全透明化的代理</li>
<li>擁有內建的監控狀態儀表板</li>
<li>支援虛擬機的使用</li>
</ul>
<a id="more"></a>
<h3 id="HAProxy-安裝"><a href="#HAProxy-安裝" class="headerlink" title="HAProxy 安裝"></a>HAProxy 安裝</h3><p>本教學會使用到一台 Proxy 節點與兩台 Web 節點，如下：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Role</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.17.0.2</td>
<td>proxy</td>
</tr>
<tr>
<td>172.17.0.3</td>
<td>web-1</td>
</tr>
<tr>
<td>172.17.0.4</td>
<td>web-2</td>
</tr>
</tbody>
</table>
<p>本篇採用 Ubuntu 作業系統，因此可透過 apt 直接安裝，以下範例是在 Ubuntu Server 環境中操作：</p>
<pre><code class="sh">$ sudo apt-get install software-properties-common python-software-properties
$ sudo apt-add-repository ppa:vbernat/haproxy-1.5
$ sudo apt-get update
$ sudo apt-get install haproxy
</code></pre>
<blockquote>
<p>若要安裝其他版本，可以修改成以下：</p>
<pre><code class="sh">sudo apt-add-repository ppa:vbernat/haproxy-1.6
</code></pre>
</blockquote>
<h3 id="HAProxy-設定"><a href="#HAProxy-設定" class="headerlink" title="HAProxy 設定"></a>HAProxy 設定</h3><p>完成安裝後，要透過編輯<code>/etc/haproxy/haproxy.cfg</code>設定檔來配置 Proxy：</p>
<pre><code class="sh">global
    log /dev/log    local0
    log /dev/log    local1 notice
    chroot /var/lib/haproxy
    stats socket /run/haproxy/admin.sock mode 660 level admin
    stats timeout 30s
    user haproxy
    group haproxy
    daemon
        maxconn 1024

    ca-base /etc/ssl/certs
    crt-base /etc/ssl/private

    ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS
    ssl-default-bind-options no-sslv3

defaults
    log    global
    mode    http
    option    httplog
    option    dontlognull
        timeout connect 5000
        timeout client  50000
        timeout server  50000
    errorfile 400 /etc/haproxy/errors/400.http
    errorfile 403 /etc/haproxy/errors/403.http
    errorfile 408 /etc/haproxy/errors/408.http
    errorfile 500 /etc/haproxy/errors/500.http
    errorfile 502 /etc/haproxy/errors/502.http
    errorfile 503 /etc/haproxy/errors/503.http
    errorfile 504 /etc/haproxy/errors/504.http

frontend nginxs_proxy
    bind 172.17.0.2:80
    mode http
    default_backend nginx_servers

backend nginx_servers
    mode http
    balance roundrobin
    option forwardfor
    http-request set-header X-Forwarded-Port %[dst_port]
    http-request add-header X-Forwarded-Proto https if { ssl_fc }
    option httpchk HEAD / HTTP/1.1\r\nHost:localhost
    server web1 172.17.0.3:80 check cookie s1
    server web2 172.17.0.4:80 check cookie s2

listen haproxy_stats
    bind 0.0.0.0:8080
    stats enable
    stats hide-version
    stats refresh 30s
    stats show-node
    stats auth username:password
    stats uri  /stats
</code></pre>
<p>完成設定後，需重啟服務：</p>
<pre><code class="sh">$ sudo service haproxy restart
</code></pre>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Load Balancer </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[透過官方 Ansible 部署 Kubernetes(Unrecommended)]]></title>
      <url>https://kairen.github.io/2016/02/24/kubernetes/deploy/official-ansible/</url>
      <content type="html"><![CDATA[<p>Kubernetes 提供了許多雲端平台與作業系統的安裝方式，本篇將使用官方 Ansible Playbook 來部署 Kubernetes 到 CentOS 7 系統上，其中 Kubernetes 將額外部署 Dashboard 與 DNS 等 Add-ons。其他更多平台的部署可以參考 <a href="https://kubernetes.io/docs/getting-started-guides/" target="_blank" rel="noopener">Creating a Kubernetes Cluster</a>。</p>
<center><img src="/images/kube/kube-ansible.png" alt=""></center>

<p>本次安裝版本為：</p>
<ul>
<li>Kubernetes v1.5.2</li>
<li>Etcd v3.1.0</li>
<li>Flannel v0.5.5</li>
<li>Docker v1.12.6</li>
</ul>
<a id="more"></a>
<h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統採用<code>CentOS 7.x</code>：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Role</th>
<th>CPU</th>
<th>Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.16.35.12</td>
<td>master1</td>
<td>2</td>
<td>4G</td>
</tr>
<tr>
<td>172.16.35.10</td>
<td>node1</td>
<td>2</td>
<td>4G</td>
</tr>
<tr>
<td>172.16.35.11</td>
<td>node2</td>
<td>2</td>
<td>4G</td>
</tr>
</tbody>
</table>
<blockquote>
<p>這邊 master 為主要控制節點，node 為應用程式工作節點。</p>
</blockquote>
<h2 id="預先準備資訊"><a href="#預先準備資訊" class="headerlink" title="預先準備資訊"></a>預先準備資訊</h2><p>首先安裝前要確認以下幾項都已將準備完成：</p>
<ul>
<li>所有節點彼此網路互通，並且不需要 SSH 密碼即可登入。</li>
<li>所有主機擁有 Sudoer 權限。</li>
<li>所有節點需要設定<code>/etc/host</code>解析到所有主機。</li>
<li><code>master1</code>或部署節點需要安裝 Ansible 與相關套件：</li>
</ul>
<pre><code class="sh">$ sudo yum install -y epel-release
$ sudo yum install -y ansible python-netaddr git
</code></pre>
<h2 id="部署-Kubernetes"><a href="#部署-Kubernetes" class="headerlink" title="部署 Kubernetes"></a>部署 Kubernetes</h2><p>首先透過 Git 工具來取得 Kubernetes 官方的 Ansible Playbook 專案，並進入到目錄：</p>
<pre><code class="sh">$ git clone &quot;https://github.com/kubernetes/contrib.git&quot;
$ cd contrib/ansible
</code></pre>
<p>編輯<code>inventory/hosts</code>檔案(inventory)，並加入以下內容：</p>
<pre><code>[masters]
master1

[etcd:children]
masters

[nodes]
node[1:2]
</code></pre><p>然後利用 Ansible ping module 來檢查節點是否可以溝通：</p>
<pre><code class="sh">$ ansible -i inventory/hosts all -m ping
master1 | SUCCESS =&gt; {
    &quot;changed&quot;: false,
    &quot;ping&quot;: &quot;pong&quot;
}
node2 | SUCCESS =&gt; {
    &quot;changed&quot;: false,
    &quot;ping&quot;: &quot;pong&quot;
}
node1 | SUCCESS =&gt; {
    &quot;changed&quot;: false,
    &quot;ping&quot;: &quot;pong&quot;
}
</code></pre>
<p>編輯<code>inventory/group_vars/all.yml</code>檔案，並修改以下內容：</p>
<pre><code>source_type: packageManager
cluster_name: cluster.kairen
networking: flannel
cluster_logging: true
cluster_monitoring: true
kube_dash: true
dns_setup: true
dns_replicas: 1
</code></pre><blockquote>
<p>其他參數可自行選擇是否啟用。</p>
</blockquote>
<p>(Option)編輯<code>roles/flannel/defaults/main.yaml</code>檔案，修改以下內容：</p>
<pre><code>flannel_options: --iface=enp0s8
</code></pre><blockquote>
<p>這邊主要解決 Vagrant 預設抓 NAT 網卡問題。</p>
</blockquote>
<p>完成後進入到<code>scripts</code>目錄，並執行以下指令進行部署：</p>
<pre><code class="sh">$ INVENTORY=../inventory/hosts ./deploy-cluster.sh
...
PLAY RECAP *********************************************************************
master1                    : ok=229  changed=93   unreachable=0    failed=0
node1                      : ok=126  changed=58   unreachable=0    failed=0
node2                      : ok=122  changed=58   unreachable=0    failed=0
</code></pre>
<p>經過一段時候就會完成，若沒有發生任何錯誤的話，就可以令用 kubectl 查看節點資訊：</p>
<pre><code class="sh">$ kubectl get nodes
NAME      STATUS    AGE
node1     Ready     3m
node2     Ready     3m
</code></pre>
<p>查看系統命名空間的 pod 與 svc 資訊：</p>
<pre><code class="sh">$ kubectl get svc --all-namespaces
NAMESPACE     NAME                    CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
default       kubernetes              10.254.0.1       &lt;none&gt;        443/TCP             3h
kube-system   elasticsearch-logging   10.254.164.5     &lt;none&gt;        9200/TCP            3h
kube-system   heapster                10.254.213.162   &lt;none&gt;        80/TCP              3h
kube-system   kibana-logging          10.254.176.124   &lt;none&gt;        5601/TCP            3h
kube-system   kube-dns                10.254.0.10      &lt;none&gt;        53/UDP,53/TCP       3h
kube-system   kubedash                10.254.68.80                   80/TCP              3h
kube-system   kubernetes-dashboard    10.254.84.138    &lt;none&gt;        80/TCP              3h
kube-system   monitoring-grafana      10.254.193.233   &lt;none&gt;        80/TCP              3h
kube-system   monitoring-influxdb     10.254.135.115   &lt;none&gt;        8083/TCP,8086/TCP   3h
</code></pre>
<blockquote>
<p>完成後，透過瀏覽器進入 <a href="http://k8s-master:8080/ui" target="_blank" rel="noopener">Dashboard</a>。</p>
</blockquote>
<h2 id="Targeted-runs"><a href="#Targeted-runs" class="headerlink" title="Targeted runs"></a>Targeted runs</h2><p>Ansible 提供 Tag 來指定執行或者忽略，這邊腳本也提供了該功能，如以下只部署 Etcd：</p>
<pre><code class="sh">$ ./deploy-cluster.sh --tags=etcd
</code></pre>
]]></content>
      
        <categories>
            
            <category> Kubernetes </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Docker </tag>
            
            <tag> Kubernetes </tag>
            
            <tag> Ansible </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Vagrant CoreOS 部署 Kubernetes 測試叢集(Unrecommended)]]></title>
      <url>https://kairen.github.io/2016/02/23/kubernetes/deploy/vagrant-multi-install/</url>
      <content type="html"><![CDATA[<p>本節將透過 Vagrant 與 CoreOS 來部署單機多節點的 Kubernetes 虛擬叢集，並使用 Kubernetest CLI 工具與 API 進行溝通。</p>
<p>本次安裝版本為：</p>
<ul>
<li>CoreOS alpha.</li>
<li>Kubernetes v1.5.4.</li>
</ul>
<a id="more"></a>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>首先必須在主機上安裝<code>Vagrant</code>工具，點選該 <a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="noopener">Vagrant downloads</a> 頁面抓取當前系統的版本，並完成安裝。</p>
<p>接著在主機上安裝<code>kubectl</code>，該程式是主要與 Kubernetes API 進行溝通的工具，透過 Curl 工具來下載。如果是 Linux 作業系統，請下載以下：</p>
<pre><code class="sh">$ curl -O &quot;https://storage.googleapis.com/kubernetes-release/release/v1.5.4/bin/linux/amd64/kubectl&quot;
$ chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/
</code></pre>
<blockquote>
<p>如果是 OS X，請取代 URL 為以下：</p>
<pre><code class="sh">$ curl -O &quot;https://storage.googleapis.com/kubernetes-release/release/v1.5.4/bin/darwin/amd64/kubectl&quot;
$ chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/
</code></pre>
</blockquote>
<h2 id="安裝-Kubernetes"><a href="#安裝-Kubernetes" class="headerlink" title="安裝 Kubernetes"></a>安裝 Kubernetes</h2><p>首先透過 Git 工具來下載 CoreOS 的 Kubernetes 專案，裡面包含了描述 Vagrant 要建立的檔案：</p>
<pre><code class="sh">$ git clone https://github.com/coreos/coreos-kubernetes.git
$ cd coreos-kubernetes/multi-node/vagrant
</code></pre>
<p>接著複製<code>config.rb.sample</code>並改成<code>config.rb</code>檔案：</p>
<pre><code class="sh">$ cp config.rb.sample config.rb
</code></pre>
<p>編輯<code>config.rb</code>設定檔，並修改成以下內容：</p>
<pre><code class="ruby">$update_channel=&quot;alpha&quot;

$controller_count=1
$controller_vm_memory=1024

$worker_count=2
$worker_vm_memory=1024

$etcd_count=1
$etcd_vm_memory=512
</code></pre>
<p>(Option)若 CNI 想使用 Calico 網路與安裝不同版本 Kubernetes 的話，需要修改<code>../generic/controller-install.sh</code>與<code>./generic/worker-install.sh</code>檔案以下內容：</p>
<pre><code class="sh">export K8S_VER=v1.5.4_coreos.0
export USE_CALICO=true
</code></pre>
<p>設定好後，即可透過以下指令來建立 SSL CA Key 與更新 Box 資訊：</p>
<pre><code class="sh">$ sudo ln -sf /usr/local/bin/openssl /opt/vagrant/embedded/bin/openssl
$ vagrant box update
</code></pre>
<p>確認完成後，執行以下指令開始建立叢集：</p>
<pre><code class="sh">$ vagrant up
</code></pre>
<blockquote>
<p>P.S. 這邊建置起來裡面虛擬機還要下載一些東西，要等一下子才會真正完成。</p>
</blockquote>
<h2 id="設定-Kubernetes-Config"><a href="#設定-Kubernetes-Config" class="headerlink" title="設定 Kubernetes Config"></a>設定 Kubernetes Config</h2><p>當完成部署後，需要配置 kubectl 連接 API，這邊可以選擇以下兩種的其中一種進行：</p>
<h3 id="使用一個-Custom-Kubernetes-Config"><a href="#使用一個-Custom-Kubernetes-Config" class="headerlink" title="使用一個 Custom Kubernetes Config"></a>使用一個 Custom Kubernetes Config</h3><pre><code class="sh">$ export KUBECONFIG=&quot;${KUBECONFIG}:$(pwd)/kubeconfig&quot;
$ kubectl config use-context vagrant-multi
</code></pre>
<h3 id="更新與使用本地的-Config"><a href="#更新與使用本地的-Config" class="headerlink" title="更新與使用本地的 Config"></a>更新與使用本地的 Config</h3><pre><code class="sh">$ kubectl config set-cluster vagrant-multi-cluster --server=&quot;https://172.17.4.101:443&quot; --certificate-authority=${PWD}/ssl/ca.pem
$ kubectl config set-credentials vagrant-multi-admin --certificate-authority=${PWD}/ssl/ca.pem --client-key=${PWD}/ssl/admin-key.pem --client-certificate=${PWD}/ssl/admin.pem
$ kubectl config set-context vagrant-multi --cluster=vagrant-multi-cluster --user=vagrant-multi-admin
$ kubectl config use-context vagrant-multi
</code></pre>
<h2 id="Kubernetes-系統驗證"><a href="#Kubernetes-系統驗證" class="headerlink" title="Kubernetes 系統驗證"></a>Kubernetes 系統驗證</h2><p>完成設定後，即可使用 kubectl 來查看節點資訊：</p>
<pre><code class="sh">$ kubectl get nodes
NAME           STATUS                     AGE
172.17.4.101   Ready,SchedulingDisabled   3m
172.17.4.201   Ready                      3m
172.17.4.202   Ready                      3m
</code></pre>
<p>查看系統命名空間的 pod 與 svc 資訊：</p>
<pre><code class="sh">$ kubectl get po --all-namespaces
NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE
kube-system   heapster-v1.2.0-4088228293-4vv12        2/2       Running   0          28m
kube-system   kube-apiserver-172.17.4.101             1/1       Running   0          29m
kube-system   kube-controller-manager-172.17.4.101    1/1       Running   0          29m
kube-system   kube-dns-782804071-w6w12                4/4       Running   0          29m
kube-system   kube-dns-autoscaler-2715466192-q1k18    1/1       Running   0          29m
kube-system   kube-proxy-172.17.4.101                 1/1       Running   0          28m
kube-system   kube-proxy-172.17.4.201                 1/1       Running   0          29m
kube-system   kube-proxy-172.17.4.202                 1/1       Running   0          29m
kube-system   kube-scheduler-172.17.4.101             1/1       Running   0          28m
kube-system   kubernetes-dashboard-3543765157-vk0mt   1/1       Running   0          29m
</code></pre>
]]></content>
      
        <categories>
            
            <category> Kubernetes </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Docker </tag>
            
            <tag> Kubernetes </tag>
            
            <tag> CoreOS </tag>
            
            <tag> Vagrant </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Ansible Playbooks]]></title>
      <url>https://kairen.github.io/2016/02/18/devops/ansible/ansible-playbook/</url>
      <content type="html"><![CDATA[<p>Playbooks 是 Ansible 的設定、部署與編配語言等。可以被用來描述一個被遠端的主機要執行的指令方案，或是一組 IT 行程執行的指令集合。</p>
<p>在基礎層面上，Playbooks 可以被用來管理部署到遠端主機的組態檔案，在更高階層上 Playbooks 可以循序對多層式架構上的伺服器執行線上的 Polling 更新內部的操作，並將操作委派給其他主機，包含過程中發生的監視器服務、負載平衡伺服器等。</p>
<a id="more"></a>
<p>Playbooks 被設計成易懂與基於 Text Language 的二次開發，有許多方式可以組合 Playbooks 與其附屬的檔案。建議在閱讀 Playbooks 時，同步閱讀 <a href="https://github.com/ansible/ansible-examples" target="_blank" rel="noopener">Example Playbooks</a>。</p>
<p>Playbooks 與 ad-hoc 相比是一種完全不同的 Ansible 應用方式，該方式也是 Ansible 強大之處。簡單來說 Playbooks 是一種組態管理系統與多機器部署系統基礎，與現有系統不同之處在於非常適合複雜的部署。若想參考範例，可以參閱 <a href="https://github.com/ansible/ansible-examples" target="_blank" rel="noopener">ansible-examples repository</a>。</p>
<h3 id="Playbook-Language-Example"><a href="#Playbook-Language-Example" class="headerlink" title="Playbook Language Example"></a>Playbook Language Example</h3><p>Playbook 採用 <a href="http://ansible-tran.readthedocs.org/en/latest/docs/YAMLSyntax.html" target="_blank" rel="noopener">YAML 語法</a>來表示。playbook 由一或多個<code>plays</code>組成的內容為元素的列表。在<code>play</code>中一組機器會被映射成定義好的角色，在 Ansible 中<code>play</code>內容也被稱為<code>tasks</code>。</p>
<p>以下是一個簡單的範例：</p>
<pre><code class="txt">---
- name: Configure cluster with apache
  hosts: cluster
  sudo: yes
  remote_user: ubuntu
  tasks:
    - name: install apache2
      apt: name=apache2 update_cache=yes state=latest

    - name: enabled mod_rewrite
      apache2_module: name=rewrite state=present
      notify:
        - restart apache2

    - name: apache2 listen on port 8081
      lineinfile: dest=/etc/apache2/ports.conf regexp=&quot;^Listen 80&quot; line=&quot;Listen 8081&quot; state=present
      notify:
        - restart apache2

    - name: apache2 virtualhost on port 8081
      lineinfile: dest=/etc/apache2/sites-available/000-default.conf regexp=&quot;^&lt;VirtualHost \*:80&gt;&quot; line=&quot;&lt;VirtualHost *:8081&gt;&quot; state=present
      notify:
        - restart apache2

  handlers:
    - name: restart apache2
      service: name=apache2 state=restarted
</code></pre>
<p>從以上範例中，可以由上往下大概知道結構如何，但我們還是要依序講解一下。</p>
]]></content>
      
        <categories>
            
            <category> DevOps </category>
            
        </categories>
        
        
        <tags>
            
            <tag> DevOps </tag>
            
            <tag> Automation Engine </tag>
            
            <tag> Ansible </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Ansible Dynamic Inventory]]></title>
      <url>https://kairen.github.io/2016/02/17/devops/ansible/ansible-dynamic-inventory/</url>
      <content type="html"><![CDATA[<p>在預設情況下，我們所使用的都是一個靜態的 Inventory 檔案，編輯主機、群組以及變數時都需要固定手動編輯完成。</p>
<p>Ansible 提供了 Dynamic Inventory 檔案，這個檔案是透過呼叫外部腳本或程式來產生指定的格式的 JSON 字串。這樣做的好處就是可以透過這個外部腳本與程式來管理系統（如 API）抓取最新資源訊息。</p>
<a id="more"></a>
<p> Ansible 使用者通常會互動於大多數的物理硬體，因此會有許多人可能也是<code>Cobbler</code>的使用者。</p>
<blockquote>
<p>Cobbler 是一個透過網路部署 Linux 的服務，而且經過調整更能夠進行 Windows 部署。該工具是使用 Python 開發，因此輕巧便利，使用簡單指令就可以完成 PXE 網路安裝環境。</p>
</blockquote>
<p> 比如說以下這個範例就是透過腳本程式產生的：</p>
<pre><code class="sh"> {
    &quot;production&quot;: [&quot;delaware.example.com&quot;, &quot;georgia.example.com&quot;,
        &quot;maryland.example.com&quot;, &quot;newhampshire.example.com&quot;,
        &quot;newjersey.example.com&quot;, &quot;newyork.example.com&quot;,
        &quot;northcarolina.example.com&quot;, &quot;pennsylvania.example.com&quot;,
        &quot;rhodeisland.example.com&quot;, &quot;virginia.example.com&quot;
    ],
    &quot;staging&quot;: [&quot;ontario.example.com&quot;, &quot;quebec.example.com&quot;],
    &quot;vagrant&quot;: [&quot;vagrant1&quot;, &quot;vagrant2&quot;, &quot;vagrant3&quot;],
    &quot;lb&quot;: [&quot;delaware.example.com&quot;],
    &quot;web&quot;: [&quot;georgia.example.com&quot;, &quot;newhampshire.example.com&quot;,
        &quot;newjersey.example.com&quot;, &quot;ontario.example.com&quot;, &quot;vagrant1&quot;
    ]
    &quot;task&quot;: [&quot;newyork.example.com&quot;, &quot;northcarolina.example.com&quot;,
        &quot;ontario.example.com&quot;, &quot;vagrant2&quot;
    ],
    &quot;redis&quot;: [&quot;pennsylvania.example.com&quot;, &quot;quebec.example.com&quot;, &quot;vagrant3&quot;],
    &quot;db&quot;: [&quot;rhodeisland.example.com&quot;, &quot;virginia.example.com&quot;, &quot;vagrant3&quot;]
}
</code></pre>
<p>使用方式如下：</p>
<ol>
<li><strong>加上執行(x)的權限給 script</strong></li>
<li><strong>將 script 與 inventory file 放在同一目錄</strong></li>
</ol>
<p>如此一來 ansible 就會自動讀取 inventory file 取得靜態的 inventory 資訊，並執行 script 取得動態的 inventory 資訊，將兩者 merge 後並使用。</p>
<p>目前官方已有提供幾個 Dynamic Inventory 的範例教學，如以下：</p>
<ul>
<li><a href="http://docs.ansible.com/ansible/intro_dynamic_inventory.html#example-the-cobbler-external-inventory-script" target="_blank" rel="noopener">Cobbler External Inventory Script</a></li>
<li><a href="http://docs.ansible.com/ansible/intro_dynamic_inventory.html#example-aws-ec2-external-inventory-script" target="_blank" rel="noopener">AWS EC2 External Inventory Script</a></li>
<li><a href="http://docs.ansible.com/ansible/intro_dynamic_inventory.html#example-openstack-external-inventory-script" target="_blank" rel="noopener">OpenStack External Inventory Script</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> DevOps </category>
            
        </categories>
        
        
        <tags>
            
            <tag> DevOps </tag>
            
            <tag> Automation Engine </tag>
            
            <tag> Ansible </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Ansible Inventory]]></title>
      <url>https://kairen.github.io/2016/02/17/devops/ansible/ansible-inventory/</url>
      <content type="html"><![CDATA[<p>Ansible 在同一時間能夠工作於多個系統，透過在 inventory file 所列舉的主機與群組來執行對應的指令，該檔案預設存於<code>/etc/ansible/hosts</code>。</p>
<p>IT 人員不只能夠使用預設的檔案，也能夠在同一時間使用多個檔案，甚至來抓取來至雲端的 inventory 檔案，這是一個是動態的 inventory ，這部分可以參考 <a href="http://docs.ansible.com/ansible/intro_dynamic_inventory.html" target="_blank" rel="noopener">Dynamic Inventory</a>。</p>
<a id="more"></a>
<h3 id="Hosts-and-Groups"><a href="#Hosts-and-Groups" class="headerlink" title="Hosts and Groups"></a>Hosts and Groups</h3><p>Inventory 是一個<code>INI-like</code>格式的檔案，如以下範例所示：</p>
<pre><code>mail.example.com

[webservers]
foo.example.com
bar.example.com

[dbservers]
one.example.com
two.example.com
three.example.com
</code></pre><p>如果 SSH 不是標準 Port 的話，可以使用<code>:</code>來對應要使用的 Port。但在 SSH config 檔案所列出來的主機將不會與 paramiko 進行連線，但是會與 OpenSSH 進行連接使用。</p>
<pre><code>badwolf.example.com:5309
</code></pre><blockquote>
<p>雖然可以使用以上方式達到不同 Port 連接，但是還是建議使用預設 Port。</p>
</blockquote>
<p>假設只有靜態 IP，但又希望透過一些別名（aliases）來表示主機，或透過不同 Port 連接的話，可以表示如以下：</p>
<pre><code>jumper ansible_port=5555 ansible_host=192.168.1.50
</code></pre><p>若要一次列出多個主機可以使用以下 Pattern：</p>
<pre><code>[webservers]
www[01:50].example.com
</code></pre><p>在數字 Pattern，前導的 0 可以根據需求刪除或加入。不只可以定義數字型，還能定義英文字母範圍：</p>
<pre><code>[databases]
db-[a:f].example.com
</code></pre><p>也可以為每台主機的設定基礎連線類型與使用者資訊：</p>
<pre><code>[targets]
localhost           ansible_connection=local
other1.example.com  ansible_connection=ssh  ansible_user=mpdehaan
other2.example.com  ansible_connection=ssh  ansible_user=mdehaan
</code></pre><h3 id="Host-Variables"><a href="#Host-Variables" class="headerlink" title="Host Variables"></a>Host Variables</h3><p>如上述範例，我們可以很容易將變數分配給將在 Playbooks 使用的主機：</p>
<pre><code>[atlanta]
host1   http_port=80    maxRequestsPerChild=808
host2   http_port=303   maxRequestsPerChild=909
</code></pre><h3 id="Group-Variables"><a href="#Group-Variables" class="headerlink" title="Group Variables"></a>Group Variables</h3><p>變數也能夠被應用到整個群組裡：</p>
<pre><code>[atlanta]
host1
host2

[atlanta:vars]
ntp_server=ntp.atlanta.example.com
proxy=proxy.atlanta.example.com
</code></pre><h3 id="Groups-of-Groups-and-Group-Variables"><a href="#Groups-of-Groups-and-Group-Variables" class="headerlink" title="Groups of Groups, and Group Variables"></a>Groups of Groups, and Group Variables</h3><p>另外，也可以用<code>:children</code> 來建立群組中的群組，並使用<code>:vars</code>來設定變數：</p>
<pre><code>[atlanta]
host1
host2

[raleigh]
host2
host3

[southeast:children]
atlanta
raleigh

[southeast:vars]
some_server=foo.southeast.example.com
halon_system_timeout=30
self_destruct_countdown=60
escape_pods=2

[usa:children]
southeast
northeast
southwest
northwest
</code></pre><h3 id="Splitting-Out-Host-and-Group-Specific-Data"><a href="#Splitting-Out-Host-and-Group-Specific-Data" class="headerlink" title="Splitting Out Host and Group Specific Data"></a>Splitting Out Host and Group Specific Data</h3><p>該部分說明想要儲存 list 與 hash table 資料，或者從 Inventory 檔案保持分離主機與群組的特定變數。在 Ansible 的第一優先作法實際上是不儲存變數於主 Inventort 檔案。</p>
<p>除了直接在 INI 檔案儲存變數外，主機與群組變數也可以儲存在個人相對的 Inventory 檔案。這些變數檔案格式為 YAML。有效的副檔名如<code>.yml</code>、<code>.yaml</code>，以及<code>.json</code>或<code>沒有副檔名</code>。</p>
<p>一般當 remote host 數量不多時，把變數定義在 inventory 中是 ok 的；但若 remote host 的數量越來越多時，將變數的宣告定義在外部的檔案中會是比較好的方式。</p>
<p>假設 Inventory 檔案路徑為：</p>
<pre><code>/etc/ansible/hosts
</code></pre><p>如果主機被命名為<code>foosball</code>以及在<code>raleigh</code>與<code>webservers</code>的群組，以下位置的 YAML 檔案變數將提供給主機使用：</p>
<pre><code class="sh"># can optionally end in &#39;.yml&#39;, &#39;.yaml&#39;, or &#39;.json&#39;
/etc/ansible/group_vars/raleigh
/etc/ansible/group_vars/webservers
/etc/ansible/host_vars/foosball
</code></pre>
<p>ansible 會自動尋找 playbook 所在的目錄中的<code>host_vars</code>目錄 以及<code>group_vars</code>目錄 中所包含的檔案，並使用定義在這兩個目錄中的變數資訊。</p>
<p>舉例來說，inventory / playbook / host_vars / group_vars 可以用類似以下的方式進行配置：</p>
<ul>
<li><strong>inventory</strong>：/home/vagrant/ansible/playbooks/inventory</li>
<li><strong>playbook</strong>：/home/vagrant/ansible/playbooks/myplaybook</li>
<li><strong>host_vars</strong>：/home/vagrant/ansible/playbooks/host_vars/prod1.example.com.tw</li>
<li><strong>group_vars</strong>：/home/vagrant/ansible/playbooks/group_vars/production</li>
</ul>
<p>變數定義的方式有兩種方式：</p>
<pre><code class="sh">db_primary_host: prod1.example.com.tw
db_replica_host: prod2.example.com.tw
db_name: widget_production
db_user: widgetuser
db_password: lastpassword
redis_host: redis_stag.example.com.tw
</code></pre>
<p>也可以用 YAML 的方式定義：</p>
<pre><code class="sh">---
db:
    user: widgetuser
    password: lastpassword
    name: widget_production
    primary:
        host: prod1.example.com.tw
        port: 5432
    replica:
        host: prod2.example.com.tw
        port: 5432
redis:
    host: redis_stag.example.com.tw
    port: 6379
</code></pre>
<p>甚至可以在繼續細分，定義檔案<code>../playbooks/group_vars/production/db</code>：</p>
<pre><code class="sh">---
db:
    user: widgetuser
    password: lastpassword
    name: widget_production
    primary:
        host: prod1.example.com.tw
        port: 5432
    replica:
        host: prod2.example.com.tw
        port: 5432
</code></pre>
<h3 id="List-of-Behavioral-Inventory-Parameters"><a href="#List-of-Behavioral-Inventory-Parameters" class="headerlink" title="List of Behavioral Inventory Parameters"></a>List of Behavioral Inventory Parameters</h3><p>正如上述提到，設定以下變數可以定義 Ansible 該如何控制以及遠端主機。如主機連線：</p>
<pre><code class="sh">ansible_connection
  Connection type to the host. Candidates are local, smart, ssh or paramiko.  The default is smart.
</code></pre>
<p>SSH connection：</p>
<pre><code>ansible_host
  The name of the host to connect to, if different from the alias you wish to give to it.
ansible_port
  The ssh port number, if not 22
ansible_user
  The default ssh user name to use.
ansible_ssh_pass
  The ssh password to use (this is insecure, we strongly recommend using --ask-pass or SSH keys)
ansible_ssh_private_key_file
  Private key file used by ssh.  Useful if using multiple keys and you don&#39;t want to use SSH agent.
ansible_ssh_common_args
  This setting is always appended to the default command line for
  sftp, scp, and ssh. Useful to configure a ``ProxyCommand`` for a
  certain host (or group).
ansible_sftp_extra_args
  This setting is always appended to the default sftp command line.
ansible_scp_extra_args
  This setting is always appended to the default scp command line.
ansible_ssh_extra_args
  This setting is always appended to the default ssh command line.
ansible_ssh_pipelining
  Determines whether or not to use SSH pipelining. This can override the
  ``pipelining`` setting in ``ansible.cfg``.
</code></pre><p>權限提升（可參閱<a href="http://docs.ansible.com/ansible/become.html" target="_blank" rel="noopener">Ansible Privilege Escalation</a>）：</p>
<pre><code>ansible_become
  Equivalent to ansible_sudo or ansible_su, allows to force privilege escalation
ansible_become_method
  Allows to set privilege escalation method
ansible_become_user
  Equivalent to ansible_sudo_user or ansible_su_user, allows to set the user you become through privilege escalation
ansible_become_pass
  Equivalent to ansible_sudo_pass or ansible_su_pass, allows you to set the privilege escalation password
</code></pre><p>遠端主機環境參數：</p>
<pre><code>ansible_shell_type
  The shell type of the target system. Commands are formatted using &#39;sh&#39;-style syntax by default. Setting this to &#39;csh&#39; or &#39;fish&#39; will cause commands executed on target systems to follow those shell&#39;s syntax instead.
ansible_python_interpreter
  The target host python path. This is useful for systems with more
  than one Python or not located at &quot;/usr/bin/python&quot; such as \*BSD, or where /usr/bin/python
  is not a 2.X series Python.  We do not use the &quot;/usr/bin/env&quot; mechanism as that requires the remote user&#39;s
  path to be set right and also assumes the &quot;python&quot; executable is named python, where the executable might
  be named something like &quot;python26&quot;.
ansible\_\*\_interpreter
  Works for anything such as ruby or perl and works just like ansible_python_interpreter.
  This replaces shebang of modules which will run on that host.
</code></pre><p>一個主機檔案範例：</p>
<pre><code>some_host         ansible_port=2222     ansible_user=manager
aws_host          ansible_ssh_private_key_file=/home/example/.ssh/aws.pem
freebsd_host      ansible_python_interpreter=/usr/local/bin/python
ruby_module_host  ansible_ruby_interpreter=/usr/bin/ruby.1.9.3
</code></pre>]]></content>
      
        <categories>
            
            <category> DevOps </category>
            
        </categories>
        
        
        <tags>
            
            <tag> DevOps </tag>
            
            <tag> Automation Engine </tag>
            
            <tag> Ansible </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Ansible Ad-Hoc 指令與 Modules]]></title>
      <url>https://kairen.github.io/2016/02/17/devops/ansible/ansible-adhoc/</url>
      <content type="html"><![CDATA[<p>ad-hoc command（特設指令）簡單說就是直接執行指令，這些指令不需要要被保存在日後使用。在進行 Ansible 的 Playbook 語言之前，了解 ad-hoc 指令也可以幫助我們做一些快速的事情，不一定要寫出一個完整的 Playbooks 指令。</p>
<p>模組（也被稱為<code>Task plugins</code>或是<code>Library plugins</code>）是 Ansible 中實際執行的功能，它們會在每個 Playbook 任務中被執行，也可以透過 ansible 直接呼叫使用。目前 Ansible 已經擁有許多模組，可參閱 <a href="http://docs.ansible.com/ansible/modules_by_category.html" target="_blank" rel="noopener">Module Index</a>。</p>
<a id="more"></a>
<p>首先我們先編輯<code>/etc/ansible/hosts</code>，加入以下內容：</p>
<pre><code class="sh">[cluster]
ansible-slave-1 ansible_host=172.16.1.206
ansible-slave-2 ansible_host=172.16.1.207
ansible-slave-3 ansible_host=172.16.1.208
</code></pre>
<h3 id="Parallelism-and-Shell-Commands"><a href="#Parallelism-and-Shell-Commands" class="headerlink" title="Parallelism and Shell Commands"></a>Parallelism and Shell Commands</h3><p>接下來我們將透過範例來說明 Ansible 的平行性與 Shell 指令，一開始我們需要將 ssh-agent 加入私有金鑰管理：</p>
<pre><code class="sh">$ ssh-agent bash
$ ssh-add ~/.ssh/id_rsa
</code></pre>
<blockquote>
<p>如果不想要透過 ssh-agent 的金鑰登入，可以在 ansible 指令使用<code>--ask-pass（-k）</code>參數，但是建議使用 ssh-agent。</p>
</blockquote>
<p>剛剛我們在 Inventroy 檔案建立了一個群組（Cluster），裡面擁有三台主機，接下來我們透過執行一個簡單的指令與參數來實現並行執行：</p>
<pre><code class="sh">$ ansible cluster -a &quot;sleep 2&quot; -f 1
</code></pre>
<blockquote>
<p>上面的指令會隨機執行一台主機，完成後接下執行下一台，然而<code>-f</code>參數可以改變一次執行的 bash，好比改成：</p>
<pre><code class="sh">$ ansible cluster -a &quot;sleep 2&quot; -f 3
</code></pre>
<p>會發現 bash 是平行執行的。</p>
</blockquote>
<p>我們除了使用預設的 user 登入以外，也可以指定要登入的使用者：</p>
<pre><code class="sh">$ ansible cluster -a &quot;echo $USER&quot; -u ubuntu
</code></pre>
<p>如果想透過特權（sudo）執行指令，可以透過以下方式：</p>
<pre><code class="sh">$ ansible cluster -a &quot;apt-get update&quot; -u ubuntu --become
</code></pre>
<blockquote>
<p>若該使用者沒有設定 sudo 不需要密碼的話，可以加入<code>--ask-sudo-pass（-k）</code>來驗證密碼。也可以使用<code>--become-method</code>來改變權限使用方法（預設為 sudo）。</p>
</blockquote>
<p>也可以透過<code>--become-user</code>來切換使用者：</p>
<pre><code class="sh">$ ansible cluster -a &quot;echo $USER&quot; -u ubuntu --become-user root
</code></pre>
<blockquote>
<p>若有密碼，可以使用<code>--ask-sudo-pass</code>。</p>
</blockquote>
<p>以上是基本的幾個指令，但當使用 ansible ad-hoc 指令時，會發現無法使用<code>shell 變數</code>以及<code>pipeline 等相關</code>，這是因為預設的 ansible ad-hoc 指令不支援，<br>故要改用 shell 模組來執行：</p>
<pre><code class="sh">$ ansible cluster -m shell -a &#39;echo $(hostname) | grep -o &quot;[0-9]&quot;&#39;
</code></pre>
<blockquote>
<p>以上指令的<code>-m</code>表示要使用的模組。但要注意！使用 ansible 指令時要留意<code>&quot;cmd&quot;</code>與<code>&#39;comd&#39;</code>的差別，比如使用<code>&quot;cmd&quot;</code>會是抓取當前系統的資訊。</p>
</blockquote>
<h3 id="File-Transfer"><a href="#File-Transfer" class="headerlink" title="File Transfer"></a>File Transfer</h3><p>Ansible 能夠以平行的方式同時<code>scp</code>大量的檔案到多台主機上，如以下範例：</p>
<pre><code class="sh">$ ansible cluster -m copy -a &quot;src=/etc/hosts dest=~/hosts&quot;
</code></pre>
<p>也可以使用<code>file</code>模組做到修改檔案的權限與屬性（這邊可以將<code>copy</code>替換成<code>file</code>）：</p>
<pre><code class="sh">$ ansible cluster -m file -a &quot;dest=~/hosts mode=600&quot;
$ ansible cluster -m file -a &quot;dest=~/hosts mode=600 owner=ubuntu group=ubuntu&quot;
</code></pre>
<p><code>file</code>模組也能夠建立目錄：</p>
<pre><code class="sh">$ ansible cluster -m file -a &quot;dest=~/data mode=755 owner=ubuntu group=ubuntu state=directory&quot;
</code></pre>
<p>若要刪除可以使用以下方式：</p>
<pre><code class="sh">$ ansible cluster -m file -a &quot;dest=~/data state=absent&quot;
</code></pre>
<h3 id="Managing-Packages"><a href="#Managing-Packages" class="headerlink" title="Managing Packages"></a>Managing Packages</h3><p>目前 Ansible 已經支援了<code>yum</code>與<code>apt</code>的模組，以下是一個<code>apt</code> 確認指定軟體名稱是否已安裝，並且不升級：</p>
<pre><code class="sh">$ ansible cluster -m apt -a &quot;name=ntp state=present&quot;
</code></pre>
<blockquote>
<p>也可以在<code>name=ntp</code>後面加版本號，如<code>name=ntp-{version}</code>。</p>
</blockquote>
<p>若要確認是否為最新版本，可以使用以下指令：</p>
<pre><code class="sh">$ ansible cluster -m apt -a &quot;name=ntp state=latest&quot;
</code></pre>
<p>若要確認一個軟體套件沒有安裝，可以使用以下指令：</p>
<pre><code class="sh">$ ansible cluster -m apt -a &quot;name=ntp state=absent&quot; --become
</code></pre>
<p>更多的指令資訊可以查看 <a href="http://docs.ansible.com/ansible/modules.html" target="_blank" rel="noopener">About Modules</a>。</p>
<h3 id="Users-and-Groups"><a href="#Users-and-Groups" class="headerlink" title="Users and Groups"></a>Users and Groups</h3><p>若想要建立系統使用者與群組，可以使用<code>user</code>模組，如以下範例：</p>
<pre><code class="sh">$ ansible all -m user -a &quot;name=food password=food&quot; --become
</code></pre>
<p>刪除則如以下：</p>
<pre><code class="sh">$ ansible all -m user -a &quot;name=food state=absent&quot; -b
</code></pre>
<blockquote>
<p><code>--become</code>與<code>-b</code>是等效的。</p>
</blockquote>
<h3 id="Deploying-From-Source-Control"><a href="#Deploying-From-Source-Control" class="headerlink" title="Deploying From Source Control"></a>Deploying From Source Control</h3><p>Ansible 不只可以透過<code>apt</code>與<code>ad-hoc 指令</code>來安裝與部署應用程式，也能用<code>git</code>模組來安裝：</p>
<pre><code class="sh">$ ansible cluster -m git -a &quot;repo=https://github.com/imac-cloud/Spark-tutorial.git dest=~/spark-tutorial&quot; -f 3
</code></pre>
<h3 id="Managing-Services"><a href="#Managing-Services" class="headerlink" title="Managing Services"></a>Managing Services</h3><p>Ansible 也可以透過<code>service</code>模組來確認指定主機是否已啟動服務：</p>
<pre><code class="sh">$ ansible cluster -m service -a &quot;name=ssh state=started&quot;
</code></pre>
<blockquote>
<p>也可以改變<code>state</code>來執行對應動作，如<code>state=restarted</code>就會重新啟動服務。</p>
</blockquote>
<h3 id="Time-Limited-Background-Operations"><a href="#Time-Limited-Background-Operations" class="headerlink" title="Time Limited Background Operations"></a>Time Limited Background Operations</h3><p>有些操作需要長時間執行於後台，在指令開始執行後，可以持續檢查執行狀態，但是若不想要獲取該資訊可以使用以下指令：</p>
<pre><code class="sh">$ ansible ansible-slave-1 -B 3600 -P 0 -a &quot;/usr/bin/long_running_operation --do-stuff&quot;
</code></pre>
<p>若要檢查執行狀態的話，可以使用<code>async_status</code>來傳入一個<code>jid</code>查看：</p>
<pre><code class="sh">$ ansible cluster -m async_status -a &quot;jid=488359678239.2844&quot;
</code></pre>
<p>獲取狀態指令如下：</p>
<pre><code class="sh">$ ansible ansible-slave-1 -B 1800 -P 60 -a &quot;/usr/bin/long_running_operation --do-stuff&quot;
</code></pre>
<blockquote>
<p><code>-B</code>表示最常執行時間，<code>-P</code>表示每隔60秒回傳狀態。</p>
</blockquote>
<h3 id="Gathering-Facts"><a href="#Gathering-Facts" class="headerlink" title="Gathering Facts"></a>Gathering Facts</h3><p>在 Playboooks 中有對 Facts 做一些描述，他表示的是一些系統<code>已知的變數</code>，若要查看所有 Facts，可以使用以下指令：</p>
<pre><code class="sh">$ ansible cluster[0] -m setup
</code></pre>
<p>接下來可以針對 <a href="http://docs.ansible.com/ansible/playbooks.html" target="_blank" rel="noopener">Playbooks</a> 與 <a href="http://docs.ansible.com/ansible/playbooks_variables.html" target="_blank" rel="noopener">Variables</a> 進行研究。</p>
]]></content>
      
        <categories>
            
            <category> DevOps </category>
            
        </categories>
        
        
        <tags>
            
            <tag> DevOps </tag>
            
            <tag> Automation Engine </tag>
            
            <tag> Ansible </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Ansible 介紹與使用]]></title>
      <url>https://kairen.github.io/2016/02/16/devops/ansible/ansible-basic/</url>
      <content type="html"><![CDATA[<p>Ansible 是最近越來越夯多 DevOps 自動化組態管理軟體，從 2013 年發起的專案，由於該架構為無 agent 程式的架構，以部署靈活與程式碼易讀而受到矚目。Ansible 除了有開源版本之外，還針對企業用戶推出 Ansible Tower 版本，已有許多知名企業採用，如 Apple、Twitter 等。</p>
<p>Ansible 架構圖如下所示，使用者透過 Ansible 編配操控公有與私有雲或 CMDB（組態管理資料庫）中的主機，其中 Ansible 編排是由<code>Inventory(主機與群組規則)</code>、<code>API</code>、<code>Modules(模組)</code>與<code>Plugins(插件)</code>組合而成。</p>
<p><center><img src="/images/devops/ansible-arch.jpg" alt=""></center><br><a id="more"></a></p>
<p><a href="https://github.com/ansible/ansible" target="_blank" rel="noopener">Ansible</a> 與其他管理工具最大差異在於不需要任何 Agent，預設使用 SSH 來做遠端操控與配置，並採用 YAML 格式來描述配置資訊。</p>
<blockquote>
<p>Ansible 提供了一個 Playbook 分享平台，可以讓管理與開發者上傳自己的功能與角色配置的 Playbook，該網址為 <a href="https://galaxy.ansible.com/intro" target="_blank" rel="noopener">Ansible Galaxy</a>。</p>
</blockquote>
<p><strong>優點：</strong></p>
<ul>
<li>開發社群活躍。</li>
<li>playbook 使用的 yaml 語言，很簡潔。</li>
<li>社群相關文件容易理解。。</li>
<li>沒有 Agent 端。</li>
<li>安裝與執行的速度快</li>
<li>配置簡單、功能強大、擴展性強</li>
<li>可透過 Python 擴展功能</li>
<li>提供用好的 Web 管理介面與 REST API 介面（AWX 平台）</li>
</ul>
<p><strong>缺點：</strong></p>
<ul>
<li>Web UI 需要收費。</li>
<li>官方資料都比較淺顯。</li>
</ul>
<h2 id="Ansible-安裝與基本操作"><a href="#Ansible-安裝與基本操作" class="headerlink" title="Ansible 安裝與基本操作"></a>Ansible 安裝與基本操作</h2><p>Ansible 有許多種安裝方式，如使用 Github 來透過 Source Code 安裝，也可以透過 python-pip 來安裝，甚至是使用作業系統的套件管理系統安裝，以下使用 Ubuntu APT 來進行安裝：</p>
<pre><code class="sh">$ sudo apt-get install software-properties-common
$ sudo apt-add-repository ppa:ansible/ansible
$ sudo apt-get update
$ sudo apt-get install ansible
</code></pre>
<p>也可以使用 Python-pip 來進行安裝：</p>
<pre><code class="sh">$ sudo easy_install pip
$ sudo pip install -U pip
$ sudo pip install ansible
</code></pre>
<h3 id="節點準備"><a href="#節點準備" class="headerlink" title="節點準備"></a>節點準備</h3><p>首先我們要在各節點先安裝 SSH Server ，並配置需要的相關環境：</p>
<pre><code class="sh">$ sudo apt-get install openssh-server
</code></pre>
<p>設定特權模式不需要輸入密碼：</p>
<pre><code class="sh">$ echo &quot;ubuntu ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/ubuntu
$ sudo chmod 440 /etc/sudoers.d/ubuntu
</code></pre>
<blockquote>
<p>這邊 User 為<code>ubuntu</code>，若使用者不一樣請更換。</p>
</blockquote>
<p>建立 SSH Key，並複製 Key 使之不用密碼登入：</p>
<pre><code class="sh">$ ssh-keygen -t rsa
$ ssh-copy-id localhost
</code></pre>
<p>新增各節點 Domain name 至<code>/etc/hosts</code>檔案：</p>
<pre><code class="sh">172.16.1.205 ansible-master
172.16.1.206 ansible-slave-1
172.16.1.207 ansible-slave-2
172.16.1.208 ansible-slave-3
</code></pre>
<p>並在 Master 節點複製所有 Slave 的 SSH Key：</p>
<pre><code class="sh">$ ssh-copy-id ubuntu@ansible-slave-1
$ ssh-copy-id ubuntu@ansible-slave-2
...
</code></pre>
<h3 id="設定-Invetory-File"><a href="#設定-Invetory-File" class="headerlink" title="設定 Invetory File"></a>設定 Invetory File</h3><p>Ansible 能夠在同一時間工作於多個基礎設施的系統中。透過作用於 Ansible 的 Inventory 檔案所列出的主機與群組，該檔案預設被存在<code>/etc/ansible/hosts</code>。</p>
<p><code>/etc/ansible/hosts</code> 是一個 INI-like  的檔案格式，如以下內容：</p>
<pre><code>ansible-slave-1
ansible-slave-2
ansible-slave-3
</code></pre><blockquote>
<p>也可以建立成 Groups，如以下內容：</p>
<pre><code class="sh">[openstack]
ansible-slave-1
ansible-slave-2
ansible-slave-3
</code></pre>
<p>若要參考更多資訊，可看 <a href="http://docs.ansible.com/ansible/intro_inventory.html" target="_blank" rel="noopener">Invetory File</a>。</p>
</blockquote>
<h3 id="基本功能操作"><a href="#基本功能操作" class="headerlink" title="基本功能操作"></a>基本功能操作</h3><p>Ansible 基本操作如以下指令：</p>
<pre><code class="sh">$ ansible &lt;pattern_goes_here&gt; -m &lt;module_name&gt; -a &lt;arguments&gt;
</code></pre>
<blockquote>
<p><code>&lt;pattern_goes_here&gt;</code>部分可以參考 <a href="http://docs.ansible.com/ansible/intro_patterns.html" target="_blank" rel="noopener">Patterns</a>。</p>
</blockquote>
<p>比如我們可以用 Ping 模組來測試是否連線成功：</p>
<pre><code class="sh">$ ansible all -m ping

ansible-slave-2 | SUCCESS =&gt; {
    &quot;changed&quot;: false,
    &quot;ping&quot;: &quot;pong&quot;
}
ansible-slave-3 | SUCCESS =&gt; {
    &quot;changed&quot;: false,
    &quot;ping&quot;: &quot;pong&quot;
}
ansible-slave-1 | SUCCESS =&gt; {
    &quot;changed&quot;: false,
    &quot;ping&quot;: &quot;pong&quot;
}
</code></pre>
<blockquote>
<p>其中<code>all</code>為所有 Invetory 的主機，<code>-m</code>為使用的模組。若使用指定的 Inventory 檔案可以使用<code>-i</code>。</p>
</blockquote>
<p>也可以執行指定指令：</p>
<pre><code class="sh">$ ansible all -a &quot;/bin/echo hello&quot;
</code></pre>
<blockquote>
<p><code>-a</code> 後面為要執行的指令。</p>
</blockquote>
<p>若要指定登入的使用者，且執行特權模式，可以使用以下指令：</p>
<pre><code class="sh">$ ansible all -a &quot;apt-get update&quot; -u vagrant -b
</code></pre>
<blockquote>
<p><code>-u</code>為登入使用者，<code>-b</code> 為切換成特權模式（root），早期版本為<code>--sudo</code>。</p>
</blockquote>
<h3 id="主機的-SSH-Key-檢查"><a href="#主機的-SSH-Key-檢查" class="headerlink" title="主機的 SSH Key 檢查"></a>主機的 SSH Key 檢查</h3><p>在 Ansible 1.2.1 與之後的版本預設都需要做主機 SSH key 檢查。</p>
<p>如果一台主機重新安裝或者在 ‘known_hosts’  有不同的 SSH Key 的話，將會導致錯誤發生，但不希望這樣的問題影響 Ansible 使用，可以在 <code>/etc/ansible/ansible.cfg</code> 或者<code>~/.ansible.cfg</code>檔案關閉檢查。</p>
<pre><code class="sh">[defaults]
host_key_checking = False
</code></pre>
<p>也可以代替為設定環境變數：</p>
<pre><code class="sh">$ export ANSIBLE_HOST_KEY_CHECKING=False
</code></pre>
<p>還要注意在 paramiko 模式主機金鑰檢查緩慢是合理的<br>，因此建議切換使用 SSH。</p>
<p>Ansible 會在遠端系統上記錄有關模組參數的一些資訊存於 syslog，除非該執行任務有標示 ‘no_log: True’。</p>
]]></content>
      
        <categories>
            
            <category> DevOps </category>
            
        </categories>
        
        
        <tags>
            
            <tag> DevOps </tag>
            
            <tag> Automation Engine </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Ceph Ansible 自動化建立 Ceph 叢集]]></title>
      <url>https://kairen.github.io/2016/02/15/ceph/deploy/ceph-ansible/</url>
      <content type="html"><![CDATA[<p>本節將介紹如何透過 <a href="https://github.com/ceph/ceph-ansible" target="_blank" rel="noopener">ceph-ansible</a> 工具安裝一個測試的 Ceph 環境，一個最簡單的 Ceph 儲存叢集至少要<code>一台 Monitor</code>與<code>三台 OSD</code>。而 MDS 則是當使用到 CephFS 的時候才需要部署。</p>
<a id="more"></a>
<h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本安裝將使用四台虛擬機器作為部署主機，虛擬機器採用 OpenStack，其規格為以下：</p>
<table>
<thead>
<tr>
<th>Role</th>
<th>RAM</th>
<th>CPUs</th>
<th>Disk</th>
<th>IP Address</th>
</tr>
</thead>
<tbody>
<tr>
<td>mon</td>
<td>2 GB</td>
<td>1vCPU</td>
<td>20 GB</td>
<td>172.16.1.200</td>
</tr>
<tr>
<td>osd1</td>
<td>2 GB</td>
<td>1vCPU</td>
<td>20 GB</td>
<td>172.16.1.201</td>
</tr>
<tr>
<td>osd2</td>
<td>2 GB</td>
<td>1vCPU</td>
<td>20 GB</td>
<td>172.16.1.202</td>
</tr>
<tr>
<td>osd3</td>
<td>2 GB</td>
<td>1vCPU</td>
<td>20 GB</td>
<td>172.16.1.203</td>
</tr>
</tbody>
</table>
<p>其中若是虛擬機，要額外建立 3 顆虛擬硬碟來作為 OSD 使用，如以下：</p>
<table>
<thead>
<tr>
<th>Dev path</th>
<th>Disk Size</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>/dev/vdb</td>
<td>25 GB</td>
<td>osd1 掛載</td>
</tr>
<tr>
<td>/dev/vdb</td>
<td>25 GB</td>
<td>osd2 掛載</td>
</tr>
<tr>
<td>/dev/vdb</td>
<td>25 GB</td>
<td>osd3 掛載</td>
</tr>
</tbody>
</table>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>首先在<code>mon</code>節點設定 SSH 到其他節點不需要密碼，請依照以下執行：</p>
<pre><code class="sh">$ ssh-keygen -t rsa
$ ssh-copy-id osd1
...
</code></pre>
<blockquote>
<p>若虛擬機的話，建立金鑰後可以直接上傳<code>公有金鑰</code>提供給其他節點。</p>
</blockquote>
<p>接著在每一個節點設定 sudo 不需要密碼：</p>
<pre><code class="sh">$ echo &quot;ubuntu ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/ubuntu &amp;&amp; sudo chmod 440 /etc/sudoers.d/ubuntu
</code></pre>
<blockquote>
<p>一般虛擬機映像檔預設就有設定。</p>
</blockquote>
<p>然後在每一台節點新增以下內容到<code>/etc/hosts</code>：</p>
<pre><code>127.0.0.1 localhost

10.21.20.99 ceph-deploy
172.16.1.200 mon
172.16.1.201 osd1
172.16.1.202 osd2
172.16.1.203 osd3
</code></pre><p>回到<code>mon</code>節點安裝部署將使用到的 ansible 工具：</p>
<pre><code class="sh">$ sudo apt-get install -y software-properties-common git cowsay
$ sudo apt-add-repository -y ppa:ansible/ansible
$ sudo apt-get update &amp;&amp; sudo apt-get install -y ansible
</code></pre>
<p>在<code>mon</code>節點編輯<code>/etc/ansible/hosts</code>，加入以下內容：</p>
<pre><code>[mons]
mon

[osds]
osd[1:3]
</code></pre><p>(option)若要安裝 rgw 與 mds 的話，可再添加以下：</p>
<pre><code class="sh">[rgws]
mon
[mdss]
mon
</code></pre>
<p>完成後透過以下指令檢查節點是否可以溝通：</p>
<pre><code class="sh">$ ansible all -m ping
172.16.1.200 | success &gt;&gt; {
    &quot;changed&quot;: false,
    &quot;ping&quot;: &quot;pong&quot;
}
...
</code></pre>
<h2 id="部署-Ansible-Ceph-叢集"><a href="#部署-Ansible-Ceph-叢集" class="headerlink" title="部署 Ansible Ceph 叢集"></a>部署 Ansible Ceph 叢集</h2><p>首先在<code>mon</code>節點透過 git 來下載 ceph-ansible 專案：</p>
<pre><code class="sh">$ git clone &quot;https://github.com/ceph/ceph-ansible.git&quot;
$ cd ceph-ansible
$ cp site.yml.sample site.yml
$ cp group_vars/all.sample group_vars/all
$ cp group_vars/mons.sample group_vars/mons
$ cp group_vars/osds.sample group_vars/osds
</code></pre>
<blockquote>
<p>若要部署 rgw 與 mds 的話，需再執行以下指令：</p>
<pre><code class="sh">$ cp group_vars/mdss.sample group_vars/mdss
$ cp group_vars/rgws.sample group_vars/rgws
</code></pre>
</blockquote>
<p>接著編輯<code>group_vars/all</code>檔案，修改以下內容：</p>
<pre><code class="yaml">ceph_origin: &#39;upstream&#39;
ceph_stable: true
monitor_interface: eth0
journal_size: 5000
public_network: 172.16.1.0/24
</code></pre>
<blockquote>
<p>其他版本可以參考官方的說明 <a href="https://github.com/ceph/ceph-ansible" target="_blank" rel="noopener">ceph-ansible</a>。</p>
</blockquote>
<p>完成後再編輯<code>group_vars/osds</code>檔案，修改以下內容：</p>
<pre><code class="yaml">journal_collocation: true
devices:
  - /dev/vdb
</code></pre>
<blockquote>
<p>這邊使用 journal，也可以選擇其他使用。若有多顆 OSD則修改<code>devices</code>。</p>
</blockquote>
<p>上述都確認無誤後，編輯<code>site.yml</code>檔案，並修改一下內容：</p>
<pre><code class="yaml">---
- hosts: mons
  become: True
  roles:
  - ceph-mon

- hosts: osds
  become: True
  roles:
  - ceph-osd
</code></pre>
<p>(option)若要部署 rgw 與 mds 的話，需再加入以下內容：</p>
<pre><code class="yaml">- hosts: mdss
  become: True
  roles:
  - ceph-mds

- hosts: rgws
  become: True
  roles:
  - ceph-rgw
</code></pre>
<p>完成後就可以透過以下指令來進行部署：</p>
<pre><code class="sh">$ ansible-playbook site.yml
</code></pre>
]]></content>
      
        <categories>
            
            <category> Ceph </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> Storage </tag>
            
            <tag> Ansible </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Foreman 管理 Puppet]]></title>
      <url>https://kairen.github.io/2016/02/14/devops/puppet-foreman/</url>
      <content type="html"><![CDATA[<p>Foreman 是一個 Puppet 的生命周期管理系統，類似 puppet-dashboard，通過它可以很直觀的查看 Puppet 所有客戶端的同步狀態與 facter 參數。</p>
<a id="more"></a>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>由於 foreman 是取決於 puppet 執行主機的組態管理，他需要部署一個 puppet master 與 agent 環境。下面的列表為在安裝之前需要備設定的項目：</p>
<ul>
<li>Root 權限：所有伺服器能夠使用<code>sudo</code>。</li>
<li>私人網路 DNS：Forward 與 reverse 的 DNS 必須被設定，可參考<a href="https://www.digitalocean.com/community/tutorials/how-to-configure-bind-as-a-private-network-dns-server-on-ubuntu-14-04" target="_blank" rel="noopener">How To Configure BIND as a Private Network DNS Server on Ubuntu 14.04</a>。</li>
<li>防火牆有開啟使用的 port： Puppet master 必須可以被存取<code>8140</code>埠口。</li>
</ul>
<h2 id="安裝-Foreman"><a href="#安裝-Foreman" class="headerlink" title="安裝 Foreman"></a>安裝 Foreman</h2><p>安裝 Foreman 最簡單的方法是使用 Foreman 安裝程式。Foreman 安裝程式與配置必要的元件來執行 Foreman，包含以下內容：</p>
<ul>
<li>Foreman</li>
<li>Puppet master and agent</li>
<li>Apache Web Server with SSL and Passenger module<br>*</li>
</ul>
<p>下載 Foreman 可以依照以下指令進行：</p>
<pre><code class="sh">$ sudo sh -c &#39;echo &quot;deb http://deb.theforeman.org/ trusty 1.5&quot; &gt; /etc/apt/sources.list.d/foreman.list&#39;
$ sudo sh -c &#39;echo &quot;deb http://deb.theforeman.org/ plugins 1.5&quot; &gt;&gt; /etc/apt/sources.list.d/foreman.list&#39;
$ wget -q http://deb.theforeman.org/pubkey.gpg -O- | sudo apt-key add -
$ sudo apt-get update &amp;&amp; sudo apt-get install foreman-installer
</code></pre>
<p>安裝完成後，要執行 Foreman Installer 可以使用以下指令：</p>
<pre><code class="sh">$ sudo foreman-installer
</code></pre>
<p>完成後會看到以下資訊：</p>
<pre><code class="sh">  Success!
  * Foreman is running at https://puppet-master.com
      Default credentials are &#39;admin:changeme&#39;
  * Foreman Proxy is running at https://puppet-master.com:8443
  * Puppetmaster is running at port 8140
  The full log is at /var/log/foreman-installer/foreman-installer.log
</code></pre>
<p>之後修改<code>puppet.conf</code>檔案，開啟<code>diff</code>選項：</p>
<pre><code class="sh">$ sudo vim /etc/puppet/puppet.conf

show_diff = true
</code></pre>
<h3 id="新增-Foreman-Host-到-Foreman-資料庫"><a href="#新增-Foreman-Host-到-Foreman-資料庫" class="headerlink" title="新增 Foreman Host 到 Foreman 資料庫"></a>新增 Foreman Host 到 Foreman 資料庫</h3><p>要新增 Host 可以使用以下指令：</p>
<pre><code class="sh">$ sudo puppet agent --test
</code></pre>
<p>完成後登入 Web，並輸入<code>admin</code>/<code>changeme</code>。</p>
<h3 id="驗證-Foreman"><a href="#驗證-Foreman" class="headerlink" title="驗證 Foreman"></a>驗證 Foreman</h3><pre><code class="sh">$ sudo puppet module install -i /etc/puppet/environments/production/modules puppetlabs/ntp

Notice: Preparing to install into /etc/puppet/environments/production/modules ...
Notice: Downloading from https://forge.puppetlabs.com ...
Notice: Installing -- do not interrupt ...
/etc/puppet/environments/production/modules
└─┬ puppetlabs-ntp (v4.1.2)
  └── puppetlabs-stdlib (v4.10.0)
</code></pre>
<h2 id="參考資源"><a href="#參考資源" class="headerlink" title="參考資源"></a>參考資源</h2><ul>
<li><a href="https://www.digitalocean.com/community/tutorials/how-to-use-foreman-to-manage-puppet-nodes-on-ubuntu-14-04" target="_blank" rel="noopener">How To Use Foreman To Manage Puppet Nodes on Ubuntu 14.04</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> DevOps </category>
            
        </categories>
        
        
        <tags>
            
            <tag> DevOps </tag>
            
            <tag> Automation Engine </tag>
            
            <tag> Puppet </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Puppet 介紹與使用]]></title>
      <url>https://kairen.github.io/2016/02/13/devops/puppet-basic/</url>
      <content type="html"><![CDATA[<p>Puppet 是一個開放原始碼專案，基於 Ruby 的系統組態管理工具，採用 Client/Server 的部署架構。是一個為了實現資料中心自動化管理，而被設計的組態管理軟體，它使用跨平台語言規範，管理組態檔案、使用者、軟體套件與系統服務等。用戶端預設每個半小時會與伺服器溝通一次，來確定是否有更新。當然也可以配置主動觸發來強制用戶端更新。這樣可以把平常的系統管理工作程式碼化，透過程式碼化的好處是可以分享、保存與避免重複勞動，也可以快速恢復以及快速的大規模環境部署伺服器。</p>
<center><img src="/images/devops/puppet-dataflow.png" alt="puppet-dataflow.png"></center>

<p><strong>優點：</strong></p>
<ul>
<li>成熟的組態管理軟體。</li>
<li>應用廣泛。</li>
<li>功能很完善。</li>
<li>提供許多資源可以配置</li>
<li>擁有許多的支持者。</li>
</ul>
<p><strong>缺點：</strong></p>
<ul>
<li>無法批次處理。</li>
<li>語言採用 DSL 與 Ruby。</li>
<li>缺少錯誤回報與檢查。</li>
<li>要透過程式定義先後順序。</li>
</ul>
<a id="more"></a>
<h2 id="基本概念介紹"><a href="#基本概念介紹" class="headerlink" title="基本概念介紹"></a>基本概念介紹</h2><h3 id="基礎設施即程式碼-Infrastructure-as-Code"><a href="#基礎設施即程式碼-Infrastructure-as-Code" class="headerlink" title="基礎設施即程式碼(Infrastructure as Code)"></a>基礎設施即程式碼(Infrastructure as Code)</h3><p>在官方可以了解到 puppet 是一個概念為<code>Infrastructure as Code</code>的工具。Infrastructure as Code 與一般撰寫的 shell scrip 類似，但是比後者更高一個層次，將這一層虛擬化，使管理者只需要定義 Infrastructure 的狀況即可。這樣除了可以模組化、reuse外，也可以清楚透過 code 了解環境安裝了什麼與設定了什麼，因此 code 就是一個 infrastructure。</p>
<h3 id="資源-Resource"><a href="#資源-Resource" class="headerlink" title="資源(Resource)"></a>資源(Resource)</h3><p>Puppet 中一個基礎元素為<code>resource</code>，一個 resource 可以是<code>file</code>、<code>package</code>或者是<code>service</code>等，透過 resource 我們可以查看環境上檔案、套件、服務狀態等。更多資訊可以參考 <a href="http://docs.puppetlabs.com/references/latest/type.html" target="_blank" rel="noopener">Resource 列表與使用方式</a>。</p>
<blockquote>
<p>P.S resource type 要注意大小寫，當作 metaparameters 的時候寫作 Type[title] Type 要大寫。</p>
</blockquote>
<h3 id="相依性-Dependencies"><a href="#相依性-Dependencies" class="headerlink" title="相依性(Dependencies)"></a>相依性(Dependencies)</h3><p>在使用 Puppet 時，通常會撰寫 manifest 檔案來定義 resource。而這些 resource 在執行時會以同步的方式完成。</p>
<blockquote>
<p>P.S 因為是同步(Sync)執行，故會有相依性的問題產生。這時候就可以用 Puppet 提供的 <code>before</code> / <code>require</code> 關鍵字來配置先後順序。</p>
</blockquote>
<h2 id="Puppet-安裝與基本操作"><a href="#Puppet-安裝與基本操作" class="headerlink" title="Puppet 安裝與基本操作"></a>Puppet 安裝與基本操作</h2><h3 id="環境建置"><a href="#環境建置" class="headerlink" title="環境建置"></a>環境建置</h3><p>我們將使用兩台 Ubuntu 14.04  主機來進行操作，一台為<code>主控節點</code>，另一台為<code>Agent 節點</code>。下面是我們將用到的伺服器的基礎資訊：</p>
<ul>
<li><strong>puupet 主控節點</strong><ul>
<li>IP：10.21.20.10</li>
<li>主機名稱：puppetmaster</li>
<li>完整主機名稱：puppetmaster.example.com</li>
</ul>
</li>
<li><strong>puupet agent 節點</strong><ul>
<li>IP：10.21.20.8</li>
<li>主機名稱：puppetslave</li>
<li>完整主機名稱：puppetslave.example.com</li>
</ul>
</li>
</ul>
<p>在每台節點完成以下步驟：</p>
<pre><code class="sh">$ sudo apt-get update &amp;&amp; sudo apt-get -y install ntp
$ sudo vim /etc/ntp.conf

server 1.tw.pool.ntp.org iburst
server 3.asia.pool.ntp.org iburst
server 2.asia.pool.ntp.org iburst
</code></pre>
<h3 id="Puppet-主控節點部署"><a href="#Puppet-主控節點部署" class="headerlink" title="Puppet 主控節點部署"></a>Puppet 主控節點部署</h3><p>首先我們要先安裝 puppet 套件，透過<code>wget</code>下載<code>puppetlabs-release.deb</code>資源庫套件：</p>
<pre><code class="sh"> $ wget https://apt.puppetlabs.com/puppetlabs-release-trusty.deb
 $ sudo dpkg -i puppetlabs-release-trusty.deb
 $ sudo apt-get update
</code></pre>
<p>完成後，我們就可以下載<code>puppetmaster-passenger</code>：</p>
<pre><code class="sh">$ sudo apt-get install puppetmaster-passenger
</code></pre>
<p>安裝過程中會發現錯誤，這部分可以忽略：</p>
<pre><code>Warning: Setting templatedir is deprecated. See http://links.puppetlabs.com/env-settings-deprecations
   (at /usr/lib/ruby/vendor_ruby/puppet/settings.rb:1139:in `issue_deprecation_warning&#39;)
</code></pre><p>安裝完後，可以透過以下指令查看版本：</p>
<pre><code class="sh">$ puppet --version
3.8.4
</code></pre>
<p>這時我們可以透過<code>resource</code>指令來查看可用資源：</p>
<pre><code class="sh">$ puppet resource [type]
$ puppet resource service

service { &#39;acpid&#39;:
  ensure =&gt; &#39;running&#39;,
  enable =&gt; &#39;true&#39;,
}
service { &#39;apache2&#39;:
  ensure =&gt; &#39;running&#39;,
}
</code></pre>
<blockquote>
<p>更多的 resource 可以查看 <a href="http://docs.puppetlabs.com/references/latest/type.html" target="_blank" rel="noopener">Type Reference</a>。</p>
</blockquote>
<p>在開始之前，我們先將 <code>apache2</code> 關閉，來讓 puppet 主控伺服器關閉：</p>
<pre><code class="sh">$ sudo service apache2 stop
</code></pre>
<p>接著我們要建立一個檔案<code>/etc/apt/preferences.d/00-puppet.pref</code>來鎖定 APT 自動更新套件，因為套件更新會造成組態檔的混亂：</p>
<pre><code class="sh">$ sudo vim /etc/apt/preferences.d/00-puppet.pref

Package: puppet puppet-common puppetmaster-passenger
Pin: version 3.8*
Pin-Priority: 501
</code></pre>
<p>Puppet 主控伺服器是一個認證推送機構，需要產生自己的認證，用於簽署所有 agent 的認證要求。首先要刪除所有該套件安裝過程建立的 ssl 憑證。預設憑證放在 <code>/var/lib/puppet/ssl</code>底下。</p>
<pre><code class="sh">$ sudo rm  -rf /var/lib/puppet/ssl
</code></pre>
<p>接著我們要修改<code>puppet.conf</code> 檔案，來配置節點之前認證溝通，這邊要註解<code>templatedir</code>這行。然後在檔案的<code>[main]</code>增加以下資訊。</p>
<pre><code class="sh">$ sudo vim /etc/puppet/puppet.conf

[main]
...
server = puppetmaster
environment = production
runinterval =  1h
strict_variables =  true
certname = puppetmaster
dns_alt_names = puppetmaster, puppetmaster.example.com
</code></pre>
<blockquote>
<p>詳細的檔案可以參閱<a href="https://docs.puppetlabs.com/puppet/latest/reference/config_file_main.html" target="_blank" rel="noopener">Main Config File (puppet.conf)
</a></p>
</blockquote>
<p>修改完後，透過<code>puppet</code>指令建立新的憑證：</p>
<pre><code class="sh">$ puppet master --verbose --no-daemonize

Info: Creating a new certificate revocation list
Info: Creating a new SSL key for puppetmaster
Info: csr_attributes file loading from /etc/puppet/csr_attributes.yaml
Info: Creating a new SSL certificate request for puppetmaster
Info: Certificate Request fingerprint (SHA256): 9B:C5:45:F8:C5:8F:C2:B1:4D:15:E3:64:5F:DB:19:AB:06:C4:60:99:48:F3:BA:8F:D3:03:7E:35:BE:BC:4E:B1
Notice: puppetmaster has a waiting certificate request
Notice: Signed certificate request for puppetmaster
Notice: Removing file Puppet::SSL::CertificateRequest puppetmaster at &#39;/var/lib/puppet/ssl/ca/requests/puppetmaster.pem&#39;
Notice: Removing file Puppet::SSL::CertificateRequest puppetmaster at &#39;/var/lib/puppet/ssl/certificate_requests/puppetmaster.pem&#39;
Notice: Starting Puppet master version 3.8.4
</code></pre>
<blockquote>
<p>當看到<code>Notice: Starting Puppet master version 3.8.4</code>代表完成，這時候可用 <code>CTRL-C</code>離開。</p>
</blockquote>
<p>檢查新產生的 SSL 憑證，可以使用以下指令：</p>
<pre><code class="sh">$ puppet cert list -all

+ &quot;puppetmaster&quot; (SHA256) 8C:5E:39:A7:81:94:2B:09:7E:20:B8:F2:46:59:60:D9:FA:5D:4A:9E:BF:27:D7:C1:1A:A4:3E:97:12:D3:BE:21 (alt names: &quot;DNS:puppet-master&quot;, &quot;DNS:puppet-master.example.com&quot;, &quot;DNS:puppetmaster&quot;)
</code></pre>
<h3 id="設定一個-Puppet-manifests"><a href="#設定一個-Puppet-manifests" class="headerlink" title="設定一個 Puppet manifests"></a>設定一個 Puppet manifests</h3><p>預設的 manifests 為<code>/etc/puppet/manifests/site.pp</code>。這個主要 manifests 檔案包括了用於在 Agent 節點執行的組態定義：</p>
<pre><code class="sh">$ sudo vim /etc/puppet/manifests/site.pp

# execute &#39;apt-get update&#39;
exec { &#39;apt-update&#39;: # exec resource named &#39;apt-update&#39;
command =&gt; &#39;/usr/bin/apt-get update&#39; # command this resource will run
}

# install apache2 package
package { &#39;apache2&#39;:
require =&gt; Exec[&#39;apt-update&#39;], # require &#39;apt-update&#39; before installing
ensure =&gt; installed,
}

# ensure apache2 service is running
service { &#39;apache2&#39;:
ensure =&gt; running,
}
</code></pre>
<blockquote>
<p>上面幾行用來部署 apache2 到 agent 節點。</p>
</blockquote>
<p>完成後，修改<code>/etc/apache2/sites-enabled/puppetmaster.conf</code>檔，修改<code>SSLCertificateFile</code>與<code>SSLCertificateKeyFile</code>對應到新的憑證：</p>
<pre><code class="sh">SSLCertificateFile      /var/lib/puppet/ssl/certs/puppetmaster.pem
SSLCertificateKeyFile   /var/lib/puppet/ssl/private_keys/puppetmaster.pem
</code></pre>
<p>然後重新開啟服務：</p>
<pre><code class="sh">$ sudo service apache2 restart
</code></pre>
<h3 id="Puppet-agent-節點部署"><a href="#Puppet-agent-節點部署" class="headerlink" title="Puppet agent 節點部署"></a>Puppet agent 節點部署</h3><p>首先在 agent 節點上使用以下指令下載 puppet labs 的套件，並安裝：</p>
<pre><code class="sh">$ wget https://apt.puppetlabs.com/puppetlabs-release-trusty.deb
$ sudo dpkg -i puppetlabs-release-trusty.deb
$ sudo apt-get update
$ sudo apt-get install -y puppet
</code></pre>
<p>由於 puppet 預設是不會啟動的，所以要編輯<code>/etc/default/puppet</code>檔案來設定：</p>
<pre><code class="sh">$ sudo vim /etc/default/puppet

START=yes
</code></pre>
<p>之後一樣設定防止 APT 更新到 puppet，修改<code>/etc/apt/preferences.d/00-puppet.pref</code>檔案：</p>
<pre><code class="sh">$ sudo vim /etc/apt/preferences.d/00-puppet.pref

Package: puppet puppet-common
Pin: version 3.8*
Pin-Priority: 501
</code></pre>
<h3 id="設定-puppet-agent"><a href="#設定-puppet-agent" class="headerlink" title="設定 puppet agent"></a>設定 puppet agent</h3><p>編輯<code>/etc/puppet/puppet.conf</code>檔案，將<code>templatedir</code>這行註解掉，並移除<code>[master]</code>部分的相關設定：</p>
<pre><code class="sh">$ sudo vim /etc/puppet/puppet.conf

[main]
logdir=/var/log/puppet
vardir=/var/lib/puppet
ssldir=/var/lib/puppet/ssl
rundir=/var/run/puppet
factpath=$vardir/lib/facter
# templatedir=$confdir/templates

[agent]
server = puppetmaster.example.com
certname = puppetslave.example.com
</code></pre>
<p>完成後啟動 puppet：</p>
<pre><code class="sh">$ sudo service puppet start
</code></pre>
<h3 id="在主控伺服器上對憑證要求進行簽證"><a href="#在主控伺服器上對憑證要求進行簽證" class="headerlink" title="在主控伺服器上對憑證要求進行簽證"></a>在主控伺服器上對憑證要求進行簽證</h3><p>當完成 master 節點與 slave 節點後，可以在主控伺服器上使用以下指令來列出當前憑證請求：</p>
<pre><code class="sh">$ puppet cert list

&quot;puppetnode.example.com&quot; (SHA256) 52:43:4C:ED:16:34:A3:EA:E7:5D:B0:97:FF:66:4F:C8:E0:51:AD:80:E6:32:95:53:FC:24:AE:15:17:17:3A:C0
</code></pre>
<p>接著使用以下指令進行簽證：</p>
<pre><code class="sh">$ puppet cert sign puppetnode.example.com

Notice: Signed certificate request for puppetnode.example.com
Notice: Removing file Puppet::SSL::CertificateRequest puppetnode.example.com at &#39;/var/lib/puppet/ssl/ca/requests/puppetnode.example.com.pem&#39;
</code></pre>
<blockquote>
<p>也可以使用<code>puppet cert sign --all</code>來一次簽署多個。</p>
<p>若想要移除可以使用<code>puppet cert clean hostname</code>。</p>
</blockquote>
<p>簽署成功後，可以用以下指令查看：</p>
<pre><code class="sh">$ puppet cert list --all

+ &quot;puppetmaster&quot;           (SHA256) 8C:5E:39:A7:81:94:2B:09:7E:20:B8:F2:46:59:60:D9:FA:5D:4A:9E:BF:27:D7:C1:1A:A4:3E:97:12:D3:BE:21 (alt names: &quot;DNS:puppet-master&quot;, &quot;DNS:puppet-master.example.com&quot;, &quot;DNS:puppetmaster&quot;)
+ &quot;puppetnode.example.com&quot; (SHA256) EF:D6:E5:7E:45:B0:5D:EC:D4:17:E6:31:A2:97:F6:C2:31:2A:19:B9:0E:9D:31:77:9A:02:93:BC:73:B9:5E:58
</code></pre>
<h3 id="部署主節點的-manifests"><a href="#部署主節點的-manifests" class="headerlink" title="部署主節點的 manifests"></a>部署主節點的 manifests</h3><p>當配置並完成 puppet manifests，現在需要部署 manifests 到 slave 節點上。要載入 puppet manifests 可以使用以下指令：</p>
<pre><code class="sh">$ puppet agent --test

Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Caching catalog for puppetnode.example.com
Info: Applying configuration version &#39;1452086629&#39;
Notice: /Stage[main]/Main/Exec[apt-update]/returns: executed successfully
Notice: Finished catalog run in 17.31 seconds
</code></pre>
<p>之後我們可以使用<code>puppet apply</code>來提交 manifests：</p>
<pre><code class="sh">$ puppet apply /etc/puppet/manifests/site.pp
</code></pre>
<p>若要指定節點，可以建立如以下的<code>*.pp</code>檔：</p>
<pre><code class="sh">$ sudo vim /etc/puppet/manifests/site-example.pp

node &#39;puppetslave1&#39;, &#39;puppetslave2&#39; {
# execute &#39;apt-get update&#39;
exec { &#39;apt-update&#39;: # exec resource named &#39;apt-update&#39;
command =&gt; &#39;/usr/bin/apt-get update&#39; # command this resource will run
}

# install apache2 package
package { &#39;apache2&#39;:
require =&gt; Exec[&#39;apt-update&#39;], # require &#39;apt-update&#39; before installing
ensure =&gt; installed,
}

# ensure apache2 service is running
service { &#39;apache2&#39;:
ensure =&gt; running,
}
}
</code></pre>
<p>Puppet 是一個很成熟的工具，已有許多模組被貢獻，我們可以透過以下方式下載模組：</p>
<pre><code class="sh">$  puppet module install puppetlabs-apache
</code></pre>
<blockquote>
<p>注意，不要在一個已經部署 Apache 的環境上使用該模組，否則會清空為沒有被 puppet 管理的 apache 配置。</p>
</blockquote>
<p>接著我們修改<code>site.pp</code>來配置 apache：</p>
<pre><code class="sh">$ sudo vim /etc/puppet/manifest/site.pp

node &#39;puppetslave&#39; {
class { &#39;apache&#39;: } # use apache module
apache::vhost { &#39;example.com&#39;: # define vhost resource
port =&gt; &#39;8080&#39;,
docroot =&gt; &#39;/var/www/html&#39;
}
}
</code></pre>
<h2 id="參考資源"><a href="#參考資源" class="headerlink" title="參考資源"></a>參考資源</h2><ul>
<li><a href="https://forge.puppetlabs.com/" target="_blank" rel="noopener">Modules Search</a></li>
<li><a href="http://www.infoq.com/cn/articles/introduction-puppet" target="_blank" rel="noopener">InfoQ Puppet 介紹</a></li>
<li><a href="http://amyhehe.blog.51cto.com/9406021/1708500" target="_blank" rel="noopener">Puppet 學習</a></li>
<li><a href="http://blog.hsatac.net/2013/02/puppet-study-note/" target="_blank" rel="noopener">Puppet 筆記</a></li>
<li><a href="http://blog.csdn.net/linux_player_c/article/details/50148415" target="_blank" rel="noopener">puppet學習筆記：puppet資源file詳細介紹</a></li>
<li><a href="https://www.digitalocean.com/community/tutorials/how-to-install-puppet-to-manage-your-server-infrastructure" target="_blank" rel="noopener">How To Install Puppet To Manage Your Server Infrastructure</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> DevOps </category>
            
        </categories>
        
        
        <tags>
            
            <tag> DevOps </tag>
            
            <tag> Automation Engine </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[SaltStack 介紹]]></title>
      <url>https://kairen.github.io/2016/02/12/devops/saltstack-basic/</url>
      <content type="html"><![CDATA[<p>Saltstack 是一套基礎設施管理開發套件、簡單易部署、可擴展到管理成千上萬的伺服器、控制速度佳(以 ms 為單位)。Saltstack 提供了動態基礎設施溝通總線用於編配、遠端執行、配置管理等等。Saltstack 是從 2011 年開始的專案，已經是很成熟的開源專案。該專案簡單的兩大基礎功能就是配置管理與遠端指令執行。</p>
<p>Saltstack 採用集中化管理，我們一般可以理解為 Puppet 的簡化版本與 <a href="https://fedorahosted.org/func/" target="_blank" rel="noopener">Func</a><br>的加強版本。Saltstack 是基於 Python 語言開發的，結合輕量級訊息佇列（ZeroMQ）以及 Python 第三方模組（Pyzmq、PyCrypto、Pyjinja2、python-msgpack與PyYAML等）。</p>
<p><img src="/images/devops/saltstack-arch.png" alt=""></p>
<a id="more"></a>
<p><strong>優點</strong>：</p>
<ul>
<li>部署簡單與方便。</li>
<li>支持大部分 UNIX/Liunx 及 Windows 環境。</li>
<li>主從集中化管理。</li>
<li>配置簡單、功能強大與擴展性強。</li>
<li>主控端（Master）與被控制端（Minion）基於憑證認證。</li>
<li>支援 API 以及自定義模組，透過 Python 輕鬆擴展。</li>
<li>社群活躍。</li>
</ul>
<p><strong>缺點</strong>：</p>
<ul>
<li>Web UI 雖然有，但是沒有報表功能。</li>
<li>需要 Agent</li>
</ul>
<p>透過 Saltstack 環境，我們可在成千上萬的伺服器進行批次的指令執行，根據不同的集中化管理配置、分散檔案、收集伺服器資料、作業系統基礎環境以及軟體套件等。</p>
<h1 id="參考資源"><a href="#參考資源" class="headerlink" title="參考資源"></a>參考資源</h1><ul>
<li><a href="http://openskill.cn/article/183?utm_source=tuicool&amp;utm_medium=referral" target="_blank" rel="noopener">SaltStack介紹和架構解析</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> DevOps </category>
            
        </categories>
        
        
        <tags>
            
            <tag> DevOps </tag>
            
            <tag> Automation Engine </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Docker 快速部署 Ceph 測試叢集]]></title>
      <url>https://kairen.github.io/2016/02/11/ceph/deploy/ceph-docker/</url>
      <content type="html"><![CDATA[<p>本節將介紹如何透過 <a href="https://github.com/ceph/ceph-docker" target="_blank" rel="noopener">ceph-docker</a> 工具安裝一個測試的 Ceph 環境，一個最簡單的 Ceph 儲存叢集至少要<code>1 Monitor</code>與<code>3 OSD</code>。另外部署 MDS 與 RGW 來進行簡單測試。</p>
<center><img src="/images/ceph/docker-ceph.jpg" alt=""></center>

<a id="more"></a>
<h2 id="節點配置"><a href="#節點配置" class="headerlink" title="節點配置"></a>節點配置</h2><p>本安裝採用一台虛擬機器來提供部署，可使用 VBox 或 OpenStack 等建立，其環境資源大小如下：</p>
<table>
<thead>
<tr>
<th>hostname</th>
<th>CPUs</th>
<th>RAM</th>
</tr>
</thead>
<tbody>
<tr>
<td>ceph-aio</td>
<td>2vCPU</td>
<td>4GB</td>
</tr>
</tbody>
</table>
<blockquote>
<p>若使用 Vagrant + VBox 的話，可以使用 <a href="https://gist.githubusercontent.com/kairen/c55a436718ddc22817ef820001aecb0f/raw/4be0a6cfa5087a4834494779b0809d76d701f67b/Vagrantfile" target="_blank" rel="noopener">Vagrantfile 腳本</a>。</p>
</blockquote>
<p>而該虛擬機要額外建立三顆虛擬區塊裝置，如下所示：</p>
<table>
<thead>
<tr>
<th>Dev path</th>
<th>Disk</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>/dev/sdb</td>
<td>20 GB</td>
<td>osd-1 使用</td>
</tr>
<tr>
<td>/dev/sdc</td>
<td>20 GB</td>
<td>osd-2 使用</td>
</tr>
<tr>
<td>/dev/sdd</td>
<td>20 GB</td>
<td>osd-3 使用</td>
</tr>
</tbody>
</table>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>首先在主機安裝 Docker Engine，可以透過以下指令進行安裝：</p>
<pre><code class="sh">$ curl -fsSL https://get.docker.com/ | sh
</code></pre>
<h2 id="部署-Ceph-測試叢集"><a href="#部署-Ceph-測試叢集" class="headerlink" title="部署 Ceph 測試叢集"></a>部署 Ceph 測試叢集</h2><p>首先為了不與預設 Docker 網路共用，這邊額外建立一網路來提供給 Ceph 使用：</p>
<pre><code class="sh">$ docker network create --driver bridge ceph-net
$ docker network inspect ceph-net
{
    &quot;Subnet&quot;: &quot;172.18.0.0/16&quot;,
    &quot;Gateway&quot;: &quot;172.18.0.1/16&quot;
}
</code></pre>
<h3 id="建立-Monitor"><a href="#建立-Monitor" class="headerlink" title="建立 Monitor"></a>建立 Monitor</h3><p>完成網路建立後，就可以開始部署 Ceph 叢集了。一開始我們必須先建立 Monitor Container：</p>
<pre><code class="sh">$ cd ~ &amp;&amp; DIR=$(pwd)
$ sudo docker run -d --net=ceph-net \
-v ${DIR}/ceph:/etc/ceph \
-v ${DIR}/lib/ceph/:/var/lib/ceph/ \
-e MON_IP=172.18.0.2 \
-e CEPH_PUBLIC_NETWORK=172.18.0.0/16 \
--name mon1 \
ceph/daemon mon
</code></pre>
<blockquote>
<p>若發生錯誤請刪除以下目錄。如以下指令：</p>
<pre><code class="sh">$ sudo rm -rf ${DIR}/etc/ceph/
$ sudo rm -rf ${DIR}/var/lib/ceph/
</code></pre>
</blockquote>
<p>檢查是否正確部署：</p>
<pre><code class="sh">$ docker exec -ti mon1 ceph -v
ceph version 10.2.2 (45107e21c568dd033c2f0a3107dec8f0b0e58374)

$ docker exec -ti mon1 ceph -s
cluster 2c254496-e948-4abb-a6dc-9aea41bbb56a
 health HEALTH_ERR
        no osds
 monmap e1: 1 mons at {1068f41de69a=172.18.0.2:6789/0}
        election epoch 3, quorum 0 1068f41de69a
 osdmap e1: 0 osds: 0 up, 0 in
        flags sortbitwise
  pgmap v2: 64 pgs, 1 pools, 0 bytes data, 0 objects
        0 kB used, 0 kB / 0 kB avail
              64 creating
</code></pre>
<h3 id="建立-OSD"><a href="#建立-OSD" class="headerlink" title="建立 OSD"></a>建立 OSD</h3><p>上面可以看到 Monitor 建立完成，但是會有錯誤，因為目前沒有 OSD。因此這邊將建立三個 OSD Container 來模擬叢集做實際儲存的功能，透過以下方式部署：</p>
<pre><code class="sh">$ cd ~ &amp;&amp; DIR=$(pwd)
$ sudo docker run -d --net=ceph-net \
--privileged=true --pid=host \
-v ${DIR}/ceph:/etc/ceph \
-v ${DIR}/lib/ceph/:/var/lib/ceph/ \
-v /dev/:/dev/ \
-e OSD_DEVICE=/dev/sdb \
-e OSD_TYPE=disk \
-e OSD_FORCE_ZAP=1 \
--name osd1 \
ceph/daemon osd
</code></pre>
<blockquote>
<p>若要建立多個 OSD，只需要修改<code>OSD_DEVICE</code>與<code>name</code>即可，這邊建議建立三個 OSD。因為預設 pool 採用三份副本，若節點數過少需要自行修改副本數或 CRUSH Map。</p>
</blockquote>
<p>完成後，可以透過以下指令檢查 Device 被使用：</p>
<pre><code class="sh">$ docker exec -ti osd1 df | grep &quot;osd&quot;
/dev/sdb1                     20857836   34924  20822912   1% /var/lib/ceph/osd/ceph-0
</code></pre>
<p>也可以直接透過 Monitor 來查看叢集安全狀態，如 PG 是否有誤等：</p>
<pre><code class="sh">$ docker exec -ti mon1 ceph -s
cluster 23fa3f2c-a401-46e0-abc1-d71b4625b348
 health HEALTH_OK
 monmap e2: 1 mons at {0b7ff674673f=172.18.0.2:6789/0}
        election epoch 4, quorum 0 0b7ff674673f
    mgr no daemons active
 osdmap e15: 3 osds: 3 up, 3 in
        flags sortbitwise,require_jewel_osds,require_kraken_osds
  pgmap v29: 64 pgs, 1 pools, 0 bytes data, 0 objects
        101 MB used, 61005 MB / 61106 MB avail
              64 active+clean
</code></pre>
<h3 id="建立-RGW"><a href="#建立-RGW" class="headerlink" title="建立 RGW"></a>建立 RGW</h3><p>當完成一個 RAODS(MON+OSD)叢集後，即可建立物件儲存閘道(RAODS Gateway)提供 S3 與 Swift 相容的 API，來儲存檔案到叢集中，一個 RGW Container 建立如下所示：</p>
<pre><code class="sh">$ cd ~ &amp;&amp; DIR=$(pwd)
$ sudo docker run -d --net=ceph-net \
-v ${DIR}/lib/ceph/:/var/lib/ceph/ \
-v ${DIR}/ceph:/etc/ceph \
-p 8080:8080 \
--name rgw1 \
ceph/daemon rgw
</code></pre>
<p>完成後，透過 curl 工具來測試是否正確部署：</p>
<pre><code class="sh">$ curl -H &quot;Content-Type: application/json&quot; &quot;http://127.0.0.1:8080&quot;
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;ListAllMyBucketsResult xmlns=&quot;http://s3.amazonaws.com/doc/2006-03-01/&quot;&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;
</code></pre>
<p>透過 Python Client 進行檔案儲存，首先下載程式：</p>
<pre><code class="sh">$ wget &quot;https://gist.githubusercontent.com/kairen/e0dec164fa6664f40784f303076233a5/raw/33add5a18cb7d6f18531d8d481562d017557747c/s3client&quot;
$ chmod u+x s3client
$ sudo pip install boto
</code></pre>
<p>接著透過以下指令建立一個使用者：</p>
<pre><code class="sh">$ docker exec -ti rgw1 radosgw-admin user create --uid=&quot;test&quot; --display-name=&quot;I&#39;m Test account&quot; --email=&quot;test@example.com&quot;

&quot;keys&quot;: [
        {
            &quot;user&quot;: &quot;test&quot;,
            &quot;access_key&quot;: &quot;PFMKGXCFD77L8X4CF0T4&quot;,
            &quot;secret_key&quot;: &quot;SA8RpGO7SoN4TIdRxYtxloc5kRSLQvhOihJdDGG3&quot;
        }
    ],
</code></pre>
<p>建立一個放置環境參數的檔案<code>s3key.sh</code>：</p>
<pre><code class="sh">export S3_ACCESS_KEY=&quot;PFMKGXCFD77L8X4CF0T4&quot;
export S3_SECRET_KEY=&quot;SA8RpGO7SoN4TIdRxYtxloc5kRSLQvhOihJdDGG3&quot;
export S3_HOST=&quot;127.0.0.1&quot;
export S3_PORT=&quot;8080&quot;
</code></pre>
<p>然後 source 檔案，並嘗試執行列出 bucket 指令：</p>
<pre><code class="sh">$ . s3key.sh
$ ./s3client list
---------- Bucket List ----------
</code></pre>
<p>建立一個 Bucket，並上傳檔案：</p>
<pre><code class="sh">$ ./s3client create files
Create [files] success ...

$ ./s3client upload files s3key.sh /
Upload [s3key.sh] success ...
</code></pre>
<p>完成後，即可透過 list 與 download 來查看與下載：</p>
<pre><code class="sh">$ ./s3client list files
---------- [files] ----------
s3key.sh                157                     2016-07-26T06:48:14.327Z

$ ./s3client download files s3key.sh
Download [s3key.sh] success ...
</code></pre>
<h3 id="建立-MDS"><a href="#建立-MDS" class="headerlink" title="建立 MDS"></a>建立 MDS</h3><p>當系統需要使用到 CephFS 時，我們將必須建立 MDS(Metadata Server) 來提供詮釋資料的儲存，一個 MDS 容器部署如下：</p>
<pre><code class="sh">$ cd ~ &amp;&amp; DIR=$(pwd)
$ sudo docker run -d --net=ceph-net \
-v ${DIR}/lib/ceph/:/var/lib/ceph/ \
-v ${DIR}/ceph:/etc/ceph \
-e CEPHFS_CREATE=1 \
--name mds1 \
ceph/daemon mds
</code></pre>
<p>透過以下指令檢查是否建立無誤：</p>
<pre><code class="sh">$ docker exec -ti mds1 ceph mds stat
e5: 1/1/1 up {0=mds-aea2f53de13a=up:active}

$ docker exec -ti mds1 ceph fs ls
name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]
</code></pre>
]]></content>
      
        <categories>
            
            <category> Ceph </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> Storage </tag>
            
            <tag> Docker </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[利用 Graphite 監控系統資料]]></title>
      <url>https://kairen.github.io/2016/02/11/devops/graphite/</url>
      <content type="html"><![CDATA[<p>Graphite 是一款開源的監控繪圖工具。Graphite 可以實時收集、存儲、顯示時間序列類型的數據（time series data）。它主要有三個部分構成：</p>
<ol>
<li><strong>Carbon</strong>：基於 Twisted 的行程，用來接收資料。</li>
<li><strong>Whisper</strong>：專門儲存時間序列類型資料的小型資料庫。</li>
<li><strong>Graphite</strong> webapp：基於 Django 的網頁應用程式。</li>
</ol>
<a id="more"></a>
<h2 id="安裝-Graphite"><a href="#安裝-Graphite" class="headerlink" title="安裝 Graphite"></a>安裝 Graphite</h2><p>在開始配置 Graphite 之前，需要先安裝系統相依套件：</p>
<pre><code class="sh">$ sudo apt-get install build-essential graphite-web graphite-carbon python-dev apache2 libapache2-mod-wsgi libpq-dev python-psycopg2
</code></pre>
<blockquote>
<p>在安裝期間<code>graphite-carbon</code>會詢問是否要刪除 whisper database files，這邊回答<code>YES</code>。</p>
</blockquote>
<h3 id="配置-Carbon"><a href="#配置-Carbon" class="headerlink" title="配置 Carbon"></a>配置 Carbon</h3><p>透過增加<code>[test]</code>到 Carbon 的<code>/etc/carbon/storage-schemas.conf</code> 檔案，這部分單純用於測試使用，如果不需要可以直接跳過：</p>
<pre><code class="txt">[carbon]
pattern = ^carbon\.
retentions = 60:90d

[test]
pattern = ^test\.
retentions = 5s:3h,1m:1d

[default_1min_for_1day]
pattern = .*
retentions = 60s:1d
</code></pre>
<blockquote>
<p>更多如何配置 Carbon storage 的資訊，可以參考 <a href="http://graphite.readthedocs.org/en/latest/config-carbon.html#storage-schemas-conf" target="_blank" rel="noopener"> storage-schemas.con</a>。</p>
</blockquote>
<p>之後複製預設的聚合組態到<code>/etc/carbon</code>：</p>
<pre><code class="sh">$ sudo cp /usr/share/doc/graphite-carbon/examples/storage-aggregation.conf.example /etc/carbon/storage-aggregation.conf
</code></pre>
<p>設定在開機時，啟動 Carbon 快取，編輯<code>/etc/default/graphite-carbon</code>：</p>
<pre><code class="sh">CARBON_CACHE_ENABLED=true
</code></pre>
<p>啟動 Carbon 服務：</p>
<pre><code class="sh">$ sudo service carbon-cache start
</code></pre>
<h3 id="安裝與配置-PostgreSQL"><a href="#安裝與配置-PostgreSQL" class="headerlink" title="安裝與配置 PostgreSQL"></a>安裝與配置 PostgreSQL</h3><p>安裝 PostgreSQL 讓 graphite-web 應用程式使用：</p>
<pre><code class="sh">$ sudo apt-get install postgresql
</code></pre>
<p>切換到<code>postgres</code>使用者，並建立資料庫使用者給 Graphite：</p>
<pre><code class="txt">$ sudo su - postgres
postgres# createuser graphite --pwprompt
</code></pre>
<p>建立<code>graphite</code>與<code>grafana</code>資料庫：</p>
<pre><code class="sh">postgres# createdb -O graphite graphite
postgres# createdb -O graphite grafana
</code></pre>
<p>切換<code>graphite</code>來檢查配置是否成功：</p>
<pre><code class="sh">$ sudo su - graphite
</code></pre>
<h3 id="設定-Graphite"><a href="#設定-Graphite" class="headerlink" title="設定 Graphite"></a>設定 Graphite</h3><p>更新 Graphite web 使用的後端資料庫與其他設定，編輯<code>/etc/graphite/local_settings.py</code>，加入以下：</p>
<pre><code class="sh">DATABASES = {
&#39;default&#39;: {
    &#39;NAME&#39;: &#39;graphite&#39;,
    &#39;ENGINE&#39;: &#39;django.db.backends.postgresql_psycopg2&#39;,
    &#39;USER&#39;: &#39;graphite&#39;,
    &#39;PASSWORD&#39;: &#39;graphiteuserpassword&#39;,
    &#39;HOST&#39;: &#39;127.0.0.1&#39;,
    &#39;PORT&#39;: &#39;&#39;
    }
}

USE_REMOTE_USER_AUTHENTICATION = True
TIME_ZONE = &#39;UTC&#39;
SECRET_KEY = &#39;some-secret-key&#39;
</code></pre>
<blockquote>
<p><code>TIME_ZONE</code> 可以查詢 <a href="https://en.wikipedia.org/wiki/List_of_tz_database_time_zones" target="_blank" rel="noopener">Wikipedia’s timezone database</a></p>
<p><code>SECRET_KEY</code>可以使用<code>openssl rand -hex 10</code>指令來建立。</p>
</blockquote>
<p>初始化資料庫：</p>
<pre><code class="sh">$ sudo graphite-manage syncdb
</code></pre>
<h3 id="設定-Graphite-使用-Apache"><a href="#設定-Graphite-使用-Apache" class="headerlink" title="設定 Graphite 使用 Apache"></a>設定 Graphite 使用 Apache</h3><p>首先複製 Graphite 的 Apache 配置樣板到 Apache sites-available 目錄：</p>
<pre><code class="sh">$ sudo cp /usr/share/graphite-web/apache2-graphite.conf /etc/apache2/sites-available
</code></pre>
<p>編輯<code>/etc/apache2/sites-available/apache2-graphite.conf</code>，修改預設監聽的 port：</p>
<pre><code>&lt;VirtualHost *:8080&gt;
</code></pre><p>編輯<code>/etc/apache2/ports.conf</code>加入監聽的 port：</p>
<pre><code class="sh">Listen 80
Listen 8080
</code></pre>
<p>取消預設 Apache 的 site：</p>
<pre><code class="sh">$ sudo a2dissite 000-default
</code></pre>
<p>啟用 Graphite 的虛擬 site，並重新載入：</p>
<pre><code class="sh">$ sudo a2ensite apache2-graphite
$ sudo service apache2 reload
</code></pre>
<p>重新啟動 apache 服務：</p>
<pre><code class="sh">$ sudo service apache2 restart
</code></pre>
<blockquote>
<p>完成後，即可登入<code>example_domain.com:8080</code>。</p>
</blockquote>
<p>測試一個簡單資料：</p>
<pre><code class="sh">$ for i in 4 6 8 16 2; do echo &quot;test.count $i `date +%s`&quot; | nc -q0 127.0.0.1 2003; sleep 6; done
</code></pre>
<h2 id="參考連結"><a href="#參考連結" class="headerlink" title="參考連結"></a>參考連結</h2><ul>
<li><a href="https://www.linode.com/docs/uptime/monitoring/deploy-graphite-with-grafana-on-ubuntu-14-04" target="_blank" rel="noopener">Deploy Graphite with Grafana on Ubuntu 14.04</a></li>
<li><a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-use-graphite-on-an-ubuntu-14-04-server" target="_blank" rel="noopener">How To Install and Use Graphite on an Ubuntu 14.04 Server</a></li>
<li><a href="http://www.vpsee.com/2015/03/a-modern-monitoring-system-built-with-grafana-collected-influxdb/" target="_blank" rel="noopener">Grafana＋collectd＋InfluxDB</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> DevOps </category>
            
        </categories>
        
        
        <tags>
            
            <tag> DevOps </tag>
            
            <tag> Monitoring </tag>
            
            <tag> Data Collect </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[kube-up 腳本部署 Kubernetes 叢集(Deprecated)]]></title>
      <url>https://kairen.github.io/2016/01/16/kubernetes/deploy/kubeup-deploy/</url>
      <content type="html"><![CDATA[<p>Kubernetes 提供了許多雲端平台與作業系統的安裝方式，本篇將使用官方腳本<code>kube-up.sh</code>來部署 Kubernetes 到 Ubuntu 14.04 系統上。其他更多平台的部署可以參考 <a href="https://kubernetes.io/docs/getting-started-guides/" target="_blank" rel="noopener">Creating a Kubernetes Cluster</a>。</p>
<p>本環境安裝資訊：</p>
<ul>
<li>Kubernetes v1.5.4</li>
<li>Etcd v2.3.0</li>
<li>Flannel v0.5.5</li>
<li>Docker v1.13.1</li>
</ul>
<a id="more"></a>
<h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本次安裝作業系統採用<code>Ubuntu 14.04 Server</code>，測試環境為 OpenStack VM 與實體主機：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Role</th>
<th>CPU</th>
<th>Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.16.35.12</td>
<td>master1</td>
<td>2</td>
<td>4G</td>
</tr>
<tr>
<td>172.16.35.10</td>
<td>node1</td>
<td>2</td>
<td>4G</td>
</tr>
<tr>
<td>172.16.35.11</td>
<td>node2</td>
<td>2</td>
<td>4G</td>
</tr>
</tbody>
</table>
<blockquote>
<p>這邊 master 為主要控制節點，node 為應用程式工作節點。</p>
</blockquote>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>安裝前需要確認叢集滿足以下幾點：</p>
<ul>
<li>目前官方只測試過 <code>Ubuntu 14.04</code>，官方說法是 15.x 也沒問題，但 16.04 上我測試無法自動完成，要自己補上各種服務的 Systemd 腳本。</li>
<li>部署節點可以透過 SSH 與其他節點溝通，並且是無密碼登入，以及有 Sudoer 權限。</li>
<li>所有節點需要安裝<code>Docker</code>或<code>rtk</code>引擎。安裝方式為以下：</li>
</ul>
<pre><code class="sh">$ curl -fsSL &quot;https://get.docker.com/&quot; | sh
$ sudo iptables -P FORWARD ACCEPT
</code></pre>
<h2 id="部署-Kubernetes-叢集"><a href="#部署-Kubernetes-叢集" class="headerlink" title="部署 Kubernetes 叢集"></a>部署 Kubernetes 叢集</h2><p>首先下載官方 Release 的原始碼程式：</p>
<pre><code class="sh">$ curl -sSL &quot;https://github.com/kubernetes/kubernetes/archive/v1.5.4.tar.gz&quot; | tar zx
$ mv kubernetes-1.5.4 kubernetes
</code></pre>
<p>接著編輯<code>kubernetes/cluster/ubuntu/config-default.sh</code>設定檔，修改以下內容：</p>
<pre><code class="sh">export nodes=${nodes:-&quot;ubuntu@172.16.35.12 ubuntu@172.16.35.10 ubuntu@172.16.35.11&quot;}
export role=&quot;ai i i&quot;
export NUM_NODES=${NUM_NODES:-3}
export SERVICE_CLUSTER_IP_RANGE=192.168.3.0/24
export FLANNEL_NET=172.16.0.0/16
SERVICE_NODE_PORT_RANGE=${SERVICE_NODE_PORT_RANGE:-&quot;30000-32767&quot;}
</code></pre>
<p>設定要部署的 Kubernetes 版本環境參數：</p>
<pre><code class="sh">export KUBE_VERSION=1.5.4
export FLANNEL_VERSION=0.5.5
export ETCD_VERSION=2.3.0
export KUBERNETES_PROVIDER=ubuntu
</code></pre>
<p>然後進入到<code>kubernetes/cluster</code>目錄，並執行以下指令：</p>
<pre><code class="sh">$ sudo sed -i &#39;s/verify-kube-binaries//g&#39; kube-up.sh
$ ./kube-up.sh
...
Cluster validation succeeded
Done, listing cluster services:

Kubernetes master is running at http://172.16.35.12:8080
</code></pre>
<p>當看到上述資訊即表示成功部署，這時候進入到<code>cluster/ubuntu/binaries</code>目錄複製 kubectl 工具：</p>
<pre><code class="sh">$ sudo cp kubectl /usr/local/bin/
</code></pre>
<p>最後透過 kubectl 工具來查看叢集節點是否成功加入：</p>
<pre><code class="sh">$ kubectl get nodes

NAME           STATUS    AGE
172.16.35.12   Ready     2m
172.16.35.10   Ready     2m
172.16.35.11   Ready     2m
</code></pre>
<h2 id="Option-部署-Add-ons"><a href="#Option-部署-Add-ons" class="headerlink" title="(Option)部署 Add-ons"></a>(Option)部署 Add-ons</h2><p>若要部署 kubernetes Dashboard 與 DNS 等額外服務的話，要修改<code>kubernetes/cluster/ubuntu/config-default.sh</code>設定檔，修改一下內容：</p>
<pre><code class="sh">ENABLE_CLUSTER_MONITORING=&quot;${KUBE_ENABLE_CLUSTER_MONITORING:-true}&quot;
ENABLE_CLUSTER_UI=&quot;${KUBE_ENABLE_CLUSTER_UI:-true}&quot;
ENABLE_CLUSTER_DNS=&quot;${KUBE_ENABLE_CLUSTER_DNS:-true}&quot;
DNS_SERVER_IP=${DNS_SERVER_IP:-&quot;192.168.3.10&quot;}
DNS_DOMAIN=${DNS_DOMAIN:-&quot;cluster.local&quot;}
</code></pre>
<blockquote>
<p>通常基本款大概為 Dashboard、DNS、Monitoring 與 Logging，。</p>
</blockquote>
<p>修改完成後，進入到<code>kubernetes/cluster/ubuntu</code>目錄，並執行以下指令：</p>
<pre><code class="sh">$ KUBERNETES_PROVIDER=ubuntu ./deployAddons.sh
</code></pre>
<p>透過 kubectl 查看資訊，這邊服務屬於系統的，所以預設會被分到<code>kube-system</code>命名空間：</p>
<pre><code class="sh">$ kubectl get pods --namespace=kube-system
</code></pre>
<p>最後就可以透過瀏覽器查看 <a href="http://172.16.35.12:8080/ui" target="_blank" rel="noopener">Dashboard</a>。</p>
<h2 id="建立-Nginx-應用程式"><a href="#建立-Nginx-應用程式" class="headerlink" title="建立 Nginx 應用程式"></a>建立 Nginx 應用程式</h2><p>Kubernetes 可以選擇使用指令直接建立應用程式與服務，或者撰寫 YAML 與 JSON 檔案來描述部署應用程式的配置，以下將使用兩種方式建立一個簡單的 Nginx 服務。</p>
<h3 id="利用-ad-hoc-指令建立"><a href="#利用-ad-hoc-指令建立" class="headerlink" title="利用 ad-hoc 指令建立"></a>利用 ad-hoc 指令建立</h3><p>kubectl 提供了 run 指令來快速建立應用程式部署，如下建立 Nginx 應用程式：</p>
<pre><code class="sh">$ kubectl run nginx --image=nginx
$ kubectl get pods -o wide

NAME                    READY     STATUS    RESTARTS   AGE       IP            NODE
nginx-701339712-w5wlq   1/1       Running   0          26m       172.16.86.2   172.16.35.11
</code></pre>
<p>而當應用程式(deploy)被建立後，我們還需要透過 Kubernetes Service 來提供給外部網路存取應用程式，如下指令：</p>
<pre><code class="sh">$ kubectl expose deploy nginx --port 80 --type NodePort
$ kubectl get svc -o wide
</code></pre>
<p>完成後要接著建立 svc（Service）來提供外部網路存取應用程式，使用以下指令建立：</p>
<pre><code class="sh">$ kubectl expose rc nginx --port=80 --type=NodePort
$ kubectl get svc

NAME         CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes   192.168.3.1     &lt;none&gt;        443/TCP        37m
nginx        192.168.3.199   &lt;nodes&gt;       80:31764/TCP   30m
</code></pre>
<blockquote>
<p>這邊採用<code>NodePort</code>，即表示任何節點 IP 位址的<code>31764</code> Port 都會 Forward 到 Nginx container 的<code>80</code> Port。</p>
</blockquote>
<p>若想刪除應用程式與服務的話，可以透過以下指令：</p>
<pre><code class="sh">$ kubectl delete deploy nginx
$ kubectl delete svc nginx
</code></pre>
<h3 id="撰寫-YAML-檔案建立"><a href="#撰寫-YAML-檔案建立" class="headerlink" title="撰寫 YAML 檔案建立"></a>撰寫 YAML 檔案建立</h3><p>Kubernetes 支援了 JSON 與 YAML 來描述要部署的應用程式資訊，這邊撰寫<code>nginx-dp.yaml</code>來部署 Nginx 應用：</p>
<pre><code>apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
</code></pre><p>接著建立 Service 來提供存取服務，這邊撰寫<code>nginx-svc.yaml</code>來建立服務：</p>
<pre><code>apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 80
    protocol: TCP
    nodePort: 30000
  selector:
    app: nginx
</code></pre><p>然後透過 kubectl 指令來指定檔案建立：</p>
<pre><code class="sh">$ kubectl create -f nginx-dp.yaml
deployment &quot;nginx&quot; created

$ kubectl create -f nginx-svc.yaml
service &quot;nginx-service&quot; created
</code></pre>
<p>完成後，可以查看一下資訊：</p>
<pre><code class="sh">$ kubectl get svc,pods,rc -o wide

NAME                CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE       SELECTOR
svc/kubernetes      192.168.3.1     &lt;none&gt;        443/TCP        51m       &lt;none&gt;
svc/nginx-service   192.168.3.155   &lt;nodes&gt;       80:30000/TCP   1m        app=nginx

NAME                        READY     STATUS    RESTARTS   AGE       IP             NODE
po/nginx-4087004473-0wrbs   1/1       Running   0          2m        172.16.101.2   172.16.35.10
</code></pre>
<p>最後要刪除的話，直接將 create 改成使用<code>delete</code>即可：</p>
<pre><code class="sh">$ kubectl delete -f nginx-dp.yaml
$ kubectl delete -f nginx-svc.yaml
</code></pre>
<h2 id="其他-Kubernetes-網路技術"><a href="#其他-Kubernetes-網路技術" class="headerlink" title="其他 Kubernetes 網路技術"></a>其他 Kubernetes 網路技術</h2><p>Kubernetes 支援多種網路整合，若 Flannel 用不爽可以改以下幾種：</p>
<ul>
<li><a href="https://github.com/kubernetes/kubernetes/blob/master/docs/admin/ovs-networking.md" target="_blank" rel="noopener">OpenVSwitch with GRE/VxLAN</a></li>
<li><a href="http://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/" target="_blank" rel="noopener">Linux Bridge L2 networks</a></li>
<li><a href="https://github.com/zettio/weave" target="_blank" rel="noopener">Weave</a></li>
<li><a href="https://github.com/Metaswitch/calico" target="_blank" rel="noopener">Calico</a>(使用 BGP Routing)</li>
</ul>
]]></content>
      
        <categories>
            
            <category> Kubernetes </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Docker </tag>
            
            <tag> Kubernetes </tag>
            
            <tag> Ubuntu </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[hyperkube 建立多節點 Kubernetes(Unrecommended)]]></title>
      <url>https://kairen.github.io/2016/01/14/kubernetes/deploy/docker-multi/</url>
      <content type="html"><![CDATA[<p>本篇將說明如何透過 Docker 來部署一個多節點的 kubernetes 叢集。其架構圖如下所示：</p>
<p><img src="/images/kube/multinode-docker.png" alt=""></p>
<p>本環境安裝資訊：</p>
<ul>
<li>Kubernetes v1.5.5</li>
<li>Docker v17.03.0-ce</li>
</ul>
<a id="more"></a>
<h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本次安裝作業系統採用<code>Ubuntu 16.04 Server</code>，測試環境為 Vagrant with Libvirt 或 Vbox：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Role</th>
<th>CPU</th>
<th>Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.16.35.12</td>
<td>master1</td>
<td>2</td>
<td>4G</td>
</tr>
<tr>
<td>172.16.35.10</td>
<td>node1</td>
<td>2</td>
<td>4G</td>
</tr>
<tr>
<td>172.16.35.11</td>
<td>node2</td>
<td>2</td>
<td>4G</td>
</tr>
</tbody>
</table>
<blockquote>
<p>這邊 master 為主要控制節點，node 為應用程式工作節點。</p>
</blockquote>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>安裝前需要確認叢集滿足以下幾點：</p>
<ul>
<li>所有節點需要安裝<code>Docker</code>或<code>rtk</code>引擎。安裝方式為以下：</li>
</ul>
<pre><code class="sh">$ curl -fsSL &quot;https://get.docker.com/&quot; | sh
$ sudo iptables -P FORWARD ACCEPT
</code></pre>
<h2 id="Kubernetes-部署"><a href="#Kubernetes-部署" class="headerlink" title="Kubernetes 部署"></a>Kubernetes 部署</h2><p>這邊將分別部署 Master 與 Node(Worker) 節點。</p>
<h3 id="建立-Master-節點"><a href="#建立-Master-節點" class="headerlink" title="建立 Master 節點"></a>建立 Master 節點</h3><p>首先下載官方 Release 的原始碼程式：</p>
<pre><code class="sh">$ git clone &quot;https://github.com/kubernetes/kube-deploy&quot;
</code></pre>
<p>接著進入部署目錄來進行部署動作，Master 執行以下指令：</p>
<pre><code class="sh">$ export IP_ADDRESS=&quot;172.16.35.12&quot;
$ cd kube-deploy/docker-multinode
$ ./master.sh
...
Master done!
</code></pre>
<p>執行後，透過 Docker 指令查看是否成功：</p>
<pre><code class="sh">$ docker ps
CONTAINER ID        IMAGE                                                    COMMAND                  CREATED              STATUS              PORTS               NAMES
bfb6461499fb        gcr.io/google_containers/hyperkube-amd64:v1.5.5          &quot;/hyperkube kubele...&quot;   4 minutes ago        Up 4 minutes                            kubelet
...
</code></pre>
<blockquote>
<p>這邊會隨時間開啟其他 Component 的 Docker Container。</p>
</blockquote>
<p>確認完成後，就可以下載 kubectl 來透過 API 管理叢集：</p>
<pre><code class="sh">$ curl -O &quot;https://storage.googleapis.com/kubernetes-release/release/v1.5.5/bin/linux/amd64/kubectl&quot;
$ chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/
</code></pre>
<p>安裝好 kubectl 後就可以透過以下指令來查看資訊：</p>
<pre><code class="sh">$ kubectl get nodes
NAME           STATUS    AGE
172.16.35.12   Ready     11s
</code></pre>
<p>查看系統命名空間的 pod 與 svc 資訊：</p>
<pre><code class="sh">$ kubectl get po --all-namespaces
NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE
kube-system   k8s-proxy-v1-bfdml                      1/1       Running   0          1m
kube-system   kube-dns-4101612645-fb1rn               4/4       Running   0          1m
kube-system   kubernetes-dashboard-3543765157-999p2   1/1       Running   0          1m
</code></pre>
<h3 id="建立-Node-Worker-節點"><a href="#建立-Node-Worker-節點" class="headerlink" title="建立 Node(Worker) 節點"></a>建立 Node(Worker) 節點</h3><p>首先下載官方 Release 的原始碼程式：</p>
<pre><code class="sh">$ git clone &quot;https://github.com/kubernetes/kube-deploy&quot;
</code></pre>
<p>接著進入部署目錄來進行部署動作，Node 執行以下指令：</p>
<pre><code class="sh">$ export MASTER_IP=&quot;172.16.35.12&quot;; export IP_ADDRESS=&quot;172.16.35.11&quot;
$ cd kube-deploy/docker-multinode
$ ./worker.sh
...
+++ [0324 07:23:06] Done. After about a minute the node should be ready
</code></pre>
<h2 id="驗證安裝"><a href="#驗證安裝" class="headerlink" title="驗證安裝"></a>驗證安裝</h2><p>完成後可以查看所有節點狀態，執行以下指令：</p>
<pre><code class="sh">$ kubectl get nodes
NAME           STATUS    AGE
172.16.35.10   Ready     3m
172.16.35.11   Ready     4m
172.16.35.12   Ready     1m
</code></pre>
<p>接著我們透過部署簡單的 Nginx 應用程式來驗證系統是否正常：</p>
<pre><code class="sh">$ kubectl run nginx --image=nginx --port=80
deployment &quot;nginx&quot; created

$ kubectl expose deploy nginx --port=80
service &quot;nginx&quot; exposed
</code></pre>
<p>透過指令檢查 Pods：</p>
<pre><code class="sh">$ kubectl get po -o wide
NAME                     READY     STATUS    RESTARTS   AGE       IP         NODE
nginx-3449338310-ttqp2   1/1       Running   0          32s       10.1.1.2   172.16.35.11
</code></pre>
<p>透過指令檢查 Service：</p>
<pre><code class="sh">$ kubectl get svc -o wide
NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE       SELECTOR
kubernetes   10.0.0.1     &lt;none&gt;        443/TCP   47m       &lt;none&gt;
nginx        10.0.0.149   &lt;none&gt;        80/TCP    37s       run=nginx
</code></pre>
<p>取得應用程式的 Service ip，並存取服務：</p>
<pre><code class="sh">$ IP=$(kubectl get svc nginx --template={{.spec.clusterIP}})
$ curl ${IP}
</code></pre>
]]></content>
      
        <categories>
            
            <category> Kubernetes </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Docker </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[hyperkube 建立單機 Kubernetes(Unrecommended)]]></title>
      <url>https://kairen.github.io/2016/01/13/kubernetes/deploy/docker-singal/</url>
      <content type="html"><![CDATA[<p>本篇將說明如何透過 Docker 來部署一個單機的 kubernetes。其架構圖如下所示：</p>
<p><img src="/images/kube/singlenode-docker.png" alt=""></p>
<a id="more"></a>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>在開始安裝前，我們必須在部署的主機或虛擬機安裝與完成以下兩點：</p>
<ul>
<li>確認安裝 Docker Engine 於主機作業系統。</li>
</ul>
<pre><code class="sh">$ curl -fsSL &quot;https://get.docker.com/&quot; | sh
$ sudo iptables -P FORWARD ACCEPT
</code></pre>
<ul>
<li>定義要使用的 Kubernetes 版本，目前支援 1.2.0+ 版本。</li>
</ul>
<pre><code class="sh">$ export K8S_VERSION=&quot;1.5.4&quot;
</code></pre>
<h2 id="部署-Kuberentes-元件"><a href="#部署-Kuberentes-元件" class="headerlink" title="部署 Kuberentes 元件"></a>部署 Kuberentes 元件</h2><p>完成上述後，透過執行以下指令進行部署：</p>
<pre><code class="sh">$ sudo docker run -d \
--volume=/:/rootfs:ro \
--volume=/sys:/sys:ro \
--volume=/var/lib/docker/:/var/lib/docker:rw \
--volume=/var/lib/kubelet/:/var/lib/kubelet:rw \
--volume=/var/run:/var/run:rw \
--net=host \
--pid=host \
--privileged=true \
--name=kubelet \
gcr.io/google_containers/hyperkube-amd64:v${K8S_VERSION} \
/hyperkube kubelet \
--containerized \
--hostname-override=&quot;127.0.0.1&quot; \
--address=&quot;0.0.0.0&quot; \
--api-servers=&quot;http://localhost:8080&quot; \
--config=/etc/kubernetes/manifests \
--cluster-dns=10.0.0.10 \
--allow-privileged=true --v=2
</code></pre>
<p>執行後，透過 Docker 指令查看是否成功：</p>
<pre><code class="sh">$ docker ps
CONTAINER ID        IMAGE                                                    COMMAND                  CREATED              STATUS              PORTS               NAMES
bfb6461499fb        gcr.io/google_containers/hyperkube-amd64:v1.5.4          &quot;/hyperkube kubele...&quot;   4 minutes ago        Up 4 minutes                            kubelet
...
</code></pre>
<blockquote>
<p>這邊會隨時間開啟其他 Component 的 Docker Container。</p>
</blockquote>
<p>確認完成後，就可以下載 kubectl 來透過 API 管理叢集：</p>
<pre><code class="sh">$ curl -O &quot;https://storage.googleapis.com/kubernetes-release/release/v${K8S_VERSION}/bin/linux/amd64/kubectl&quot;
$ chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/
</code></pre>
<p>接著設定 kubectl config 來使用測試叢集：</p>
<pre><code class="sh">$ kubectl config set-cluster test-doc --server=http://localhost:8080
$ kubectl config set-context test-doc --cluster=test-doc
$ kubectl config use-context test-doc
</code></pre>
<h2 id="驗證安裝"><a href="#驗證安裝" class="headerlink" title="驗證安裝"></a>驗證安裝</h2><p>當完成所有步驟後，就可以檢查節點狀態：</p>
<pre><code class="sh">$ kubectl get nodes
NAME        STATUS    AGE
127.0.0.1   Ready     6m
</code></pre>
<p>查看系統命名空間的 pod 與 svc 資訊：</p>
<pre><code class="sh">$ kubectl get po --all-namespaces
kubectl get po --all-namespaces
NAMESPACE     NAME                                    READY     STATUS             RESTARTS   AGE
kube-system   k8s-etcd-127.0.0.1                      1/1       Running            0          15m
kube-system   k8s-master-127.0.0.1                    4/4       Running            2          15m
kube-system   k8s-proxy-127.0.0.1                     1/1       Running            0          15m
kube-system   kube-addon-manager-127.0.0.1            2/2       Running            0          15m
</code></pre>
<p>接著我們透過部署簡單的 Nginx 應用程式來驗證系統是否正常：</p>
<pre><code class="sh">$ kubectl run nginx --image=nginx --port=80
deployment &quot;nginx&quot; created

$ kubectl expose deploy nginx --port=80
service &quot;nginx&quot; exposed
</code></pre>
<p>透過指令檢查 Pods：</p>
<pre><code class="sh">$ kubectl get po -o wide
NAME                    READY     STATUS    RESTARTS   AGE       NODE
nginx-198147104-u9lt6   1/1       Running   0          3m        127.0.0.1
</code></pre>
<p>透過指令檢查 Service：</p>
<pre><code class="sh">$ kubectl get svc -o wide
NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE       SELECTOR
kubernetes   10.0.0.1     &lt;none&gt;        443/TCP   11m       &lt;none&gt;
nginx        10.0.0.133   &lt;none&gt;        80/TCP    3m        run=nginx
</code></pre>
<p>取得應用程式的 Service ip，並存取服務：</p>
<pre><code class="sh">$ IP=$(kubectl get svc nginx --template={{.spec.clusterIP}})
$ curl ${IP}
</code></pre>
]]></content>
      
        <categories>
            
            <category> Kubernetes </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Docker </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[學習 Docker Network 之間的差別]]></title>
      <url>https://kairen.github.io/2016/01/05/container/docker-network/</url>
      <content type="html"><![CDATA[<p>Docker 的網路是透過 Linux 的網路命名空間與虛擬網路裝置(Veth pair)實現而成。然而 Docker 的網路支援了不同類型功能，每一種都有其用意，本篇將針對以下幾項 Docker network mode 進行實作與介紹：</p>
<ul>
<li>Bridge Mode (default)</li>
<li>Host Mode</li>
<li>None Mode</li>
<li>Container Mode</li>
</ul>
<a id="more"></a>
<h2 id="Bridge-Mode"><a href="#Bridge-Mode" class="headerlink" title="Bridge Mode"></a>Bridge Mode</h2><p>Bridge Mode 是 Docker 預設使用的 network mode 。若你在已安裝 Docker 的環境中使用 <code>ifconfig</code> 指令，你可以看到有一個名為 docker0 的 network interface：</p>
<center><img src="/images/docker/network/bridge-mode-host-interface.png" alt="bridge-mode-interface" title="bridge-mode-interface"></center>

<p>此時，你可以在這個 Docker 環境中執行一個 docker container ：</p>
<pre><code>$ docker run -ti busybox sh
</code></pre><p>一樣使用 <code>ifconfig</code> 指令，你也會看到一個 eth0 network interface：</p>
<center><img src="/images/docker/network/bridge-mode-container-interface.png" alt="bridge-mode-interface" title="bridge-mode-interface"></center>

<p>這就是為什麼在 host 上可以直接使用 <code>ping</code> 指令與 container 進行通訊，因為 veth796c087 讓 docker0(172.17.0.1) 與 eth0(172.17.0.2) 位於同一個區網。</p>
<center><img src="/images/docker/network/bridge-mode-bridge.png" alt="bridge-mode-bridge" title="bridge-mode-bridge"></center>

<p>我們用一張簡單的圖來表示這三者的關係：</p>
<center><img src="/images/docker/network/bridge-mode.png" alt="bridge-mode" title="bridge-mode"></center>

<p>由上圖你可以發現，Container 1 可透過 eth0 經過 veth4fd8759 與 docker0 進行溝通，Container 2 也是如此，且 Container 1 與 Container 2 也可以進行溝通。</p>
<h2 id="Host-Mode"><a href="#Host-Mode" class="headerlink" title="Host Mode"></a>Host Mode</h2><p>Host Mode 可以把他想像成建立一個與 Host 擁有同樣的 network interface 的 Container ，使用方式：</p>
<pre><code>$ docker run --net=host -ti busybox sh
</code></pre><h2 id="None-Mode"><a href="#None-Mode" class="headerlink" title="None Mode"></a>None Mode</h2><p>None Mode 是建置最簡潔的 Container ，也就是沒有任何 network interface 的 Container。使用方式是在建立 Container 的同時給與 <code>--net=none</code> 的參數：</p>
<pre><code>$ docker run --net=none -ti busybox sh
</code></pre><p>此時，你若使用 <code>ifconfig</code> 指令，會發現這個 Container 沒有任何 network interface。</p>
<center><img src="/images/docker/network/none-mode.png" alt="none-mode" title="none-mode"></center>

<h2 id="Container-Mode"><a href="#Container-Mode" class="headerlink" title="Container Mode"></a>Container Mode</h2><p>首先，我們要先啟動一個 Container，並且使用這個 Container 的 Container ID 建立另外一個 Container：</p>
<center><img src="/images/docker/network/container-list.png" alt="container-list" title="container-list"></center>

<p>建置第二個 Container 的方式，將第一個 Container ID 當參數進行建置：</p>
<pre><code>docker run -ti --net=container:d16d87a29be3 busybox sh
</code></pre><p>此時，你會發現兩個 Container 的 IP 都是 172.17.0.2 ，雖然他們是不同的 Container 但是被放置同一個 Namespace 內，三者的關係如下：</p>
<center><img src="/images/docker/network/container-mode.png" alt="container-mode" title="none-mode"></center>
]]></content>
      
        <categories>
            
            <category> Container </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux Container </tag>
            
            <tag> Docker </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[用 ELK 做監控系統]]></title>
      <url>https://kairen.github.io/2016/01/03/devops/elk/</url>
      <content type="html"><![CDATA[<p>ELK 是由三個套件的開頭英文組成的縮寫，其 E 表示<code>Elasticsearch</code>，L 表示<code>Logstash</code>，K 表示<code>Kibana</code>，作為收集資料、資料索引以及資料視覺化的工具集合，以下分別簡單介紹三個套件。</p>
<h2 id="Logstash"><a href="#Logstash" class="headerlink" title="Logstash"></a>Logstash</h2><p>Logstash 可以簡單、有效、快速的處理Log資料，不過Logstash的主要功能是處理時間類型的Log，也就是在Log檔中有時間戳記（TimeStamp）的資料，而分析Log資料主要就是分析事件發生的時間和內容</p>
<h3 id="Logstash-Forwarder"><a href="#Logstash-Forwarder" class="headerlink" title="Logstash Forwarder"></a>Logstash Forwarder</h3><p>可傳送所收集到的 Log 訊息到 Logstash。</p>
<h2 id="Elasticsearch"><a href="#Elasticsearch" class="headerlink" title="Elasticsearch"></a>Elasticsearch</h2><p>Elasticsearch 是一個開源的資料搜尋分析系統，它可以解決現在 Web 去做資料庫的搜尋的種種問題，嚴格來說也不只是 web，(有可能是為了撈資料的效能，或是 schema free,  real-time 等等)。</p>
<h2 id="Kibana"><a href="#Kibana" class="headerlink" title="Kibana"></a>Kibana</h2><p>Kibana 是一個開源和免費的工具，他可以幫助您匯總、分析和搜索重要數據日志並提供友好的web界面</p>
<h2 id="系統與安裝版本"><a href="#系統與安裝版本" class="headerlink" title="系統與安裝版本"></a>系統與安裝版本</h2><ul>
<li>OS: Ubuntu 14.04</li>
<li>Elasticsearch 1.4.4</li>
<li>Logstash 1.5.0</li>
<li>Kibana 4</li>
</ul>
<h2 id="進行安裝"><a href="#進行安裝" class="headerlink" title="進行安裝"></a>進行安裝</h2><h3 id="首先安裝-Java-Oracle"><a href="#首先安裝-Java-Oracle" class="headerlink" title="首先安裝 Java Oracle"></a>首先安裝 Java Oracle</h3><pre><code>sudo apt-get purge openjdk*
sudo apt-get -y autoremove
sudo add-apt-repository -y ppa:webupd8team/java
sudo apt-get update
echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections
echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections
sudo apt-get -y install oracle-java7-installer
</code></pre><h3 id="安裝-Elasticsearch"><a href="#安裝-Elasticsearch" class="headerlink" title="安裝 Elasticsearch"></a>安裝 Elasticsearch</h3><p>匯入 Elasticsearch public GPG key 到 apt</p>
<pre><code>wget -O - http://packages.elasticsearch.org/GPG-KEY-elasticsearch | sudo apt-key add -
</code></pre><p>建立 Elasticsearch source list：</p>
<pre><code>echo &#39;deb http://packages.elasticsearch.org/elasticsearch/1.4/debian stable main&#39; | sudo tee /etc/apt/sources.list.d/elasticsearch.list
</code></pre><p>更新套件：</p>
<pre><code>sudo apt-get update
</code></pre><p>安裝 elasticsearch 1.4.4：</p>
<pre><code>sudo apt-get -y install elasticsearch=1.4.4
</code></pre><p>安裝完成，開啟配置檔：</p>
<pre><code>sudo vi /etc/elasticsearch/elasticsearch.yml
</code></pre><p>如果想限制給外界存取 Elasticsearch，可找到  <code>network.host</code> ，將內容取代成”localhost”，如下：</p>
<pre><code>network.host: localhost
</code></pre><p>開啟 Elasticsearch：</p>
<pre><code>sudo service elasticsearch restart
</code></pre><p>重開機立即啟動 Elasticsearch ：</p>
<pre><code>sudo update-rc.d elasticsearch defaults 95 10
</code></pre><h3 id="安裝-Kibana"><a href="#安裝-Kibana" class="headerlink" title="安裝 Kibana"></a>安裝 Kibana</h3><p>下載 Kibana 4 到 opt 資料夾</p>
<pre><code>cd /opt
</code></pre><p>使用<code>wget</code>下載 Kibana 套件壓縮檔：</p>
<pre><code>wget https://download.elasticsearch.org/kibana/kibana/kibana-4.0.1-linux-x64.tar.gz
</code></pre><p>解壓縮檔案：</p>
<pre><code>tar xvf kibana-*.tar.gz
</code></pre><p>開啟 Kibana 配置檔：</p>
<pre><code>vi ~/kibana-4*/config/kibana.yml
</code></pre><p>配置檔中找到<code>host</code>將 IP address “0.0.0.0” 取代成 “localhost”，此設定讓 Kibana 只能被 localhost 存取，如下：</p>
<pre><code>host: &quot;localhost&quot;
</code></pre><p>將下載完的 kibana 資料夾名稱改成 kibana：</p>
<pre><code>sudo mv kibana-4.0.1-linux-x64 kibana
</code></pre><p>Kibana 執行 <code>/opt/kibana/bin/kibana</code> 來開啟，但我們想用 service 的方式開啟。</p>
<p>下載 Kibana 4 init 腳本:</p>
<pre><code>cd /etc/init.d &amp;&amp; sudo wget https://gist.githubusercontent.com/thisismitch/8b15ac909aed214ad04a/raw/bce61d85643c2dcdfbc2728c55a41dab444dca20/kibana4
</code></pre><p>開啟 Kibana service：</p>
<pre><code>sudo chmod +x /etc/init.d/kibana4
sudo update-rc.d kibana4 defaults 96 9
sudo service kibana4 start
</code></pre><h3 id="安裝-Logstash"><a href="#安裝-Logstash" class="headerlink" title="安裝 Logstash"></a>安裝 Logstash</h3><p>建立 Logstash source list：</p>
<pre><code>echo &#39;deb http://packages.elasticsearch.org/logstash/1.5/debian stable main&#39; | sudo tee /etc/apt/sources.list.d/logstash.list
</code></pre><p>更新套件：</p>
<pre><code>sudo apt-get update
</code></pre><p>安裝 Logstash：</p>
<pre><code>sudo apt-get install logstash
</code></pre><h3 id="產生-SSL-認證"><a href="#產生-SSL-認證" class="headerlink" title="產生 SSL 認證"></a>產生 SSL 認證</h3><p>因為我們將使用 Logstash Forwarder 收集 logs並傳送到 Logstash Server ，所以我們必須建立一對SSL 認證的 key：</p>
<pre><code>sudo mkdir -p /etc/pki/tls/certs
sudo mkdir /etc/pki/tls/private
</code></pre><p>設定 openssl 配置：</p>
<pre><code>sudo vi /etc/ssl/openssl.cnf
</code></pre><p>配置檔中找到 <code>[ v3_ca ]</code> ，並新增以下內容：</p>
<pre><code>subjectAltName = IP:logstash_server_private_ip
</code></pre><p>產生 SSL 認證和 private key 到 <code>/etc/pki/tls/</code> ，如下：</p>
<pre><code>cd /etc/pki/tls
</code></pre><p>設定 SSL 驗證：</p>
<pre><code>sudo openssl req -config /etc/ssl/openssl.cnf -x509 -days 3650 -batch -nodes -newkey rsa:2048 -keyout private/logstash-forwarder.key -out certs/logstash-forwarder.crt
</code></pre><h3 id="配置-Logstash"><a href="#配置-Logstash" class="headerlink" title="配置 Logstash"></a>配置 Logstash</h3><p>新增配置檔 <code>01-lumberjack-input.conf</code>：</p>
<pre><code>sudo vi /etc/logstash/conf.d/01-lumberjack-input.conf
</code></pre><p>新增以下配置內容：</p>
<pre><code>input {
  lumberjack {
    port =&gt; 5000
    type =&gt; &quot;logs&quot;
    ssl_certificate =&gt; &quot;/etc/pki/tls/certs/logstash-forwarder.crt&quot;
    ssl_key =&gt; &quot;/etc/pki/tls/private/logstash-forwarder.key&quot;
  }
}
</code></pre><p>新增配置檔 <code>10-syslog.conf</code>：</p>
<pre><code>sudo vi /etc/logstash/conf.d/10-syslog.conf
</code></pre><p>新增以下配置內容：</p>
<pre><code>filter {
  if [type] == &quot;syslog&quot; {
    grok {
      match =&gt; { &quot;message&quot; =&gt; &quot;%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}&quot; }
      add_field =&gt; [ &quot;received_at&quot;, &quot;%{@timestamp}&quot; ]
      add_field =&gt; [ &quot;received_from&quot;, &quot;%{host}&quot; ]
    }
    syslog_pri { }
    date {
      match =&gt; [ &quot;syslog_timestamp&quot;, &quot;MMM  d HH:mm:ss&quot;, &quot;MMM dd HH:mm:ss&quot; ]
    }
  }
}
</code></pre><p>新增配置檔 <code>30-lumberjack-output.conf</code>：</p>
<pre><code>sudo vi /etc/logstash/conf.d/30-lumberjack-output.conf
</code></pre><p>新增以下配置內容：</p>
<pre><code>output {
  elasticsearch { host =&gt; localhost }
  stdout { codec =&gt; rubydebug }
}
</code></pre><p>重啟 Logstash：</p>
<pre><code>sudo service logstash restart
</code></pre><p>完成後就可以設置<code>Logstash Forwarder</code>（簡單說就是加入 Client）。</p>
<h4 id="複製-SSL-Certificate-與-Logstash-Forwarder-套件-On-Logstash-Server"><a href="#複製-SSL-Certificate-與-Logstash-Forwarder-套件-On-Logstash-Server" class="headerlink" title="複製 SSL Certificate 與 Logstash Forwarder 套件 (On Logstash Server)"></a>複製 SSL Certificate 與 Logstash Forwarder 套件 (On Logstash Server)</h4><pre><code>scp /etc/pki/tls/certs/logstash-forwarder.crt user@client_server_private_address:/tmp
</code></pre><h4 id="安裝-Logstash-Forwarder-套件-On-Client"><a href="#安裝-Logstash-Forwarder-套件-On-Client" class="headerlink" title="安裝 Logstash Forwarder 套件 (On Client)"></a>安裝 Logstash Forwarder 套件 (On Client)</h4><p>Logstash Forwarder source list：</p>
<pre><code>echo &#39;deb http://packages.elasticsearch.org/logstashforwarder/debian stable main&#39; | sudo tee /etc/apt/sources.list.d/logstashforwarder.list
</code></pre><p>一樣可使用Elasticsearch的 GPG key 來安裝：</p>
<pre><code>wget -O - http://packages.elasticsearch.org/GPG-KEY-elasticsearch | sudo apt-key add -
</code></pre><p>安裝 Logstash Forwarder package：</p>
<pre><code>sudo apt-get update
sudo apt-get install logstash-forwarder
</code></pre><p>複製 Logstash server’s SSL認證到 <code>/etc/pki/tls/certs</code>：</p>
<pre><code>sudo mkdir -p /etc/pki/tls/certs
sudo cp /tmp/logstash-forwarder.crt /etc/pki/tls/certs/
</code></pre><h4 id="配置-Logstash-Forwarder"><a href="#配置-Logstash-Forwarder" class="headerlink" title="配置 Logstash Forwarder"></a>配置 Logstash Forwarder</h4><p>設定Logstash Forwarder 配置檔(<code>On Client Server</code>)：</p>
<pre><code>sudo vi /etc/logstash-forwarder.conf
</code></pre><p>配置檔中找到 <code>network</code> ，底下加入以下內容：</p>
<pre><code>&quot;servers&quot;: [ &quot;logstash_server_private_address:5000&quot; ],
    &quot;timeout&quot;: 15,
    ssl ca&quot;: &quot;/etc/pki/tls/certs/logstash-forwarder.crt&quot;
</code></pre><p>配置檔中找到 <code>files</code> ，底下加入以下內容：</p>
<pre><code>   {
      &quot;paths&quot;: [
        &quot;/var/log/syslog&quot;,
        &quot;/var/log/auth.log&quot;
       ],
      &quot;fields&quot;: { &quot;type&quot;: &quot;syslog&quot; }
    }
</code></pre><p>重啟 Logstash Forwarder：</p>
<pre><code>sudo service logstash-forwarder restart
</code></pre><p>完成後，即可開啟瀏覽器，網址列輸入<a href="locahost:5601" target="_blank" rel="noopener">locahost:5601</a>。</p>
]]></content>
      
        <categories>
            
            <category> DevOps </category>
            
        </categories>
        
        
        <tags>
            
            <tag> DevOps </tag>
            
            <tag> Monitoring </tag>
            
            <tag> Visualization </tag>
            
            <tag> Logging </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[自己建立 Docker Registry]]></title>
      <url>https://kairen.github.io/2016/01/02/container/docker-registry/</url>
      <content type="html"><![CDATA[<p>Docker Registry 是被用來儲存 Docker 所建立的映像檔的地方，我們可以把自己建立的映像檔透過上傳到 Registries 來分享給其他人。Registries 也被分為了公有與私有，一般公有的 Registries 是 <a href="https://hub.docker.com/" target="_blank" rel="noopener">Docker Hub</a>、<a href="https://quay.io/" target="_blank" rel="noopener">QUAY</a> 與 <a href="https://console.cloud.google.com/gcr/images/google-containers/GLOBAL" target="_blank" rel="noopener">GCP registry</a>，提供了所有基礎的映像檔與全球使用者上傳的映像檔。私人的則是企業或者個人環境建置的，可參考 <a href="https://docs.docker.com/registry/deploying/" target="_blank" rel="noopener">Deploying a registry server</a>。</p>
<a id="more"></a>
<h2 id="預先準備資訊"><a href="#預先準備資訊" class="headerlink" title="預先準備資訊"></a>預先準備資訊</h2><p>本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統可採用<code>Ubuntu 16.x</code>與<code>CentOS 7.x</code>：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Role</th>
<th>CPU</th>
<th>Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.16.35.13</td>
<td>docker-registry</td>
<td>1</td>
<td>2G</td>
</tr>
</tbody>
</table>
<h3 id="安裝"><a href="#安裝" class="headerlink" title="安裝"></a>安裝</h3><p>首先進入到<code>docker-registry</code>節點，安裝 Docker engine：</p>
<pre><code class="sh">$ curl -fsSL &quot;https://get.docker.com/&quot; | sh
</code></pre>
<p>完成安裝後，接著透過以下指令建立一個 Docker registry 容器：</p>
<pre><code class="sh">$ docker run -d -p 5000:5000 --restart=always --name registry \
-v $(pwd)/data:/var/lib/registry \
registry:2
</code></pre>
<blockquote>
<p>-v 為 host 與 container 要進行同步的目錄，主要存放 docker images 資料</p>
</blockquote>
<p>接著為了方便檢視 Docker image，這邊另外部署 Docker registry UI：</p>
<pre><code class="sh">$ docker run -d -p 5001:80 \
-e ENV_DOCKER_REGISTRY_HOST=172.16.35.13 \
-e ENV_DOCKER_REGISTRY_PORT=5000 \
konradkleine/docker-registry-frontend:v2
</code></pre>
<p>完成後就可以透過瀏覽器進入 <a href="172.16.35.13:5001" target="_blank" rel="noopener">Docker registry UI</a> 查看資訊。也可以透過以下指令檢查是否部署成功：</p>
<pre><code class="sh">$ docker pull ubuntu:14.04
$ docker tag ubuntu:14.04 localhost:5000/ubuntu:14.04
$ docker push localhost:5000/ubuntu:14.04

The push refers to a repository [localhost:5000/ubuntu]
447f88c8358f: Pushed
df9a135a6949: Pushed
...
</code></pre>
<blockquote>
<p>其他 Docker registry 列表：</p>
<ul>
<li><a href="https://github.com/SUSE/Portus" target="_blank" rel="noopener">Portus</a></li>
<li><a href="http://www.projectatomic.io/registry/" target="_blank" rel="noopener">Atomic Registry</a></li>
<li><a href="https://docs.rancher.com/os/configuration/private-registries/" target="_blank" rel="noopener">Private Registries in RancherOS</a></li>
<li><a href="https://github.com/vmware/harbor" target="_blank" rel="noopener">VMware Harbor</a></li>
</ul>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Container </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux Container </tag>
            
            <tag> Docker </tag>
            
            <tag> Docker registry </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Docker Machine Driver 使用]]></title>
      <url>https://kairen.github.io/2015/12/06/container/docker-machine-driver/</url>
      <content type="html"><![CDATA[<p>Docker machine 是 Docker 早期提供 Windows 與 Mac OS X 建立 Docker 環境的工具，其採用 VirtualBox 來提供一個 Container OS，再利用 Docker client 來進行操作。然而 Docker Machine 除了使用 VirtualBox 外，還可以使用<code>Generic Driver</code>與<code>OpenStack Driver</code>來建立雲端平台上的 Docker 環境。</p>
<a id="more"></a>
<h2 id="使用-Generic-Driver"><a href="#使用-Generic-Driver" class="headerlink" title="使用 Generic Driver"></a>使用 Generic Driver</h2><p>以下範例使用 Generic Driver 來連接一個已存在的 OpenStack instance：</p>
<pre><code class="sh">$ docker-machine create --driver generic \
--generic-ssh-user ubuntu \
--generic-ssh-key ~/.ssh/id_rsa \
--generic-ip-address 10.26.1.82 \
docker-engine-1
</code></pre>
<blockquote>
<ul>
<li><code>--generic-ssh-user</code>為要遠端的使用者。</li>
<li><code>--generic-ssh-key</code>為要使用的 SSH Key。</li>
<li><code>--generic-ip-address</code>為要使用的虛擬機 IP。</li>
</ul>
</blockquote>
<h2 id="使用-OpenStack-Driver"><a href="#使用-OpenStack-Driver" class="headerlink" title="使用 OpenStack Driver"></a>使用 OpenStack Driver</h2><p>Docker Machine 提供了使用者可以建立 Docker 容器於 OpenStack 虛擬機。以下範例是使用 OpenStack Driver 來連接 OpenStack 並建立虛擬機：</p>
<pre><code class="sh">$ docker-machine create --driver openstack \
--openstack-username &quot;&lt;KEYSTONE_USERNAME&gt;&quot; \
--openstack-password &quot;&lt;KEYSTONE_PASSWD&gt;&quot; \
--openstack-tenant-name &quot;&lt;KEYSTONE_PROJECT_NAME&gt;&quot; \
--openstack-auth-url &quot;&lt;KEYSTONE_URL&gt;&quot; \
--openstack-flavor-name &quot;m1.medium&quot; \
--openstack-image-name &quot;Ubuntu-14.04-Server-Cloud&quot; \
--openstack-net-name &quot;admin-net&quot; \
--openstack-floatingip-pool &quot;internal-net&quot; \
--openstack-ip-version 4 \
--openstack-ssh-user &quot;ubuntu&quot; \
--openstack-sec-groups &quot;ALL_PASS&quot; \
openstack-docker
</code></pre>
<blockquote>
<ul>
<li><code>--openstack-username</code>為 Keystone 使用者帳號。</li>
<li><code>--openstack-password</code>為 Keystone 使用者密碼。</li>
<li><code>--openstack-tenant-name</code>為要使用的 project 名稱。</li>
<li><code>--openstack-auth-url</code>為 Keystone URL。</li>
<li><code>--openstack-flavor-name</code>為要使用的 Flavor。</li>
<li><code>--openstack-image-name</code>為要使用的 Image 名稱。</li>
<li><code>--openstack-net-name</code>為要使用的私有網路名稱。</li>
<li><code>--openstack-floatingip-pool</code>為要使用的 Floating 網路名稱。</li>
<li><code>--openstack-ip-versionl</code>為要使用的網路版本。</li>
<li><code>--openstack-ssh-user</code>為要遠端的使用者名稱。</li>
<li><code>--openstack-sec-groups</code>為要使用的 Security Grous 名稱，可多個如以下<code>ALL_PASS, SSH</code>。</li>
</ul>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Container </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux Container </tag>
            
            <tag> Docker </tag>
            
            <tag> OpenStack </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Ubuntu Metal as a Service 安裝]]></title>
      <url>https://kairen.github.io/2015/12/04/linux/ubuntu/ubuntu-maas-install/</url>
      <content type="html"><![CDATA[<p>Ubuntu MAAS 提供了裸機服務，將雲端的語言帶到物理伺服器中，其功能可以讓企業一次大量部署伺服器硬體環境的作業系統與基本設定等功能。</p>
<a id="more"></a>
<h2 id="安裝與設定"><a href="#安裝與設定" class="headerlink" title="安裝與設定"></a>安裝與設定</h2><p>首先為了方便系統操作，將<code>sudo</code>設置為不需要密碼：</p>
<pre><code class="sh">$ echo &quot;ubuntu ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/ubuntu &amp;&amp; sudo chmod 440 /etc/sudoers.d/ubuntu
</code></pre>
<blockquote>
<p><code>ubuntu</code> 為 User Name。</p>
</blockquote>
<p>安裝相關套件：</p>
<pre><code class="sh">$ sudo apt-get install software-properties-common
</code></pre>
<p>新增 MAAS 與 JuJu 的資源庫：</p>
<pre><code class="sh">sudo add-apt-repository -y ppa:juju/stable
sudo add-apt-repository -y ppa:maas/stable
sudo add-apt-repository -y ppa:cloud-installer/stable
sudo apt-get update
</code></pre>
<h3 id="安裝-MAAS"><a href="#安裝-MAAS" class="headerlink" title="安裝 MAAS"></a>安裝 MAAS</h3><p>接著安裝 MAAS：</p>
<pre><code class="sh">$ sudo apt-get install -y maas
</code></pre>
<p>安裝完成後，建立一個 admin 使用者帳號：</p>
<pre><code class="sh">$ sudo maas-region createadmin
</code></pre>
<blockquote>
<p>這時候就可以登入 <a href="http://&lt;maas.ip&gt;/MAAS/" target="_blank" rel="noopener">MAAS</a>。</p>
</blockquote>
<p>登入後，需要設定 PXE 網路與下載 Image。當完成後就可以開其他主機來測試 PXE-boot。</p>
<h3 id="安裝-JuJu"><a href="#安裝-JuJu" class="headerlink" title="安裝 JuJu"></a>安裝 JuJu</h3><p>安裝 JuJu quick start：</p>
<pre><code class="sh">$ sudo apt-get update &amp;&amp; sudo apt-get install juju-quickstart
</code></pre>
<p>完成後，透過以下指令來部署 JuJu GUI：</p>
<pre><code class="sh">$ juju-quickstart --gui-port 4242
</code></pre>
<h3 id="上傳客製化-Images"><a href="#上傳客製化-Images" class="headerlink" title="上傳客製化 Images"></a>上傳客製化 Images</h3><p>首先安裝 Maas Image Builder 來建立映像檔：</p>
<pre><code class="sh">$ sudo add-apt-repository -y ppa:blake-rouse/mib-daily
$ sudo apt-get install maas-image-builder
</code></pre>
<p>安裝完成後，我們可以使用以下指令建立映像檔，如以下為建立一個<code>CentOS 7</code>的映像檔：</p>
<pre><code class="sh">$ sudo maas-image-builder -a amd64 -o centos7-amd64-root-tgz centos --edition 7
</code></pre>
<blockquote>
<p>目前 CentOS 支援了<code>6.5</code>與<code>7.0</code>版本。</p>
</blockquote>
<p>當映像檔建立完成後，就可以上傳到 MAAS 了，在這之前需要登入 MAAS，才有權限上傳：</p>
<pre><code class="sh">$ maas login &lt;user_name&gt; &lt;maas_url&gt; &lt;api_key&gt;
</code></pre>
<blockquote>
<p><code>user_name&gt;</code>為使用者帳號名稱。<br><code>&lt;maas_url&gt;</code>為 MAAS 網址，如 <a href="http://192.168.1.2/MAAS" target="_blank" rel="noopener">http://192.168.1.2/MAAS</a><br><code>&lt;api_key&gt;</code>為帳號 API Key。r00tme</p>
</blockquote>
<p>登入後，就可以使用以下指令進行上傳客製化的映像檔了：</p>
<pre><code class="sh">$ maas &lt;user_name&gt; boot-resources create name=centos/centos7 architecture=amd64/generic content@=centos7-amd64-root-tgz
</code></pre>
<blockquote>
<p>登入時，要輸入指令需加<code>&lt;user_name&gt;</code>。</p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux </tag>
            
            <tag> PXE </tag>
            
            <tag> Bare Metal </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[NFS 簡單安裝與使用]]></title>
      <url>https://kairen.github.io/2015/11/24/linux/ubuntu/nfs/</url>
      <content type="html"><![CDATA[<p>網路檔案系統(Network FileSystem，NFS)是早期由 SUN 公司所開發出來的<code>分散式檔案系統</code>協定。主要透過 RPC Service 使檔案能夠共享於網路中，NFS 的好處是它支援了不同系統與機器的溝通能力，使資料能夠很輕易透過網路共享給別人。</p>
<a id="more"></a>
<h2 id="安裝與設定"><a href="#安裝與設定" class="headerlink" title="安裝與設定"></a>安裝與設定</h2><p>首先在 NFS 節點安裝以下套件：</p>
<pre><code class="sh">$ sudo apt-get -y install nfs-kernel-server
</code></pre>
<p>編輯<code>/etc/idmapd.conf</code>設定檔，然後設定 Domain：</p>
<pre><code>Domain = kyle.bai.example
</code></pre><p>接著編輯<code>/etc/exports</code>檔案，加入以下內容：</p>
<pre><code class="sh">/var/nfs/images 10.0.0.0/24(rw,sync,no_root_squash,no_subtree_check)
/var/nfs/vms 10.0.0.0/24(rw,sync,no_root_squash,no_subtree_check)
/var/nfs/volumes 10.0.0.0/24(rw,sync,no_root_squash,no_subtree_check)
</code></pre>
<p>然後重新啟動 NFS Server，如以下指令：</p>
<pre><code class="sh">$ sudo /etc/init.d/nfs-kernel-server restart
</code></pre>
<p>接著到 Client 端，安裝 NFS 工具：</p>
<pre><code class="sh">$ sudo apt-get -y install nfs-common
</code></pre>
<p>編輯<code>/etc/idmapd.conf</code>設定檔，然後設定 Domain：</p>
<pre><code>Domain = kyle.bai.example
</code></pre><p>然後透過以下指令來掛載使用：</p>
<pre><code class="sh">$ sudo mount -t nfs kyle.bai.example:/var/nfs/images /var/nfs/images
</code></pre>
<p>完成後，透過以下指令來檢查：</p>
<pre><code class="sh">$ df -hT
Filesystem                Type      Size  Used Avail Use% Mounted on
udev                      devtmpfs  7.9G  8.0K  7.9G   1% /dev
tmpfs                     tmpfs     1.6G  776K  1.6G   1% /run
/dev/sda1                 ext4      459G  8.3G  427G   2% /
none                      tmpfs     4.0K     0  4.0K   0% /sys/fs/cgroup
none                      tmpfs     5.0M     0  5.0M   0% /run/lock
none                      tmpfs     7.9G     0  7.9G   0% /run/shm
none                      tmpfs     100M     0  100M   0% /run/user
10.0.0.61:/var/nfs/images nfs4      230G  5.1G  213G   3% /var/nfs/images
</code></pre>
<p>編輯<code>/etc/fstab</code>檔案來提供開機掛載：</p>
<pre><code>10.0.0.61:/var/nfs/vms /var/lib/nova/instances nfs defaults 0 0
</code></pre><p>也可以安裝自動掛載工具，透過以下指令安裝：</p>
<pre><code class="sh">$ sudo apt-get -y install autofs
</code></pre>
<p>編輯<code>/etc/auto.master</code>檔案，加入以下內容到最後面：</p>
<pre><code>/-    /etc/auto.mount
</code></pre><p>然後編輯<code>/etc/auto.mount</code>檔案，設定以下內容：</p>
<pre><code class="sh"># create new : [mount point] [option] [location]
 /mntdir -fstype=nfs,rw  kyle.bai.example:/home
</code></pre>
<p>建立掛載用目錄：</p>
<pre><code class="sh">$ sudo mkdir /mntdir
</code></pre>
<p>啟動 auto-mount 服務：</p>
<pre><code class="sh">$ sudo initctl restart autofs
</code></pre>
<p>完成後透過以下方式檢查：</p>
<pre><code class="sh">$ cat /proc/mounts | grep mntdir
</code></pre>
<h2 id="Cinder-使用-NFS"><a href="#Cinder-使用-NFS" class="headerlink" title="Cinder 使用 NFS"></a>Cinder 使用 NFS</h2><p>OpenStack Cinder 也支援了 NFS 的驅動，因此只需要在<code>/etc/cinder/cinder.conf</code>設定以下即可：</p>
<pre><code>[DEFAULT]
...
enabled_backends = nfs

[nfs]
nfs_shares_config = /etc/cinder/nfs_shares
volume_driver = cinder.volume.drivers.nfs.NfsDriver
volume_backend_name = nfs-backend
nfs_sparsed_volumes = True
</code></pre><p>建立 Cinder backend 來提供不同的 Backend 的使用：</p>
<pre><code>$ cinder type-create TYPE
$ cinder type-key TYPE set volume_backend_name=BACKEND
</code></pre>]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Storage </tag>
            
            <tag> File System </tag>
            
            <tag> OpenStack </tag>
            
            <tag> Linux </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Ceph FS 基本操作]]></title>
      <url>https://kairen.github.io/2015/11/21/ceph/cephfs/</url>
      <content type="html"><![CDATA[<p>Ceph FS 底層的部分同樣是由 RADOS(OSDs + Monitors + MDSs) 提供，在上一層同樣與 librados 溝通，最上層則是有不同的 library 將其轉換成標準的 POSIX 檔案系統供使用。</p>
<center><img src="/images/ceph/cephfs.png" alt=""></center>

<a id="more"></a>
<h2 id="建立一個-Ceph-File-System"><a href="#建立一個-Ceph-File-System" class="headerlink" title="建立一個 Ceph File System"></a>建立一個 Ceph File System</h2><p>首先將一個叢集建立完成，並提供 Metadata Server Node 與 Client，建立 Client 可以透過以下指令：</p>
<pre><code class="sh">$ ceph-deploy install &lt;myceph-client&gt;
</code></pre>
<p>建立 MDS 節點可以透過以下指令：</p>
<pre><code class="sh">$ ceph-deploy mds create mds-node
</code></pre>
<p>當 Ceph 叢集已經提供了MDS後，可以建立 Data Pool 與 Metadata Pool：</p>
<pre><code class="sh">$ ceph osd pool create cephfs_data 128
$ ceph osd pool create cephfs_metadata 128
</code></pre>
<blockquote>
<p><strong>How to judge PG number</strong>：</p>
<ul>
<li>Less than 5 OSDs set pg_num to 128</li>
<li>Between 5 and 10 OSDs set pg_num to 512</li>
<li>Between 10 and 50 OSDs set pg_num to 4096</li>
<li>If you have more than 50 OSDs, you need to understand the tradeoffs and how to calculate the pg_num value by yourself</li>
</ul>
</blockquote>
<p>完成 Pool 建立後，我們將儲存池拿來給 File System 使用，並建立檔案系統：</p>
<pre><code class="sh">$ ceph fs new cephfs cephfs_metadata cephfs_data
</code></pre>
<p>取得 Client 驗證金鑰：</p>
<pre><code class="sh">$ cat /etc/ceph/ceph.client.admin.keyring
[client.admin]
    key = AQC/mo9VxqsXDBAAQ/LQtTmR+GTPs65KBsEPrw==
</code></pre>
<p>建立，並儲存到檔案<code>admin.secret</code>：</p>
<pre><code class="sh">AQC/mo9VxqsXDBAAQ/LQtTmR+GTPs65KBsEPrw==
</code></pre>
<p>檢查 MDS 與 FS：</p>
<pre><code class="sh">$ ceph fs ls
$ ceph mds stat
</code></pre>
<p>建立 Mount 用目錄，並且 Mount File System：</p>
<pre><code class="sh">$ sudo mkdir /mnt/mycephfs
$ sudo mount -t ceph {ip-address-of-monitor}:6789:/ /mnt/mycephfs/ -o name=admin,secretfile=admin.secret
</code></pre>
<p>檢查系統 DF 與 Mount 結果：</p>
<pre><code class="sh">$ sudo df -l
$ sudo mount
</code></pre>
<blockquote>
<p>使用CEPH檔案系統時，要注意是否安裝了元資料伺服器(Metadata Server)。且請確認CEPH版本為是<code>0.84</code>之後的版本。</p>
</blockquote>
<h2 id="Ceph-Filesystem-FUSE-File-System-in-User-Space"><a href="#Ceph-Filesystem-FUSE-File-System-in-User-Space" class="headerlink" title="Ceph Filesystem FUSE (File System in User Space)"></a>Ceph Filesystem FUSE (File System in User Space)</h2><p>首先在MDS節點上安裝ceph-fuse 套件：</p>
<pre><code class="sh">$ sudo apt-get install -y ceph-fuse
</code></pre>
<p>完成後，我們就可以Mount起來使用：</p>
<pre><code class="sh">$ sudo mkdir /mnt/myceph-fuse
$ sudo ceph-fuse -m {ip-address-of-monitor}:6789 /mnt/myceph-fuse
</code></pre>
<p>當 Mount 成功後，就可以到該目錄檢查檔案。</p>
<blockquote>
<p><strong>FUSE</strong>：使用者空間檔案系統（Filesystem in Userspace，簡稱FUSE）是作業系統中的概念，指完全在使用者態實作的檔案系統。目前Linux通過內核模組對此進行支援。一些檔案系統如ZFS，glusterfs和lustre使用FUSE實作。</p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Ceph </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> Storage </tag>
            
            <tag> File System </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[使用 ceph-deploy 工具部署 Ceph 叢集]]></title>
      <url>https://kairen.github.io/2015/11/20/ceph/deploy/ceph-deploy/</url>
      <content type="html"><![CDATA[<p>本節將介紹如何透過 <a href="https://github.com/ceph/ceph-deploy" target="_blank" rel="noopener">ceph-deploy</a> 工具安裝一個測試的 Ceph 環境，一個最簡單的 Ceph 儲存叢集至少要一台<code>Monitor</code>與三台<code>OSD</code>。而 MDS 則是當使用到 CephFS 的時候才需要部署。</p>
<a id="more"></a>
<h2 id="環境準備"><a href="#環境準備" class="headerlink" title="環境準備"></a>環境準備</h2><p>在開始部署 Ceph 叢集之前，我們需要在每個節點做一些基本的準備，來確保叢集安裝的過程是流暢的，本次安裝會擁有 6 台節點，叢集拓樸圖如下所示：</p>
<pre><code class="sh">                           +------------------+
                           | [ Deploy  Node ] |
                           |   10.21.20.99    |
                           | Ceph deploy tool |
                           +--------+---------+
                                    |
                                    |           
     +------------------+           |           +-----------------+
     |  [ Admin Node ]  |           |           |[ Monitor  Node ]|
     |   10.21.20.100   |-----------+-----------|   10.21.20.101  |
     |  Ceph admin ops  |           |           |  Ceph mon Node  |
     +------------------+           |           +-----------------+
                                    |
        +---------------------------+--------------------------+
        |                           |                          |
        |                           |                          |
+-------+----------+       +--------+---------+       +--------+---------+
|  [ OSD Node 1 ]  |       |  [ OSD Node 2 ]  |       |  [ OSD Node 3 ]  |
|   10.21.20.121   +-------+   10.21.20.122   +-------+   10.21.20.123   |
|  Object Storage  |       |  Object Storage  |       |  Object Storage  |
|   Disk  *  2     |       |   Disk  *  2     |       |   Disk  *  2     |
+------------------+       +------------------+       +------------------+
</code></pre>
<blockquote>
<p>P.S. 上面磁碟分為兩個，因為這邊教學不將 journal 分開來，故一顆當作系統使用，一顆為資料儲存與 journal 使用。<br>P.S. 這邊網路建議設定為<code>static</code>，若有支援 Jumbo frame 也可以開啟。</p>
</blockquote>
<p>首先在每一台節點新增以下內容到<code>/etc/hosts</code>：</p>
<pre><code>127.0.0.1 localhost

10.21.20.99 ceph-deploy
10.21.20.100 ceph-admin
10.21.20.101 ceph-mon1
10.21.20.121 ceph-osd1
10.21.20.122 ceph-osd2
10.21.20.123 ceph-osd3
</code></pre><p>然後在每台節點執行以下指令來使<code>sudo</code>不需要輸入密碼：</p>
<pre><code class="sh">$ echo &quot;ubuntu ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/ubuntu &amp;&amp; sudo chmod 440 /etc/sudoers.d/ubuntu
</code></pre>
<blockquote>
<p>上面 <code>ubuntu</code> 是節點的使用者名稱，這邊都是一樣。<br>P.S. 當然若要注意安全考量，而不讓該使用者直接使用有權限的資源，可以使用 root user。</p>
</blockquote>
<p>接著在設定<code>Deploy</code>節點能夠以無密碼方式進行 SSH 登入其他節點，請依照以下執行：</p>
<pre><code class="sh">$ ssh-keygen -t rsa
$ ssh-copy-id ceph-mon1
$ ssh-copy-id ceph-mds
...
</code></pre>
<blockquote>
<p>若 Deploy 節點的使用者與其他不同的話，編輯<code>~/.ssh/config</code>加入以下內容：</p>
<pre><code class="sh">Host ceph-admin
    Hostname ceph-admin
    User ubuntu
Host ceph-mds
    Hostname ceph-mds
    User mds
...
</code></pre>
</blockquote>
<p>之後在<code>Deploy</code>節點安裝部署工具，首先安裝基本相依套件，使用<code>apt-get</code>來進行安裝，再透過<code>python-pip</code>進行安裝部署工具：</p>
<pre><code class="sh">$ sudo apt-get install -y python-pip
$ sudo pip install ceph-deploy
</code></pre>
<blockquote>
<p>P.S. ceph-deploy 所安裝的 ceph 版本，會受到 ceph-deply 工具版本不同而有所差異。</p>
</blockquote>
<p>完成後即可開始部署 Ceph 環境。</p>
<h3 id="環境部署"><a href="#環境部署" class="headerlink" title="環境部署"></a>環境部署</h3><p>首先建立一個名稱為<code>mycluster</code>的目錄，並進入該目錄：</p>
<pre><code>$ sudo mkdir mycluster
$ cd mycluster
</code></pre><p>採用 ceph-deploy 工具部署環境時，需要依照以下步驟進行，首先建立要當任 Monitor 的節點，透過以下方式：</p>
<pre><code class="sh">$ ceph-deploy new ceph-mon1 &lt;other_nodes&gt;
</code></pre>
<blockquote>
<p>當執行該指令時，不是直接讓 ceph-mon1 節點安裝成為 Monitor，而只是新增一個<code>conf</code>，並標示誰是 Monitor，當在初始化階段時，才將該設定檔給對應節點，讓它啟動是設定為 Monitor 角色。</p>
</blockquote>
<p>接著我們需要先讓每個節點（ceph-deploy除外）安裝 ceph-common 套件，透過以下方式安裝：</p>
<pre><code class="sh">$ ceph-deploy install ceph-admin ceph-mds &lt;other_nodes&gt;
</code></pre>
<p>當完成安裝後，才能開始真正的部署節點的角色，第一先將 Monitor 都完成部署，才能讓叢集先正常被運作，透過以下指令來將 Monitors 初始化：</p>
<pre><code class="sh">$ ceph-deploy mon create-initial
</code></pre>
<p>上述沒有問題後，就可以開始部署實際作為儲存的 OSD 節點，我們可以透過以下指令進行：</p>
<pre><code class="sh">$ ceph-deploy osd prepare ceph-osd1:/dev/sdb &lt;other_nodes&gt;:&lt;data_disk&gt;
</code></pre>
<blockquote>
<p>若要將 journal 分離，可以使用以下方式：</p>
<pre><code class="sh">$ ceph-deploy osd prepare ceph-osd1:/dev/sdb:/dev/sdc &lt;other_nodes&gt;:&lt;data_disk&gt;:&lt;journal_disk&gt;
</code></pre>
</blockquote>
<p>部署沒有問題的話，即可啟用該 OSD：</p>
<pre><code class="sh">$ ceph-deploy osd activate ceph-osd1:/dev/sdb &lt;other_nodes&gt;:&lt;data_disk&gt;
</code></pre>
<blockquote>
<p>P.S. 較新的版本該指令可以省略，因為在準備期間就會幫你直接啟動。</p>
</blockquote>
<p>這樣一個簡單的叢集就部署完成了，這時候我們可以隨需求加入<code>admin</code>與<code>MDS</code>節點，可以透過以下方式進行：</p>
<pre><code class="sh">$ ceph-deploy admin ceph-admin
$ ceph-deploy mds create ceph-mds
</code></pre>
<p>完成後，可以透過以下指令檢查 ceph 叢集狀態：</p>
<pre><code class="sh">$ ceph health
HEALTH_OK

$ ceph status
cluster e2432059-e219-4555-8d37-c32d5b16e4a4
 health HEALTH_OK
 monmap e1: 1 mons at {ceph-mon1=10.21.20.101:6789/0}
        election epoch 6, quorum 0, ceph-mon1
 osdmap e119: 3 osds: 3 up, 3 in
        flags sortbitwise
  pgmap v813: 128 pgs, 2 pools, 91289 kB data, 22856 objects
        691 MB used, 2152 GB / 2152 GB avail
             128 active+clean
</code></pre>
<blockquote>
<ul>
<li>如果出現<code>ERROR: missing keyring, cannot use cephx for authentication</code>，請注意這個檔案<code>/etc/ceph/ceph.client.admin.keyring</code>是否有權限讀取。</li>
<li>如果出現<code>too few PGs per</code>，修改<code>pg_num</code>與<code>pgp_num</code>。範例如下：</li>
</ul>
<pre><code>$ ceph osd pool set rbd pg_num 128
$ ceph osd pool set rbd pgp_num 128
</code></pre></blockquote>
<p>若要檢查 mds 可以使用以下指令：</p>
<pre><code class="sh">$ ceph mds stat
</code></pre>
<p>若想檢查 OSDs 的目前狀態可以使用以下幾個指令：</p>
<pre><code class="sh">$ ceph osd stat
osdmap e119: 3 osds: 3 up, 3 in
       flags sortbitwise

$ ceph osd dump
$ ceph osd tree
</code></pre>
<p>最後如果進行多台 Monitor 的部署的話，要注意讓這些節點的時間同步。Ceph 使用多 Monitort 來避免單點故障問題，部署的比例可自行定義，比如 1 台、3:1 台、5:3 台等等。</p>
]]></content>
      
        <categories>
            
            <category> Ceph </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> Storage </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Ceph 分散式儲存系統介紹]]></title>
      <url>https://kairen.github.io/2015/11/19/ceph/ceph-intro/</url>
      <content type="html"><![CDATA[<p>Ceph 提供了<code>Ceph 物件儲存</code>以及<code>Ceph 區塊儲存</code>，除此之外 Ceph 也提供了自身的<code>Ceph 檔案系統</code>，所有的 Ceph 儲存叢集的部署都開始於 Ceph 各節點，透過網路與 Ceph 的叢集溝通。最簡單的 Ceph 儲存叢集至少要建立一個 Monitor 與兩個 OSD（Object storage daemon），但是當需要運行 Ceph 檔案系統時，就需要再加入Metadata伺服器。</p>
<center><img src="/images/ceph/ceph.jpeg" alt="Ceph"></center>

<a id="more"></a>
<h2 id="Ceph-目標"><a href="#Ceph-目標" class="headerlink" title="Ceph 目標"></a>Ceph 目標</h2><p>開發檔案系統是一種複雜的投入，但是如果能夠準確地解決問題的話，則擁有著不可估量的價值。我們可以把 Ceph 的目標可以簡單定義為以下：</p>
<ul>
<li><strong>容易擴充到 <a href="http://en.wikipedia.org/wiki/Petabyte" target="_blank" rel="noopener">PB 級別</a>的儲存容量</strong></li>
<li><strong>在不同負載情況下的高效能（每秒輸入/輸出操作數[IPOS]、帶寬）</strong></li>
<li><strong>高可靠性</strong></li>
</ul>
<p>但這些目標彼此間相互矛盾(例如:可擴充性會減少或阻礙效能，或影響可靠性)。 Ceph 開發了一些有趣的概念(例如<strong>動態 metadata 分區</strong>、<strong>資料分散</strong>、<strong>複製</strong>)。</p>
<p>Ceph 的設計也實作了<code>容錯性</code>來防止<code>單一節點故障問題（SOPF）</code>，並假設，大規模（PB 級）中儲存的故障是一種常態，而非異常。最後，它的設計沒有假設特定的工作負荷，而是包含了可變的分散式工作負荷的適應能力，從而提供最佳的效能。它以<a href="http://zh.wikipedia.org/wiki/POSIX" target="_blank" rel="noopener">POSIX</a> 兼容為目標完成這些工作，允許它透明的部署於那些依賴於 POSIX 語義上現有的應用(通過Ceph增強功能)。最後，Ceph 是開源分散式儲存和 Linux 主流核心的一部分。</p>
<center><img src="/images/ceph/stack.png" alt="完整架構"></center>

<p>其中最底層的 RADOS 是由 OSD、Monitor 與 MDS 三種所組成。</p>
<h2 id="Ceph-架構"><a href="#Ceph-架構" class="headerlink" title="Ceph 架構"></a>Ceph 架構</h2><p>現在，讓我們先在上層探討 Ceph 架構及其核心元素。之後深入到其它層次，來解析 Ceph 的一些主要方面，從而進行更詳細的分析。</p>
<p>Ceph 生態系統可以大致劃分為四部分（圖1）：</p>
<ul>
<li><strong>客戶端</strong> (資料使用者)</li>
<li><strong>metadata 伺服器</strong> (快取及同步分散的metadata)</li>
<li><strong>物件儲存叢集</strong> (以物件方式儲存資料與 metadata，實現其它主要職責)</li>
<li><strong>叢集監控</strong> (實現監控功能)</li>
</ul>
<center><img src="/images/ceph/part.png" alt="part"></center>

<p>如圖所示，客戶使用 metadata 伺服器，執行 metadata 操作(來確定資料位置)。metadata 伺服器管理資料位置，以及在何處儲存取新資料。值得注意的是，metadata 儲存在一個儲存叢集上（標記為 “metadata I/O”）。實際的檔案 I/O 發生在客戶和物件儲存叢集之間。這樣一來，提供了更高層次的 POSIX 功能(例如，開啟、關閉、重新命名)就由 metadata 伺服器管理，不過 POSIX 功能（例如讀和寫）則直接由物件儲存叢集管理。</p>
<center><img src="/images/ceph/figure2.gif" alt="figure2.gif"></center>

<p>上面分層視圖說明了，一系列伺服器透過一個客戶端介面存取 Ceph 生態圈系統，這就明白了 metadata 伺服器和物件級儲存之間的關係。分散式儲存系統可以在一些層面中查看，包括一個儲存設備的格式（Extent and B-tree-based Object File System [EBOFS]或者一個備選），還有一個設計用於管理資料複製、故障檢測、復原，以及隨後資料遷移的覆蓋管理層，叫做<code>Reliable Autonomic Distributed Object Storage（RADOS）</code>。最後，監視器用於區別元件中的故障，包括隨後的通知。</p>
<h2 id="Ceph-元件成員"><a href="#Ceph-元件成員" class="headerlink" title="Ceph 元件成員"></a>Ceph 元件成員</h2><h3 id="簡單的-Ceph-生態系統"><a href="#簡單的-Ceph-生態系統" class="headerlink" title="簡單的 Ceph 生態系統"></a>簡單的 Ceph 生態系統</h3><p>了解 Ceph 概念架構後，我們來探討另一個層次，了解在 Ceph 中實現的主要元件。 Ceph 和傳統檔案系統之間的重要差異之一，就是智能部分都用在了生態環境，而不是檔案系統本身。</p>
<center><img src="/images/ceph/architecture.png" alt="architecture"></center>

<p>圖中顯示了一個簡單的Ceph生態系統。Ceph Client 是 Ceph 檔案系統的用戶。Ceph Metadata Daemon 提供了 metadata 伺服器，而 Ceph Object Storage Daemon 提供了實際儲存（對資料和 metadata 兩者）。最後 Ceph Monitor 提供了叢集管理。要注意的是 Ceph 客戶，物件儲存端點，metadata 伺服器（根據檔案系統的容量）可以有許多，而且至少有一對冗餘的監視器。那麼這個檔案系統是如何分散的呢？</p>
<h3 id="Ceph-Client"><a href="#Ceph-Client" class="headerlink" title="Ceph Client"></a>Ceph Client</h3><p>因為Linux顯示檔案系統的一個共有介面（通過虛擬檔案系統交換機[VFS]），Ceph 的用戶透視圖就是透明的。然而管理者的透視圖肯定是不同的，考慮到很多伺服器會包含儲存系統這一潛在因素（要查看更多建立的 Ceph 叢集的資訊，可看參考資料部分）。從用戶的角度來看，存取大容量的儲存系統，卻不知道下面聚合成一個大容量的<strong>儲存池</strong>的 metadata 伺服器、監視器、還有獨立的對象儲存裝置。用戶只是簡單地看到一個安裝點，在這點上可以執行標准檔案 I/O。</p>
<blockquote>
<p>P.S <strong>核心或使用者空間</strong> : 早期版本的 Ceph 利用在 User SpacE（FUSE）的 Filesystems，它把文件系統推入到用戶空間，還可以很大程度上簡化其開發。但是今天，Ceph 已經被集成到主線內核，使其更快速，因為用戶空間上下文交換機對文件系統 I/O 已經不再需要。</p>
</blockquote>
<h3 id="Ceph-Metadata-Server（MDS）"><a href="#Ceph-Metadata-Server（MDS）" class="headerlink" title="Ceph Metadata Server（MDS）"></a>Ceph Metadata Server（MDS）</h3><p>作為處理 Ceph File System 的 metadata 之用，若僅使用 Block or Object storage，就不會用到這部分功能。</p>
<center><img src="/images/ceph/metadata.png" alt="metadata"></center>

<blockquote>
<p><strong>P.S 若只使用 Ceph Object Storage 與 Ceph Block Device 的話，將不需要部署 MDS 節點。</strong></p>
</blockquote>
<h3 id="Ceph-Monitor（MON）"><a href="#Ceph-Monitor（MON）" class="headerlink" title="Ceph Monitor（MON）"></a>Ceph Monitor（MON）</h3><p>Ceph 包含實施叢集映射管理的監控者，但是故障管理的一些要素是在物件儲存本身執行的。當物件儲存裝置發生故障或者新裝置加入時，監控者就會檢測和維護一個有效的叢集映射。這個功能會按一種分散式的方法執行，這種方式中映射升級可以和當前的流量溝通。Ceph 使用<a href="http://zh.wikipedia.org/zh-tw/Paxos%E7%AE%97%E6%B3%95" target="_blank" rel="noopener">Paxos</a>，它是一系列分散式共識演算法。</p>
<p>在每一個 Ceph 儲存中，都有一到多個 Monitor 存在，為了有效的在分散式的環境下存取檔案，每一個 Ceph Monitor daemon 維護著很多份與 叢集相關的映射資料(map)，包含：</p>
<ul>
<li><strong>Monitor map</strong>：包含叢集的 fsid 、位置、名稱、IP 位址和 Port，也包括目前 epoch、此狀態圖何時建立與最近修改時間。</li>
<li><strong>OSD map</strong>：包含叢集 fsid 、此狀態圖何時建立、最近修改時間、儲存池（Pools）列表 、副本數量、放置群組（PG）數量、 OSD 列表與其狀態（如 up 、 in ）</li>
<li><strong>Placement Group map</strong>：包含放置群組版本、其時間戳記、最新的 OSD epoch、佔用率以及各放置群組詳細，如放置群組 ID 、 up set 、 acting set 、 PG 狀態（如 active+clean），和各儲存池的資料使用情況統計。</li>
<li><strong>CRUSH map</strong>： 包含儲存裝置列表、故障域樹狀結構（如裝置、主機、機架、row、機房等等），以及儲存資料時如何利用此樹狀結構的規則。</li>
<li><strong>MDS map</strong>：包含當前 MDS map 的 epoch、建立於何時與最近修改時間，還包含了儲存 metadata 的儲存池、metadata 伺服器列表、還有哪些 metadata 伺服器是 <code>up</code> 且<code>in</code> 的。</li>
</ul>
<p>主要是用來監控 Ceph 儲存叢集上的即時狀態，確保 Ceph 可以運作無誤。</p>
<h3 id="Ceph-Object-Storage-Daemon（OSD）"><a href="#Ceph-Object-Storage-Daemon（OSD）" class="headerlink" title="Ceph Object Storage Daemon（OSD）"></a>Ceph Object Storage Daemon（OSD）</h3><p>與傳統的物件儲存類似，Ceph 儲存節點不僅包括儲存，還包含了智能容錯與恢復。傳統的驅動是只響應來自啟動者下達的命令中簡單目標。但是物件儲存裝置是智能裝置，它能作為目標和啟動者，支持與其他物件儲存裝置的溝通與合作。</p>
<p>實際與 Client 溝通進行資料存取的即為 OSD。每個 object 被儲存到多個 PG（Placement Group）中，每個 PG 再被存放到多個 OSD 中。為了達到 Active + Clean 的狀態，確認每份資料都有兩份的儲存，每個 Ceph 儲存叢集中至少都必須要有兩個 OSD。</p>
<p>主要功能為實際資料（object）的儲存，處理資料複製、恢復、回填（backfilling, 通常發生於新 OSD 加入時）與重新平衡（發生於新 OSD 加入 CRUSH map 時），並向 Monitor 提供鄰近 OSD 的心跳檢查資訊。</p>
<center><img src="/images/ceph/osd.png" alt="osd"></center>

<h3 id="CRUSH"><a href="#CRUSH" class="headerlink" title="CRUSH"></a>CRUSH</h3><p>Ceph 透過 CRUSH（Controlled, Scalable, Decentralized Placement of Replicated Data） 演算法來計算資料儲存位置，來確認如何儲存和檢索資料，CRUSH 授權 Ceph client 可以直接連接 OSD，而不再是透過集中式伺服器或者中介者來儲存與讀取資料。該演算法與架構特性使 Ceph 可以避免單一節點故障問題、效能瓶頸與擴展的限制。</p>
<p>CRUSH 是一種偽隨機資料分散算法，能夠再階層結構的儲存叢集很好地分散物件的副本，該演算法實作了偽隨機(確定性)的函式，會透過輸入o​​bject id、object group id等參數，來回傳一組儲存裝置（即儲存物件的副本）。CRUSH 有以下兩個重要的資訊：</p>
<ul>
<li><strong>Cluster Map</strong>：被用來描述儲存叢集的階層結構。</li>
<li><strong>CRUSH Rule</strong>：被用來描述副本的分散策略。</li>
</ul>
<p>CRUSH 主要提供了以下功能：</p>
<ul>
<li>將資料有效率的存放在不同的儲存裝置中。</li>
<li>即使移除整個 cluster中的儲存裝置，也不會影響到資料正常的存取。</li>
<li>不需要有任何主要的管理裝置(or 節點)來做為控管之用。</li>
<li>可依照使用者所定義的規則來處理資料分散的方式。</li>
</ul>
<p>透過以上資訊，CRUSH 可以將資料分散存放在不同的儲存實體位置，並避免單點錯誤造成資料無法存取的狀況發生。</p>
<h3 id="Pools"><a href="#Pools" class="headerlink" title="Pools"></a>Pools</h3><p>Pool 是儲存物件邏輯分區。Ceph Client 從 Monitor 取得 Cluster map，並把物件寫入 Pool。Pool 的副本數、CRUSH Ruleset 和 PG 數量決定著 Ceph 該如何放置資料。</p>
<center><img src="/images/ceph/pool-flow.png" alt=""></center>

<p>一個 Pool 可以設定許多參數，但最少需要正確設定以下資訊：</p>
<ul>
<li>物件擁有權與存取權限</li>
<li>Placement Group 數量</li>
<li>CRUSH Ruleset 設定與使用</li>
</ul>
<h3 id="Placement-Group"><a href="#Placement-Group" class="headerlink" title="Placement Group"></a>Placement Group</h3><p>Ceph 把物件映射到放置群組（PG），PG 是一種邏輯的物件 Pool 片段，這些物件會組成一個群組後，再存到 OSD 中。PG 減少了各物件存入對應的 OSD 時 metadata 的數量，更多的 PG（如：每個OSD 有 100 個群組）可以使<code>負載平衡</code>更好。</p>
<center><img src="/images/ceph/pg.png" alt="pg"></center>

<blockquote>
<p><strong>P.S 多個 PG 也可以對應到同一個 OSD，因此 PG 與 OSD 其實是種多對多的關係。</strong></p>
</blockquote>
<h4 id="PG-ID-計算"><a href="#PG-ID-計算" class="headerlink" title="PG ID 計算"></a>PG ID 計算</h4><p>當 Client 在綁定某個 Monitor 時，會先取得最新的 Cluster map 副本，該 map 可讓 Client 端知道叢集有多少 Monitor、OSD 與 MDS。但是無法知道要存取的物件位置。</p>
<p>在 Ceph 中，物件的位置是透過計算得知的。因此 Client 只需要傳入 Object id 與 Pool 即可知道物件位置。當 Client 需要有名稱物件（如 mysql、archive 等）時，Ceph 會用物件名稱來計算 PG（是一個 Hash value）、 OSD 編號與 Pool。流程如下：</p>
<ul>
<li>Client 輸入 pool id 與 object id（e.g., pool=’vms’, object id=’instance-1’）</li>
<li>CRUSH 取得 object id，並進行 Hash 取得值</li>
<li>CRUSH 在以 OSD 數量對 Hash value 進行 mod 運算，來取得 pg id（e.g., 58）</li>
<li>CRUSH 再透過取得 pool name 來取得 pool id（e.g., ‘vms’ = ‘4’）<br>5* CRUSH 在把 pool id 加到 pg id 前面（e.g, 4.58）</li>
</ul>
<p>透過計算物件位置的方式，改善了傳統查詢定位的效能問題。CRUSH 演算法讓 Client 計算物件應該存放到哪裡，並連接該 Primary OSD 進行物件儲存與檢索。</p>
<h2 id="Ceph-三大儲存服務系統特性與功能"><a href="#Ceph-三大儲存服務系統特性與功能" class="headerlink" title="Ceph 三大儲存服務系統特性與功能"></a>Ceph 三大儲存服務系統特性與功能</h2><p>Ceph  是一個統一的儲存系統，提供了物件、區塊與檔案系統功能，且擁有高可靠、高擴展。該三大儲存服務功能與特性如下：</p>
<table>
<thead>
<tr>
<th>CEPH 物件儲存</th>
<th>CEPH 區塊裝置</th>
<th>CEPH 檔案系統</th>
</tr>
</thead>
<tbody>
<tr>
<td>RESTful 介面</td>
<td>精簡空間配置</td>
<td>與 POSIX 語意相容</td>
</tr>
<tr>
<td>與 S3 和 Swift 相容的 API</td>
<td>映像檔大小最大支援 16 EB（exabytes）</td>
<td>Metadata 與資料分離</td>
</tr>
<tr>
<td>S3 風格的子域名</td>
<td>可組態的等量化（Configurable striping）</td>
<td>動態重新平衡(Dynamic rebalancing)</td>
</tr>
<tr>
<td>統一的 S3/Swift 命名空間</td>
<td>記憶體快取</td>
<td>子目錄快照</td>
</tr>
<tr>
<td>使用者管理</td>
<td>快照</td>
<td>可組態的等量化（Configurable striping）</td>
</tr>
<tr>
<td>利用率追蹤</td>
<td>寫入時複製 cloning</td>
<td>核心驅動的支援</td>
</tr>
<tr>
<td>等量化物件（Striped objects）</td>
<td>支援 KVM 與 libvirt</td>
<td>支援使用者空間檔案系統（FUSE）</td>
</tr>
<tr>
<td>雲端解決方案整合</td>
<td>可作為雲端解決方案的後端</td>
<td>可作為 NFS/CIFS 部署</td>
</tr>
<tr>
<td>多站點部署</td>
<td>累積備份</td>
<td>可用於 Hadoop 上（替代 HDFS ）</td>
</tr>
<tr>
<td>災難復原</td>
</tr>
</tbody>
</table>
<h2 id="參考"><a href="#參考" class="headerlink" title="參考"></a>參考</h2><ul>
<li><a href="http://mirrors.myccdn.info/ceph/doc/docs_zh/output/html/" target="_blank" rel="noopener">Ceph中文文件</a></li>
<li><a href="http://godleon.blogspot.tw/2014/10/ceph.html" target="_blank" rel="noopener">小信豬 Ceph</a></li>
<li><a href="http://ceph.com/docs/master/" target="_blank" rel="noopener">Ceph 官方文件</a></li>
<li><a href="http://ceph.com/resources/development/" target="_blank" rel="noopener">Ceph Development</a></li>
<li><a href="http://cloud.51cto.com/art/201504/470780_all.htm" target="_blank" rel="noopener">Ceph 整合案例</a></li>
<li><a href="https://books.google.com.tw/books?hl=zh-TW&amp;lr&amp;id=HftzBgAAQBAJ&amp;oi=fnd&amp;pg=PP1&amp;dq=openstack%20ceph&amp;ots=OWwptmw5Mm&amp;sig=d1uC5NThemjRudfktDXmuQ6TqA0&amp;redir_esc=y#v=onepage&amp;q=openstack%20ceph&amp;f=false" target="_blank" rel="noopener">Ceph Book</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Ceph </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> Storage </tag>
            
            <tag> Distribution System </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Apache Cassandra 分散式資料庫]]></title>
      <url>https://kairen.github.io/2015/11/17/data-engineer/apache-cassandra/</url>
      <content type="html"><![CDATA[<p>Cassandra 是最初由 Facebook 開發，之後貢獻給 Apache 基金會維護的分散式 NoSQL 資料庫系統，一般被認為 Amazon Dyname 與 Google BigTable 的結合體，主要是分散性像 Dynamo，然而資料模型卻如 BigTable。目前有多家公司採用，可運行上千台節點來提供超過 200 TB 的資料。</p>
<p>Cassandra 擁有幾個特點，也因為這些特點讓許多人選擇使用該資料庫，以下幾個項目簡單列出其特點：</p>
<ul>
<li>完全去中心化，且不是主從架構的備份</li>
<li>統一類型的節點</li>
<li>以 P2P 協定串連起網路，清除 SPOF（Single Point Of Failure）問題</li>
<li>高擴展性，新增與刪除節點容易</li>
<li>可呼叫的一致性，並支援強一致性與弱一致性</li>
<li>支援跨區域的叢集架構</li>
<li>每個區域儲存一份完整資料，來提供存取局部性、容錯與災難復原</li>
<li><strong>寫入效能理論上比讀取好（但近期有證實現在讀取也很不錯），適合串流資料儲存</strong></li>
<li>比 HBase 的隨機存取效能要好上許多，但不擅長區間掃描</li>
<li>可作為 HBase 的即時查詢快取。</li>
</ul>
<a id="more"></a>
<center><img src="/images/spark/cassandra_arch.png" alt=""></center>

<p>若要瞭解更多 Cassandra 可以閱讀 <a href="http://wiki.apache.org/cassandra/GettingStarted" target="_blank" rel="noopener">Cassandra Wiki
</a>。</p>
<h2 id="部署多節點叢集"><a href="#部署多節點叢集" class="headerlink" title="部署多節點叢集"></a>部署多節點叢集</h2><p>本節將安裝一個簡單的 Cassandra 叢集，來提供 NoSQL 資料庫以存取資料，以下為節點配置：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>HostName</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.17.0.2</td>
<td>cassandra-1</td>
</tr>
<tr>
<td>172.17.0.3</td>
<td>cassandra-2</td>
</tr>
<tr>
<td>172.17.0.4</td>
<td>cassandra-3</td>
</tr>
</tbody>
</table>
<p>首先需在每個節點安裝 Java，這邊採用 Oracle 的 Java 來進行安裝：</p>
<pre><code>$ sudo apt-get install -y software-properties-common
$ sudo add-apt-repository -y ppa:webupd8team/java
$ sudo apt-get update
$ echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections
$ echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections
$ sudo apt-get -y install oracle-java7-installer
</code></pre><blockquote>
<p>若要安裝<code>java8</code>請修改成<code>oracle-java8-installer</code>。</p>
</blockquote>
<p>接著在各節點安裝 Cassandra 套件，這邊採用<code>apt-get</code>安裝，首先加入 Repository：</p>
<pre><code class="sh">$ echo &quot;deb http://www.apache.org/dist/cassandra/debian 22x main&quot; | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list
</code></pre>
<blockquote>
<p>這邊安裝<code>2.2.x</code>版本，若要其他版本則修改<code>22x</code>，如改為<code>21x</code>。</p>
</blockquote>
<p>為了避免軟體套件更新軟體時有簽證警告，需加入 Apache 基金會與套件資源庫相關的三個公有金鑰：</p>
<pre><code class="sh">$ gpg --keyserver pgp.mit.edu --recv-keys F758CE318D77295D
$ gpg --export --armor F758CE318D77295D | sudo apt-key add -
$ gpg --keyserver pgp.mit.edu --recv-keys 2B5C1B00
$ gpg --export --armor 2B5C1B00 | sudo apt-key add -
$ gpg --keyserver pgp.mit.edu --recv-keys 0353B12C
$ gpg --export --armor 0353B12C | sudo apt-key add -
</code></pre>
<p>完成後更新 apt-get Repository：</p>
<pre><code class="sh">$ sudo apt-get update
</code></pre>
<p>安裝 Cassandra NoSQL 於每個節點上：</p>
<pre><code>$ sudo apt-get install -y cassandra
</code></pre><p>完成後，我們必須開始配置各節點來組成一個叢集，先把每個節點的 Cassandra 服務關閉：</p>
<pre><code class="sh">$ sudo service cassandra stop
</code></pre>
<p>關閉後，在每個節點編輯<code>/etc/cassandra/cassandra.yaml</code>檔案，並修改一下內容：</p>
<pre><code class="sh">cluster_name: &#39;examples&#39;  
num_tokens: 256  
seed_provider:  
    - class_name: org.apache.cassandra.locator.SimpleSeedProvider
      parameters:
          - seeds: &quot;172.17.0.2, 172.17.0.3, 172.17.0.4&quot;

listen_address: 172.17.0.2  
broadcast_address: 172.17.0.2
rpc_address: 0.0.0.0  
broadcast_rpc_address: 172.17.0.2
</code></pre>
<blockquote>
<p>P.S. 這邊要注意不同節點會有不同，如<code>listen_address</code>與<code>seeds</code>等。</p>
</blockquote>
<p>都完成設定檔後，就可以重啟每台的 Cassandra 服務：</p>
<pre><code class="sh">$ sudo service cassandra restart
</code></pre>
<p>當確認所有節點重新啟動後，在<code>其中一個節點</code>建立 keyspaces 來做 replication：</p>
<pre><code class="sh">$ cqlsh
cqlsh&gt; create keyspace Spark_smack WITH REPLICATION = { &#39;class&#39; : &#39;SimpleStrategy&#39;, &#39;replication_factor&#39; : &#39;3&#39; };  
exit
</code></pre>
<blockquote>
<p><code>Spark_smack</code> 該值是可以依個人名稱修改。</p>
</blockquote>
<p>建立完成後，可以使用<code>nodetool</code>指令來檢查是否成功：</p>
<pre><code class="sh">$ nodetool status spark_smack

Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address     Load       Tokens       Owns (effective)  Host ID                               Rack
UN  172.17.0.3  255.68 KB  256          100.0%            9c9eb117-f787-47bd-825f-3daf49eba489  rack1
UN  172.17.0.2  252.03 KB  256          100.0%            7a4adb77-42d1-402f-b57b-4a40ad013e2c  rack1
UN  172.17.0.4  127.2 KB   256          100.0%            5823fd78-45f2-4328-9470-f1053bb3fc3b  rack1
</code></pre>
<h2 id="驗證系統"><a href="#驗證系統" class="headerlink" title="驗證系統"></a>驗證系統</h2><p>我們透過 Client 程式來連到<code>cassandra-1</code>節點來驗證叢集是否建立成功：</p>
<pre><code class="sh">$ cqlsh
cqlsh&gt; use &lt;keyspace name&gt;;
</code></pre>
<blockquote>
<p>這邊<code>&lt;keyspace name&gt;</code>範例為 spark_smack。</p>
</blockquote>
<p>然後建立一個資料表：</p>
<pre><code class="sql">CREATE TABLE emp(  
   emp_id int PRIMARY KEY,
   emp_name text,
   emp_city text,
   emp_sal varint,
   emp_phone varint
   );
</code></pre>
<p>接著插入一筆資料到資料表：</p>
<pre><code class="sql">INSERT INTO emp (emp_id, emp_name, emp_city )  
  VALUES(1, &#39;Kyle&#39;, &#39;Taichung&#39; );
</code></pre>
<p>若都沒有任何錯誤的話，現在連接到其他節點來檢查是否有同步資料：</p>
<pre><code class="sh">$ cqlsh
cqlsh&gt; use spark_smack;
cqlsh&gt; select * from emp;

 emp_id | emp_city | emp_name | emp_phone | emp_sal
--------+----------+----------+-----------+---------
      1 | Taichung |     Kyle |      null |    null
</code></pre>
<blockquote>
<p>P.S 若發生如以下錯誤，請依照指示解決：</p>
<pre><code class="sh">Connection error: (&#39;Unable to connect to any servers&#39;, {&#39;127.0.0.1&#39;: error(111, &quot;Tried connecting to [(&#39;127.0.0.1&#39;, 9042)]. Last error: Connection refused&quot;)})
</code></pre>
<p>首先檢查設定檔<code>/etc/cassandra/cassandra.yaml</code>裡面的<code>rpc_address</code>是否為<code>0.0.0.0</code>。</p>
</blockquote>
<p>最後，若要復原與備份可以閱讀 <a href="http://blog.powerupcloud.com/2015/09/13/cassandra-backup-and-recovery/" target="_blank" rel="noopener">Cassandra Backup and Recovery</a>。</p>
<h2 id="參考資源"><a href="#參考資源" class="headerlink" title="參考資源"></a>參考資源</h2><ul>
<li><a href="https://www.byvoid.com/blog/cassandra-intro" target="_blank" rel="noopener">Cassandra简介</a></li>
<li><a href="http://blog.powerupcloud.com/2016/01/10/install-and-configure-a-3-node-cassandra-cluster-on-ubuntu-14-04/" target="_blank" rel="noopener">Install and Configure a 3 node Cassandra Cluster on Ubuntu 14.04</a></li>
<li><a href="http://www.infoq.com/cn/articles/cassandra-mythology" target="_blank" rel="noopener">關於 Cassandra 的錯誤觀點</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Spark </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Database </tag>
            
            <tag> NoSQL </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Kuberentes 是什麼？]]></title>
      <url>https://kairen.github.io/2015/11/10/kubernetes/k8s-intro/</url>
      <content type="html"><![CDATA[<p>Kubernetes 是 Google 的開源專案，主要用於管理跨主機容器化叢集系統。該專案脫胎於 Google 內部叢集管理工具 Borg，早期主要貢獻者更是參與 Borg 專案的人員，大概基本上都認為 Kubernetes 許多概念與架構是來至 Google 十餘年的設計、部署、管理大規模容器的經驗。</p>
<p>Kubernetes 在 Docker 叢集管理中是很重要的一員，其實作了許多功能，包含應用程式部署、叢集節點擴展、自動容錯機制等，並且能夠統一管理跨主機的管理。其架構如下圖所示。</p>
<center><img src="/images/kube/kubernets-arch.png" alt=""></center>

<a id="more"></a>
<p>使用 Kubernetes 的好處，網路上已有許多相關資訊，這邊列舉幾個：</p>
<ul>
<li>輕易的 Scale out/in 容器</li>
<li>自動化容錯與擴展，如容器的部署與副本</li>
<li>以多個容器組成群組，並提供容器負載平衡</li>
<li>容易擴展節點</li>
<li>以叢集的方式來管理跨機器的容器。</li>
<li>基於 Docker 的應用程式封裝、實例化與執行。</li>
</ul>
<p>兩種節點角色：</p>
<ul>
<li><p><strong>Node</strong>：是運行 k8s worker 的實體或者虛擬機器，一般稱為 Minion 節點。該節點會運作幾個 k8s 關鍵元件：</p>
<ul>
<li>kubelet：為 master 節點的 agent。</li>
<li>kube-proxy： Sevice 使用其將服務請求連接路由到 Pod 的容器中。</li>
<li>docker engine（或 rocket）：主要建立容器的引擎。</li>
</ul>
</li>
<li><p><strong>Master</strong>：每一個 k8s 叢集都會有一個或多個 Master，主要提供 REST APIs、管理 k8s 工具、運行 Etcd 、Scheduler 以及 Pod 的 Replication Controller 等服務。</p>
</li>
</ul>
<p>如架構圖所示，Kubernetes 由多個元件組合而成，然而在了解 Kubernetes 元件之前，我們需要先知道 Kubernetes 的生態圈中的一些重要概念，這些概念將影響對於 Kubernetes 進一步的探索，其主要概念如下：</p>
<ul>
<li><p><strong>Pods（po）</strong>：是 k8s 的最基本執行單元，即包含一組容器與 Volumes。在同一個 Pod 裡的容器會共享使用一個網路命名空間，並利用 localhost 溝通，Pod 生命週期是短暫的。</p>
</li>
<li><p><strong>Services（svc）</strong>：由於 k8s 的 Pods 中的容器 IP 會隨著排程、故障等因素而改變，因此 k8s 利用 Service 來定義一系列存取這些 Pod 應用程式服務的抽象層。Service 透過 Proxy 的 port 與 Selector 來決定服務請求要傳送給後端哪些 Pods 中的容器，因此對外是單一的存取介面，。</p>
</li>
<li><p><strong>Replication Controllers（rc）</strong>：主要確保 k8s 叢集所指定 Pod 中的容器被正常運作的副本數量，當正在執行的容器發生故障，而少於副本數時，Replication Controller 會在任一節點啟動一個新的容器，然後若是多於副本數量則會自動移除一個容器。</p>
</li>
<li><p><strong>Lable</strong>：被用來區分 Pod、Service 與 Replication Controllers 的 Key/Vaule 標籤，用來傳遞使用者定義的屬性。Label 是 rc 與 svc 的執行基礎，rc 透過標示容器 label 來讓 svc 服務請求正確選擇要存取容器。</p>
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> Kubernetes </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux Container </tag>
            
            <tag> Docker </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Docker 基礎介紹]]></title>
      <url>https://kairen.github.io/2015/11/09/container/docker/</url>
      <content type="html"><![CDATA[<p><a href="http://www.docker.com/" target="_blank" rel="noopener">Docker</a> 是一個開源的專案，主要提供容器化應用程式的部署與自動化的管理，透過部署 Docker engine 於 Linux 作業系統上，提供軟體抽象層以及作業系統層虛擬化自動管理機制。Docker 使用一些 Linux Kernel 中的軟體，如 Control Groups（cgroups）、namespaces（核心命名空間）、Another UnionFS 等來達到建置獨立的環境與存取 CPU、Memory 以及網路資源等，最終提供一個輕量級的虛擬化。Docker 的發展非常迅速，從 0.9.0 版本後，採用自己開發的 Libcontainer 函式庫作為直接存取由 Linux Kernel 提供的虛擬化基礎設施。Docker 也支援了多平台，從個人電腦到私有、公有雲都能夠進行使用與部署輕量級虛擬化。</p>
<blockquote>
<p>Docker 並非一個新興技術，而是過往的 Linux Container 技術的基礎上進行封裝。因此使用者只需要關注應用程式如何被部署與執行就好，而不用管容器是如何被建置出來。</p>
</blockquote>
<center><img src="/images/docker/docker-logo.png" alt="Docker"></center>

<a id="more"></a>
<p>使用 Docker 有以下幾項好處：</p>
<ul>
<li>更快速的交付和部署</li>
<li>高效能的虛擬化環境</li>
<li>易遷移和擴展服務</li>
<li>管理簡單化</li>
<li>更有效的使用實體主機資源</li>
<li>可建置任何語言的 Dockerized 應用程式</li>
<li>跨平台的管理、部署與使用</li>
</ul>
<blockquote>
<p>微軟過往許多需要相依於 Windows 語言應用程式也慢慢被支援到 Docker 了。</p>
</blockquote>
<h2 id="Docker-Container-vs-傳統虛擬化"><a href="#Docker-Container-vs-傳統虛擬化" class="headerlink" title="Docker Container vs 傳統虛擬化"></a>Docker Container vs 傳統虛擬化</h2><p>傳統虛擬化技術一般是透過在 Host OS 上安裝 Hypervisor（VMM），然後由 Hypervisor 來管理不同虛擬主機，每個虛擬主機都需要安裝不同的作業系統，因此每個環境被獨立的完全隔離。</p>
<center><img src="/images/docker/vmm-layer.png" alt="虛擬化"></center>

<p>然而 Docker 提供應用程式在獨立的 Container 中執行，這些Container 並不需要像虛擬化一樣額外依附在 Hypervisor 甚至 Guest OS 上，而是透過 Docker engine 來進行管理。</p>
<center><img src="/images/docker/container-layer.png" alt="Docker Container"></center>

<h2 id="Docker-基本概念"><a href="#Docker-基本概念" class="headerlink" title="Docker 基本概念"></a>Docker 基本概念</h2><p>Docker 擁有幾個基本概念，其中包含 Docker 三大重要部分與元件。我們將針對這些概念進行概述，也會在其他章節進一步說明。</p>
<ul>
<li><strong>Docker images</strong>：Image（映像檔）被用來啟動容器的實際執行得應用程式環境。這概念類似 VM 的映像檔，VM 透過映像檔來啟動作業系統，並執行許多服務，但 Docker 的映像檔則只是檔案系統的儲存狀態，是一個唯讀的板模。</li>
<li><p><strong>Docker containers</strong>：Container（容器）是一個應用程式執行的實例，Docker 會提供獨立、安全的環境給應用程式執行，容器是從映像檔建立，並運作於主機上。</p>
<blockquote>
<p>P.S 盡量不要在一個 Container 執行過多的服務。</p>
</blockquote>
</li>
<li><p><strong>Docker registries</strong>：Registries 是被用來儲存 Docker 所建立的映像檔的地方，我們可以把自己建立的映像檔透過上傳到 Registries 來分享給其他人。Registries 也被分為了公有與私有，一般公有的 Registries 是 <a href="https://hub.docker.com/" target="_blank" rel="noopener">Docker Hub</a>，提供了所有基礎的映像檔與全球使用者上傳的映像檔。私人的則是企業或者個人環境建置的，可參考 <a href="https://docs.docker.com/registry/deploying/" target="_blank" rel="noopener">Deploying a registry server</a>。</p>
</li>
</ul>
<p>Docker 的推出與發展非常迅速，相關的部署工具與資源相繼出現，更因此讓原名為 dotcloud 變成 Docker, Inc。Docker 也在 2014 - 2015 年推出了以下三大工具：</p>
<ul>
<li><strong><a href="https://docs.docker.com/machine/overview/" target="_blank" rel="noopener">docker-machine</a></strong>：Docker machine 是可以透過指令來安裝 Docker engine 的工具。該工具可以讓使用者不需要學習一堆安裝指令來部署容器環境，目前已經支援了許多驅動程式，例如：OpenStack、Amazon EC2、Google Cloud Engine 與 Microsoft Azure等，更可以被用來建立混合環境。</li>
<li><strong><a href="https://docs.docker.com/compose/overview/" target="_blank" rel="noopener">docker-compose</a></strong>：Compose 是 Docker 的編配工具，可以用來建置 Swarm 上的多節點容器化叢集與單一節點的應用程式。Compose 的前身是 Fig，使用者可以透過定義 YAML 檔案來描述與維護所有應用程式服務定義與部署，如多個服務之間如何連接等，使用 Compose 部署的應用程式可以在不影響其他服務情況下自動更新。</li>
<li><strong><a href="https://docs.docker.com/swarm/overview/" target="_blank" rel="noopener">docker-swarm</a></strong>：Swarm 是 Docker 的原生叢集與調度工具，它基於應用程式生命週期、容器使用、效能需求自動優化分散式應用程式的基礎架構。且 Swarm 可以透過許多服務發現（Service Discovery）套件來打造 HA 的叢集，Swarm 也提供很高的靈活性，使應用程式可以簡單的分散部署在多主機環境上。</li>
</ul>
<h2 id="使用者操作-Docker-的方法"><a href="#使用者操作-Docker-的方法" class="headerlink" title="使用者操作 Docker 的方法"></a>使用者操作 Docker 的方法</h2><p>Docker 主機上會執行一個 Docker daemon，就能夠開啟許多 Container。如果要對 Docker 進行操作的話，可以使用 Docker client 軟體，如 docker client、docker-py、Kitematic，這些工具會分別採用以下兩種方式來對部署 Docker daemon 進行管理：</p>
<ol>
<li><a href="http://en.wikipedia.org/wiki/Unix_domain_socket" target="_blank" rel="noopener">UNIX Sockets</a></li>
<li><a href="http://en.wikipedia.org/wiki/Representational_state_transfer" target="_blank" rel="noopener">RESTful API</a></li>
</ol>
<p>溝通方式如下圖所示，其中 Docker daemon 可以同時安裝 Docker client 來直接進行 Docker 使用（一般安裝都會有），詳細資訊可以參閱 <a href="https://docs.docker.com/reference/api/docker_remote_api/" target="_blank" rel="noopener">Docker Remote API - Docker Documentation</a>。</p>
<center><img src="/images/docker/communication.png" alt="溝通方式"></center>

<p>最後有興趣看每週 Docker 的新聞可以訂閱 Docker Weekly，參閱   <a href="https://www.docker.com/newsletter-subscription" target="_blank" rel="noopener">Docker Newsletter</a></p>
<h2 id="參考資源"><a href="#參考資源" class="headerlink" title="參考資源"></a>參考資源</h2><ul>
<li><a href="https://www.docker.com/" target="_blank" rel="noopener">Docker 官方</a></li>
<li><a href="https://blog.docker.com/2016/03/swarmweek-swarm-maintainers/?mkt_tok=3RkMMJWWfF9wsRonuqTMZKXonjHpfsX54uQqUKO1lMI%2F0ER3fOvrPUfGjI4DS8piI%2BSLDwEYGJlv6SgFQ7LMMaZq1rgMXBk%3D" target="_blank" rel="noopener">Docker Blog</a></li>
<li><a href="http://www.infoq.com/cn/dockerdeep/" target="_blank" rel="noopener">InfoQ Docker 深入淺出</a></li>
<li><a href="https://www.gitbook.com/book/philipzheng/docker_practice/details" target="_blank" rel="noopener">Docker —— 從入門到實踐</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Container </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux Container </tag>
            
            <tag> Docker </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[簡單設定 Apache2 Proxy 與 VirtualHost]]></title>
      <url>https://kairen.github.io/2015/11/04/linux/ubuntu/apache2-proxy/</url>
      <content type="html"><![CDATA[<p>Apache2 是一套經過測試與用於生產環境的 HTTP 伺服器，在許多網頁伺服器中被廣泛的採用，Apache2 除了本身能力強大外，其也整合了許多的額外模組來提供更多的擴展功能。</p>
<a id="more"></a>
<h2 id="Apache2-安裝與設定"><a href="#Apache2-安裝與設定" class="headerlink" title="Apache2 安裝與設定"></a>Apache2 安裝與設定</h2><p>要安裝 Apache 伺服器很簡單，只需要透過 APT 進行安裝即可：</p>
<pre><code class="sh">$ sudo apt-get update
$ sudo apt-get install -y libapache2-mod-proxy-html libxml2-dev apache2 build-essential
</code></pre>
<h3 id="啟用-Proxy-Modules"><a href="#啟用-Proxy-Modules" class="headerlink" title="啟用 Proxy Modules"></a>啟用 Proxy Modules</h3><p>這邊可以透過以下指令來逐一啟動模組：</p>
<pre><code class="sh">a2enmod proxy
a2enmod proxy_http
a2enmod proxy_ajp
a2enmod rewrite
a2enmod deflate
a2enmod headers
a2enmod proxy_balancer
a2enmod proxy_connect
a2enmod proxy_html
</code></pre>
<h3 id="設定-Default-conf-來啟用"><a href="#設定-Default-conf-來啟用" class="headerlink" title="設定 Default conf 來啟用"></a>設定 Default conf 來啟用</h3><p>編輯<code>/etc/apache2/sites-available/000-default.conf</code>設定檔，加入 Proxy 與 VirtualHost 資訊：</p>
<pre><code class="sh"># 簡單 Proxypass 範例
&lt;VirtualHost *:80&gt;
        ErrorLog ${APACHE_LOG_DIR}/laravel-error.log
        CustomLog ${APACHE_LOG_DIR}/laravel-access.log combined
        ProxyPass / http://192.168.20.10/
        ProxyPassReverse / http://192.168.20.10/
        ProxyPreserveHost On
        ServerName laravel.kairen.com
        ServerAlias laravel.kairen.com
        ServerAlias *.laravel.kairen.com
&lt;/VirtualHost&gt;

# 簡單 Load balancer 範例
&lt;Proxy balancer://api-gateways&gt;
    # Server 1
    BalancerMember http://192.168.20.11:8080/
    # Server 2
    BalancerMember http://192.168.20.12:8080/
&lt;/Proxy&gt;

&lt;VirtualHost *:*&gt;
    ProxyPass / balancer://api-gateways
&lt;/VirtualHost&gt;
</code></pre>
<p>完成後重新啟動服務即可：</p>
<pre><code class="sh">$ sudo service apache2 restart
</code></pre>
<h3 id="使用-SSL-Reverse-Proxy"><a href="#使用-SSL-Reverse-Proxy" class="headerlink" title="使用 SSL Reverse-Proxy"></a>使用 SSL Reverse-Proxy</h3><p>如果需要設定 SSL 連線與認證的話，可以透過以下設定方式來提供：</p>
<pre><code>Listen 443
NameVirtualHost *:443
&lt;VirtualHost *:443&gt;
    SSLEngine On
    SSLCertificateFile /etc/apache2/ssl/file.pem
    ProxyPass / http://192.168.20.11:8080/
    ProxyPassReverse / http://192.168.20.11:8080/
&lt;/VirtualHost&gt;
</code></pre><p>完成後重新啟動服務即可：</p>
<pre><code class="sh">$ sudo service apache2 restart
</code></pre>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux </tag>
            
            <tag> HTTP Server </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Apache Kafka 叢集]]></title>
      <url>https://kairen.github.io/2015/10/13/data-engineer/kafka-install/</url>
      <content type="html"><![CDATA[<p>Apache Kafka 是一個分散式的訊息佇列框架，是由 LinkedIn 公司使用 Scala 語言開發的系統，被廣泛用來處理高吞吐量與容易水平擴展，目前許多巨量資料運算框架以都有整合 Kafka，諸如：Spark、Cloudera、Apache Storm等，</p>
<p>Kafka 是基於<code>Publish/Subscribe</code>的訊息系統，主要設計由以下特點：</p>
<ul>
<li>在 TB 級以上資料也能確保常數時間複雜度的存取效能，且時間複雜度為O(1)的訊息持久化。</li>
<li>高吞吐量，在低階的商業電腦上也能提供單機<code>100k/sec</code>條以上的訊息傳輸。</li>
<li>支援 Kafka Server 之間的訊息分區(Partition)以及分散式發送，並保證每個分區內的訊息循序傳輸。</li>
<li>同時支援離線資料處理與即時資料處理</li>
<li>容易的服務不中斷水平擴展。</li>
</ul>
<a id="more"></a>
<p>Kafka 從架構上來看，Kafka 會擁有以下幾個角色：</p>
<ul>
<li><strong>Producer</strong>：主要為 Publish 訊息到 Topic。</li>
<li><strong>Consumer</strong>：主要為 Subscribe Topic 來取得訊息。</li>
<li><strong>Broker</strong>：訊息的中介者，可看錯是一台訊息 Server，可部署單機至多台叢集。</li>
<li><strong>Topic</strong>：拿來做訊息的分類。</li>
<li><strong>Zookeeper</strong>：Zookeeper 不算是 Kafka 一員，但 Kafka 依賴 Zookeeper 來做到 Sync。</li>
</ul>
<center><img src="/images/spark/kafka.jpg" alt=""></center>

<p>Apache Kafka 的一個簡單應用架構可以參考下圖，透過 Spark Streaming 來進行串接做快速的串流資料收集，並利用 Spark 框架進行分析後取得結果存於 Cassandra 資料庫叢集，最後在由應用程式或前端網頁來顯示處理過的資料。</p>
<h2 id="安裝-Apache-Kafka"><a href="#安裝-Apache-Kafka" class="headerlink" title="安裝 Apache Kafka"></a>安裝 Apache Kafka</h2><p>一個簡單的節點配置如下：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Role</th>
<th>zookeeper id</th>
<th>broker.id</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.17.0.2</td>
<td>kafka-1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>172.17.0.3</td>
<td>kafka-2</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>172.17.0.4</td>
<td>kafka-3</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<p>首先要在每台節點安裝 Java，這邊採用 Oracle 的 Java 來進行安裝：</p>
<pre><code>$ sudo apt-get install -y software-properties-common
$ sudo add-apt-repository -y ppa:webupd8team/java
$ sudo apt-get update
$ echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections
$ echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections
$ sudo apt-get -y install oracle-java7-installer
</code></pre><p>完成後，接著在每台節點安裝 Zookeeper 服務，這邊採用<code>apt-get</code>來進行安裝：</p>
<pre><code class="sh">$ sudo apt-get install -y zookeeperd
</code></pre>
<blockquote>
<p>完成安裝後，zookeeper 會自動開啟服務於 port 2181。若沒啟動使用以下指令：</p>
<pre><code class="sh">$ sudo service zookeeper restart
</code></pre>
<p>若想部署多節點 Zookeeper，請修改每台節點的<code>/etc/zookeeper/conf/zoo.cfg</code>檔案，加入以下內容：(Option)</p>
<pre><code class="sh">server.1=172.17.0.2:2888:3888
server.2=172.17.0.3:2888:3888
server.3=172.17.0.4:2888:3888
</code></pre>
<p>並設定 ID，如下指令：</p>
<pre><code class="sh">$ echo &quot;1&quot; | sudo tee /etc/zookeeper/conf/myid
</code></pre>
</blockquote>
<p>測試 Zookeeper 是否啟動，可以透過 Telnet 來進行：</p>
<pre><code class="sh">$ telnet localhost 2181
</code></pre>
<p>當節點上述完成後就可以下載 Kafka 套件，並解壓縮到<code>/opt/</code>底下：</p>
<pre><code class="sh">$ sudo curl -s &quot;http://ftp.tc.edu.tw/pub/Apache/kafka/0.9.0.1/kafka_2.10-0.9.0.1.tgz&quot; | sudo tar -xz -C /opt/
$ sudo mv /opt/kafka_2.10-0.9.0.1 /opt/kafka
</code></pre>
<p>下載完成後，編輯<code>/opt/kafka/config/server.properties</code>，並加入以下內容：</p>
<pre><code class="sh">broker.id=0

host.name=172.17.0.2

zookeeper.connect=172.17.0.2:2181,172.17.0.3:2181,172.17.0.4:2181
</code></pre>
<blockquote>
<p>這邊的<code>broker.id</code>需跟著節點數變動，從 0  開始計數。若使用<code>OpenStack</code>或者<code>Docker</code>這些虛擬化的話，需在設定檔加入：</p>
<pre><code class="sh">advertised.host.name=&lt;Advertised IP&gt;
advertised.port=9092
</code></pre>
</blockquote>
<p>編輯完以後就分別啟動這三台 Broker：</p>
<pre><code class="sh">$ cd /opt/kafka
$ bin/kafka-server-start.sh config/server.properties &amp;
</code></pre>
<h2 id="驗證服務"><a href="#驗證服務" class="headerlink" title="驗證服務"></a>驗證服務</h2><p>當所有 Server 啟動完成後，就可以透過建立 Topic 來確認是否成功部署完成：</p>
<pre><code class="sh">$ /opt/kafka/bin/kafka-topics.sh --create \
--zookeeper localhost:2181 \
--replication-factor 3 \
--partitions 1 \
--topic test
</code></pre>
<blockquote>
<p>可以試著將<code>--replication-factor</code>改為 4，若成功會看到以下錯誤訊息：</p>
<pre><code>replication factor: 4 larger than available brokers: 3
</code></pre><p>原因是我們只有建立 3 台叢集。</p>
</blockquote>
<p>建立完成後，可以用以下指令查看：</p>
<pre><code class="sh">$ /opt/kafka/bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test

# 會看到類似以下資訊
Topic:test    PartitionCount:1    ReplicationFactor:3    Configs:
    Topic: test    Partition: 0    Leader: 2    Replicas: 2,0,1    Isr: 2,0,1
</code></pre>
<p>接下來透過 Publish 來傳送訊息：</p>
<pre><code class="sh">$ /opt/kafka/bin/kafka-console-producer.sh \
--broker-list localhost:9092 \
--topic test

# 輸入
ggeeder
ggeeder
</code></pre>
<p>接著就要讀取訊息，透過 Subscribe 來訂閱收取資料：</p>
<pre><code class="sh">$ /opt/kafka/bin/kafka-console-consumer.sh \
--zookeeper 172.17.0.2:2181,172.17.0.3:2181,172.17.0.4:2181 \
--topic test \
--from-beginning
</code></pre>
<p>這時透過手動方式關閉該 broker，來測試 replication 是否有正確運作：</p>
<pre><code class="sh">$ jps
83 Kafka

$ sudo kill -9 83
</code></pre>
<p>接著可以先去看該 Topic 的 Leader 是否有變化：</p>
<pre><code class="sh">$ /opt/kafka/bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test
</code></pre>
<p>此時看 Consume message，會發現訊息應該還是會保存的完整無缺：</p>
<pre><code class="sh">$ /opt/kafka/bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic test
</code></pre>
]]></content>
      
        <categories>
            
            <category> Spark </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Kafka </tag>
            
            <tag> Message Queue </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Spark on Mesos 多節點部署]]></title>
      <url>https://kairen.github.io/2015/10/12/data-engineer/spark-mesos/</url>
      <content type="html"><![CDATA[<p>Spark + Mesos 叢集是由多個主節點與工作節點組合而成，它實作了兩層的排程（Scheduler）來提供粗/細粒度的排程。在 Mesos 中主節點（Master）主要負責資料的分配與排程，然而從節點（Slave）則是主要執行任務負載的角色。Mesos 也提供了高可靠的部署模式，可利用多個主節點的 ZooKeeper 來做服務發現。</p>
<a id="more"></a>
<center><img src="/images/spark/mesos.png" alt=""></center>

<p>在 Mesos 上所執行的應用程式都被稱為<code>框架（Framework）</code>，該框架會被 Mesos 以 API 方式處理資源的提供，並將任務提交給 Mesos。其任務執行流程有以下幾個步驟構成：</p>
<ul>
<li>Slave 提供可用資源給 Master</li>
<li>Master 向 Framework 的資源供應，並說明 Slave 資源</li>
<li>Framework Scheduler 回應任務以及每個任務的資源需求</li>
<li>Master 將任務發送到適當的 Slave 執行器（Executor）</li>
</ul>
<center><img src="/images/spark/mesos-framework.jpg" alt=""></center>

<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>以下為節點配置：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>HostName</th>
</tr>
</thead>
<tbody>
<tr>
<td>192.168.1.10</td>
<td>mesos-master</td>
</tr>
<tr>
<td>192.168.1.11</td>
<td>mesos-slave-1</td>
</tr>
<tr>
<td>192.168.1.12</td>
<td>mesos-slave-2</td>
</tr>
</tbody>
</table>
<p>首先我們要在各節點先安裝 ssh-server 與 Java JDK，並配置需要的相關環境：</p>
<pre><code class="sh">$ sudo apt-get install openssh-server
</code></pre>
<p>設定<user>(hadoop)不用需打 sudo 密碼：</user></p>
<pre><code class="sh">$ echo &quot;hadoop ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/hadoop &amp;&amp; sudo chmod 440 /etc/sudoers.d/hadoop
</code></pre>
<blockquote>
<p>P.S 要注意 <code>hadoop</code> 要隨著現在使用的 User 變動。</p>
</blockquote>
<p>建立ssh key，並複製 key 使之不用密碼登入：</p>
<pre><code class="sh">$ ssh-keygen -t rsa
$ ssh-copy-id localhost
</code></pre>
<p>安裝Java 1.8 JDK：</p>
<pre><code>$ sudo apt-get purge openjdk*
$ sudo apt-get -y autoremove
$ sudo apt-get install -y software-properties-common
$ sudo add-apt-repository -y ppa:webupd8team/java
$ sudo apt-get update
$ echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections
$ echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections
$ sudo apt-get -y install oracle-java8-installer
</code></pre><p>新增各節點 Hostname 至<code>/etc/hosts</code>檔案：</p>
<pre><code class="sh">127.0.0.1 localhost

192.168.1.10 mesos-master
192.168.1.11 mesos-slave-1
192.168.1.12 mesos-slave-2
</code></pre>
<p>並在<code>Master</code>節點複製所有<code>Slave</code>的 ssh key：</p>
<pre><code class="sh">$ ssh-copy-id ubuntu@mesos-slave-1
$ ssh-copy-id ubuntu@mesos-slave-2
</code></pre>
<h2 id="Mesos-安裝"><a href="#Mesos-安裝" class="headerlink" title="Mesos 安裝"></a>Mesos 安裝</h2><p>首先要安裝 Mesos 於系統上，可以採用以下方式獲取最新版本的 Respository：</p>
<pre><code class="sh">$ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv E56151BF
$ DISTRO=$(lsb_release -is | tr &#39;[:upper:]&#39; &#39;[:lower:]&#39;)
$ CODENAME=$(lsb_release -cs)
$ echo &quot;deb http://repos.mesosphere.com/${DISTRO} ${CODENAME} main&quot; | sudo tee /etc/apt/sources.list.d/mesosphere.list
</code></pre>
<p>加入 key 與 repository 後，即可透過<code>apt-get</code>安裝：</p>
<pre><code class="sh">sudo apt-get update
sudo apt-get -y install mesos
</code></pre>
<blockquote>
<p>P.S <code>Master</code>需要再安裝 Marathon</p>
</blockquote>
<h3 id="Master-節點設定"><a href="#Master-節點設定" class="headerlink" title="Master 節點設定"></a>Master 節點設定</h3><p>首先設定 Zookeeper ID：</p>
<pre><code class="sh">$ echo 1 | sudo tee /etc/zookeeper/conf/myid
</code></pre>
<p>設定 Zookeeper configuration：</p>
<pre><code class="sh">HOST_IP=$(ip route get 8.8.8.8 | awk &#39;{print $NF; exit}&#39;)
echo server.1=$HOST_IP:2888:3888 | sudo tee -a /etc/zookeeper/conf/zoo.cfg
</code></pre>
<blockquote>
<p>若要部署 HA 需要加入多個 Master 節點的 Zookeeper。</p>
</blockquote>
<p>完成後，重新啟動 Zookeeper 服務：</p>
<pre><code class="sh">$ sudo service zookeeper restart
</code></pre>
<p>接著設定 Mesos zk configuration：</p>
<pre><code class="sh">$ echo zk://$HOST_IP:2181/mesos | sudo tee /etc/mesos/zk
</code></pre>
<p>設定 Mesos quorum 參數：</p>
<pre><code class="sh">$ echo 1 | sudo tee /etc/mesos-master/quorum
</code></pre>
<blockquote>
<p>若是 OpenStack VM 需要設定 Host IP 和 EXENTAL_IP 為 區網 IP 而非 Flaot IP：（Optional）</p>
<pre><code class="sh">EXENTAL_IP=&#39;192.168.1.10&#39;
echo $EXENTAL_IP | sudo tee /etc/mesos-master/hostname
echo $HOST_IP | sudo tee /etc/mesos-master/ip
echo &#39;mesos-cluster&#39; | sudo tee /etc/mesos-master/cluster
</code></pre>
</blockquote>
<p>接著設定<code>advertise_ip</code>：</p>
<pre><code class="sh">$ echo $HOST_IP | sudo tee /etc/mesos-master/advertise_ip
</code></pre>
<p>當設定完成，要接著設定 Marathon，首先建立組態目錄：</p>
<pre><code class="sh">sudo mkdir /etc/marathon/
sudo mkdir /etc/marathon/conf
</code></pre>
<p>設定 hostname：</p>
<pre><code>$ echo $EXENTAL_IP | sudo tee /etc/marathon/conf/hostname
</code></pre><p>設定 master ip ：</p>
<pre><code class="sh">$ echo zk://$HOST_IP:2181/mesos | sudo tee /etc/marathon/conf/master
</code></pre>
<p>設定 master zookeeper ：</p>
<pre><code class="sh">$ echo zk://$HOST_IP:2181/marathon | sudo tee /etc/marathon/conf/zk
</code></pre>
<p>關閉 Master 節點的<code>mesos-slave</code> service：</p>
<pre><code class="sh">sudo service mesos-slave stop
sudo sh -c &quot;echo manual &gt; /etc/init/mesos-slave.override&quot;
</code></pre>
<p>重新啟動 Mesos 與 Marathon 服務：</p>
<pre><code class="sh">sudo service mesos-master restart
sudo service marathon restart
</code></pre>
<h3 id="Slave-節點設定"><a href="#Slave-節點設定" class="headerlink" title="Slave 節點設定"></a>Slave 節點設定</h3><p>由於我們是使用 ubuntu 套件，Zookeeper 會以相依套件被自動下載至環境上，故我們要手動停止服務：</p>
<pre><code class="sh">sudo service zookeeper stop
sudo sh -c &quot;echo manual &gt; /etc/init/zookeeper.override&quot;
</code></pre>
<p>設定 Mesos 與 Marathon：</p>
<blockquote>
<p>若使用 OpenStack VM，需要將 MASTER_IP 和 PUBlIC_IP 設定為區網 IP</p>
<pre><code class="sh">MASTER_IP=&quot;192.168.1.10&quot;
PUBlIC_IP=&quot;192.168.1.11&quot;
HOST_IP=$(ip route get 8.8.8.8 | awk &#39;{print $NF; exit}&#39;)
echo zk://$MASTER_IP:2181/mesos | sudo tee /etc/mesos/zk
</code></pre>
<p>設定 Hostname 可以使用 OpenStack Float IP（Optional）：</p>
<pre><code class="sh">echo $PUBlIC_IP | sudo tee /etc/mesos-slave/hostname
</code></pre>
</blockquote>
<p>設定 slave ip：</p>
<pre><code class="sh">$ echo $HOST_IP | sudo tee /etc/mesos-slave/ip
</code></pre>
<p>關閉 mesos-master 服務，並取消自動開機啟動：</p>
<pre><code class="sh">sudo service mesos-master stop
sudo sh -c &quot;echo manual &gt; /etc/init/mesos-master.override&quot;
</code></pre>
<p>重新啟動 Mesos slave 服務：</p>
<pre><code class="sh">$ sudo service mesos-slave restart
</code></pre>
<h2 id="驗證安裝結果"><a href="#驗證安裝結果" class="headerlink" title="驗證安裝結果"></a>驗證安裝結果</h2><p>當安裝完成，我們要驗證系統是否正常運行，可以透過以下指令運行：</p>
<pre><code class="sh">MASTER=$(mesos-resolve `cat /etc/mesos/zk`)
mesos-execute --master=$MASTER --name=&quot;cluster-test&quot; --command=&quot;sleep 5&quot;
</code></pre>
<blockquote>
<p>若要查看細節資訊，可以用瀏覽器開啟 <a href="http://&lt;master-ip&gt;:5050" target="_blank" rel="noopener">Mesos Console</a>、<a href="http://&lt;master-ip&gt;:8080" target="_blank" rel="noopener">Marathon console</a></p>
</blockquote>
<h2 id="安裝-Spark-Driver"><a href="#安裝-Spark-Driver" class="headerlink" title="安裝 Spark Driver"></a>安裝 Spark Driver</h2><p>首先下載 Spark，並修改權限：</p>
<pre><code class="sh">$ curl -s &quot;http://archive.apache.org/dist/spark/spark-1.5.2/spark-1.5.2-bin-hadoop2.6.tgz&quot; | sudo tar -xz -C /opt/
$ sudo mv /opt/spark-1.5.2-bin-hadoop2.6 /opt/spark
$ sudo chown $USER:$USER -R /opt/spark
</code></pre>
<p>之後到<code>spark/conf</code>目錄，將<code>spark-env.sh.template</code>複製為<code>park-env.sh</code>：</p>
<pre><code class="sh">$ cp spark-env.sh.template spark-env.sh
</code></pre>
<p>在<code>spark-env.sh</code>這內容最下方增加這幾筆環境參數：</p>
<pre><code class="sh">export MESOS_NATIVE_JAVA_LIBRARY=&quot;/usr/lib/libmesos.so&quot;
export MASTER=&quot;mesos://192.168.1.10:5050&quot;
export SPARK_EXECUTOR_URI=&quot;/opt/spark-1.5.2.tgz&quot;

export JAVA_HOME=$(readlink -f /usr/bin/java | sed &quot;s:jre/bin/java::&quot;)

export SPARK_LOCAL_IP=$(ifconfig eth0 | awk &#39;/inet addr/{print substr($2,6)}&#39;)
export SPARK_LOCAL_HOSTNAME=$(ifconfig eth0 | awk &#39;/inet addr/{print substr($2,6)}&#39;)
</code></pre>
<p>接著下載一個新的<code>spark-1.5.2-bin-hadoop2.6.tgz</code>，並解壓縮：</p>
<pre><code class="sh">$ cd ~/
$ wget &quot;http://archive.apache.org/dist/spark/spark-1.5.2/spark-1.5.2-bin-hadoop2.6.tgz&quot;
$ tar -xvf spark-1.5.2-bin-hadoop2.6.tgz
$ sudo mv spark-1.5.2-bin-hadoop2.6 spark-1.5.2
$ sudo vim spark-1.5.2/conf/spark-env.sh
export MESOS_NATIVE_LIBRARY=/usr/local/lib/libmesos.so
export SPARK_EXECUTOR_URI=&quot;/opt/spark-1.5.2.tgz&quot;
export MASTER=mesos://192.168.1.10:5050
export JAVA_HOME=$(readlink -f /usr/bin/java | sed &quot;s:jre/bin/java::&quot;)
</code></pre>
<p>完成後壓縮資料夾：</p>
<pre><code class="sh">$ sudo tar -czvf spark-1.5.2.tgz spark-1.5.2/
</code></pre>
<p>並在<code>Master</code>節點複製到所有<code>Slave</code>並解壓縮：</p>
<pre><code class="sh">$ scp spark-1.5.2.tgz mesos-slave-1:~/ &amp;&amp; ssh mesos-slave-1 sudo mv ~/spark-1.5.2.tgz /opt
$ scp spark-1.5.2.tgz mesos-slave-2:~/ &amp;&amp; ssh mesos-slave-2 sudo mv ~/spark-1.5.2.tgz /opt
$ sudo tar -xvf /opt/spark-1.5.2.tgz
</code></pre>
<p>設定使用者環境參數：</p>
<pre><code class="sh">$ echo &quot;export SPARK_HOME=/opt/spark&quot; | sudo tee -a ~/.bashrc
$ echo &quot;export PATH=\$SPARK_HOME/bin:\$PATH&quot; | sudo tee -a ~/.bashrc
</code></pre>
<p>執行<code>spark-shell</code>，來驗證 Spark 可否正常執行：</p>
<pre><code class="sh">$ spark-shell --master mesos://192.168.1.34:5050
val data = 1 to 10000
val distData = sc.parallelize(data)
distData.filter(_&lt; 10).collect()
</code></pre>
<p>或使用範例程式提交 Job：</p>
<pre><code class="sh">$ spark-submit --class org.apache.spark.examples.SparkPi \
--master mesos://192.168.1.10:5050 \
--num-executors 1 \
--executor-memory 1g \
--executor-cores 1 \
lib/spark-examples*.jar \
1
</code></pre>
]]></content>
      
        <categories>
            
            <category> Spark </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Mesos </tag>
            
            <tag> HDFS </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Ubuntu PXE 安裝與設定]]></title>
      <url>https://kairen.github.io/2015/10/11/linux/ubuntu/ubuntu-pxe/</url>
      <content type="html"><![CDATA[<p>預啟動執行環境（Preboot eXecution Environment，PXE，也被稱為預執行環境)提供了一種使用網路介面（Network Interface）啟動電腦的機制。這種機制讓電腦的啟動可以不依賴本地資料儲存裝置（如硬碟）或本地已安裝的作業系統。</p>
<p><img src="/images/linux/pxe.png" alt="PXE"></p>
<a id="more"></a>
<p>PXE 伺服器必須要提供至少含有 DHCP 以及 TFTP :</p>
<ul>
<li>DHCP 服務必須要能夠提供用戶端的網路參數之外，還得要告知用戶端 TFTP 所在的位置為何才行</li>
<li>TFTP 則是提供用戶端 boot loader 及 kernel file 下載點的重要服務</li>
</ul>
<h2 id="Kickstart"><a href="#Kickstart" class="headerlink" title="Kickstart"></a>Kickstart</h2><p>我們在手動安裝作業系統時，會針對需求安裝作業系統的相關套件、設定、disk切割等，當我們重複的輸入這些資訊時，隨著需安裝的電腦越多會越裝越阿雜，如果有人可以幫你完成這樣一套輸入資訊的話，就可以快速的自動化部署多台電腦，除了方便外，心情也格外爽快。</p>
<p>kickstart是Red Hat公司針對自動化安裝Red Had、Fedora、CentOS而制定的問題回覆規範，透過這個套件可以指定回覆設定問題，更能夠指定作業系統安裝其他套裝軟體，也可以執行Script(sh, bash)，通常kickstart設定檔(.cfg)是透過system-config-kickstart產生。也可以利用GUI的CentOS下產生安裝用的cfg檔案。</p>
<h2 id="Preseed"><a href="#Preseed" class="headerlink" title="Preseed"></a>Preseed</h2><p>相對於kickstart，preseed是Debain/Ubuntu的自動化安裝回覆套件。</p>
<h2 id="其他工具"><a href="#其他工具" class="headerlink" title="其他工具"></a>其他工具</h2><ul>
<li>Stacki 3</li>
<li>Ubuntu MAAS</li>
<li>Foreman</li>
<li>LinMin</li>
<li>OpenStack Ironic</li>
<li>Crowbar</li>
</ul>
<h2 id="PXE-安裝與設定"><a href="#PXE-安裝與設定" class="headerlink" title="PXE 安裝與設定"></a>PXE 安裝與設定</h2><p>首先安裝相關軟體，如 TFTP、DHCP等：</p>
<pre><code class="sh">sudo apt-get install -y tftpd-hpa isc-dhcp-server lftp openbsd-inetd
</code></pre>
<h3 id="DHCP-設定"><a href="#DHCP-設定" class="headerlink" title="DHCP 設定"></a>DHCP 設定</h3><p>首先編輯 <code>/etc/dhcp/dhcpd.conf</code>檔案，在下面配置 DHCP：</p>
<pre><code>ddns-update-style none;
default-lease-time 600;
max-lease-time 7200;
log-facility local7;

subnet 10.21.10.0 netmask 255.255.255.0 {
    range 10.21.10.200 10.21.10.250;
    option subnet-mask 255.255.255.0;
    option routers 10.21.10.254;
    option broadcast-address 10.21.10.255;
    filename &quot;pxelinux.0&quot;;
    next-server 10.21.10.240;
}
</code></pre><p>完成後，重新啟動 DHCP 服務：</p>
<pre><code class="sh">$ sudo service isc-dhcp-server restart

 * Stopping ISC DHCP server dhcpd [fail]
 * Starting ISC DHCP server dhcpd [ OK ]
</code></pre>
<p>檢查 DHCP 是否正確被啟動：</p>
<pre><code class="sh">$ sudo netstat -lu | grep boot

udp        0      0 *:bootps                *:*
</code></pre>
<h3 id="TFTP-Server-設定"><a href="#TFTP-Server-設定" class="headerlink" title="TFTP Server 設定"></a>TFTP Server 設定</h3><p>編輯<code>/etc/inetd.conf</code>檔案，在最下面加入以下內容：</p>
<pre><code>tftp dgram udp wait root /usr/sbin/in.tftpd  /usr/sbin/in.tftpd -s /var/lib/tftpboot
</code></pre><p>接著設定 Boot 時啟動服務，以及重新啟動相關服務：</p>
<pre><code class="sh">$ sudo update-inetd --enable BOOT
$ sudo service openbsd-inetd restart

 * Restarting internet superserver inetd [ OK ]

$ sudo service tftpd-hpa restart
</code></pre>
<p>檢查 TFTP Server 是否正確啟動：</p>
<pre><code class="sh">$ netstat -lu | grep tftp

udp        0      0 *:tftp                  *:*
</code></pre>
<h3 id="建立開機選單"><a href="#建立開機選單" class="headerlink" title="建立開機選單"></a>建立開機選單</h3><p>完成後安裝 syslinux:</p>
<pre><code class="sh">sudo apt-get -y install syslinux
</code></pre>
<p>複製 syslinux 設定檔至<code>/var/lib/tftpboot</code>目錄中：</p>
<pre><code class="sh">sudo cp /usr/lib/syslinux/menu.c32  /var/lib/tftpboot
sudo cp /usr/lib/syslinux/vesamenu.c32 /var/lib/tftpboot
sudo cp /usr/lib/syslinux/pxelinux.0 /var/lib/tftpboot
sudo cp /usr/lib/syslinux/memdisk /var/lib/tftpboot
sudo cp /usr/lib/syslinux/mboot.c32 /var/lib/tftpboot
sudo cp /usr/lib/syslinux/chain.c32 /var/lib/tftpboot
</code></pre>
<p>建立<code>/var/lib/tftpboot/pxelinux.cfg</code>目錄：</p>
<pre><code class="sh">$ sudo mkdir /var/lib/tftpboot/pxelinux.cfg
</code></pre>
<p>接著編輯<code>/var/lib/tftpboot/pxelinux.cfg/default</code>檔案，設定開機選單，以下為簡單設定範例：</p>
<pre><code>UI vesamenu.c32
TIMEOUT 100
MENU TITLE Welcom to KaiRen.Lab PXE Server System

LABEL local
  MENU LABEL Boot from local drive
  MENU DEFAULT
  localboot 0

LABEL Custom CentOS 6.5
  MENU LABEL Install Custom CentOS 6.5
  kernel ./centos/vmlinuz
  append initrd=./centos/initrd.img ksdevice=bootif ip=dhcp ks=http://10.21.10.240/centos-ks/default_ks.cfg

LABEL Hadoop CentOS 6.5
  MENU LABEL Install Hadoop CentOS 6.5
  kernel ./centos/vmlinuz
  append initrd=./centos/initrd.img ksdevice=bootif ip=dhcp ks=http://10.21.10.240/centos-ks/hdp_ks.cfg

LABEL Ubuntu Server 14.04
  MENU LABEL Install Ubuntu Server 14.04
  kernel ./ubuntu/server/14.04/linux
  append initrd=./ubuntu/server/14.04/initrd.gz method=http://10.21.10.240/ubuntu/server/14.04/
</code></pre>]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux </tag>
            
            <tag> PXE </tag>
            
            <tag> Bare Metal </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[CentOS 6.5 PXE 安裝與設定]]></title>
      <url>https://kairen.github.io/2015/10/03/linux/centos/centos-pxe/</url>
      <content type="html"><![CDATA[<p>預啟動執行環境（Preboot eXecution Environment，PXE，也被稱為預執行環境)提供了一種使用網路介面（Network Interface）啟動電腦的機制。這種機制讓電腦的啟動可以不依賴本地資料儲存裝置（如硬碟）或本地已安裝的作業系統。</p>
<a id="more"></a>
<h2 id="安裝環境"><a href="#安裝環境" class="headerlink" title="安裝環境"></a>安裝環境</h2><ul>
<li>CentOS 6.5 Minimal Install</li>
<li>Intel(R) Core(TM)2 Quad CPU Q8400  @ 2.66GHz</li>
<li>500 GB</li>
<li>4G RAM</li>
<li>Two Eth Card<ul>
<li>Inner eth = PEX DHCP</li>
<li>Outer eth = Public network</li>
</ul>
</li>
</ul>
<h2 id="PXE-安裝與設定"><a href="#PXE-安裝與設定" class="headerlink" title="PXE 安裝與設定"></a>PXE 安裝與設定</h2><p>首先安装 Setuptool 於 CentOS 上</p>
<pre><code class="sh">$ sudo yum install -y setuptool ntsysv iptables system-config-network-tui
</code></pre>
<p>關閉防火牆與 SElinux，避免驗證時被阻擋：</p>
<pre><code class="sh">$ sudo service iptables stop
$ sudo setenforce 0
</code></pre>
<p>接著編輯<code>/etc/selinux/config</code>，修改以下內容:</p>
<pre><code>SELINUX=disabled
</code></pre><p>然後編輯<code>/etc/sysconfig/network-scripts/ifcfg-ethx</code>設定與確認 IP Address 是否正確：</p>
<pre><code class="sh">DEVICE=ethx
HWADDR=C4:6E:1F:04:60:24    #依照個人eth
TYPE=Ethernet
UUID=ada7e5dc-a2e9-4a89-9c93-e1f559cd05f2
ONBOOT=yes
NM_CONTROLLED=yes
BOOTPROTO=none
IPADDR=192.168.28.130       #依照網路
NETMASK=255.255.255.0
USERCTL=no
</code></pre>
<h2 id="DHCP-Server-安裝與設定"><a href="#DHCP-Server-安裝與設定" class="headerlink" title="DHCP Server 安裝與設定"></a>DHCP Server 安裝與設定</h2><p>DHCP是「 動態主機配置協定」(Dynamic Host Configuration Protocol)。<br>DHCP是可自動將IP位址指派給登入TCP/IP網路的用戶端的一種軟體(這種IP位址稱為「動態IP位址」)。這邊安裝方式為以下：</p>
<pre><code class="sh">$ sudo yum -y install dhcp
</code></pre>
<p>完成後編輯<code>/etc/dhcp/dhcpd.conf</code>，並修改以下設定:</p>
<pre><code>ddns-update-style none;
ignore client-updates;
allow booting;
allow bootp;
option ip-forwarding false;
option mask-supplier false;
option broadcast-address 192.168.28.255;

subnet 192.168.28.0 netmask 255.255.255.0 {
        option routers 192.168.28.130
        range 192.168.28.50 192.168.28.60;
        #option subnet-mask 255.255.255.0;
        #option domain-name &quot;i4502.dic.ksu&quot;;
        option domain-name-servers 10.21.20.1;

        next-server 192.168.28.130;
        filename        &quot;pxelinux.0&quot;;
}
</code></pre><p>設定完後，重新啟動 DHCP 服務：</p>
<pre><code class="sh">$ sudo service dhcpd start
$ sudo chkconfig dhcpd on
</code></pre>
<h2 id="TFTP-Server-安裝與設定"><a href="#TFTP-Server-安裝與設定" class="headerlink" title="TFTP Server 安裝與設定"></a>TFTP Server 安裝與設定</h2><p>簡單文件傳輸協議或稱小型文件傳輸協議（英文：Trivial File Transfer Protocol，縮寫TFTP），是一種簡化的文件傳輸協議。小型文件傳輸協議非常簡單，通過少量存儲器就能輕鬆實現——這在當時是很重要的考慮因素。所以TFTP被用於引導計算機，例如沒有大容量存儲器的路由器。安裝方式為以下：</p>
<pre><code class="sh">$ sudo yum -y install tftp-server tftp
</code></pre>
<p>安裝完成後編輯<code>/etc/xinetd.d/tftp</code>，修改以下內容：</p>
<pre><code>service tftp
{
        socket_type             = dgram
        protocol                = udp
        wait                    = yes
        user                    = root
        server                  = /usr/sbin/in.tftpd
        server_args             = -s /install/tftpboot
        disable                 = yes
        per_source              = 11
        cps                     = 100 2
        flags                   = IPv4
}
</code></pre><p>P.S 如果不修改 server_args，預設為 <code>/var/lib/tftpboot/</code>。</p>
<p>接著建立<code>/install/tftpboot</code>來存放 Boot 映像檔：</p>
<pre><code class="sh">sudo mkdir -p /install/tftpboot
sudochcon --reference /var /install

sudo service xinetd restart
sudo chkconfig xinetd on
sudo chkconfig tftp on
</code></pre>
<h2 id="安裝-syslinu"><a href="#安裝-syslinu" class="headerlink" title="安裝 syslinu"></a>安裝 syslinu</h2><p>如果要使用 PXE 的開機管理程式與開機選單的話，那就得要安裝 CentOS 內建提供的 syslinux 軟體，從裡面撈出兩個檔案即可。當然啦，這兩個檔案得要放置在 TFTP 的根目錄下才好！整個實作的過程如下。</p>
<pre><code class="sh">yum -y install syslinux
cp /usr/share/syslinux/menu.c32 /install/tftpboot/
cp /usr/share/syslinux/vesamenu.c32 /install/tftpboot/
cp /usr/share/syslinux/pxelinux.0 /install/tftpboot/
mkdir /install/tftpboot/pxelinux.cfg
ll /install/tftpboot/
</code></pre>
<h2 id="掛載CentOS-映像檔"><a href="#掛載CentOS-映像檔" class="headerlink" title="掛載CentOS 映像檔"></a>掛載CentOS 映像檔</h2><p>已CentOS 6.5 Minimal為範例。</p>
<pre><code class="sh">mount -o loop CentOS-6.5-x86_64-minimal.iso /mnt
mkdir -p /install/tftpboot/kernel/centos6.5

cp /mnt/isolinux/vmlinuz /install/tftpboot/kernel/centos6.5
cp /mnt/isolinux/initrd.img /install/tftpboot/kernel/centos6.5
cp /mnt/isolinux/isolinux.cfg /install/tftpboot/pxelinux.cfg/demo
umount /mnt
</code></pre>
<ul>
<li>vmlinuz：就是安裝軟體的核心檔案 (kernel file)</li>
<li>initrd.img：就是開機過程中所需要的核心模組參數</li>
<li>isolinux.cfg –&gt; demo：作為未來 PXE 所需要的開機選單之參考</li>
</ul>
<h2 id="設定開機選單"><a href="#設定開機選單" class="headerlink" title="設定開機選單"></a>設定開機選單</h2><pre><code class="sh">vim /install/tftpboot/pxelinux.cfg/default
</code></pre>
<p><strong>修改：</strong></p>
<pre><code class="sh">UI vesamenu.c32
TIMEOUT 300
DISPLAY ./boot.msg
MENU TITLE Welcome to KAIREN&#39;s PXE Server System

LABEL local
  MENU LABEL Boot from local drive
  MENU DEFAULT
  localboot 0

LABEL ubuntu
  MENU LABEL Install CentOS 6.5
  kernel ./kernel/centos6.5/vmlinuz
  append initrd=./kernel/centos6.5/initrd.img
</code></pre>
<h3 id="修改額外開機選單訊息"><a href="#修改額外開機選單訊息" class="headerlink" title="修改額外開機選單訊息"></a>修改額外開機選單訊息</h3><pre><code class="sh">vim /install/tftpboot/boot.msg
</code></pre>
<p><strong>訊息：</strong></p>
<pre><code class="sh">Welcome to KAI-REN&#39;s PXE Server System.

The 1st menu can let you system goto hard disk menu.
The 2nd menu can goto interactive installation step.
</code></pre>
<h2 id="提供NFS-Server-提供映像檔"><a href="#提供NFS-Server-提供映像檔" class="headerlink" title="提供NFS Server 提供映像檔"></a>提供NFS Server 提供映像檔</h2><p>NFS 就是 Network FileSystem 的縮寫，最早之前是由 Sun 這家公司所發展出來的。 它最大的功能就是可以透過網路，讓不同的機器、不同的作業系統、可以彼此分享個別的檔案 (share files)。這個 NFS 伺服器可以讓你的 PC 來將網路遠端的 NFS 伺服器分享的目錄，掛載到本地端的機器當中， 在本地端的機器看起來，那個遠端主機的目錄就好像是自己的一個磁碟分割槽一樣 (partition)。</p>
<pre><code class="sh">mkdir -p /install/nfs_share/centos6.5
vim /etc/fstab
</code></pre>
<p><strong>在最底下加入：</strong></p>
<pre><code class="sh">/root/CentOS-6.5-x86_64-minimal.iso /install/nfs_share/centos6.5 iso9660 defaults,loop 0 0
</code></pre>
<p><strong>安裝並提供分享目錄</strong></p>
<pre><code class="sh">mount -a
df

yum -y install nfs-utils
vim /etc/exports
</code></pre>
<p><strong>加入：</strong></p>
<pre><code class="sh">/install/nfs_share/  192.168.28.0/24(ro,async,nohide,crossmnt)  localhost(ro,async,nohide,crossmnt)
</code></pre>
<p><strong>修改System nfs conf</strong></p>
<pre><code class="sh">vim /etc/sysconfig/nfs
</code></pre>
<p><strong>如下(P.S 找到上面這幾個設定值，我們得要設定好固定的 port 來開放防火牆給用戶處理)：</strong></p>
<pre><code class="sh">RQUOTAD_PORT=901
LOCKD_TCPPORT=902
LOCKD_UDPPORT=902
MOUNTD_PORT=903
STATD_PORT=904
</code></pre>
<p><strong>修改NFS 不需要對映帳號</strong></p>
<pre><code class="sh">vim /etc/idmapd.conf
</code></pre>
<p><strong>如下：</strong></p>
<pre><code class="sh">[General]
Domain = &quot;kairen.pxe.com&quot;
[Mapping]
Nobody-User = nfsnobody
Nobody-Group = nfsnobody
</code></pre>
<p><strong>重開服務</strong></p>
<pre><code class="sh">service rpcbind restart
service nfs restart
service rpcidmapd restart
service nfslock restart

chkconfig rpcbind on
chkconfig nfs on
chkconfig rpcidmapd on
chkconfig nfslock on
rpcinfo -p

showmount -e localhost
</code></pre>
<p>如果看到<strong>Export list for localhost:<br>/install/nfs_share 192.168.28.0/24,localhost</strong>就是成功了。</p>
<h2 id="提供-HTTP-Server"><a href="#提供-HTTP-Server" class="headerlink" title="提供 HTTP Server"></a>提供 HTTP Server</h2><p>Apache HTTP Server（簡稱Apache）是Apache軟體基金會的一個開放原始碼的網頁伺服器軟體，可以在大多數電腦作業系統中運行，由於其跨平台和安全性。</p>
<pre><code class="sh">yum -y install httpd
service httpd start
chkconfig httpd on
</code></pre>
<p><strong>建立CentOS 6.5目錄</strong></p>
<pre><code class="sh">mkdir -p /var/www/html/install/centos6.5
vim /etc/fstab
</code></pre>
<p><strong>加入到最下方：</strong></p>
<pre><code class="sh">/root/CentOS-6.5-x86_64-minimal.iso /var/www/html/install/centos6.5 iso9660 defaults,loop 0 0
</code></pre>
<p><strong>掛載起來</strong></p>
<pre><code class="sh">mount -a
df
</code></pre>
<h2 id="提供-FTP-Server"><a href="#提供-FTP-Server" class="headerlink" title="提供 FTP Server"></a>提供 FTP Server</h2><pre><code class="sh">yum -y install vsftpd
service vsftpd start
chkconfig vsftpd on

mkdir -p /var/ftp/install/centos6.5
vim /etc/fstab
</code></pre>
<p><strong>一樣加入Mount :</strong></p>
<pre><code class="sh">/root/CentOS-6.5-x86_64-minimal.iso /var/ftp/install/centos6.5 iso9660 defaults,loop,context=system_u:object_r:public_content_t:s0 0 0
</code></pre>
<p><strong>掛載起來</strong></p>
<pre><code class="sh">mount -a
df
</code></pre>
<ul>
<li><a href="http://192.168.28.130/install/centos6.5" target="_blank" rel="noopener">HTTP</a></li>
<li><a href="ftp://192.168.28.130/install/centos6.5" target="_blank" rel="noopener">FTP</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux </tag>
            
            <tag> PXE </tag>
            
            <tag> Bare Metal </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Spark on Hadoop YARN 單機安裝]]></title>
      <url>https://kairen.github.io/2015/09/19/data-engineer/spark-yarn/</url>
      <content type="html"><![CDATA[<p>本教學為安裝 Spark on Hadoop YARN 的 all-in-one 版本，將 Spark 應用程式執行於 YARN 上，來讓應用程式執行於不同的工作節點上。</p>
<a id="more"></a>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>首先我們要先安裝 ssh-server 與 Java JDK，並配置需要的相關環境：</p>
<pre><code class="sh">$ sudo apt-get install openssh-server
</code></pre>
<p>設定<user>(hadoop)不用需打 sudo 密碼：</user></p>
<pre><code class="sh">$ echo &quot;hadoop ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/hadoop &amp;&amp; sudo chmod 440 /etc/sudoers.d/hadoop
</code></pre>
<blockquote>
<p>P.S 要注意 <code>hadoop</code> 要隨著現在使用的 User 變動。</p>
</blockquote>
<p>建立ssh key，並複製 key 使之不用密碼登入：</p>
<pre><code class="sh">$ ssh-keygen -t rsa
$ ssh-copy-id localhost
</code></pre>
<p>安裝Java 1.7 JDK：</p>
<pre><code>$ sudo apt-get purge openjdk*
$ sudo apt-get -y autoremove
$ sudo apt-get install -y software-properties-common
$ sudo add-apt-repository -y ppa:webupd8team/java
$ sudo apt-get update
$ echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections
$ echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections
$ sudo apt-get -y install oracle-java7-installer
</code></pre><h2 id="安裝-Hadoop-YARN"><a href="#安裝-Hadoop-YARN" class="headerlink" title="安裝 Hadoop YARN"></a>安裝 Hadoop YARN</h2><p>首先我們須先將 Hadoop YARN 安裝完成，詳細步驟如下所示。<br>下載Hadoop 2.6.0 or laster version：</p>
<pre><code class="sh">$ curl -s &quot;https://archive.apache.org/dist/hadoop/core/hadoop-2.6.0/hadoop-2.6.0.tar.gz&quot; | sudo tar -xz -C /opt/
$ sudo mv /opt/hadoop-2.6.0 /opt/hadoop
</code></pre>
<blockquote>
<p>若要下載不同版本可以到官方查看。</p>
</blockquote>
<p>到 hadoop 底下的 /etc/hadoop 設定所有conf檔與<code>evn.sh</code>檔：</p>
<pre><code class="sh">$ cd /opt/hadoop/etc/hadoop
$ sudo vim hadoop-env.sh
# 修改裡面的Java_Home
export JAVA_HOME=/usr/lib/jvm/java-7-oracle
</code></pre>
<p>修改<code>mapred-site.xml.template</code>檔案：</p>
<pre><code class="sh">$ sudo mv mapred-site.xml.template mapred-site.xml
$ sudo vim mapred-site.xml

# 修改以下放置到&lt;configuration&gt;&lt;/configuration&gt;裡面
&lt;property&gt;
   &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
   &lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<p>修改<code>hdfs-site.xml</code>檔案：</p>
<pre><code class="sh">$ sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode
$ sudo mkdir -p /usr/local/hadoop_store/hdfs/datanode
$ sudo chown -R $USER:$USER /usr/local/hadoop_store
$ sudo vim hdfs-site.xml

# 修改以下放置到&lt;configuration&gt;&lt;/configuration&gt;裡面
&lt;property&gt;
   &lt;name&gt;dfs.replication&lt;/name&gt;
   &lt;value&gt;1&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
   &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
   &lt;value&gt;/usr/local/hadoop_store/hdfs/namenode&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
   &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
   &lt;value&gt;/usr/local/hadoop_store/hdfs/datanode&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<p>修改<code>core-site.xml</code>檔案：</p>
<pre><code class="sh">$ sudo mkdir -p /app/hadoop/tmp
$ sudo chown $USER_NAME:$USER_NAME /app/hadoop/tmp
$ sudo vim core-site.xml

# 修改以下放置到&lt;configuration&gt;&lt;/configuration&gt;裡面
    &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
        &lt;value&gt;/app/hadoop/tmp&lt;/value&gt;
        &lt;description&gt;A base for other temporary directories.&lt;/description&gt;
    &lt;/property&gt;
</code></pre>
<p>修改<code>yarn-site.xml</code>檔案：</p>
<pre><code class="sh">$ sudo vim yarn-site.xml

# 修改以下放置到&lt;configuration&gt;&lt;/configuration&gt;裡面
 &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<p>進行 Namenode 格式化：</p>
<pre><code class="sh">$ cd /opt/hadoop/bin
$ ./hadoop namenode -format
</code></pre>
<p>沒出錯的話，就可以開啟Hadoop對應服務：</p>
<pre><code class="sh">$ cd /opt/hadoop/sbin
$ ./start-yarn.sh
$ ./start-dfs.sh
</code></pre>
<p>檢查是否開啟以下服務：</p>
<pre><code class="sh">$ jps
3457 ResourceManager
7087 Jps
3593 NodeManager
4190 DataNode
4025 NameNode
4383 SecondaryNameNode
</code></pre>
<blockquote>
<p>開啟 <a href="http://localhost:8088" target="_blank" rel="noopener">Website YARN Dashboard</a> 與 <a href="http://localhost:50070" target="_blank" rel="noopener">HDFS Dashboard</a></p>
</blockquote>
<p>設定環境變數：</p>
<pre><code class="sh">$ cd
$ sudo vim .bashrc
# 加入以下到最後一行
export HADOOP_HOME=&quot;/opt/hadoop&quot;
export PATH=PATH:$HADOOP_HOME
export HADOOP_BIN=&quot;/opt/hadoop/bin&quot;
export PATH=$PATH:$HADOOP_BIN
</code></pre>
<p>透過<code>source</code>指令引用環境變數：</p>
<pre><code class="sh">$ source .bashrc
</code></pre>
<h3 id="驗證系統"><a href="#驗證系統" class="headerlink" title="驗證系統"></a>驗證系統</h3><p>為了驗證系統是否建置成功，可執行一個範例程式來看看是否能夠正常執行，如下所示：<br>首先上傳資料到HDFS上：</p>
<pre><code class="sh">$ sudo vim words.txt

# 加入以下，可以自行在多加
AA
CD
BB
DE
AA
AA
# 加入以上

$ hadoop fs -mkdir /example
$ hadoop fs -put words.txt /example
</code></pre>
<p>執行範例程式：</p>
<pre><code class="sh">$ cd /opt/hadoop/share/hadoop/mapreduce
$ hadoop jar hadoop-mapreduce-examples-2.6.0.jar wordcount /example/words.txt /example/output
</code></pre>
<h2 id="Spark-安裝"><a href="#Spark-安裝" class="headerlink" title="Spark 安裝"></a>Spark 安裝</h2><p>不管單機或叢集，安裝 Spark 只需要在 Master 節點上進行即可，步驟如下：</p>
<p>首先下載 Spark，並修改權限：</p>
<pre><code class="sh">$ curl -s https://d3kbcqa49mib13.cloudfront.net/spark-1.5.2-bin-hadoop2.6.tgz | sudo tar -xz -C /opt/
$ sudo mv /opt/spark-1.5.2-bin-hadoop2.6 /opt/spark
$ sudo chown $USER:$USER -R /opt/spark
</code></pre>
<blockquote>
<p>其他 Hadoop 版本可以到這邊<a href="http://d3kbcqa49mib13.cloudfront.net" target="_blank" rel="noopener">Spark-Hadoop</a>查看。</p>
</blockquote>
<p>設定 Spark 環境參數：</p>
<pre><code class="sh">$ echo &quot;export HADOOP_CONF_DIR=\$HADOOP_HOME/etc/hadoop&quot; | sudo tee -a /opt/spark/conf/spark-env.sh
$ echo &quot;export YARN_CONF_DIR=\$HADOOP_HOME/etc/hadoop&quot; | sudo tee -a /opt/spark/conf/spark-env.sh
$ echo &quot;export SPARK_HOME=/opt/spark&quot; | sudo tee -a /opt/spark/conf/spark-env.sh
$ echo &quot;export SPARK_JAR=/opt/spark/lib/spark-assembly-1.5.2-hadoop2.6.0.jar&quot; | sudo tee -a /opt/spark/conf/spark-env.sh
$ echo &quot;export PATH=\$SPARK_HOME/bin:\$PATH&quot; | sudo tee -a /opt/spark/conf/spark-env.sh
</code></pre>
<p>設定使用者環境參數：</p>
<pre><code class="sh">$ echo &quot;export SPARK_HOME=/opt/spark&quot; | sudo tee -a ~/.bashrc
$ echo &quot;export PATH=\$SPARK_HOME/bin:\$PATH&quot; | sudo tee -a ~/.bashrc
</code></pre>
<h3 id="驗證系統-1"><a href="#驗證系統-1" class="headerlink" title="驗證系統"></a>驗證系統</h3><p>為了驗證 Spark 是否成功安裝，可以透過執行一個範例程式來看看結果，如下所示：</p>
<pre><code class="sh">$ cd /opt/spark
$ spark-submit --class org.apache.spark.examples.SparkPi \
--master yarn-cluster \
--num-executors 1 \
--executor-memory 1g \
--executor-cores 1 \
lib/spark-examples*.jar \
1
</code></pre>
]]></content>
      
        <categories>
            
            <category> Spark </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Mesos </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Spark Standalone 模擬分散式運算]]></title>
      <url>https://kairen.github.io/2015/09/18/data-engineer/spark-standalone/</url>
      <content type="html"><![CDATA[<p>本教學為安裝 Spark Standalone 的叢集版本，將 Spark 應用程式執行於自己的分散式機制與各台機器連結上，來讓應用程式執行於不同的工作節點上。</p>
<a id="more"></a>
<h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>首先我們要在各節點先安裝 ssh-server 與 Java JDK，並配置需要的相關環境：</p>
<pre><code class="sh">$ sudo apt-get install openssh-server
</code></pre>
<p>設定<user>(hadoop)不用需打 sudo 密碼：</user></p>
<pre><code class="sh">$ echo &quot;hadoop ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/hadoop &amp;&amp; sudo chmod 440 /etc/sudoers.d/hadoop
</code></pre>
<blockquote>
<p>P.S 要注意 <code>hadoop</code> 要隨著現在使用的 User 變動。</p>
</blockquote>
<p>建立ssh key，並複製 key 使之不用密碼登入：</p>
<pre><code class="sh">$ ssh-keygen -t rsa
$ ssh-copy-id localhost
</code></pre>
<p>安裝Java 1.7 JDK：</p>
<pre><code>$ sudo apt-get purge openjdk*
$ sudo apt-get -y autoremove
$ sudo apt-get install -y software-properties-common
$ sudo add-apt-repository -y ppa:webupd8team/java
$ sudo apt-get update
$ echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections
$ echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections
$ sudo apt-get -y install oracle-java7-installer
</code></pre><p>新增各節點 Hostname 至 <code>/etc/hosts</code> 檔案：</p>
<pre><code class="sh">127.0.0.1 localhost

192.168.1.10 hadoop-master
192.168.1.11 hadoop-slave1
192.168.1.11 hadoop-slave2
</code></pre>
<p>並在<code>Master</code>節點複製所有<code>Slave</code>的 ssh key：</p>
<pre><code class="sh">$ ssh-copy-id ubuntu@hadoop-slave1
$ ssh-copy-id ubuntu@hadoop-slave2
</code></pre>
<h2 id="安裝-Spark"><a href="#安裝-Spark" class="headerlink" title="安裝 Spark"></a>安裝 Spark</h2><p>首先下載 Spark，並修改權限：</p>
<pre><code class="sh">$ curl -s https://d3kbcqa49mib13.cloudfront.net/spark-1.5.2-bin-hadoop2.6.tgz | sudo tar -xz -C /opt/
$ sudo mv /opt/spark-1.5.2-bin-hadoop2.6 /opt/spark
$ sudo chown $USER:$USER -R /opt/spark
</code></pre>
<p>之後到<code>spark/conf</code>目錄，將<code>spark-env.sh.template</code>複製為<code>spark-env.sh</code>：</p>
<pre><code class="sh">$ cp spark-env.sh.template spark-env.sh
</code></pre>
<p>在<code>spark-env.sh</code>這內容最下方增加這幾筆環境參數：</p>
<pre><code class="sh">export SPARK_MASTER_IP=&quot;hadoop-master&quot;  
export SPARK_MASTER_PORT=&quot;7077&quot;
export SPARK_MASTER_WEBUI_PORT=&quot;8090&quot;
</code></pre>
<blockquote>
<p><code>SPARK_MASTER_IP</code>為主節點（Master）的 IP。<br><code>SPARK_MASTER_PORT</code>為主節點（Master）的 Port。<br><code>SPARK_MASTER_WEBUI_PORT</code>為 WebUI 的 Port，預設為 8080。</p>
</blockquote>
<p>接著複製<code>slaves.template</code>為<code>slaves</code>：</p>
<pre><code class="sh">$ cp slaves.template slaves
</code></pre>
<p>在最下方增加每台機器的 hostname：</p>
<pre><code class="sh">hadoop-slave1
hadoop-slave2
</code></pre>
<p>完成後將設定檔複製給其他台機器：</p>
<pre><code class="sh">scp -r /opt/spark ubuntu@hadoop-slave1:/opt
scp -r /opt/spark ubuntu@hadoop-slave2:/opt
</code></pre>
<p>啟動 Spark ：</p>
<pre><code class="sh">/opt/spark/sbin/start-all.sh
</code></pre>
<blockquote>
<p>這樣 Spark 就啟動完成了，開啟 <a href="http://&lt;ip&gt;:8090" target="_blank" rel="noopener">Web UI</a> 來檢查狀態。</p>
</blockquote>
<p>設定使用者環境參數：</p>
<pre><code class="sh">$ echo &quot;export SPARK_HOME=/opt/spark&quot; | sudo tee -a ~/.bashrc
$ echo &quot;export PATH=\$SPARK_HOME/bin:\$PATH&quot; | sudo tee -a ~/.bashrc
</code></pre>
<h2 id="驗證系統"><a href="#驗證系統" class="headerlink" title="驗證系統"></a>驗證系統</h2><p>為了驗證 Spark 是否成功安裝，可以透過執行一個範例程式來看看結果，如下所示：</p>
<pre><code class="sh">$ cd /opt/spark
$ spark-submit --class org.apache.spark.examples.SparkPi \
--master spark://hadoop-master:7077 \
--num-executors 1 \
--executor-memory 1g \
--executor-cores 1 \
lib/spark-examples*.jar \
1
</code></pre>
]]></content>
      
        <categories>
            
            <category> Spark </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
        
    </entry>
    
  
  
    
    <entry>
      <title><![CDATA[關於我]]></title>
      <url>https://kairen.github.io/about/index.html</url>
      <content type="html"><![CDATA[<p>大家好!我是 Kyle，目前任職於 inwinSTACK 擔任軟體工程師，主要開發與研究虛擬化、分散式儲存與容器化應用，無聊時就嘗試一些技術性東西，並將筆記整理至這邊，有任何問題也歡迎聯絡。</p>
<p>自己在⼤學期間主要撰寫於 java 與 objc 程式語⾔，並專注開發 iOS Mobile 應用程式，期間共開發 20 項相關專案，而次要研讀 Hadoop 與 Spark 巨量資料運算框架，以及 Linux 相關技術，在大學期間從 Backend、Frontend 至 Mobile 都有參與開發及設計。自己本身喜歡接觸有挑戰性的事情，在學期間利用產學合作，與業界進行 Co-work 開發該企業軟體，合作企業橫跨不同領域，諸如：旅遊產業、雲端產業、健⾝器材產業等等，幫助許多企業降低⽣產成本與提升產品附加價值，在開發專案與不同領域的夥伴相互合作，來⾃不同領域科系互補與意⾒，體會產品與作品需深思熟慮很多層⾯。</p>
<p>由於雲端的普及，虛擬化技術⽇漸成熟，因應其需求，在研究所主要專研於雲端相關知識與技術，並以雲端開源軟體 OpenStack、Ceph 分散式檔案儲存系統與 Docker 輕量虛擬化做深⼊研究，進⼀步了解其架構與運作⽅式，並將其建置與應⽤於實務上，也透過參與相關活動及貢獻來與國際接軌，學習不同領域知識與技術，希望未來能把所學發揮出來。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[categories]]></title>
      <url>https://kairen.github.io/categories/index.html</url>
      <content type="html"></content>
    </entry>
    
    <entry>
      <title><![CDATA[tags]]></title>
      <url>https://kairen.github.io/tags/index.html</url>
      <content type="html"></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.10/pki/admin-csr.json</url>
      <content type="html"><![CDATA[{"CN":"admin","key":{"algo":"rsa","size":2048},"names":[{"C":"TW","ST":"Taipei","L":"Taipei","O":"system:masters","OU":"Kubernetes-manual"}]}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.10/pki/apiserver-csr.json</url>
      <content type="html"><![CDATA[{"CN":"kube-apiserver","key":{"algo":"rsa","size":2048},"names":[{"C":"TW","ST":"Taipei","L":"Taipei","O":"Kubernetes","OU":"Kubernetes-manual"}]}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.10/pki/ca-config.json</url>
      <content type="html"><![CDATA[{"signing":{"default":{"expiry":"87600h"},"profiles":{"kubernetes":{"usages":["signing","key encipherment","server auth","client auth"],"expiry":"87600h"}}}}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.10/pki/ca-csr.json</url>
      <content type="html"><![CDATA[{"CN":"kubernetes","key":{"algo":"rsa","size":2048},"names":[{"C":"TW","ST":"Taipei","L":"Taipei","O":"Kubernetes","OU":"Kubernetes-manual"}]}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.10/pki/etcd-ca-csr.json</url>
      <content type="html"><![CDATA[{"CN":"etcd","key":{"algo":"rsa","size":2048},"names":[{"C":"TW","ST":"Taipei","L":"Taipei","O":"etcd","OU":"Etcd Security"}]}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.10/pki/etcd-csr.json</url>
      <content type="html"><![CDATA[{"CN":"etcd","key":{"algo":"rsa","size":2048},"names":[{"C":"TW","ST":"Taipei","L":"Taipei","O":"etcd","OU":"Etcd Security"}]}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.10/pki/front-proxy-ca-csr.json</url>
      <content type="html"><![CDATA[{"CN":"kubernetes","key":{"algo":"rsa","size":2048}}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.10/pki/front-proxy-client-csr.json</url>
      <content type="html"><![CDATA[{"CN":"front-proxy-client","key":{"algo":"rsa","size":2048}}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.10/pki/kube-proxy-csr.json</url>
      <content type="html"><![CDATA[{"CN":"system:kube-proxy","key":{"algo":"rsa","size":2048},"names":[{"C":"TW","ST":"Taipei","L":"Taipei","O":"system:kube-proxy","OU":"Kubernetes-manual"}]}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.10/pki/kubelet-csr.json</url>
      <content type="html"><![CDATA[{"CN":"system:node:$NODE","key":{"algo":"rsa","size":2048},"names":[{"C":"TW","L":"Taipei","ST":"Taipei","O":"system:nodes","OU":"Kubernetes-manual"}]}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.10/pki/manager-csr.json</url>
      <content type="html"><![CDATA[{"CN":"system:kube-controller-manager","key":{"algo":"rsa","size":2048},"names":[{"C":"TW","ST":"Taipei","L":"Taipei","O":"system:kube-controller-manager","OU":"Kubernetes-manual"}]}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.10/pki/scheduler-csr.json</url>
      <content type="html"><![CDATA[{"CN":"system:kube-scheduler","key":{"algo":"rsa","size":2048},"names":[{"C":"TW","ST":"Taipei","L":"Taipei","O":"system:kube-scheduler","OU":"Kubernetes-manual"}]}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.8/pki/admin-csr.json</url>
      <content type="html"><![CDATA[{"CN":"admin","key":{"algo":"rsa","size":2048},"names":[{"C":"TW","ST":"Taipei","L":"Taipei","O":"system:masters","OU":"Kubernetes-manual"}]}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.8/pki/apiserver-csr.json</url>
      <content type="html"><![CDATA[{"CN":"kube-apiserver","key":{"algo":"rsa","size":2048},"names":[{"C":"TW","ST":"Taipei","L":"Taipei","O":"Kubernetes","OU":"Kubernetes-manual"}]}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.8/pki/ca-config.json</url>
      <content type="html"><![CDATA[{"signing":{"default":{"expiry":"87600h"},"profiles":{"kubernetes":{"usages":["signing","key encipherment","server auth","client auth"],"expiry":"87600h"}}}}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.8/pki/ca-csr.json</url>
      <content type="html"><![CDATA[{"CN":"kubernetes","key":{"algo":"rsa","size":2048},"names":[{"C":"TW","ST":"Taipei","L":"Taipei","O":"Kubernetes","OU":"Kubernetes-manual"}]}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.8/pki/etcd-ca-csr.json</url>
      <content type="html"><![CDATA[{"CN":"etcd","key":{"algo":"rsa","size":2048},"names":[{"C":"TW","ST":"Taipei","L":"Taipei","O":"etcd","OU":"Etcd Security"}]}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.8/pki/etcd-csr.json</url>
      <content type="html"><![CDATA[{"CN":"etcd","hosts":["127.0.0.1","172.16.35.12"],"key":{"algo":"rsa","size":2048},"names":[{"C":"TW","ST":"Taipei","L":"Taipei","O":"etcd","OU":"Etcd Security"}]}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.8/pki/front-proxy-ca-csr.json</url>
      <content type="html"><![CDATA[{"CN":"kubernetes","key":{"algo":"rsa","size":2048}}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.8/pki/front-proxy-client-csr.json</url>
      <content type="html"><![CDATA[{"CN":"front-proxy-client","key":{"algo":"rsa","size":2048}}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.8/pki/kube-proxy-csr.json</url>
      <content type="html"><![CDATA[{"CN":"system:kube-proxy","key":{"algo":"rsa","size":2048},"names":[{"C":"TW","ST":"Taipei","L":"Taipei","O":"system:kube-proxy","OU":"Kubernetes-manual"}]}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.8/pki/kubelet-csr.json</url>
      <content type="html"><![CDATA[{"CN":"system:node:$NODE","key":{"algo":"rsa","size":2048},"names":[{"C":"TW","L":"Taipei","ST":"Taipei","O":"system:nodes","OU":"Kubernetes-manual"}]}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.8/pki/manager-csr.json</url>
      <content type="html"><![CDATA[{"CN":"system:kube-controller-manager","key":{"algo":"rsa","size":2048},"names":[{"C":"TW","ST":"Taipei","L":"Taipei","O":"system:kube-controller-manager","OU":"Kubernetes-manual"}]}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/manual-v1.8/pki/scheduler-csr.json</url>
      <content type="html"><![CDATA[{"CN":"system:kube-scheduler","key":{"algo":"rsa","size":2048},"names":[{"C":"TW","ST":"Taipei","L":"Taipei","O":"system:kube-scheduler","OU":"Kubernetes-manual"}]}]]></content>
    </entry>
    
    <entry>
      <title></title>
      <url>https://kairen.github.io/files/openstack/keystone/webhook-policy.json</url>
      <content type="html"><![CDATA[[{"resource":{"verbs":["get","list","watch"],"resources":["pods"],"version":"*","namespace":"default"},"match":[{"type":"role","values":["k8s-admin","k8s-viewer","k8s-editor"]}]},{"resource":{"verbs":["create","update","delete"],"resources":["pods"],"version":"*","namespace":"default"},"match":[{"type":"role","values":["k8s-admin","k8s-editor"]},{"type":"project","values":["PROJECT_ID"]}]}]]]></content>
    </entry>
    
  
</search>
