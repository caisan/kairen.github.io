<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8" />

    

    

    <title>Deploy OpenStack on Kubernetes using OpenStack-helm | KaiRen&#39;s Blog</title>
    <meta name="author" content="Kyle Bai" />
    <meta name="version" content="1.0.0" />
    <meta name="keywords" content="undefined" />
    <meta name="description" content="OpenStack Helm 是一個提供部署建置的專案，其目的是為了推動 OpenStack 生產環境的解決方案，而這種部署方式採用容器化方式，並執行於 Kubernetes 系統上來提供 OpenStack 服務的管理與排程等使用。


而本篇文章將說明如何建置多節點的 OpenStack Helm 環境來進行功能驗證。
節點與安裝版本以下為各節點的硬體資訊。



IP Address
Role
CPU
Memory




172.22.132.10
vip
-
-


172.22.13" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no" />
    <meta name="baidu-site-verification" content="F0CXvmUgA9" />

    
    
    <link rel="icon" href="/images/favicon.png">
    

    <link rel="stylesheet" href="/css/style.css">
</head>
<body>

    <div class="app">
        <header class="header clearfix">
    <div id="nav" class="nav">
    <button id="open-panel" class="open-panel"><i class="icon-library"></i></button>

    <nav class="nav-inner">

        
        
        <li class="nav-item">
            <a class="nav-link" href="/">Home</a>
        </li>
        
        
        
        <li class="nav-item">
            <a class="nav-link" href="/categories">Category</a>
        </li>
        
        
        
        <li class="nav-item">
            <a class="nav-link" href="/archives">Archive</a>
        </li>
        
        
        
        <li class="nav-item nav-item-tag">
            <a id="nav-tag" class="nav-link" href="#">Tag</a>
            <div id="nav-tags" class="nav-tag-wrap">
                <i class="nav-tag-arrow"></i>
                
  <div class="widget-wrap">
    <h3 class="widget-title">
        <i class="icon-tag vm"></i>
        <span class="vm">Tags</span>
    </h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AWS/">AWS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ansible/">Ansible</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Automation-Engine/">Automation Engine</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bare-Metal/">Bare Metal</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bare-metal/">Bare-metal</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Blockchain/">Blockchain</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BlueStore/">BlueStore</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNCF/">CNCF</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Calico/">Calico</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CentOS/">CentOS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ceph/">Ceph</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Concurrent/">Concurrent</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Container/">Container</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CoreOS/">CoreOS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DC-OS/">DC/OS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DL-ML/">DL/ML</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Collect/">Data Collect</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Database/">Database</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DevOps/">DevOps</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DevStack/">DevStack</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Distribution-System/">Distribution System</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Docker/">Docker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Docker-Swarm/">Docker Swarm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Docker-registry/">Docker registry</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ethereum/">Ethereum</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Federation/">Federation</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/File-System/">File System</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GPU/">GPU</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Git/">Git</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Go-lang/">Go lang</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Golang/">Golang</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HDFS/">HDFS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HTTP-Server/">HTTP Server</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Helm/">Helm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/High-Availability/">High Availability</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/">Java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kafka/">Kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Keystone/">Keystone</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kops/">Kops</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kubernetes-RBAC/">Kubernetes RBAC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LDAP/">LDAP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux-Container/">Linux Container</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LinuxKit/">LinuxKit</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Load-Balancer/">Load Balancer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Logging/">Logging</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Maven/">Maven</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mesos/">Mesos</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Message-Queue/">Message Queue</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Microkernel/">Microkernel</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Moby/">Moby</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Monitoring/">Monitoring</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NVIDIA-GPU/">NVIDIA GPU</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NoSQL/">NoSQL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/OOP/">OOP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/OS-X/">OS X</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/OpenStack/">OpenStack</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Openstack/">Openstack</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PXE/">PXE</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Programming/">Programming</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Prometheus/">Prometheus</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Puppet/">Puppet</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SPDK/">SPDK</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SSD/">SSD</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Smart-Contract/">Smart Contract</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Solidity/">Solidity</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Storage/">Storage</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TensorFlow/">TensorFlow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ubuntu/">Ubuntu</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vagrant/">Vagrant</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Visualization/">Visualization</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/iOS/">iOS</a></li></ul>
    </div>
  </div>


            </div>
        </li>
        
        
        
        <li class="nav-item">
            <a class="nav-link" href="/about">About</a>
        </li>
        
        
        
        <li class="nav-item">
            <a class="nav-link" href="/atom.xml">Feed</a>
        </li>
        
        
        

    </nav>
</div>

    <aside id="aside" class="aside">
    <div id="aside-mask" class="aside-mask"></div>
    <div id="aside-inner" class="aside-inner">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit"><i class="icon-search-stroke"></i></button><input type="hidden" name="sitesearch" value="https://kairen.github.io"></form>

        
        
        
        

        
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#節點與安裝版本"><span class="toc-number">1.</span> <span class="toc-text">節點與安裝版本</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kubernetes-叢集"><span class="toc-number">2.</span> <span class="toc-text">Kubernetes 叢集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#初始化與設定基本需求"><span class="toc-number">2.1.</span> <span class="toc-text">初始化與設定基本需求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#安裝與設定-Kube-ansible"><span class="toc-number">2.2.</span> <span class="toc-text">安裝與設定 Kube-ansible</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#部屬-Kubernetes-叢集"><span class="toc-number">2.3.</span> <span class="toc-text">部屬 Kubernetes 叢集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#OpenStack-helm-叢集"><span class="toc-number">3.</span> <span class="toc-text">OpenStack-helm 叢集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Helm-init"><span class="toc-number">3.1.</span> <span class="toc-text">Helm init</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#事前準備"><span class="toc-number">3.2.</span> <span class="toc-text">事前準備</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Ceph-Chart"><span class="toc-number">3.3.</span> <span class="toc-text">Ceph Chart</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#OpenStack-Chart"><span class="toc-number">3.4.</span> <span class="toc-text">OpenStack Chart</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#測試-OpenStack-功能"><span class="toc-number">4.</span> <span class="toc-text">測試 OpenStack 功能</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Refers"><span class="toc-number">5.</span> <span class="toc-text">Refers</span></a></li></ol>
        
    </div>
</aside>

</header>

        <div id="content" class="content"><article class="article" itemscope itemprop="blogPost">
    
    <header class="article-header">
        
        <h1 itemprop="name">
            Deploy OpenStack on Kubernetes using OpenStack-helm
        </h1>
        
        <div class="article-meta clearfix">
            <a class="article-date" href="/2017/11/29/openstack/openstack-helm/">
    
    <i class="icon-calendar"></i>
    
    <time datetime="2017-11-29T08:23:01.000Z" itemprop="datePublished">2017-11-29</time>
</a>

            
<div class="article-tag-list">
    <i class="icon-tag"></i>
    <a class="article-tag-link" href="/tags/Helm/">Helm</a>, <a class="article-tag-link" href="/tags/Kubernetes/">Kubernetes</a>, <a class="article-tag-link" href="/tags/Openstack/">Openstack</a>
</div>


        </div>
    </header>
    
    <section class="article-body markdown-body">
        
        <p><a href="https://github.com/openstack/openstack-helm" target="_blank" rel="noopener">OpenStack Helm</a> 是一個提供部署建置的專案，其目的是為了推動 OpenStack 生產環境的解決方案，而這種部署方式採用容器化方式，並執行於 Kubernetes 系統上來提供 OpenStack 服務的管理與排程等使用。</p>
<p><img src="https://i.imgur.com/8sMjowM.png" alt=""></p>
<a id="more"></a>
<p>而本篇文章將說明如何建置多節點的 OpenStack Helm 環境來進行功能驗證。</p>
<h2 id="節點與安裝版本"><a href="#節點與安裝版本" class="headerlink" title="節點與安裝版本"></a>節點與安裝版本</h2><p>以下為各節點的硬體資訊。</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Role</th>
<th>CPU</th>
<th>Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.22.132.10</td>
<td>vip</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>172.22.132.101</td>
<td>master1</td>
<td>4</td>
<td>16G</td>
</tr>
<tr>
<td>172.22.132.22</td>
<td>node1</td>
<td>4</td>
<td>16G</td>
</tr>
<tr>
<td>172.22.132.24</td>
<td>node2</td>
<td>4</td>
<td>16G</td>
</tr>
<tr>
<td>172.22.132.28</td>
<td>node3</td>
<td>4</td>
<td>16G</td>
</tr>
</tbody>
</table>
<p>使用 Kernel、作業系統與軟體版本：</p>
<table>
<thead>
<tr>
<th></th>
<th>資訊描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>作業系統版本</td>
<td>16.04.3 LTS (Xenial Xerus)</td>
</tr>
<tr>
<td>Kernel 版本</td>
<td>4.4.0-101-generic</td>
</tr>
<tr>
<td>Kubernetes</td>
<td>v1.8.4</td>
</tr>
<tr>
<td>Docker</td>
<td>Docker 17.09.0-ce</td>
</tr>
<tr>
<td>Calico</td>
<td>v2.6.2</td>
</tr>
<tr>
<td>Etcd</td>
<td>v3.2.9</td>
</tr>
<tr>
<td>Ceph</td>
<td>v10.2.10</td>
</tr>
<tr>
<td>Helm</td>
<td>v2.7.0</td>
</tr>
</tbody>
</table>
<h2 id="Kubernetes-叢集"><a href="#Kubernetes-叢集" class="headerlink" title="Kubernetes 叢集"></a>Kubernetes 叢集</h2><p>本節說明如何建立 Kubernetes Cluster，這邊採用 <a href="https://github.com/kairen/kube-ansible" target="_blank" rel="noopener">kube-ansible</a> 工具來建立。</p>
<h3 id="初始化與設定基本需求"><a href="#初始化與設定基本需求" class="headerlink" title="初始化與設定基本需求"></a>初始化與設定基本需求</h3><p>安裝前需要確認以下幾個項目：</p>
<ul>
<li>所有節點的網路之間可以互相溝通。</li>
<li><code>部署節點</code>對其他節點不需要 SSH 密碼即可登入。</li>
<li>所有節點都擁有 Sudoer 權限，並且不需要輸入密碼。</li>
<li>所有節點需要安裝<code>Python</code>。</li>
<li>所有節點需要設定<code>/etc/host</code>解析到所有主機。</li>
<li><code>部署節點</code>需要安裝 <strong>Ansible &gt;= 2.4.0</strong>。</li>
</ul>
<pre><code class="shell"># Ubuntu install
$ sudo apt-get install -y software-properties-common
$ sudo apt-add-repository -y ppa:ansible/ansible
$ sudo apt-get update &amp;&amp; sudo apt-get install -y ansible git make

# CentOS install
$ sudo yum install -y epel-release
$ sudo yum -y install ansible cowsay
</code></pre>
<h3 id="安裝與設定-Kube-ansible"><a href="#安裝與設定-Kube-ansible" class="headerlink" title="安裝與設定 Kube-ansible"></a>安裝與設定 Kube-ansible</h3><p>首先取得最新穩定版本的 Kubernetes Ansible:</p>
<pre><code class="shell">$ git clone https://github.com/kairen/kube-ansible.git
$ cd kube-ansible
</code></pre>
<p>然後新增<code>inventory</code>檔案來描述要部屬的主機角色:</p>
<pre><code>[etcds]
172.22.132.101 ansible_user=ubuntu

[masters]
172.22.132.101 ansible_user=ubuntu

[nodes]
172.22.132.22 ansible_user=ubuntu
172.22.132.24 ansible_user=ubuntu
172.22.132.28 ansible_user=ubuntu

[kube-cluster:children]
masters
nodes

[kube-addon:children]
masters
</code></pre><p>接著編輯<code>group_vars/all.yml</code>檔案來添加與修改以下內容：</p>
<pre><code class="yaml"># Kubenrtes version, only support 1.8.0+.
kube_version: 1.8.4

# CNI plugin
# Support: flannel, calico, canal, weave or router.
network: calico
pod_network_cidr: 10.244.0.0/16
# CNI opts: flannel(--iface=enp0s8), calico(interface=enp0s8), canal(enp0s8).
cni_iface: &quot;&quot;

# Kubernetes cluster network.
cluster_subnet: 10.96.0
kubernetes_service_ip: &quot;{{ cluster_subnet }}.1&quot;
service_ip_range: &quot;{{ cluster_subnet }}.0/12&quot;
service_node_port_range: 30000-32767
api_secure_port: 5443

# Highly Available configuration.
haproxy: true
keepalived: true # set `lb_vip_address` as keepalived vip, if this enable.
keepalived_vip_interface: &quot;{{ ansible_default_ipv4.interface }}&quot;

lb_vip_address: 172.22.132.10
lb_secure_port: 6443
lb_api_url: &quot;https://{{ lb_vip_address }}:{{ lb_secure_port }}&quot;

etcd_iface: &quot;&quot;

insecure_registrys:
- &quot;172.22.132.253:5000&quot; # 有需要的話

ceph_cluster: true
</code></pre>
<blockquote>
<ul>
<li>這邊<code>insecure_registrys</code>為 deploy 節點的 Docker registry ip 與 port。</li>
<li>Extra addons 部分針對需求開啟，預設不會開啟。</li>
<li>若想把 Etcd, VIP 與 Network plugin 綁定在指定網路的話，請修改<code>etcd_iface</code>, <code>keepalived_vip_interface</code> 與 <code>cni_iface</code>。其中<code>cni_iface</code>需要針對不同 Plugin 來改變。</li>
<li>若想要修改部署版本的 Packages 的話，請編輯<code>roles/commons/packages/defaults/main.yml</code>來修改版本。</li>
</ul>
</blockquote>
<p>接著由於 OpenStack-helm 使用的 Kubernetes Controller Manager 不同，因此要修改<code>roles/commons/container-images/defaults/main.yml</code>的 Image 來源如下：</p>
<pre><code class="yaml">...
  manager:
  name: kube-controller-manager
  repos: kairen/
  tag: &quot;v{{ kube_version }}&quot;
...
</code></pre>
<p>完後成修改 storage roles 設定版本並進行安裝。</p>
<p>首先編輯<code>roles/storage/ceph/defaults/main.yml</code>修改版本為以下：</p>
<pre><code class="yaml">ceph_version: jewel
</code></pre>
<p>接著編輯<code>roles/storage/ceph/tasks/main.yml</code>修改成以下內容：</p>
<pre><code class="yaml">---

- name: Install Ceph dependency packages
  include_tasks: install-ceph.yml

# - name: Create and copy generator config file
#   include_tasks: gen-config.yml
#   delegate_to: &quot;{{ groups['masters'][0] }}&quot;
#   run_once: true
#
# - name: Deploy Ceph components on Kubernetes
#   include_tasks: ceph-on-k8s.yml
#   delegate_to: &quot;{{ groups['masters'][0] }}&quot;
#   run_once: true

# - name: Label all storage nodes
#   shell: &quot;kubectl label nodes node-type=storage&quot;
#   delegate_to: &quot;{{ groups['masters'][0] }}&quot;
#   run_once: true
#   ignore_errors: true
</code></pre>
<h3 id="部屬-Kubernetes-叢集"><a href="#部屬-Kubernetes-叢集" class="headerlink" title="部屬 Kubernetes 叢集"></a>部屬 Kubernetes 叢集</h3><p>確認<code>group_vars/all.yml</code>與其他設定都完成後，就透過 ansible ping 來檢查叢集狀態：</p>
<pre><code class="shell">$ ansible -i inventory all -m ping
...
172.22.132.101 | SUCCESS =&gt; {
    &quot;changed&quot;: false,
    &quot;failed&quot;: false,
    &quot;ping&quot;: &quot;pong&quot;
}
...
</code></pre>
<p>接著就可以透過以下指令進行部署叢集：</p>
<pre><code class="shell">$ ansible-playbook cluster.yml
...
TASK [cni : Apply calico network daemonset] *********************************************************************************************************************************
changed: [172.22.132.101 -&gt; 172.22.132.101]

PLAY RECAP ******************************************************************************************************************************************************************
172.22.132.101             : ok=155  changed=58   unreachable=0    failed=0
172.22.132.22              : ok=117  changed=28   unreachable=0    failed=0
172.22.132.24              : ok=50   changed=18   unreachable=0    failed=0
172.22.132.28              : ok=51   changed=19   unreachable=0    failed=0
</code></pre>
<p>完成後，進入<code>master</code>節點執行以下指令確認叢集：</p>
<pre><code class="shell">$ kubectl get node
NAME           STATUS    ROLES     AGE       VERSION
kube-master1   Ready     master    1h        v1.8.4
kube-node1     Ready     &lt;none&gt;    1h        v1.8.4
kube-node2     Ready     &lt;none&gt;    1h        v1.8.4
kube-node3     Ready     &lt;none&gt;    1h        v1.8.4

$ kubectl -n kube-system get po
NAME                                       READY     STATUS    RESTARTS   AGE
calico-node-js6qp                          2/2       Running   2          1h
calico-node-kx9xn                          2/2       Running   2          1h
calico-node-lxrjl                          2/2       Running   2          1h
calico-node-vwn5f                          2/2       Running   2          1h
calico-policy-controller-d549764f6-9kn9l   1/1       Running   1          1h
haproxy-kube-master1                       1/1       Running   1          1h
keepalived-kube-master1                    1/1       Running   1          1h
kube-apiserver-kube-master1                1/1       Running   1          1h
kube-controller-manager-kube-master1       1/1       Running   1          1h
kube-dns-7bd4879dc9-kxmx6                  3/3       Running   3          1h
kube-proxy-7tqkm                           1/1       Running   1          1h
kube-proxy-glzmm                           1/1       Running   1          1h
kube-proxy-krqxs                           1/1       Running   1          1h
kube-proxy-x9zdb                           1/1       Running   1          1h
kube-scheduler-kube-master1                1/1       Running   1          1h
</code></pre>
<p>檢查 kube-dns 是否連 host 都能夠解析:</p>
<pre><code class="shell">$ nslookup kubernetes
Server:        10.96.0.10
Address:    10.96.0.10#53

Non-authoritative answer:
Name:    kubernetes.default.svc.cluster.local
Address: 10.96.0.1
</code></pre>
<p>接著安裝 Ceph 套件：</p>
<pre><code class="sh">$ ansible-playbook storage.yml
</code></pre>
<h2 id="OpenStack-helm-叢集"><a href="#OpenStack-helm-叢集" class="headerlink" title="OpenStack-helm 叢集"></a>OpenStack-helm 叢集</h2><p>本節說明如何建立 OpenStack on Kubernetes 使用 Helm，部署是使用 <a href="https://github.com/openstack/openstack-helm" target="_blank" rel="noopener">openstack-helm</a>。過程將透過 OpenStack-helm 來在 Kubernetes 建置 OpenStack 叢集。以下所有操作都在<code>kube-master1</code>上進行。</p>
<h3 id="Helm-init"><a href="#Helm-init" class="headerlink" title="Helm init"></a>Helm init</h3><p>在開始前需要先將 Helm 進行初始化，以提供後續使用，然而這邊由於使用到 RBAC 的關係，因此需建立一個 Service account 來提供給 Helm 使用：</p>
<pre><code class="shell">$ kubectl -n kube-system create sa tiller
$ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
$ helm init --service-account tiller
</code></pre>
<blockquote>
<p>由於 <code>kube-ansible</code> 本身包含 Helm 工具, 因此不需要自己安裝，只需要依據上面指令進行 init 即可。</p>
</blockquote>
<p>新增一個檔案<code>openrc</code>來提供環境變數：</p>
<pre><code class="shell">export HELM_HOST=$(kubectl describe svc/tiller-deploy -n kube-system | awk &#39;/Endpoints/{print $2}&#39;)
export OSD_CLUSTER_NETWORK=172.22.132.0/24
export OSD_PUBLIC_NETWORK=172.22.132.0/24
export WORK_DIR=local
export CEPH_RGW_KEYSTONE_ENABLED=true
</code></pre>
<blockquote>
<ul>
<li><code>OSD_CLUSTER_NETWORK</code>與<code>OSD_PUBLIC_NETWORK</code>都是使用實體機器網路，這邊 daemonset 會使用 hostNetwork。</li>
<li><code>CEPH_RGW_KEYSTONE_ENABLED</code> 在 Kubernetes 版本有點不穩，可依需求關閉。</li>
</ul>
</blockquote>
<p>完成後，透過 source 指令引入:</p>
<pre><code class="shell">$ source openrc
$ helm version
Client: &amp;version.Version{SemVer:&quot;v2.7.0&quot;, GitCommit:&quot;08c1144f5eb3e3b636d9775617287cc26e53dba4&quot;, GitTreeState:&quot;clean&quot;}
Server: &amp;version.Version{SemVer:&quot;v2.7.0&quot;, GitCommit:&quot;08c1144f5eb3e3b636d9775617287cc26e53dba4&quot;, GitTreeState:&quot;clean&quot;}
</code></pre>
<h3 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h3><p>首先透過 Kubernetes label 來標示每個節點的角色：</p>
<pre><code class="shell">kubectl label nodes openstack-control-plane=enabled --all
kubectl label nodes ceph-mon=enabled --all
kubectl label nodes ceph-osd=enabled --all
kubectl label nodes ceph-mds=enabled --all
kubectl label nodes ceph-rgw=enabled --all
kubectl label nodes ceph-mgr=enabled --all
kubectl label nodes openvswitch=enabled --all
kubectl label nodes openstack-compute-node=enabled --all
</code></pre>
<blockquote>
<p>這邊為了避免過度的節點污染，因此不讓 masters 充當任何角色：</p>
<pre><code class="shell">kubectl label nodes kube-master1 openstack-control-plane-
kubectl label nodes kube-master1 ceph-mon-
kubectl label nodes kube-master1 ceph-osd-
kubectl label nodes kube-master1 ceph-mds-
kubectl label nodes kube-master1 ceph-rgw-
kubectl label nodes kube-master1 ceph-mgr-
kubectl label nodes kube-master1 openvswitch-
kubectl label nodes kube-master1 openstack-compute-node-
</code></pre>
</blockquote>
<p>由於使用 Kubernetes RBAC，而目前 openstack-helm 有 bug，不會正確建立 Service account 的 ClusterRoleBindings，因此要手動建立(這邊偷懶一下直接使用 Admin roles)：</p>
<pre><code class="shell">$ cat &lt;&lt;EOF | kubectl create -f -
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: ceph-sa-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: system:serviceaccount:ceph:default
EOF

$ cat &lt;&lt;EOF | kubectl create -f -
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: openstack-sa-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: system:serviceaccount:openstack:default
EOF
</code></pre>
<blockquote>
<p>若沒有建立的話，會有類似以下的錯誤資訊：</p>
<pre><code>Error from server (Forbidden): error when creating &quot;STDIN&quot;: secrets is forbidden: User &quot;system:serviceaccount:ceph:default&quot; cannot create secrets in the namespace &quot;ceph&quot;
</code></pre></blockquote>
<p>下載最新版本 openstack-helm 專案：</p>
<pre><code class="shell">$ git clone https://github.com/openstack/openstack-helm.git
$ cd openstack-helm
</code></pre>
<p>現在須建立 openstack-helm chart 來提供部署使用：</p>
<pre><code class="shell">$ helm serve &amp;
$ helm repo add local http://localhost:8879/charts
$ make
# output
...
1 chart(s) linted, no failures
if [ -d congress ]; then helm package congress; fi
Successfully packaged chart and saved it to: /root/openstack-helm/congress-0.1.0.tgz
make[1]: Leaving directory &#39;/root/openstack-helm&#39;
</code></pre>
<h3 id="Ceph-Chart"><a href="#Ceph-Chart" class="headerlink" title="Ceph Chart"></a>Ceph Chart</h3><p>在部署 OpenStack 前，需要先部署 Ceph 叢集，這邊透過以下指令建置：</p>
<pre><code class="shell">$ helm install --namespace=ceph ${WORK_DIR}/ceph --name=ceph \
  --set endpoints.identity.namespace=openstack \
  --set endpoints.object_store.namespace=ceph \
  --set endpoints.ceph_mon.namespace=ceph \
  --set ceph.rgw_keystone_auth=${CEPH_RGW_KEYSTONE_ENABLED} \
  --set network.public=${OSD_PUBLIC_NETWORK} \
  --set network.cluster=${OSD_CLUSTER_NETWORK} \
  --set deployment.storage_secrets=true \
  --set deployment.ceph=true \
  --set deployment.rbd_provisioner=true \
  --set deployment.client_secrets=false \
  --set deployment.rgw_keystone_user_and_endpoints=false \
  --set bootstrap.enabled=true
</code></pre>
<blockquote>
<ul>
<li><code>CEPH_RGW_KEYSTONE_ENABLED</code>是否啟動 Ceph RGW Keystone。</li>
<li><code>OSD_PUBLIC_NETWORK</code>與<code>OSD_PUBLIC_NETWORK</code>為 Ceph 叢集網路。</li>
</ul>
</blockquote>
<p>成功安裝 Ceph chart 後，就可以透過 kubectl 來查看結果：</p>
<pre><code class="shell">$ kubectl -n ceph get po
NAME                                   READY     STATUS    RESTARTS   AGE
ceph-mds-57798cc8f6-r898r              1/1       Running   2          10min
ceph-mon-96p9r                         1/1       Running   0          10min
ceph-mon-check-bd8875f87-whvhd         1/1       Running   0          10min
ceph-mon-qkj95                         1/1       Running   0          10min
ceph-mon-zx7tw                         1/1       Running   0          10min
ceph-osd-5fvfl                         1/1       Running   0          10min
ceph-osd-kvw9b                         1/1       Running   0          10min
ceph-osd-wcf5j                         1/1       Running   0          10min
ceph-rbd-provisioner-599ff9575-mdqnf   1/1       Running   0          10min
ceph-rbd-provisioner-599ff9575-vpcr6   1/1       Running   0          10min
ceph-rgw-7c8c5d4f6f-8fq9c              1/1       Running   3          10min
</code></pre>
<p>確認 Ceph 叢集建立正確：</p>
<pre><code class="shell">$ MON_POD=$(kubectl get pods \
  --namespace=ceph \
  --selector=&quot;application=ceph&quot; \
  --selector=&quot;component=mon&quot; \
  --no-headers | awk &#39;{ print $1; exit }&#39;)
$ kubectl exec -n ceph ${MON_POD} -- ceph -s

    cluster 02ad8724-dee0-4f55-829f-3cc24e2c7571
     health HEALTH_WARN
            too many PGs per OSD (856 &gt; max 300)
     monmap e2: 3 mons at {kube-node1=172.22.132.22:6789/0,kube-node2=172.22.132.24:6789/0,kube-node3=172.22.132.28:6789/0}
            election epoch 8, quorum 0,1,2 kube-node1,kube-node2,kube-node3
      fsmap e5: 1/1/1 up {0=mds-ceph-mds-57798cc8f6-r898r=up:active}
     osdmap e21: 3 osds: 3 up, 3 in
            flags sortbitwise,require_jewel_osds
      pgmap v6053: 856 pgs, 10 pools, 3656 bytes data, 191 objects
            43091 MB used, 2133 GB / 2291 GB avail
                 856 active+clean
</code></pre>
<blockquote>
<p>Warn 這邊忽略，OSD 機器太少….。</p>
</blockquote>
<p>接著為了讓 Ceph 可以在其他 Kubernetes namespace 中存取 PVC，這邊要產生 client secret key 於 openstack namespace 中來提供給 OpenStack 元件使用，這邊執行以下 Chart 來產生：</p>
<pre><code class="shell">$ helm install --namespace=openstack ${WORK_DIR}/ceph --name=ceph-openstack-config \
  --set endpoints.identity.namespace=openstack \
  --set endpoints.object_store.namespace=ceph \
  --set endpoints.ceph_mon.namespace=ceph \
  --set ceph.rgw_keystone_auth=${CEPH_RGW_KEYSTONE_ENABLED} \
  --set network.public=${OSD_PUBLIC_NETWORK} \
  --set network.cluster=${OSD_CLUSTER_NETWORK} \
  --set deployment.storage_secrets=false \
  --set deployment.ceph=false \
  --set deployment.rbd_provisioner=false \
  --set deployment.client_secrets=true \
  --set deployment.rgw_keystone_user_and_endpoints=false
</code></pre>
<p>檢查 pod 與 secret 是否建立成功：</p>
<pre><code class="shell">$ kubectl -n openstack get secret,po -a
NAME                          TYPE                                  DATA      AGE
secrets/default-token-q2r87   kubernetes.io/service-account-token   3         2m
secrets/pvc-ceph-client-key   kubernetes.io/rbd                     1         2m

NAME                                           READY     STATUS      RESTARTS   AGE
po/ceph-namespace-client-key-generator-w84n4   0/1       Completed   0          2m
</code></pre>
<h3 id="OpenStack-Chart"><a href="#OpenStack-Chart" class="headerlink" title="OpenStack Chart"></a>OpenStack Chart</h3><p>確認沒問題後，就可以開始部署 OpenStack chart 了。首先先安裝 Mariadb cluster:</p>
<pre><code class="shell">$ helm install --name=mariadb ./mariadb --namespace=openstack
</code></pre>
<blockquote>
<p>這邊跑超久…34mins…，原因可能是 Storage 效能問題。</p>
</blockquote>
<p>這邊正確執行後，會依序依據 StatefulSet 建立起 Pod 組成 Cluster：</p>
<pre><code class="shell">$ kubectl -n openstack get po
NAME        READY     STATUS    RESTARTS   AGE
mariadb-0   1/1       Running   0          37m
mariadb-1   1/1       Running   0          4m
mariadb-2   1/1       Running   0          2m
</code></pre>
<p>當 Mariadb cluster 完成後，就可以部署一些需要的服務，如 RabbitMQ, OVS 等：</p>
<pre><code class="shell">helm install --name=memcached ./memcached --namespace=openstack
helm install --name=etcd-rabbitmq ./etcd --namespace=openstack
helm install --name=rabbitmq ./rabbitmq --namespace=openstack
helm install --name=ingress ./ingress --namespace=openstack
helm install --name=libvirt ./libvirt --namespace=openstack
helm install --name=openvswitch ./openvswitch --namespace=openstack
</code></pre>
<p>上述指令若正確執行的話，會分別建立起以下服務：</p>
<pre><code class="shell">$ kubectl -n openstack get po
NAME                                   READY     STATUS    RESTARTS   AGE
etcd-5c9bc8c97f-jpm2k                  1/1       Running   0          4m
ingress-api-jhjjv                      1/1       Running   0          4m
ingress-api-nx5qm                      1/1       Running   0          4m
ingress-api-vr8xf                      1/1       Running   0          4m
ingress-error-pages-86b9db69cc-mmq4p   1/1       Running   0          4m
libvirt-94xq5                          1/1       Running   0          4m
libvirt-lzfzs                          1/1       Running   0          4m
libvirt-vswxb                          1/1       Running   0          4m
mariadb-0                              1/1       Running   0          42m
mariadb-1                              1/1       Running   0          9m
mariadb-2                              1/1       Running   0          7m
memcached-746fcc894-cwhpr              1/1       Running   0          4m
openvswitch-db-7fjr2                   1/1       Running   0          4m
openvswitch-db-gtmcr                   1/1       Running   0          4m
openvswitch-db-hqmbt                   1/1       Running   0          4m
openvswitch-vswitchd-gptp9             1/1       Running   0          4m
openvswitch-vswitchd-s4cwd             1/1       Running   0          4m
openvswitch-vswitchd-tvxlg             1/1       Running   0          4m
rabbitmq-6fdb8879df-6vmz8              1/1       Running   0          4m
rabbitmq-6fdb8879df-875zz              1/1       Running   0          4m
rabbitmq-6fdb8879df-h5wj6              1/1       Running   0          4m
</code></pre>
<p>一旦所有基礎服務與元件都建立完成後，就可以開始部署 OpenStack 的專案 Chart，首先建立 Keystone 來提供身份認證服務：</p>
<pre><code class="shell">$ helm install --namespace=openstack --name=keystone ./keystone \
  --set pod.replicas.api=1

$ kubectl -n openstack get po -l application=keystone
NAME                            READY     STATUS     RESTARTS   AGE
keystone-api-74c774d448-dkqmj   0/1       Init:0/1   0          4m
keystone-bootstrap-xpdtl        0/1       Init:0/1   0          4m
keystone-db-sync-2bxtp          1/1       Running    0          4m        0          29s
</code></pre>
<blockquote>
<p>這邊由於叢集規模問題，副本數都為一份。</p>
</blockquote>
<p>這時候會先建立 Keystone database tables，完成後將啟動 API pod，如以下結果：</p>
<pre><code class="shell">$ kubectl -n openstack get po -l application=keystone
NAME                            READY     STATUS    RESTARTS   AGE
keystone-api-74c774d448-dkqmj   1/1       Running   0          11m
</code></pre>
<p>如果安裝支援 RGW 的 Keystone endpoint 的話，可以使用以下方式建立：</p>
<pre><code class="shell">$ helm install --namespace=openstack ${WORK_DIR}/ceph --name=radosgw-openstack \
  --set endpoints.identity.namespace=openstack \
  --set endpoints.object_store.namespace=ceph \
  --set endpoints.ceph_mon.namespace=ceph \
  --set ceph.rgw_keystone_auth=${CEPH_RGW_KEYSTONE_ENABLED} \
  --set network.public=${OSD_PUBLIC_NETWORK} \
  --set network.cluster=${OSD_CLUSTER_NETWORK} \
  --set deployment.storage_secrets=false \
  --set deployment.ceph=false \
  --set deployment.rbd_provisioner=false \
  --set deployment.client_secrets=false \
  --set deployment.rgw_keystone_user_and_endpoints=true

$ kubectl -n openstack get po -a -l application=ceph
NAME                                        READY     STATUS      RESTARTS   AGE
ceph-ks-endpoints-vfg4l                     0/3       Completed   0          1m
ceph-ks-service-tr9xt                       0/1       Completed   0          1m
ceph-ks-user-z5tlt                          0/1       Completed   0          1m
</code></pre>
<p>完成後，安裝 Horizon chart 來提供 OpenStack dashbaord：</p>
<pre><code class="shell">$ helm install --namespace=openstack --name=horizon ./horizon \
  --set network.enable_node_port=true \
  --set network.node_port=31000

$ kubectl -n openstack get po -l application=horizon
NAME                       READY     STATUS    RESTARTS   AGE
horizon-7c54878549-45668   1/1       Running   0          3m
</code></pre>
<p>接著安裝 Glance chart 來提供 OpenStack image service。目前 Glance 支援幾個 backend storage:</p>
<ul>
<li><strong>pvc</strong>: 一個簡單的 Kubernetes PVCs 檔案後端。</li>
<li><strong>rbd</strong>: 使用 Ceph RBD 來儲存 images。</li>
<li><strong>radosgw</strong>: 使用 Ceph RGW 來儲存 images。</li>
<li><strong>swift</strong>: 另用 OpenStack switf 所提供的物件儲存服務來儲存 images.</li>
</ul>
<p>這邊可以利用以下方式來部署不同的儲存後端：</p>
<pre><code class="shell">$ export GLANCE_BACKEND=radosgw
$ helm install --namespace=openstack --name=glance ./glance \
  --set pod.replicas.api=1 \
  --set pod.replicas.registry=1 \
  --set storage=${GLANCE_BACKEND}

$ kubectl -n openstack get po -l application=glance
NAME                               READY     STATUS    RESTARTS   AGE
glance-api-6cd8b856d6-lhzfs        1/1       Running   0          14m
glance-registry-599f8b857b-gt4c6   1/1       Running   0          14m
</code></pre>
<p>接著安裝 Neutron chart 來提供 OpenStack 虛擬化網路服務：</p>
<pre><code class="shell">$ helm install --namespace=openstack --name=neutron ./neutron \
  --set pod.replicas.server=1

$ kubectl -n openstack get po -l application=neutron
NAME                              READY     STATUS    RESTARTS   AGE
neutron-dhcp-agent-2z49d          1/1       Running   0          9h
neutron-dhcp-agent-d2kn8          1/1       Running   0          9h
neutron-dhcp-agent-mrstl          1/1       Running   0          9h
neutron-l3-agent-9f9mw            1/1       Running   0          9h
neutron-l3-agent-cshzw            1/1       Running   0          9h
neutron-l3-agent-j5vb9            1/1       Running   0          9h
neutron-metadata-agent-6bfb2      1/1       Running   0          9h
neutron-metadata-agent-kxk9c      1/1       Running   0          9h
neutron-metadata-agent-w8cnl      1/1       Running   0          9h
neutron-ovs-agent-j2549           1/1       Running   0          9h
neutron-ovs-agent-plj9t           1/1       Running   0          9h
neutron-ovs-agent-xlx7z           1/1       Running   0          9h
neutron-server-6f45d74b87-6wmck   1/1       Running   0          9h
</code></pre>
<p>接著安裝 Nova chart 來提供 OpenStack 虛擬機運算服務:</p>
<pre><code class="shell">$ helm install --namespace=openstack --name=nova ./nova \
  --set pod.replicas.api_metadata=1 \
  --set pod.replicas.osapi=1 \
  --set pod.replicas.conductor=1 \
  --set pod.replicas.consoleauth=1 \
  --set pod.replicas.scheduler=1 \
  --set pod.replicas.novncproxy=1

$ kubectl -n openstack get po -l application=nova
NAME                                 READY     STATUS    RESTARTS   AGE
nova-api-metadata-84fdc84fd7-ldzrh   1/1       Running   1          9h
nova-api-osapi-57f599c6d6-pqrjv      1/1       Running   0          9h
nova-compute-8rvm9                   2/2       Running   0          9h
nova-compute-cbk7h                   2/2       Running   0          9h
nova-compute-tf2jb                   2/2       Running   0          9h
nova-conductor-7f5bc76d79-bxwnb      1/1       Running   0          9h
nova-consoleauth-6946b5884f-nss6n    1/1       Running   0          9h
nova-novncproxy-d789dccff-7ft9q      1/1       Running   0          9h
nova-placement-api-f7c79578f-hj2g9   1/1       Running   0          9h
nova-scheduler-778866f555-mmksg      1/1       Running   0          9h
</code></pre>
<p>接著安裝 Cinfer chart 來提供 OpenStack 區塊儲存服務:</p>
<pre><code class="shell">$ helm install --namespace=openstack --name=cinder ./cinder \
  --set pod.replicas.api=1

$ kubectl -n openstack get po -l application=cinder
NAME                                READY     STATUS    RESTARTS   AGE
cinder-api-5cc89f5467-ssm8k         1/1       Running   0          32m
cinder-backup-67c4d8dfdb-zfsq4      1/1       Running   0          32m
cinder-scheduler-65f9dd49bf-6htwg   1/1       Running   0          32m
cinder-volume-69bfb67b4-bmst2       1/1       Running   0          32m
</code></pre>
<p>(option)都完成後，將 Horizon 服務透過 NodePort 方式曝露出來(如果上面 Horizon chart 沒反應的話)，執行以下指令編輯：</p>
<pre><code class="shell">$ kubectl -n openstack edit svc horizon-int
# 修改 type:
  type: NodePort
</code></pre>
<p>最後連接 <a href="http://172.22.132.10:31000" target="_blank" rel="noopener">Horizon Dashboard</a>，預設使用者為<code>admin/password</code>。</p>
<p><img src="https://i.imgur.com/8yunUPy.png" alt=""></p>
<p>其他 Chart 可以利用以下方式來安裝，如 Heat chart：</p>
<pre><code class="shell">$ helm install --namespace=openstack --name=heat ./heat

$ kubectl -n openstack get po -l application=heat
NAME                              READY     STATUS    RESTARTS   AGE
heat-api-5cf45d9d44-qrt69         1/1       Running   0          13m
heat-cfn-79dbf55789-bq4wh         1/1       Running   0          13m
heat-cloudwatch-bcc4647f4-4c4ln   1/1       Running   0          13m
heat-engine-55cfcc86f8-cct4m      1/1       Running   0          13m
</code></pre>
<h2 id="測試-OpenStack-功能"><a href="#測試-OpenStack-功能" class="headerlink" title="測試 OpenStack 功能"></a>測試 OpenStack 功能</h2><p>在<code>kube-master1</code>安裝 openstack client:</p>
<pre><code class="shell">$ sudo pip install python-openstackclient
</code></pre>
<p>建立<code>adminrc</code>來提供 client 環境變數：</p>
<pre><code class="shell">export OS_PROJECT_DOMAIN_NAME=default
export OS_USER_DOMAIN_NAME=default
export OS_PROJECT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=password
export OS_AUTH_URL=http://keystone.openstack.svc.cluster.local:80/v3
export OS_IDENTITY_API_VERSION=3
export OS_IMAGE_API_VERSION=2
</code></pre>
<p>引入環境變數，並透過 openstack client 測試：</p>
<pre><code class="shell">$ source adminrc
$ openstack user list
+----------------------------------+-----------+
| ID                               | Name      |
+----------------------------------+-----------+
| 42f0d2e7823e413cb469f9cce731398a | glance    |
| 556a2744811f450098f64b37d34192d4 | nova      |
| a97ec73724aa4445b2d575be54f23240 | cinder    |
| b28a5dcfd18948419e14acba7ecf6f63 | swift     |
| d1f312b6bb7c460eb7d8d78c8bf350fc | admin     |
| dc326aace22c4314a0100865fe4f57c2 | neutron   |
| ec5d6d3c529847b29a1c9187599c8a6b | placement |
+----------------------------------+-----------+
</code></pre>
<p>接著需要設定對外網路來提供給 VM 存取，在有<code>neutron-l3-agent</code>節點上，新增一個腳本<code>setup-gateway.sh</code>：</p>
<pre><code class="shell">#!/bin/bash
set -x

# Assign IP address to br-ex
OSH_BR_EX_ADDR=&quot;172.24.4.1/24&quot;
OSH_EXT_SUBNET=&quot;172.24.4.0/24&quot;
sudo ip addr add ${OSH_BR_EX_ADDR} dev br-ex
sudo ip link set br-ex up

# Setup masquerading on default route dev to public subnet
DEFAULT_ROUTE_DEV=&quot;enp3s0&quot;
sudo iptables -t nat -A POSTROUTING -o ${DEFAULT_ROUTE_DEV} -s ${OSH_EXT_SUBNET} -j MASQUERADE
</code></pre>
<blockquote>
<ul>
<li>網卡記得修改<code>DEFAULT_ROUTE_DEV</code>。</li>
<li>這邊因為沒有額外提供其他張網卡，所以先用 bridge 處理。</li>
</ul>
</blockquote>
<p>然後透過執行該腳本建立一個 bridge 網路：</p>
<pre><code class="shell">$ chmod u+x setup-gateway.sh
$ ./setup-gateway.sh
</code></pre>
<p>確認完成後，接著建立 Neutron ext net，透過以下指令進行建立：</p>
<pre><code class="shell">$ openstack network create \
   --share --external \
   --provider-physical-network external \
   --provider-network-type flat ext-net

$ openstack subnet create --network ext-net \
    --allocation-pool start=172.24.4.10,end=172.24.4.100 \
    --dns-nameserver 8.8.8.8 --gateway 172.24.4.1 \
    --subnet-range 172.24.4.0/24 \
    --no-dhcp ext-subnet

$ openstack router create router1
$ neutron router-gateway-set router1 ext-net
</code></pre>
<p>直接進入 Dashboard 新增 Self-service Network:<br><img src="https://i.imgur.com/lqMrgqs.png" alt=""></p>
<p>加入到 router1:<br><img src="https://i.imgur.com/4aNnF3O.png" alt=""></p>
<p>完成後，就可以建立 instance，這邊都透過 Dashboard 來操作：<br><img src="https://i.imgur.com/fCYkxSC.png" alt=""></p>
<p>透過 SSH 進入 instance：<br><img src="https://i.imgur.com/Ijylo9X.png" alt=""></p>
<h2 id="Refers"><a href="#Refers" class="headerlink" title="Refers"></a>Refers</h2><ul>
<li><a href="https://github.com/portdirect/sydney-workshop" target="_blank" rel="noopener">sydney-workshop</a></li>
<li><a href="https://docs.openstack.org/openstack-helm/latest/install/multinode.html" target="_blank" rel="noopener">Multi Node</a></li>
</ul>

        
    </section>
</article>



<div class="comments">
    <div id="disqus_thread">
        <p class="comment-tips">国内查看评论需要代理~</p>
    </div>
    <script>
    window.disqus_config = function () {
        this.language = 'zh';
        this.page.url = 'https://kairen.github.io/2017/11/29/openstack/openstack-helm/';
        this.page.title = 'Deploy OpenStack on Kubernetes using OpenStack-helm';
        this.page.identifier = '2017/11/29/openstack/openstack-helm/';
    };
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = 'https://k2r2bai.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

</div>
        <footer class="footer">
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, Theme by <a href="https://github.com/sanonz/hexo-theme-concise" target="_blank">Concise</a>
</footer>


    </div>

    <script type="text/javascript" src="https://cdn.bootcss.com/jquery/1.9.0/jquery.min.js"></script>
    
    <script type="text/javascript" src="/js/scrollspy.min.js"></script>
    
    <script type="text/javascript">
        $(function() {
            var nodes = {
                nav: $('#nav'),
                aside: $('#aside'),
                navTags: $('#nav-tags')
            };

            $('#open-panel, #aside-mask').on('click', function() {
                nodes.aside.toggleClass('panel-show');
            });
            $('#nav-tag').on('click', function(event) {
                event.preventDefault();console.log(nodes.navTags.attr('class'))
                nodes.navTags.toggleClass('tag-show');console.log(nodes.navTags.attr('class'))
            })/*.hover(function() {
                nodes.navTags.addClass('tag-show');
            }, function() {
                nodes.navTags.removeClass('tag-show');
            });*/

            
            $(document.body).scrollspy({target: '#aside-inner'});
            
        });
    </script>

</body>
</html>
